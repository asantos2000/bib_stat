@article{Navas-Loro2019,
   abstract = {This paper introduces ContractFrames, a framework able to translate natural language texts referring to the different events related to the status of a purchase contract to logic clauses from a legal reasoning system called PROLEG. Diverse frames and rules have been developed for the extraction and storage of this event-centric information before its conversion to logic clauses. Our framework uses natural language tools and rules to extract relevant information, store it in the form of frames, and return the logic clauses of the input text. Also an ontology, called the Contract Workflow Ontology, has been developed to represent all the relevant information of the events related to a contract. The framework has been tested in a synthetic dataset, and showed promising results.},
   author = {María Navas-Loro and Ken Satoh and Víctor Rodríguez-Doncel},
   doi = {10.1007/978-3-030-31605-1_9/COVER},
   isbn = {9783030316044},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Contract life-cycle,Legal NLP,Legal ontology,PROLEG},
   pages = {101-114},
   publisher = {Springer},
   title = {ContractFrames: Bridging the Gap Between Natural Language and Logics in Contract Law},
   volume = {11717 LNAI},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-31605-1_9},
   year = {2019},
}
@article{Maynard2020,
   abstract = {Traditionally, there has been a disconnect between custom-built applications used to solve real-world information extraction problems in industry, and automated learning-based approaches developed in academia. Despite approaches such as transfer-based learning, adapting these to more customised solutions where the task and data may be different, and where training data may be largely unavailable, is still hugely problematic, with the result that many systems still need to be custom-built using expert hand-crafted knowledge, and do not scale. In the legal domain, a traditional slow adopter of technology, black box machine learning-based systems are too untrustworthy to be widely used. In industrial settings, the fine-grained highly specialised knowledge of human experts is still critical, and it is not obvious how to integrate this into automated classification systems. In this paper, we examine two case studies from recent work combining this expert human knowledge with automated NLP technologies.},
   author = {Diana Maynard and Adam Funk},
   doi = {10.1007/978-3-030-58323-1_1/COVER},
   isbn = {9783030583224},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Information extraction,Natural language processing,Ontologies},
   pages = {3-10},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Combining expert knowledge with NLP for specialised applications},
   volume = {12284 LNAI},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-58323-1_1},
   year = {2020},
}
@article{Ganguly2023,
   abstract = {Artificial Intelligence (AI), Machine Learning (ML), Information Retrieval (IR) and Natural Language Processing (NLP) are transforming the way legal professionals and law firms approach their work. The significant potential for the application of AI to Law, for instance, by creating computational solutions for legal tasks, has intrigued researchers for decades. This appeal has only been amplified with the advent of Deep Learning (DL). It is worth noting that working with legal text is far more challenging as compared to the other subdomains of IR/NLP, mainly due to the typical characteristics of legal text, such as considerably longer documents, complex language and lack of large-scale annotated datasets. In this tutorial, we introduce the audience to these characteristics of legal text, and with it, the challenges associated with processing the legal documents. We touch upon the history of AI and Law research, and how it has evolved over the years from relatively simpler approaches to more complex ones, such as those involving DL. We organize the tutorial as follows. First, we provide a brief introduction to state-of-the-art research in the general domain of IR and NLP. We then discuss in more detail IR/NLP tasks specific to the legal domain. We outline the methodologies (both from an academic and industry perspective), and the available tools and datasets to evaluate the methodologies. This is then followed by a hands-on coding/demo session.},
   author = {Debasis Ganguly and Jack G. Conrad and Kripabandhu Ghosh and Saptarshi Ghosh and Pawan Goyal and Paheli Bhattacharya and Shubham Kumar Nigam and Shounak Paul},
   doi = {10.1007/978-3-031-28241-6_34/COVER},
   isbn = {9783031282409},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {AI & Law,Legal data analytics,Legal information retrieval,Natural language processing},
   pages = {331-340},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Legal IR and NLP: The History, Challenges, and State-of-the-Art},
   volume = {13982 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-28241-6_34},
   year = {2023},
}
@article{Soavi2020,
   abstract = {We are interested in semi-automating the process of generating a formal specification from a legal contract in natural language text form. Towards this end, we present a tool, named ContracT, that annotates legal contract text using an ontology for legal contracts. In the last part of the paper, we present results from a preliminary empirical evaluation of the tool that provided encouraging results in identifying contract concepts in text and discuss critical points to be tackled in future studies.},
   author = {Michele Soavi and Nicola Zeni and John Mylopoulos and Luisa Mich},
   doi = {10.1007/978-3-030-63479-7_9/COVER},
   isbn = {9783030634780},
   issn = {18651356},
   journal = {Lecture Notes in Business Information Processing},
   keywords = {Legal contract,Ontology for contracts,Semantic annotation,Structure model},
   pages = {124-137},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {ContracT – from Legal Contracts to Formal Specifications: Preliminary Results},
   volume = {400},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-63479-7_9},
   year = {2020},
}
@article{Goossens2022,
   abstract = {Vanbreda Risk & Benefits, a large Belgian insurance broker and risk consultant, allocates a substantial amount of time and resources to answer contract related questions from customers. This requires employees to manually search the relevant parameters in the contracts. In this paper, a solution is proposed and evaluated that automatically extracts insurance parameters from contracts using regular expressions and Natural Language Processing. While Natural Language Processing has been used in insurance for optimising premiums, detecting fraudulent claims, or underwriting, limited work has been done regarding parameter extraction. The proposed solution has been developed on 127 different contracts and two different contract types in terms of accuracy and time performance. Moreover, the automatic parameter extraction has been compared to manual parameter extraction. We conclude that automatic parameter extraction using regular expressions achieves better accuracy than manual extraction on top of being significantly faster, allowing Vanbreda Risk & Benefits to invest more time into providing better customer service.},
   author = {Alexandre Goossens and Laure Berth and Emilia Decoene and Ziboud Van Veldhoven and Jan Vanthienen},
   doi = {10.1007/978-3-031-04216-4_3/COVER},
   isbn = {9783031042157},
   issn = {18651356},
   journal = {Lecture Notes in Business Information Processing},
   keywords = {Insurance industry,NLP,Regular expressions,Service automation},
   pages = {27-38},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Automatically Extracting Insurance Contract Knowledge Using NLP},
   volume = {444 LNBIP},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-04216-4_3},
   year = {2022},
}
@article{Thonssen2013,
   abstract = {Contract Management becomes increasingly important for companies and public administrations alike. Obligations and liabilities are described in contract clauses that are often buried in documents of a hundred pages and more. Although commercial Contract Management Systems (CMS) are available, with a few exceptions relevant information has to be extracted manually which is time consuming and error prone. But even if information extraction is automated and contracts are managed using a CMS, dealing with obligations is still a challenge. Whereas the CMSs deal well with time triggered obligations like periodical payments by setting up corresponding workflows, they fail to trigger obligations based on events, as this knowledge is out of the systems' scope. We introduce an approach to fill the gap as we relate information about the obligations managed in a CMS with background knowledge modelled in an ontology. The ontology is a formal representation of an enterprise architecture extended by top-level concepts. Motivating scenario for the approach is the contract management of a large company. For proof of concept a prototype has been developed. © Springer-Verlag Berlin Heidelberg 2013.},
   author = {Barbara Thönssen and Jonas Lutz},
   doi = {10.1007/978-3-642-54105-6_23/COVER},
   isbn = {9783642541049},
   issn = {18650929},
   journal = {Communications in Computer and Information Science},
   keywords = {Contract Management,Enterprise Architecture,Enterprise Ontology,Information Extraction,Obligation Management,Risk Management},
   pages = {337-349},
   publisher = {Springer Verlag},
   title = {Semantically Enriched Obligation Management: An Approach for Improving the Handling of Obligations Represented in Contracts},
   volume = {415},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-54105-6_23},
   year = {2013},
}
@article{Andrew2018,
   abstract = {In recent years, the journalists and computer sciences speak to each other to identify useful technologies which would help them in extracting useful information. This is called”computational Journalism”. In this paper, we present a method that will enable the journalists to automatically identifies and annotates entities such as names of people, organizations, role and functions of people in legal documents; the relationship between these entities are also explored. The system uses a combination of both statistical and rule based technique. The statistical method used is Conditional Random Fields and for the rule based technique, document and language specific regular expressions are used.},
   author = {Judith Jeyafreeda Andrew and Xavier Tannier},
   doi = {10.18653/V1/W18-2401},
   issn = {0736587X},
   journal = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
   pages = {1-8},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {Automatic Extraction of Entities and Relation from Legal Documents},
   year = {2018},
}
@article{Aejas2022,
   abstract = {A contract is a binding document between two or more parties for executing any kind of activities that are defined clearly in its clauses. The parties who are assigned specific roles and rights must review and act accordingly until the contract expires. Automation of contract management is an emerging topic in various fields such as supply chain and legal domains. Recognition of various entities related to the document and their extraction are the main tasks to be performed for automating the contract management process. Named Entity Recognition is a well-known task in NLP that deals with the recognition of named entities such as person and date from a text. But traditional NER models perform poorly for domain-specific entity recognition and extraction such as legal and contract documents. For domain-specific entity recognition, we need to train the model with a dataset from the specific domain. In this paper, various approaches in the task of entity extraction from legal and contract documents are reviewed and discussed with the aim of proposing a new automated method for contract management using NLP.},
   author = {Bajeela Aejas and Abdelaziz Bouras and Abdelhak Belhi and Houssem Gasmi},
   doi = {10.1007/978-981-16-2102-4_69/COVER},
   isbn = {9789811621017},
   issn = {23673389},
   journal = {Lecture Notes in Networks and Systems},
   keywords = {Contracts,NER,Natural Language Processing,Supply Chain Management},
   pages = {763-771},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {A Review of Contract Entity Extraction},
   volume = {217},
   url = {https://link.springer.com/chapter/10.1007/978-981-16-2102-4_69},
   year = {2022},
}
@article{Ferneda2012,
   abstract = {In order to avoid ambiguity and to ensure, as far as possible, a strict interpretation of law, legal texts usually define the specific lexical terms used within their discourse by means of normative rules. With an often large amount of rules in effect in a given domain, extracting these definitions manually would be a costly undertaking. This paper presents an approach to cope with this problem based in a variation of an automated technique of natural language processing of Brazilian Portuguese texts. For the sake of generality, the proposed solution was developed to address the more general problem of building a glossary from domain specific texts that contain definitions amongst their content. This solution was applied to a corpus of texts on the telecommunications regulations domain and the results are reported. The usual pipeline of natural language processing has been followed: preprocessing, segmentation, and part-of-speech tagging. A set of feature extraction functions is specified and used along with reference glossary information on whether or not a text fragment is a definition, to train a SVM classifier. At last, the definitions are extracted from the texts and evaluated upon a testing corpus, which also contains the reference glossary annotations on definitions. The results are then discussed in light of other definition extraction techniques. © 2012 Springer-Verlag.},
   author = {Edilson Ferneda and Hércules Antonio Do Prado and Augusto Herrmann Batista and Marcello Sandi Pinheiro},
   doi = {10.1007/978-3-642-31137-6_48/COVER},
   isbn = {9783642311369},
   issn = {03029743},
   issue = {PART 3},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Definition extraction,Information extraction,Natural Language Processing},
   pages = {631-646},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Extracting definitions from Brazilian legal texts},
   volume = {7335 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-31137-6_48},
   year = {2012},
}
@article{Das2020,
   abstract = {Artificial Intelligence (AI) as an emerging technology, facilitates the mapping of human intelligence with computerized mechanism. This capability of thinking like human being provides computer with boundless potential for user interaction and prediction of logical solution (i.e. decision) for any particular event using its knowledge base obtained through previous experience. To generate faster solution, Artificial Intelligence (AI) can be utilized in multivariate service sectors, which are accustomed to generate logical solution in slow pace due to various constraints like, manpower, fund, infrastructure, policy paralysis, etc. In a developing country like India, which is striving hard to have a constant growth rate, application of Artificial Intelligence (AI) based tools have wide window open in various service sectors like judiciary, healthcare, business, education, agriculture, banking, etc. Generally in case of judiciary, end users have to wait for a long time to get their desired justice which directly affects their efficiency, contribution and well-being in this society. Using the concept of Artificial Intelligence (AI), Legal knowledge based tools may accelerate the service delivery of legal professionals from typical searching of related case journals to extraction of precise information in a customized manner. This paper focuses on legal representation of legal knowledge base to study the changing trends using Artificial Intelligence (AI).},
   author = {Tanaya Das and Abhishek Roy and A. K. Majumdar},
   doi = {10.1007/978-3-030-38040-3_73/COVER},
   issn = {23674520},
   journal = {Lecture Notes on Data Engineering and Communications Technologies},
   keywords = {Artificial Intelligence,Legal knowledge base,Ontology},
   pages = {647-653},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {A study on legal knowledge base creation using artificial intelligence and ontology},
   volume = {46},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-38040-3_73},
   year = {2020},
}
@article{Bansal2019,
   abstract = {The Amount of legal information that is being produced on a daily basis in the law courts is increasing enormously and nowadays this information is available in electronic form also. The application of various machine learning and deep learning methods for processing of legal documents has been receiving considerate attention over the last few years. Legal document classification, translation, summarization, contract review, case prediction and information retrieval are some of the tasks that have received concentrated efforts from the research community. In this survey, we have performed a comprehensive study of various deep learning methods applied in the legal domain and classified various legal tasks into three broad categories, viz. legal data search, legal text analytics and legal intelligent interfaces. The proposed study suggests that deep learning models like CNNs, RNNs, LSTM and GRU, and multi-task deep learning models are being used actively to solve wide variety of legal tasks and are giving state-of-the-art performance.},
   author = {Neha Bansal and Arun Sharma and R. K. Singh},
   doi = {10.1007/978-3-030-19823-7_31/TABLES/1},
   isbn = {9783030198220},
   issn = {18684238},
   journal = {IFIP Advances in Information and Communication Technology},
   keywords = {Classification,Deep learning,Legal text analytics,Prediction systems},
   pages = {374-381},
   publisher = {Springer New York LLC},
   title = {A Review on the Application of Deep Learning in Legal Domain},
   volume = {559},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-19823-7_31},
   year = {2019},
}
@article{Isemann2013,
   abstract = {Tasks and difficulties inherent in the largely open problem of temporal information extraction from legal text are outlined. We demonstrate the efficacy of tools and concepts available "off-the-shelf" and suggest refinements for such applications. In particular, the frequent references between regulatory texts have to be addressed as a separate named entity recognition task that bears relevance to an analysis of the temporal ordering of legislation. A regular expression-based approach as a robust first step towards addressing this problem is tested. © 2013 Springer-Verlag.},
   author = {Daniel Isemann and Khurshid Ahmad and Tim Fernando and Carl Vogel},
   doi = {10.1007/978-3-642-41278-3_60},
   isbn = {9783642412776},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {legal information extraction,named entities,temporality},
   pages = {497-504},
   title = {Temporal dependence in legal documents},
   volume = {8206 LNCS},
   year = {2013},
}
@article{Rodriguez-Doncel2021,
   city = {Cham},
   doi = {10.1007/978-3-030-89811-3},
   editor = {Víctor Rodríguez-Doncel and Monica Palmirani and Michał Araszkiewicz and Pompeu Casanovas and Ugo Pagallo and Giovanni Sartor},
   isbn = {978-3-030-89810-6},
   publisher = {Springer International Publishing},
   title = {AI Approaches to the Complexity of Legal Systems XI-XII},
   volume = {13048},
   url = {https://link.springer.com/10.1007/978-3-030-89811-3},
   year = {2021},
}
@article{Navas-Loro2021,
   abstract = {In this paper we present a suite of tools named TimeLex, that includes different systems able to process temporal information from legal texts. The first tool, called lawORdate, helps preprocessing legal references in texts in Spanish that can be misleading when trying to find dates in texts. The second one, Añotador, is a temporal tagger (this is, a tool that finds temporal expressions, such as dates or durations) that identifies temporal expressions in texts and provides a standard value for each of them. Finally, a third tool, called WhenTheFact, extracts relevant events from judgments, allowing a full processing of the temporal dimension of this kind of texts, and being a first step towards the complete temporal information processing in the legal domain.},
   author = {María Navas-Loro and Víctor Rodríguez-Doncel},
   doi = {10.1007/978-3-030-89811-3_18/COVER},
   isbn = {9783030898106},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Events,Legal texts,Temporal expressions,Timeline generation},
   pages = {260-266},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {TimeLex: A Suite of Tools for Processing Temporal Information in Legal Texts},
   volume = {13048 LNAI},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-89811-3_18},
   year = {2021},
}
@article{Patil2021,
   abstract = {Information retrieval is the process of extracting a pertinent set of facts from a text or a document. The documents are of unstructured format, and thus, information retrieval techniques aim at organizing this data. Named Entity Recognition is one of the information retrieval techniques which classifies a particular word or a phrase in its appropriate class. NER can thus, also be used in extracting entities from legal documents, which would help in providing an effective way to represent these documents. This would reduce the task of a lawyer scrutinizing the document, multiple times, to look for the same set of information. NER systems can be developed with different approaches, one of which is utilizing an NLP library. However, these pretrained NLP libraries may or may not be suitable for a particular use case. Hence, in this paper, we depict an approach to analyze rental documents by custom training spaCy NLP library for tagging named entities such as a person, address, amount, date, etc. The system will provide an interface for the user to upload rent documents, and the result analysis will be stored for quick insights into the document.},
   author = {Chinmay Patil and Sushant Patil and Komal Nimbalkar and Dhiraj Chavan and Sharmila Sengupta and Devesh Rajadhyax},
   doi = {10.1007/978-981-15-7062-9_38/COVER},
   isbn = {9789811570612},
   issn = {21903026},
   journal = {Smart Innovation, Systems and Technologies},
   keywords = {Annotation,Classifier,Cognie,Named entity recognition,spaCy},
   pages = {389-397},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Named Entity Recognition for Rental Documents Using NLP},
   volume = {196},
   url = {https://link.springer.com/chapter/10.1007/978-981-15-7062-9_38},
   year = {2021},
}
@article{Nguyen2022,
   abstract = {General Data Protection Regulation (GDPR) is an important framework for data protection that applies to all European Union countries. Recently, DAPRECO knowledge base (KB) which is a repository of if-then rules written in LegalRuleML as a formal logic representation of GDPR has been introduced to assist compliance checking. DAPRECO KB is, however, constructed manually and the current version does not cover all the articles in GDPR. Looking for an automated method, we present our machine translation approach to obtain a semantic parser translating the regulations in GDPR to their logic representation on DAPRECO KB. We also propose a new version of GDPR Semantic Parsing data by splitting each complex regulation into simple subparagraph-like units and re-annotating them based on published data from DAPRECO project. Besides, to improve the performance of our semantic parser, we propose two mechanisms: Sub-expression intersection and PRESEG. The former deals with the problem of duplicate sub-expressions while the latter distills knowledge from pre-trained language model BERT. Using these mechanisms, our semantic parser obtained a performance of 60.49% F1 in sub-expression level, which outperforms the baseline model by 5.68%.},
   author = {Minh Phuong Nguyen and Thi Thu Trang Nguyen and Vu Tran and Ha Thanh Nguyen and Le Minh Nguyen and Ken Satoh},
   doi = {10.1007/978-3-031-21743-2_35/COVER},
   isbn = {9783031217425},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {GDPR,Logic representation,Semantic parsing},
   pages = {442-454},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Learning to Map the GDPR to Logic Representation on DAPRECO-KB},
   volume = {13757 LNAI},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-21743-2_35},
   year = {2022},
}
@article{Azevedo2022,
   abstract = {Products, services, among many other things in life have a quality standard, are inclusive, or do not harm customers. Regulations required from their manufacturers or providers make it possible. This type of requirement also exists in the finance sector. Governments, international agencies, or civil institutions are responsible for creating, applying, and inspecting these regulations. Regulators from all spheres (federal, state, and municipal) constantly demand changes in the finance sector to meet current needs adequately. This paper presents the constant evolution of a banking compliance application in Brazil. It aims to classify the relevance or irrelevance of regulatory documents published by more than 100 Brazilian regulators, affecting the businesses of more than 40 departments of Banco do Brasil. The application uses a hybrid strategy, combining machine learning and rules for a binary classification challenge involving each company department. This work also presents a particular type of corpus imbalance called The Imbalance Within Class.},
   author = {Rafael Faria de Azevedo and Tiago Nunes Silva and Henrique Tibério Brandão Vieira Augusto and Paulo Oliveira Sampaio Reis and Isadora Bastos Chaves and Samara Beatriz Naka de Vasconcellos and Liliany Aparecida dos Anjos Pereira and Mauro Melo de Souza Biccas and André Luiz Monteiro and Alexandre Rodrigues Duarte},
   doi = {10.1007/978-3-030-98305-5_13/COVER},
   isbn = {9783030983048},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Banking compliance,Finance sector regulation,Imbalance within class,Machine Learning,Rules},
   pages = {137-147},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Banking Regulation Classification in Portuguese},
   volume = {13208 LNAI},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-98305-5_13},
   year = {2022},
}
@article{Batarseh2022,
   abstract = {In this manuscript, we propose an alternative to conventional policy making procedures. The presented policy pipeline leverages intelligent methods that factor for causal relations and economic factors to produce explainable outcomes. Attribution-based methods for analyzing the effects of technology policies are deployed for all American states. Legal codes are analyzed using natural language processing methods to detect similarity, and K-nearest neighbor (Knn) is applied to group laws by influence on state’s technological descriptors, such as broadband and internet use. Additionally, we classify which laws are excitatory and which ones are inhibitory regarding the overall quality of technology services. Our pipeline allows for explaining the ‘goodness of a policy’ using task-based and end-to-end learning; a notion that has not been explored prior. Data are collected from multiple state statutes, intelligent models are developed, experimental work is performed, and the results are presented and discussed.},
   author = {Feras A. Batarseh and Dominick Perini and Qasim Wani and Laura Freeman},
   doi = {10.1007/978-3-031-08421-8_43/COVER},
   isbn = {9783031084201},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Attributions,Data-driven law,Public policy,XAI},
   pages = {624-637},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Explainable Artificial Intelligence for Technology Policy Making Using Attribution Networks},
   volume = {13196 LNAI},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-08421-8_43},
   year = {2022},
}
@article{Benthem2019,
   abstract = {We identify a pervasive contrast between implicit and explicit stances in logical analysis and system design. Implicit systems change received meanings of logical constants and sometimes also the notion of consequence, while explicit systems conservatively extend classical systems with new vocabulary. We illustrate the contrast for intuitionistic and epistemic logic, then take it further to information dynamics, default reasoning, and other areas, to show its wide scope. This gives a working understanding of the contrast, though we stop short of a formal definition, and acknowledge limitations and borderline cases. Throughout we show how awareness of the two stances suggests new logical systems and new issues about translations between implicit and explicit systems, linking up with foundational concerns about identity of logical systems. But we also show how a practical facility with these complementary working styles has philosophical consequences, as it throws doubt on strong philosophical claims made by just taking one design stance and ignoring alternative ones. We will illustrate the latter benefit for the case of logical pluralism and hyper-intensional semantics.},
   author = {Johan van Benthem},
   doi = {10.1007/S10992-018-9485-Y},
   issn = {15730433},
   issue = {3},
   journal = {Journal of Philosophical Logic},
   keywords = {Explicit,Implicit,Logic,Modality,Translation,Vocabulary},
   month = {6},
   pages = {571-601},
   publisher = {Springer Netherlands},
   title = {Implicit and Explicit Stances in Logic},
   volume = {48},
   year = {2019},
}
@article{Wang2022,
   abstract = {Lattice theory has close connections with modal logic via algebraic semantics and lattices of modal logics. However, one less explored direction is to view lattices as relational structures based on partial orders, and study the modal logic over them. In this paper, following the earlier steps of Burgess and van Benthem in the 1980s, we use the basic tense logic and its nominal extensions with binary modalities of infimum and supremum to talk about lattices via standard Kripke semantics. As the main results, we obtain a series of complete axiomatizations of lattices, (un)bounded lattices over partial orders or strict preorders. In particular, we solve an axiomatization problem left open by Burgess (1984).},
   author = {Xiaoyang Wang and Yanjing Wang},
   doi = {10.1007/978-3-031-15298-6_5/COVER},
   isbn = {9783031152979},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {completeness,hybrid logic,lattice,modal logic,polyadic modal logic,step-by-step,tense logic},
   pages = {70-87},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Tense Logics over Lattices},
   volume = {13468 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-15298-6_5},
   year = {2022},
}
@article{Libal2021,
   abstract = {The GDPR is one of many legal texts which can greatly benefit from the support of automated reasoning. Since its introduction, efforts were made to formalize it in order to support various automated operations. Nevertheless, widespread and efficient automated reasoning over the GDPR has not yet been achieved. In this paper, a tool called the NAI suite is being used in order to annotate article 13 of the GDPR. The annotation results in a fully formalized version of the article, which deals with the transparency requirements of data collection and processing. Automated compliance checking is then being demonstrated via several simple queries. By using the public API of the NAI suite, arbitrary tools can use this procedure to support trust management and GDPR compliance functions.},
   author = {Tomer Libal},
   doi = {10.1007/978-3-030-73959-1_1/COVER},
   isbn = {9783030739584},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Automated reasoning,Compliance checking,Data protection},
   pages = {3-19},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Towards Automated GDPR Compliance Checking},
   volume = {12641 LNAI},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-73959-1_1},
   year = {2021},
}
@article{Mauritz2021,
   abstract = {The fourth industrial revolution is driven by Software-enabled automation. To fully realize the potential of this digital transformation in a way that is beneficial to society, automation needs to become programmable by domain experts—the vision being a Software-assisted increase in productivity instead of replacing workers with Software. While domain experts, e.g., workers in production, typically have extensive experience with processes and workflows involving cyber-physical systems, e.g., production machines, they have little to no knowledge of programming and formal logic. In this paper, we present a framework for expressing executable rules in the context of a cyber-physical system at the conceptual level, akin to human reasoning, in almost natural sentences (e.g., if a person is within 1 m of the machine then the light will turn red). These requirements are automatically transformed by our framework into formal logic and can be executed and evaluated by a rule engine without additional input by domain experts. The framework is designed in a modular way that enables domain engineering, i.e., the development of new languages for individual application domains, with minimal effort. Only domain-specific entities and predicates (e.g., is within) need to be defined and implemented for a new domain. We demonstrate our framework in a logistics scenario on a shop floor that requires human-machine collaboration.},
   author = {Malte Mauritz and Moritz Roidl},
   doi = {10.1007/978-3-030-89159-6_11/COVER},
   isbn = {9783030891589},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Domain-specific languages,Language model transformation,Language programming,Logistics,Runtime monitoring},
   pages = {162-177},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {From Requirements to Executable Rules: An Ensemble of Domain-Specific Languages for Programming Cyber-Physical Systems in Warehouse Logistics},
   volume = {13036 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-89159-6_11},
   year = {2021},
}
@article{Ran2023,
   abstract = {Since the launch of Ethereum in 2013, the smart contract has been a momentous part of the blockchain systems due to its character of automatic execution. The generation of smart contracts has also attracted extensive attention from the academic community. However, the preparation and generation of smart contracts are still mainly manual so far, which limits the scalability of the smart contracts. In this paper, we put forward a new method to generate smart contracts automatically based on knowledge extraction and Unified Modeling Language (UML), which can significantly accelerate the generation of smart contracts. We will describe this method in more detail based on the logistics supply chain.},
   author = {Peiyun Ran and Mingsheng Liu and Jianwu Zheng and Zakirul Alam Bhuiyan and Jianhua Li and Gang Li and Shiyuan Yu and Lifeng Wang and Song Tang and Peng Zhao},
   doi = {10.1007/978-3-031-28124-2_44/COVER},
   isbn = {9783031281235},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {automatic code generation,blockchain,knowledge extraction,smart contract,unified modeling language},
   pages = {461-474},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Automatic Smart Contract Generation with Knowledge Extraction and Unified Modeling Language},
   volume = {13828 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-28124-2_44},
   year = {2023},
}
@article{Zhang2021,
   abstract = {The objective of this research is to present an innovative technique of extracting and presenting knowledge in construction documents. A construction project can generate a huge number of documents such as contract, correspondences, meeting minutes, quality and...},
   author = {Qiqi Zhang and Zirui Hong and Xing Su},
   doi = {10.1007/978-981-15-8892-1_59},
   isbn = {978-981-15-8892-1},
   journal = {Proceedings of the 24th International Symposium on Advancement of Construction Management and Real Estate},
   pages = {823-837},
   publisher = {Springer, Singapore},
   title = {Content Analysis Based on Knowledge Graph: A Practice on Chinese Construction Contracts},
   url = {https://link.springer.com/chapter/10.1007/978-981-15-8892-1_59},
   year = {2021},
}
@article{Aejas2023,
   abstract = {Identifying and extracting information from contracts is an important task of contract analysis, which is mostly performed manually by lawyers and legal specialists. This manual analysis is a time-consuming, error-prone task. We can overcome this by automating the task of legal entity extraction using the Natural Language Processing (NLP) techniques. For extracting information from the natural language text, we can use popular NLP methods Named Entity Recognition (NER) and relation extraction (RE). Most NER and RE methods rely on machine learning and deep learning to identify relevant entities in natural language text. The main concern in adapting the AI methods for contract element extraction is the scarcity of annotated datasets in the legal field. Aiming at tackling this challenge, we decided to prepare the contract datasets for NER and RE tasks by manually annotating publicly available English contracts. This work is a part of the research aimed at automating the conversion of natural language contracts into Smart Contracts in the blockchain-based Supply Chain context. This paper explains the implementation and comparison of NER models using the deep learning methods BiLSTM and transformer-based BERT for evaluating the dataset.},
   author = {Bajeela Aejas and Abdelhak Belhi and Abdelaziz Bouras},
   doi = {10.1007/978-981-19-7663-6_70/COVER},
   isbn = {9789811976629},
   issn = {23673389},
   journal = {Lecture Notes in Networks and Systems},
   keywords = {Dataset,Deep learning,Legal domain,NER,NLP,RE},
   pages = {751-759},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Toward an NLP Approach for Transforming Paper Contracts into Smart Contracts},
   volume = {579},
   url = {https://link.springer.com/chapter/10.1007/978-981-19-7663-6_70},
   year = {2023},
}
@article{Balbiani2009,
   abstract = {In nowadays applications of temporal deontic logic to the verification of security policies, an issue arises concerning the temporal inheritance of future directed obligations that have net yet been met. We investigate decision procedures for temporal deontic logics that account for this particular interaction between time and obligation. © 2009 Elsevier B.V. All rights reserved.},
   author = {Philippe Balbiani and Jan Broersen and Julien Brunel},
   doi = {10.1016/J.ENTCS.2009.02.030},
   issn = {15710661},
   issue = {C},
   journal = {Electronic Notes in Theoretical Computer Science},
   keywords = {axiomatization,deontic logic,tableaux method,temporal logic},
   month = {3},
   pages = {69-89},
   title = {Decision Procedures for a Deontic Logic Modeling Temporal Inheritance of Obligations},
   volume = {231},
   year = {2009},
}
@article{Khoja2022,
   abstract = {Contracts in business life, and in particular company purchase agreements, often comprise a large number of provisions and are correspondingly long and complex. In practice, it is therefore a great challenge to keep track of their regulatory context and to identify and avoid inconsistencies in such contracts. Against this background, we propose a semi-formal as well as a formal logical modeling of this type of contracts, using decidable first-order theories. We also present the tool ContractCheck, which performs fully automated inconsistency analyses on the considered contracts using Satisfiability Modulo Theories (SMT) solving.},
   author = {Alan Khoja and Martin Kölbl and Stefan Leue and Rüdiger Wilhelmi},
   doi = {10.1007/978-3-031-15077-7_1/COVER},
   isbn = {9783031150760},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {1-23},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Automated Consistency Analysis for Legal Contracts},
   volume = {13255 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-15077-7_1},
   year = {2022},
}
@article{Bonifacio2021,
   abstract = {Contracts play an important role in business management where relationships among different parties are dictated by legal rules. Electronic contracts have emerged mostly due to technological advances and electronic trading between companies and customers. New challenges have then arisen to guarantee reliability among the stakeholders in electronic negotiations. In this scenario, automatic verification of electronic contracts appeared as an imperative support, specially the conflict detection task of multi-party contracts. The problem of checking contracts has been largely addressed in the literature, but there are few, if any, methods and practical tools that can deal with multi-party contracts using a contract language with deontic and dynamic aspects as well as relativizations, over the same formalism. In this work we present an automatic checker for finding conflicts on multi-party contracts modeled by an extended contract language with deontic operators and relativizations. Moreover a well-known case study of sales contract is modeled and automatically verified by our tool. Further, we performed practical experiments in order to evaluate the efficiency of our method and the practical tool.},
   author = {Adilson Luiz Bonifacio and Wellington Aparecido Della Mura},
   doi = {10.1007/S10506-020-09276-Y},
   issn = {15728382},
   issue = {3},
   journal = {Artificial Intelligence and Law},
   keywords = {Conflict detection,Experiments,Multi-party contracts,Relativized contract language},
   month = {9},
   pages = {287-310},
   publisher = {Springer Science and Business Media B.V.},
   title = {Automatically running experiments on checking multi-party contracts},
   volume = {29},
   year = {2021},
}
@article{Castro2007,
   abstract = {In this paper we present a propositional deontic logic, with the goal of using it to specify fault-tolerant systems, and an axiomatization of it. We prove several results about this logic: completeness, soundness, compactness and decidability. The main technique used during the completeness proof is based on standard techniques for modal logics, but it has some new characteristics introduced for dealing with this logic. In addition, the logic provides several operators which appear useful for use in practice, in particular to model fault-tolerant systems and to reason about their fault tolerance properties. © Springer-Verlag Berlin Heidelberg 2007.},
   author = {Pablo F. Castro and T. S.E. Maibaum},
   doi = {10.1007/978-3-540-75292-9_8},
   isbn = {9783540752905},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Deontic logic,Fault tolerance,Modal logic,Software specification,Temporal logic},
   pages = {109-123},
   publisher = {Springer Verlag},
   title = {A complete and compact propositional deontic logic},
   volume = {4711 LNCS},
   year = {2007},
}
@article{Boley2010,
   abstract = {RuleML is a family of languages, whose modular system of XML schemas permits high-precision Web rule interchange. The family's top-level distinction is deliberation rules vs. reaction rules. Deliberation rules include modal and derivation rules, which themselves include facts, queries (incl. integrity constraints), and Horn rules (incl. Datalog). Reaction rules include Complex Event Processing (CEP), Knowledge Representation (KR), and Event-Condition- Action (ECA) rules, as well as Production (CA) rules. RuleML rules can combine all parts of both derivation and reaction rules. This allows uniform XML serialization across all kinds of rules. After its use in SWRL and SWSL, RuleML has provided strong input to W3C RIF on several levels. This includes the use of 'striped' XML as well as the structuring of rule classes into sublanguages with partial mappings between, e.g., Datalog RuleML and RIF-Core, Hornlog RuleML and RIF-BLD, as well as Production RuleML and RIF-PRD. We discuss the rationale and key features of RuleML 1.0 as the overarching specification of Web rules that encompasses RIF RuleML as a subfamily, and takes into account corresponding OASIS, OMG (e.g., PRR, SBVR), and ISO (e.g., Common Logic) specifications. © Her Majesty the Queen in Right of Canada 2010.},
   author = {Harold Boley and Adrian Paschke and Omair Shafiq},
   doi = {10.1007/978-3-642-16289-3_15},
   isbn = {3642162886},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {162-178},
   publisher = {Springer Verlag},
   title = {RuleML 1.0: The overarching specification of web rules},
   volume = {6403 LNCS},
   year = {2010},
}
@article{Zhao2022,
   abstract = {Due to the imbalance between a large number of litigation cases and the number of judicial personnel, many legal documents to be processed greatly increase the burden of legal practitioners. So the intelligent processing of legal documents is especially important. At present, machine learning and deep learning have made great achievements in the intelligent processing of legal documents, including the elements extraction of legal documents, classification of legal documents, generation of legal documents, abstract extraction of legal documents etc. The main aim of this paper is to present a review of legal documents intelligent processing based on deep learning from legal documents representation, elements extraction of legal documents, classification of legal documents, automatic generation of legal documents.},
   author = {Guolong Zhao and Yuling Liu and E. Erdun},
   doi = {10.1007/978-3-031-06794-5_55/COVER},
   isbn = {9783031067938},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Automatic generation of legal documents,Classification of legal documents,Deep learning,Elements extraction of legal documents,Intelligent justice,Legal documents representation},
   pages = {684-695},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Review on Intelligent Processing Technologies of Legal Documents},
   volume = {13338 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-06794-5_55},
   year = {2022},
}
@article{Nay2016,
   abstract = {We compare policy differences across institutions by embedding representations of the entire legal corpus of each institution and the vocabulary shared across all corpora into a continuous vector space. We apply our method, Gov2Vec, to Supreme Court opinions, Presidential actions, and official summaries of Congressional bills. The model discerns meaningful differences between government branches. We also learn representations for more finegrained word sources: individual Presidents and (2-year) Congresses. The similarities between learned representations of Congresses over time and sitting Presidents are negatively correlated with the bill veto rate, and the temporal ordering of Presidents and Congresses was implicitly learned from only text. With the resulting vectors we answer questions such as: how does Obama and the 113th House differ in addressing climate change and how does this vary from environmental or economic perspectives? Our work illustrates vectorarithmetic- based investigations of complex relationships between word sources based on their texts. We are extending this to create a more comprehensive legal semantic map.},
   author = {John J. Nay},
   doi = {10.18653/v1/w16-5607},
   isbn = {9781945626265},
   journal = {NLP + CSS 2016 - EMNLP 2016 Workshop on Natural Language Processing and Computational Social Science, Proceedings of the Workshop},
   pages = {49-54},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {Gov2Vec: Learning Distributed Representations of Institutions and Their Legal Text},
   year = {2016},
}
@article{Sulis2022,
   abstract = {This paper describes an application of textual similarity techniques in the Legal Informatics domain. In European law, a relevant interest relates to the transposition of EU directives by the Member States, which can be complete, partial, or eventually absent. As part of an European project, legal experts annotated transpositions of six directives on a per-article basis. Following an established NLP pipeline, we explore a similarity-based technique to identify correspondences between transpositions of national implementations. Early results are promising and show the role that Artificial Intelligence may play within the process of harmonization and standardization of domestic legal systems as a result of the adoption of EU legislation.},
   author = {Emilio Sulis and Llio Bryn Humphreys and Davide Audrito and Luigi Di Caro},
   doi = {10.1007/978-3-031-08421-8_13/COVER},
   isbn = {9783031084201},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Harmonization of laws,Legal informatics,Natural language processing,Text similarity},
   pages = {185-197},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Exploiting Textual Similarity Techniques in Harmonization of Laws},
   volume = {13196 LNAI},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-08421-8_13},
   year = {2022},
}
@article{Soavi2022,
   abstract = {Legal contracts have been used for millennia to conduct business transactions world-wide. Such contracts are expressed in natural language, and usually come in written form. We are interested in producing formal specifications from such legal text that can be used to formally analyze contracts, also serve as launching pad for generating smart contracts, information systems that partially automate, monitor and control the execution of legal contracts. We have been developing a method for transforming legal contract documents into specifications, adopting a semantic approach where transformation is treated as a text classification, rather than a natural language processing problem. The method consists of five steps that (a) Identify domain terms in the contract and manually disambiguate them when necessary, in consultation with stakeholders; (b) Semantically annotate text identifying obligations, powers, contracting parties, assets and situations; (c) Identify relationships among the concepts mined in (b); (d) Generate a domain model based on the terms identified in (a), as well as parameters and local variables for the contract; (e) Generate expressions that formalize the conditions of obligations and powers using terms identified in earlier steps in a contract specification language. This paper presents the method through an illustrative example, also reports on a prototype implementation of an environment that supports the method.},
   author = {Michele Soavi and Nicola Zeni and John Mylopoulos and Luisa Mich},
   doi = {10.1007/978-3-031-05760-1_20/COVER},
   isbn = {9783031057595},
   issn = {18651356},
   journal = {Lecture Notes in Business Information Processing},
   keywords = {Domain model,Formal specification,Legal contract,Ontology,Semantic annotation,Smart contract},
   pages = {338-353},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Contratto – A Method for Transforming Legal Contracts into Formal Specifications},
   volume = {446 LNBIP},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-05760-1_20},
   year = {2022},
}
@article{Giri2017,
   abstract = {With crimes increasing at an alarming rate, it becomes essential to impart justice to the victims readily. To come to a final decision, lawyers need to study several previous judgments for research purposes. Reducing the time spent on research can speed up the judicial process drastically. The time consumption mostly happens in two areas - searching for the right document and understanding that document. To start with, being able to get hold of the appropriate judgments or other legal documents is the most essential task for any legal professional, especially lawyers. Once a document is obtained, the next most integral and inevitable task is to read and re-read it, and come to necessary as well as needful conclusions after a comprehensive analysis. To resolve the first issue, there is a need for an efficient search system which can provide searching options based upon multiple views. This system is an effort at improving the search for users by providing them with search options based upon either the semantics of the word or based upon the IPC sections. It is important that laymen can access all the related judgments by entering just one keyword or phrase without bothering about the legal jargon. Post retrieval of documents, the lengthy texts have to be scrutinized for meaningful inferences. To reduce the time spent in reading texts, we intend to present the information in the judgments visually through semantic networks. Lawyers will be benefitted by this system because it will enable them to skip the complexity of the often verbose language of the legal documents. This IR System provides the features of semantic and IPC section based search to users, deriving information from semantic networks that are representative of the documents, so that a more efficient search system on legal documents can be put in place.},
   author = {Rachayita Giri and Yosha Porwal and Vaibhavi Shukla and Palak Chadha and Rishabh Kaushal},
   doi = {10.1109/IC3.2017.8284324},
   isbn = {9781538630778},
   journal = {2017 10th International Conference on Contemporary Computing, IC3 2017},
   month = {7},
   pages = {1-6},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Approaches for information retrieval in legal documents},
   volume = {2018-January},
   year = {2017},
}
@article{Francesconi2010,
   city = {Berlin, Heidelberg},
   doi = {10.1007/978-3-642-12837-0},
   editor = {Enrico Francesconi and Simonetta Montemagni and Wim Peters and Daniela Tiscornia},
   isbn = {978-3-642-12836-3},
   publisher = {Springer Berlin Heidelberg},
   title = {Semantic Processing of Legal Texts},
   volume = {6036},
   url = {http://link.springer.com/10.1007/978-3-642-12837-0},
   year = {2010},
}
@article{Gangemi2005,
   abstract = {The increasing development of legal ontologies seems to offer interesting solutions to legal knowledge formalization, which in past experiences lead to a limited exploitation of legal expert systems for practical use. The paper describes how a constructive approach to ontology can provide useful components to create newly designed legal decision support systems either as local or Web-based semantic services. We describe the relation of our research to AI&Law and legal philosophy, the components of our Core Legal Ontology, the JurWordNet semantic lexicon, and some examples of use of legal ontologies for both norm conformity and compatibility. Our legal ontologies are based on DOLCE+, an extension of the DOLCE foundational ontology developed in the WonderWeb and Metokis EU projects. © 2005 Springer-Verlag.},
   author = {Aldo Gangemi and Maria Teresa Sagri and Daniela Tiscornia},
   doi = {10.1007/978-3-540-32253-5_7},
   isbn = {3540250638},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {97-124},
   title = {A constructive framework for legal ontologies},
   volume = {3369 LNAI},
   year = {2005},
}
@article{Gomes2022,
   abstract = {This work aims to systematize the knowledge on emerging Intelligent Information Retrieval (IIR) practices in scenarios whose context is similar to the field of tax law. It is a part of a project that covers the emerging techniques of IIR and its applicability to the tax law domain. Furthermore, it presents an overview of different approaches for representing legal data and exposes the challenging task of providing quality insights to support decision-making in a dedicated legal environment. It also offers an overview of the related background and prior research referring to the techniques for information retrieval in legal documents, establishing the current state-of-the-art, and identifying its main drawbacks. A summary of the most appropriate technologies and research approaches of the technologies that apply artificial intelligence technology to help legal tasks is also depicted.},
   author = {Marco Gomes and Bruno Oliveira and Cristóvão Sousa},
   doi = {10.1007/978-3-031-16474-3_11/COVER},
   isbn = {9783031164736},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Artificial intelligence,Information Retrieval,Legal domain,Legal knowledge},
   pages = {119-130},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Enriching Legal Knowledge Through Intelligent Information Retrieval Techniques: A Review},
   volume = {13566 LNAI},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-16474-3_11},
   year = {2022},
}
@article{Ashley2013,
   abstract = {Users of commercial legal information retrieval (IR) systems often want argument retrieval (AR): retrieving not merely sentences with highlighted terms, but arguments and argument-related information. Using a corpus of argument-annotated legal cases, we conducted a baseline study of current legal IR systems in responding to standard queries. We identify ways in which they cannot meet the need for AR and illustrate how additional argument-relevant information could address some of those inadequacies. We conclude by indicating our approach to developing an AR system to retrieve arguments from legal decisions. © 2013 The authors and IOS Press.},
   author = {Kevin D. Ashley and Vern R. Walker},
   doi = {10.3233/978-1-61499-359-9-29},
   isbn = {9781614993582},
   issn = {09226389},
   journal = {Frontiers in Artificial Intelligence and Applications},
   keywords = {Argument retrieval,Default-logic framework,Legal information retrieval,Pragmatics,Presuppositional annotation,Semantics},
   pages = {29-38},
   title = {From information retrieval (IR) to argument retrieval (AR) for legal cases: Report on a baseline study},
   volume = {259},
   year = {2013},
}
@article{Dalmonte2022,
   abstract = {We propose a minimal deontic logic, called MIND, based on intuitionistic logic. This logic gives a very simple solution to handling conflicting obligations: the presence of two conflicting obligations does not entail the triviality of the set of norms. Moreover, the logic supports the claim that there may be no obligations at all, so that a logical truth is not obligatory. Like in intuitionistic/constructive modal logic, in this logic, the two deontic modalities Obligation and Permission are not dual, however as a difference with so-called constructive modal logic, it supports distribution of permission over disjunction, but it does not satisfy aggregation of obligations. The logic MIND is a non-normal modal logic based on intuitionistic logic and it is semantically characterised by a suitable neighbourhood semantics. We further present a simple cut-free sequent calculus for this logic. By means of this calculus we show that logic MIND is decidable and that it satisfies the disjunction property.},
   author = {Tiziano Dalmonte and Charles Grellois and Nicola Olivetti},
   doi = {10.1007/978-3-031-15298-6_18/COVER},
   isbn = {9783031152979},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Deontic logic,Intuitionistic modal logic,Neighbourhood semantics,Non-normal modal logic,Sequent calculus},
   pages = {280-294},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Towards an Intuitionistic Deontic Logic Tolerating Conflicting Obligations},
   volume = {13468 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-15298-6_18},
   year = {2022},
}
@article{Athan2015,
   abstract = {This tutorial presents the principles of the OASIS Legal- RuleML applied to the legal domain and discusses why, how, and when LegalRuleML is well-suited for modelling norms. To provide a framework of reference, we present a comprehensive list of requirements for devising rule interchange languages that capture the peculiarities of legal rule modelling in support of legal reasoning. The tutorial comprises syntactic, semantic, and pragmatic foundations, a LegalRuleML primer, as well as use case examples from the legal domain.},
   author = {Tara Athan and Guido Governatori and Monica Palmirani and Adrian Paschke and Adam Wyner},
   doi = {10.1007/978-3-319-21768-0_6},
   isbn = {9783319217673},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Legal rule modeling,LegalRuleML,Meta Model,RuleML},
   pages = {151-188},
   publisher = {Springer Verlag},
   title = {LegalRuleML: Design principles and foundations},
   volume = {9203},
   year = {2015},
}
@article{Steen2022,
   abstract = {LegalRuleML is a comprehensive XML-based representation framework for modeling and exchanging normative rules. The TPTP input and output formats, on the other hand, are general-purpose standards for the interaction with automated reasoning systems. In this paper we provide a bridge between the two communities by (i) defining a logic-pluralistic normative reasoning language based on the TPTP format, (ii) providing a translation scheme between relevant fragments of LegalRuleML and this language, and (iii) proposing a flexible architecture for automated normative reasoning based on this translation. We exemplarily instantiate and demonstrate the approach with three different normative logics.},
   author = {Alexander Steen and David Fuenmayor},
   doi = {10.1007/978-3-031-21541-4_16/COVER},
   isbn = {9783031215407},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Automated reasoning,Deontic logics,LegalRuleML},
   pages = {244-260},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Bridging Between LegalRuleML and TPTP for Automated Normative Reasoning},
   volume = {13752 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-21541-4_16},
   year = {2022},
}
@article{Goossens2022,
   abstract = {Decision models are increasingly being used in modeling business processes. Hence, extracting decision models automatically from texts would help decision modellers by reducing modeling time and supporting them in their analysis. In this paper, deep learning techniques are investigated to extract decision dependencies and conditional clauses directly from text. By using a large dataset of labeled and tagged sentences and NLP, deep learning techniques (BERT and BI-LSTM-CRF) are trained and tested on the identification of these items. The results show that the performance is sufficiently high to extract decision dependency and logic (semi)-automatically from text which provides a big step towards automatic decision modelling.},
   author = {Alexandre Goossens and Michelle Claessens and Charlotte Parthoens and Jan Vanthienen},
   doi = {10.1007/978-3-030-94343-1_27/COVER},
   isbn = {9783030943424},
   issn = {18651356},
   journal = {Lecture Notes in Business Information Processing},
   keywords = {Decision Model and Notation (DMN),Decision model extraction,Deep learning},
   pages = {349-361},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Extracting Decision Dependencies and Decision Logic from Text Using Deep Learning Techniques},
   volume = {436 LNBIP},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-94343-1_27},
   year = {2022},
}
@article{Novotna2022,
   abstract = {Legal formalization is a necessary step for automated legal reasoning because it aims to capture the meaning of legal texts in a machine-readable form. Therefore, there are many attempts to create better legal formalization methods and to make legal reasoning techniques more efficient. However, these methods are rarely evaluated and thus, it is difficult to recognize the“good” legal formalization method. In this article, the authors provide a categorization of necessary properties of a “good” formalization that is based on a literature review of recent state-of-the-art methods. They conclude in favour of the legal experts’ evaluation method as the most suitable one for assessing the quality of legal formalization.},
   author = {Tereza Novotná and Tomer Libal},
   doi = {10.1007/978-3-031-15565-9_12/COVER},
   isbn = {9783031155642},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Evaluation,Legal formalization,Literature review},
   pages = {189-203},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {An Evaluation of Methodologies for Legal Formalization},
   volume = {13283 LNAI},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-15565-9_12},
   year = {2022},
}
@article{Javed2022,
   abstract = {Process compliance with relevant regulations and de-facto standards is a mandatory requirement for certifying critical systems. However, it is often carried out manually, and therefore perceived as complex and labour-intensive. Ontology-based Natural Language Processing (NLP) provides an efficient support for compliance management with critical software system engineering standards. This, however, has not been considered in the literature. Accordingly, the approach presented in this paper focuses on ontology-based NLP for compliance management of software engineering processes with standard documents. In the developed ontology, the process concerns, such as stakeholders, tasks and work products are captured for better interpretation. The rules are created for extracting and structuring information, in which both syntactic features (captured using NLP tasks) and semantic features (captured using ontology) are encoded. During the planning phase, we supported the generation of requirements, process models and compliance mappings in Eclipse Process Framework (EPF) Composer. In the context of reverse compliance, the gaps with standard documents are detected, potential measures for their resolution are provided, and adaptions are made after the process engineer approval. The applicability of the proposed approach is demonstrated by processing ECSS-E-ST-40C, a space software engineering standard, generating models and mappings, as well as reverse compliance management of extended process model.},
   author = {Muhammad Atif Javed and Faiz Ul Muram and Samina Kanwal},
   doi = {10.1007/978-3-030-96648-5_14/COVER},
   isbn = {9783030966478},
   issn = {18650937},
   journal = {Communications in Computer and Information Science},
   keywords = {Compliance management,EPF composer,Natural language processing,Ontology,Process,Rules,SPEM 2.0,Standards},
   pages = {309-327},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Ontology-Based Natural Language Processing for Process Compliance Management},
   volume = {1556 CCIS},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-96648-5_14},
   year = {2022},
}
@article{Bennett2022,
   abstract = {Standards are a necessary adjunct to many kinds of new technology development. This paper explores an emerging new kind of standard in which business meaning is standardized using formal ontology. Some issues are identified in terms of the characteristics of different ontologies and how these may apply to different intended uses. By way of illustration, an account of the origins, history and intended uses of an industry standard called the Financial Industry Business Ontology (FIBO) is given, along with a summary of other financial industry standards. It is suggested that an understanding of the ontology characterizations described here may be instructive in developing industry standards at the level of semantics and in making use of these standards.},
   author = {Michael G. Bennett},
   doi = {10.1007/978-3-030-89906-6_47/COVER},
   isbn = {9783030899059},
   issn = {23673389},
   journal = {Lecture Notes in Networks and Systems},
   keywords = {FIBO,Finance,Ontology,Standards},
   pages = {717-729},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Standards for Knowledge Graphs in the Financial Sector},
   volume = {358 LNNS},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-89906-6_47},
   year = {2022},
}
@article{Beller2008,
   abstract = {Deontic reasoning is thinking about whether actions are forbidden or allowed, obligatory or not obligatory. It is proposed that social norms, imposing constraints on individual actions, constitute the fundamental concept for the system of these four deontic modalities, and that people reason from such norms flexibly according to deontic core principles. Two experiments are presented, one on deontic conditional reasoning, the other on "pure" deontic reasoning. Both experiments demonstrate people's high deontic competence and confirm the proposed representational and inferential principles. Experiment 1 additionally shows small effects of the conditional formulations. These findings support the dual source approach (Beller & Spada, 2003) that distinguishes between domain-specific and domain-general inferences. Implications for other theories of deontic reasoning are discussed. © 2008 Psychology Press.},
   author = {Sieghard Beller},
   doi = {10.1080/13546780802222258},
   issn = {13546783},
   issue = {4},
   journal = {Thinking and Reasoning},
   keywords = {Conditionals,Content effects,Deontic reasoning,Dual source approach},
   month = {7},
   pages = {305-341},
   title = {Deontic norms, deontic reasoning, and deontic conditionals},
   volume = {14},
   year = {2008},
}
@article{Akaichi2023,
   abstract = {Robust Usage Control (UC) mechanisms are necessary to protect sensitive data and resources, especially when these are distributed across multiple nodes or users. Existing solutions have limitations in expressing and enforcing usage control policies due to difficulties in capturing complex requirements and the lack of formal semantics necessary for automated compliance checking. To address these challenges, we propose GUCON, a generic policy framework that allows for the expression of and reasoning over granular UC policies. This is achieved by leveraging the expressiveness and semantics of graph pattern expressions, as well as the flexibility of deontic concepts. Additionally, GUCON incorporates algorithms for conflict detection, resolution, compliance and requirements checking, ensuring active policy enforcement. We demonstrate the effectiveness of our framework by proposing instantiations using SHACL, OWL and ODRL. We show how instantiations provide a bridge between abstract formalism and concrete implementations, thus allowing existing reasoners and implementations to be leveraged.},
   author = {Ines Akaichi and Giorgos Flouris and Irini Fundulaki and Sabrina Kirrane},
   doi = {10.1007/978-3-031-45072-3_3/COVER},
   isbn = {9783031450716},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Deontic Rules,Enforcement,Policy,Reasoning,Usage Control},
   pages = {34-53},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {GUCON: A Generic Graph Pattern Based Policy Framework for Usage Control Enforcement},
   volume = {14244 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-45072-3_3},
   year = {2023},
}
@article{,
   city = {Cham},
   doi = {10.1007/978-3-031-45072-3},
   editor = {Anna Fensel and Ana Ozaki and Dumitru Roman and Ahmet Soylu},
   isbn = {978-3-031-45071-6},
   publisher = {Springer Nature Switzerland},
   title = {Rules and Reasoning},
   volume = {14244},
   url = {https://link.springer.com/10.1007/978-3-031-45072-3},
   year = {2023},
}
@article{Francesconi2023,
   abstract = {This paper presents an approach for legal compliance checking in the Semantic Web which can be effectively applied for applications in the Linked Open Data environment. It is based on modeling deontic norms in terms of ontology classes and ontology property restrictions. It is also shown how this approach can handle norm defeasibility. Such methodology is implemented by decidable fragments of OWL 2, while legal reasoning is carried out by available decidable reasoners. The approach is generalised by presenting patterns for modeling deontic norms and norms compliance checking.},
   author = {Enrico Francesconi and Guido Governatori},
   doi = {10.1007/S10506-022-09317-8},
   issn = {15728382},
   issue = {3},
   journal = {Artificial Intelligence and Law},
   keywords = {Legal reasoning,Norm compliance,OWL 2,Semantic Web},
   month = {9},
   pages = {445-464},
   publisher = {Institute for Ionics},
   title = {Patterns for legal compliance checking in a decidable framework of linked open data},
   volume = {31},
   year = {2023},
}
@article{Bonatti2020,
   abstract = {This paper shows how knowledge representation and reasoning techniques can be used to support organizations in complying with the GDPR, that is, the new European data protection regulation. This work is carried out in a European H2020 project called SPECIAL. Data usage policies, the consent of data subjects, and selected fragments of the GDPR are encoded in a fragment of OWL2 called PL (policy language); compliance checking and policy validation are reduced to subsumption checking and concept consistency checking. This work proposes a satisfactory tradeoff between the expressiveness requirements on PL posed by the modeling of the GDPR, and the scalability requirements that arise from the use cases provided by SPECIAL's industrial partners. Real-time compliance checking is achieved by means of a specialized reasoner, called PLR, that leverages knowledge compilation and structural subsumption techniques. The performance of a prototype implementation of PLR is analyzed through systematic experiments, and compared with the performance of other important reasoners. Moreover, we show how PL and PLR can be extended to support richer ontologies, by means of import-by-query techniques. We prove novel tractability and intractability results related to PL, and some negative results about the restrictions posed on ontology import.},
   author = {Piero A. Bonatti and Luca Ioffredo and Iliana M. Petrova and Luigi Sauro and Ida R. Siahaan},
   doi = {10.1016/J.ARTINT.2020.103389},
   issn = {00043702},
   journal = {Artificial Intelligence},
   keywords = {GDPR,Import-by-query,Knowledge compilation,Semantic policy languages,Structural subsumption,Tractable OWL2 fragments},
   month = {12},
   publisher = {Elsevier B.V.},
   title = {Real-time reasoning in OWL2 for GDPR compliance},
   volume = {289},
   year = {2020},
}
@article{Bonatti2020,
   abstract = {The European General Data Protection Regulation (GDPR) calls for technical and organizational measures to support its implementation. Towards this end, the SPECIAL H2020 project aims to provide a set of tools that can be used by data controllers and processors to automatically check if personal data processing and sharing complies with the obligations set forth in the GDPR. The primary contributions of the project include: (i) a policy language that can be used to express consent, business policies, and regulatory obligations; and (ii) two different approaches to automated compliance checking that can be used to demonstrate that data processing performed by data controllers/processors complies with consent provided by data subjects, and business processes comply with regulatory obligations set forth in the GDPR.},
   author = {Piero A. Bonatti and Sabrina Kirrane and Iliana M. Petrova and Luigi Sauro},
   doi = {10.1007/S13218-020-00677-4},
   issn = {16101987},
   issue = {3},
   journal = {KI - Kunstliche Intelligenz},
   keywords = {Compliance checking,GDPR,Policies},
   month = {9},
   pages = {303-315},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Machine Understandable Policies and GDPR Compliance Checking},
   volume = {34},
   year = {2020},
}
@article{Broersen2004,
   abstract = {Dynamic deontic logics reduce normative assertions about explicit complex actions to standard dynamic logic assertions about the relation between complex actions and violation conditions. We address two general, but related problems in this field. The first is to find a formalization of the notion of 'action negation' that (1) has an intuitive interpretation as an action forming combinator and (2) does not impose restrictions on the use of other relevant action combinators such as sequence and iteration, and (3) has a meaningful interpretation in the normative context. The second problem we address concerns the reduction from deontic assertions to dynamic logic assertions. Our first point is that we want this reduction to obey the free-choice semantics for norms. For ought-to- be deontic logics it is generally accepted that the free-choice semantics is counter-intuitive. But for dynamic deontic logics we actually consider it a viable, if not, the better alternative. Our second concern with the reduction is that we want it to be more liberal than the ones that were proposed before in the literature. For instance, Meyer's reduction does not leave room for action whose normative status is neither permitted nor forbidden. We test the logics we define in this paper against a set of minimal logic requirements. © 2004 Elsevier B.V. All rights reserved.},
   author = {Jan Broersen},
   doi = {10.1016/J.JAL.2004.01.007},
   issn = {15708683},
   issue = {1 SPEC. ISS.},
   journal = {Journal of Applied Logic},
   keywords = {Action negation and refraining,Action theory,Deontic logic,Dynamic logic},
   pages = {153-168},
   publisher = {Elsevier BV},
   title = {Action negation and alternative reductions for dynamic deontic logics},
   volume = {2},
   year = {2004},
}
@article{Su2023,
   abstract = {A normative sentence contains information which is used for either describing some deontic situations or prescribing a new norm. Considering the differences between them, we introduced the notion of relativized conditional obligations based on ideality sequences. On the one hand, each ideality sequence can be treated as a normative system which prescribes the relative ideality of states of affairs. On the other hand, a bare structure provides a factual background. Once these are done, every betterness structure based on a given ideality sequence and a bare structure describes the conditional obligations. Deletion and postfixing are two updates on the normative system which can bring about corresponding obligations successfully or not. Jørgensen’s dilemma can be conceptualized by using the notion of successful updates. A sound and strongly complete axiom system for the logic of relativized conditional obligations PCDL is established.},
   author = {Xingchi Su},
   doi = {10.1007/978-3-031-45558-2_20/COVER},
   isbn = {9783031455575},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Jørgensen’s dilemma,axiomatization,conditional obligation,ideality sequence,normative system,successful update,update normative system},
   pages = {260-268},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Making Norms and Following Norms},
   volume = {14329 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-45558-2_20},
   year = {2023},
}
@article{Yufang2023,
   abstract = {In recent years, massive multi-source heterogeneous South China Sea data have been widely used in the construction of South China Sea digital resources, such as the South China Sea Sovereign Evidence Chain Project. Due to the data sparsity, a large number of isolated data are generated, which seriously affects the analysis effect of the South China Sea Big Data. In this paper, we proposed a novel data association method. We collected data from the South China Sea Library Digital Resources as South China Sea evidence data, which is a sentence or paragraph containing time, place, people, institutions and events can prove the sovereignty of the South China Sea. According to the definition of the evidence weight by the International Court of Justice, the logical relationship of South China Sea evidence data was constructed. Firstly, we randomly selected 3068 data from 21174 evidence data to label the logical relationship. Secondly, we used the BERT pre-training model to extract the logical relationship of evidence data. Finally, the Knowledge Graph technology is used to retrieve and visualize the logical relationship of evidence data. In this paper, we applied the BERT to extract the logical relationships of evidence data with an accuracy of 0.78, which indicates that the model has some feasibility. This paper could help to improve the correlation of the South China Sea Big Data and to enhance the ability of data processing.},
   author = {Peng Yufang and Xu Hao and Jin Weijian and Yang Haiping},
   doi = {10.1007/978-981-99-3300-6_16/COVER},
   isbn = {9789819932993},
   issn = {18650937},
   journal = {Communications in Computer and Information Science},
   keywords = {Deep learning,Knowledge Graph,Logical Relationship,The South China Sea Big Data},
   pages = {219-233},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Logical Relationship Extraction of Multimodal South China Sea Big Data Using BERT and Knowledge Graph},
   volume = {1796 CCIS},
   url = {https://link.springer.com/chapter/10.1007/978-981-99-3300-6_16},
   year = {2023},
}
@article{Beltagy2020,
   abstract = {Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.},
   author = {Iz Beltagy and Matthew E. Peters and Arman Cohan},
   month = {4},
   title = {Longformer: The Long-Document Transformer},
   url = {http://arxiv.org/abs/2004.05150},
   year = {2020},
}
@article{Huang2023,
   abstract = {Legal artificial intelligence (LegalAI) is an emerging field that leverages AI technology to enhance legal services. Similar Case Matching (SCM), which calculates the relevance between a candidate and a target case, is a critical technique in LegalAI to enable diverse legal intelligences. Existing approaches mainly rely the on single query texts or specific keywords for retrieval, yet neglected the domain complexity and multi-faceted nature of queries. Thus, a multi-example matching paradigm is motivated where three inherent challenges reveal. 1) Relevance assessment across multiple examples is complex. 2) The inherent lengthy and structured property of legal documents. 3) Lacking datasets containing golden labels for multi-example-based legal text matching. To address these challenges, this paper develops a novel multi-example dataset, and a Multi-level Correlation Semantic Matching (MCSM) is devised to extract similarity between cases given multi-example inputs. The proposed multi-level scheme can be interpreted as two aspects. Firstly, we consider both content and structure correlations to evaluate the relevance. Secondly, by dividing legal documents into distinctive segments, we can hierarchically learn the intra- and inter-segment dependencies to model the long-term dependencies across components of legal documents. An attention mechanism is employed to capture the complex interconnections among these examples and enable an attentive matching aggregation of content and structure. With multiple examples, the MCSM tackles the intricate and diverse nature of legal queries, providing a comprehensive and multi-dimensional description view. Extensive experimental evaluations show that the proposed MCSM outperforms baseline methods.},
   author = {Ting Huang and Xike Xie and Xiufeng Liu},
   doi = {10.1007/978-981-99-7254-8_48/COVER},
   isbn = {9789819972531},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Legal Artificial Intelligence,Natural Language Processing,Semantic Matching},
   pages = {621-632},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Multi-level Correlation Matching for Legal Text Similarity Modeling with Multiple Examples},
   volume = {14306 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-981-99-7254-8_48},
   year = {2023},
}
@article{Bennett2013,
   abstract = {This article describes the Financial Industry Business Ontology (FIBO) as a set of formal models that define unambiguous shared meaning for financial industry concepts. An account is given of the history and development of the FIBO series of standards and the theoretical underpinnings of these as a business or 'conceptual' model. Some initial proof of concept work is described, demonstrating how in addition to the use of FIBO as a conceptual model, it is possible to derive semantic technology-based applications that may be used to carry out novel types of processing on data. The development roadmap of the FIBO series of standards within the Object Management Group is also described, so that readers can have an idea of what to expect from FIBO and when. © 2013 Macmillan Publishers Ltd.},
   author = {Mike Bennett},
   doi = {10.1057/JBR.2013.13},
   issn = {17456452},
   issue = {3-4},
   journal = {Journal of Banking Regulation},
   keywords = {FIBO,OWL,big data,ontology,semantics,vocabulary},
   month = {7},
   pages = {255-268},
   title = {The financial industry business ontology: Best practice for big data},
   volume = {14},
   year = {2013},
}
@article{Georgoudi2023,
   abstract = {Documents contain textual information, which is of the utmost importance for all the organizations. Document management systems have been used to store vast amounts of unstructured textual data described with minimal metadata, a method that has several limitations. In order to convert hidden knowledge into machine-readable data with rich connections, this paper presents work in progress on the development of the first end-to-end guided approach to construct a Knowledge Graph from Greek governmental documents from the Greek open government portal. The resulted Knowledge graph constitutes a proof-of-concept graph, that illustrates the beneficial semantic relationships between the textual data.},
   author = {Amalia Georgoudi and Nikolaos Stylianou and Ioannis Konstantinidis and Georgios Meditskos and Thanassis Mavropoulos and Stefanos Vrochidis and Nick Bassiliades},
   doi = {10.1007/978-3-031-36819-6_26/COVER},
   isbn = {9783031368189},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Government Documents,Knowledge Graph Construction,Knowledge Representation,Natural Language Processing,Ontologies,RDF Triples},
   pages = {294-299},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Towards Knowledge Graph Creation from Greek Governmental Documents},
   volume = {13925 LNAI},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-36819-6_26},
   year = {2023},
}
@article{Stylianou2022,
   abstract = {Document management systems (DMS) have been used for decades to store large amounts of information in textual form. Their technology paradigm is based on storing vast quantities of textual information enriched with metadata to support searchability. However, this exhibits limitations as it treats textual information as a black box and is based exclusively on user-created metadata, a process that suffers from quality and completeness shortcomings. The use of knowledge graphs in DMS can substantially improve searchability, providing the ability to link data and enabling semantic searching. Recent approaches focus on either creating knowledge graphs from document collections or updating existing ones. In this paper, the authors introduce Doc2KG (Document-to-Knowledge-Graph), an intelligent framework that handles both creation and real-time updating of a knowledge graph, while also exploiting domain-specific ontology standards. They use DIAVGEIA (clarity), an award-winning Greek open government portal, as the case-study and discuss new capabilities for the portal by implementing Doc2KG.},
   author = {Nikolaos Stylianou and Danai Vlachava and Ioannis Konstantinidis and Nick Bassiliades and Vassilios Peristeras},
   doi = {10.4018/IJSWIS.295552},
   issn = {15526291},
   issue = {1},
   journal = {International Journal on Semantic Web and Information Systems},
   keywords = {EGovernment,Government portals,Linked data,Machine learning,Natural language processing,Open data,Semantic web},
   month = {1},
   publisher = {IGI Global},
   title = {Doc2KG: Transforming Document Repositories to Knowledge Graphs},
   volume = {18},
   year = {2022},
}
@article{Izumigawa2023,
   abstract = {Ontologies are human- and machine-readable conceptualizations that define domain concepts and their relationships. The context provided by these representations are essential for advanced reasoning applications and explainable artificial intelligence efforts. Despite their advantages, however, automating ontology generation from text is difficult due to a number of challenges. To bring greater awareness to those challenges and to initiate discussion on possible solutions, we provide a definition of ontologies, the motivation behind our work, a set of key challenges, and an overview of adjacent solutions in recent work.},
   author = {Christianne Izumigawa and Bethany Taylor and Jonathan Sato},
   doi = {10.1007/978-3-031-36004-6_59/COVER},
   isbn = {9783031360039},
   issn = {18650937},
   journal = {Communications in Computer and Information Science},
   keywords = {Natural Language Processing,Ontology Learning},
   pages = {433-438},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Automated Ontology Generation},
   volume = {1836 CCIS},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-36004-6_59},
   year = {2023},
}
@article{Liga2023,
   abstract = {The aim of this work is to employ Tree Kernel algorithms to classify natural language in the legal domain (i.e. deontic sentences and rules). More precisely, an innovative way of extracting labelled legal data is proposed, which combines the information provided by two famous LegalXML formats: Akoma Ntoso and LegalRuleML. We then applied this method on the European General Data Protection Regulation (GDPR) to train a Tree Kernel classifier on deontic and non-deontic sentences which were reconstructed using Akoma Ntoso, and labelled using the LegalRuleML representation of the GDPR. To prove the non-triviality of the task we reported the results of a stratified baseline classifier on two classification scenarios.},
   author = {Davide Liga and Monica Palmirani},
   doi = {10.1007/978-3-031-16072-1_4/COVER},
   isbn = {9783031160714},
   issn = {23673389},
   journal = {Lecture Notes in Networks and Systems},
   keywords = {Akoma Ntoso,Deontic,GDPR,Legal AI,LegalRuleML,NLP},
   pages = {54-73},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Deontic Sentence Classification Using Tree Kernel Classifiers},
   volume = {542 LNNS},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-16072-1_4},
   year = {2023},
}
@article{Chalkidis2018,
   abstract = {We consider the task of detecting contractual obligations and prohibitions. We show that a self-attention mechanism improves the performance of a BILSTM classifier, the previous state of the art for this task, by allowing it to focus on indicative tokens. We also introduce a hierarchical BILSTM, which converts each sentence to an embedding, and processes the sentence embeddings to classify each sentence. Apart from being faster to train, the hierarchical BILSTM outperforms the flat one, even when the latter considers surrounding sentences, because the hierarchical model has a broader discourse view.},
   author = {Ilias Chalkidis and Ion Androutsopoulos and Achilleas Michos},
   doi = {10.18653/v1/p18-2041},
   isbn = {9781948087346},
   journal = {ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
   pages = {254-259},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {Obligation and prohibition extraction using hierarchical RNNs},
   volume = {2},
   year = {2018},
}
@article{Prince-Tritto2024,
   abstract = {The utilization of machine learning methods for the analysis and interpretation of legal documents has been growing over the years, yet their potential and limitations remain under-explored. This study aims to address this gap, using unsupervised machine learning...},
   author = {Philippe Prince-Tritto and Hiram Ponce},
   doi = {10.1007/978-3-031-47640-2_5/COVER},
   isbn = {9783031476396},
   issn = {16113349},
   pages = {52-67},
   publisher = {Springer, Cham},
   title = {Exploring the Challenges and Limitations of Unsupervised Machine Learning Approaches in Legal Concepts Discovery},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-47640-2_5},
   year = {2024},
}
@article{Graham2023,
   abstract = {The contract review process can be a costly and time-consuming task for lawyers and clients alike, requiring significant effort to identify and evaluate the legal implications of individual clauses. To address this challenge, we propose the use of natural language processing techniques, specifically text classification based on deontic tags, to streamline the process. Our research question is whether natural language processing techniques, specifically dense vector embeddings, can help semi-automate the contract review process and reduce time and costs for legal professionals reviewing deontic modalities in contracts. In this study, we create a domain-specific dataset and train both baseline and neural network models for contract sentence classification. This approach offers a more efficient and cost-effective solution for contract review, mimicking the work of a lawyer. Our approach achieves an accuracy of 0.90, showcasing its effectiveness in identifying and evaluating individual contract sentences.},
   author = {S. Georgette Graham and Hamidreza Soltani and Olufemi Isiaq},
   doi = {10.1007/S10506-023-09379-2/METRICS},
   issn = {15728382},
   journal = {Artificial Intelligence and Law},
   keywords = {Annotation,Deep learning,Deontic reasoning,Legal text classification,Natural language processing},
   month = {11},
   pages = {1-22},
   publisher = {Institute for Ionics},
   title = {Natural language processing for legal document review: categorising deontic modalities in contracts},
   url = {https://link.springer.com/article/10.1007/s10506-023-09379-2},
   year = {2023},
}
@article{Liu2023,
   abstract = {Few-shot learning aims to discriminate images from novel categories using only a few available training examples. While existing few-shot methods can adapt quickly and precisely to new categories, they often struggle to retain knowledge of base categories that were used in the training phase. To address this challenge and support lifelong learning, generalized few-shot learning has been introduced to enable few-shot models to classify both base and novel categories. However, as the number of categories increases, few-shot models can lose efficiency due to the limited amount of visual information available for each category. To address this limitation, we propose the knowledge-augmented weight generation (KAWG) method, which incorporates semantic information in addition to visual features. Specifically, KAWG combines textual descriptions and entity relationships extracted from knowledge graphs and visual features to generate more robust classifiers for generalized few-shot learning tasks. Through our meta-training strategy, KAWG can retain the knowledge learned from base categories to the greatest extent when transferring to novel classes. Experiments show that our approach achieves state-of-the-art performance on some generalized few-shot benchmarks.},
   author = {Dianqi Liu and Liang Bai and Tianyuan Yu},
   doi = {10.1007/S11063-023-11278-1/METRICS},
   issn = {1573773X},
   issue = {6},
   journal = {Neural Processing Letters},
   keywords = {Few-shot learning,Image classification,Knowledge graph,Meta-learn},
   month = {12},
   pages = {7649-7666},
   publisher = {Springer},
   title = {Generalized Few-Shot Classification with Knowledge Graph},
   volume = {55},
   url = {https://link.springer.com/article/10.1007/s11063-023-11278-1},
   year = {2023},
}
@article{Abdallah2023,
   abstract = {Answering questions related to the legal domain is a complex task, primarily due to the intricate nature and diverse range of legal document systems. Providing an accurate answer to a legal query typically necessitates specialized knowledge in the relevant domain, which makes this task all the more challenging, even for human experts. Question answering (QA) systems are designed to generate answers to questions asked in human languages. QA uses natural language processing to understand questions and search through information to find relevant answers. QA has various practical applications, including customer service, education, research, and cross-lingual communication. However, QA faces challenges such as improving natural language understanding and handling complex and ambiguous questions. Answering questions related to the legal domain is a complex task, primarily due to the intricate nature and diverse range of legal document systems. Providing an accurate answer to a legal query typically necessitates specialized knowledge in the relevant domain, which makes this task all the more challenging, even for human experts. At this time, there is a lack of surveys that discuss legal question answering. To address this problem, we provide a comprehensive survey that reviews 14 benchmark datasets for question-answering in the legal field as well as presents a comprehensive review of the state-of-the-art Legal Question Answering deep learning models. We cover the different architectures and techniques used in these studies and the performance and limitations of these models. Moreover, we have established a public GitHub repository where we regularly upload the most recent articles, open data, and source code. The repository is available at: \url\{https://github.com/abdoelsayed2016/Legal-Question-Answering-Review\}.},
   author = {Abdelrahman Abdallah and Bhawna Piryani and Adam Jatowt},
   doi = {10.1186/s40537-023-00802-8},
   issue = {1},
   journal = {Journal of Big Data},
   keywords = {Information retrieval,Legal information extraction,Legal question answering,Machine learning,Natural language processing,Transformers},
   month = {4},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Exploring the State of the Art in Legal QA Systems},
   volume = {10},
   url = {http://arxiv.org/abs/2304.06623 http://dx.doi.org/10.1186/s40537-023-00802-8},
   year = {2023},
}
@article{Acosta2019,
   city = {Cham},
   doi = {10.1007/978-3-030-33220-4},
   editor = {Maribel Acosta and Philippe Cudré-Mauroux and Maria Maleshkova and Tassilo Pellegrini and Harald Sack and York Sure-Vetter},
   isbn = {978-3-030-33219-8},
   publisher = {Springer International Publishing},
   title = {Semantic Systems. The Power of AI and Knowledge Graphs},
   volume = {11702},
   url = {http://link.springer.com/10.1007/978-3-030-33220-4},
   year = {2019},
}
@article{Kim2020,
   abstract = {This study proposes the optimization method of the associative knowledge graph using TF-IDF based ranking scores. The proposed method calculates TF-IDF weights in all documents and generates term ranking. Based on the terms with high scores from TF-IDF based ranking, optimized transactions are generated. News data are first collected through crawling and then are converted into a corpus through preprocessing. Unnecessary data are removed through preprocessing including lowercase conversion, removal of punctuation marks and stop words. In the document term matrix, words are extracted and then transactions are generated. In the data cleaning process, the Apriori algorithm is applied to generate association rules and make a knowledge graph. To optimize the generated knowledge graph, the proposed method utilizes TF-IDF based ranking scores to remove terms with low scores and recreate transactions. Based on the result, the association rule algorithm is applied to create an optimized knowledge model. The performance is evaluated in rule generation speed and usefulness of association rules. The association rule generation speed of the proposed method is about 22 seconds faster. And the lift value of the proposed method for usefulness is about 0.43 to 2.51 higher than that of each one of conventional association rule algorithms.},
   author = {Hyun Jin Kim and Ji Won Baek and Kyungyong Chung},
   doi = {10.3390/app10134590},
   issn = {20763417},
   issue = {13},
   journal = {Appl Sci},
   keywords = {Apriori,Association rule,Associative knowledge graph,FP-tree,TF-IDF},
   month = {7},
   pages = {4590},
   publisher = {MDPI AG},
   title = {Optimization of associative knowledge graph using TF-IDF based ranking score},
   volume = {10},
   year = {2020},
}
@article{Sood2023,
   abstract = {Frauds accounted for significant losses in the financial sector and emerged as the industry’s biggest challenge. Companies invest significant amounts to prevent such fraud. It has been reported that 63.6% of the financial institutions that use Automated Fraud prevention methods successfully prevented frauds before their occurrence. Some estimations suggest that 80% of specialists are confident in cutting down fraud using Artificial Intelligence (AI)-based platforms. Several research studies have also administered AI-based techniques for fraud prevention. This study takes a systematic literature review approach to uncover the emerging areas of fraud detection using AI. The authors have analyzed 241 research articles published in the last 20 years. The Scopus database was the source of the articles in the literature review. The meta-analysis and network analysis were carried out, and the output shows the up trend of this research domain. Author-coauthor network collaboration is analyzed using the VOSviewer tool. K-means clustering was performed to identify the critical research domain, and future research areas were also identified. This research will act as a reference for future scholars who want to perform analysis on the application of AI techniques in financial fraud detection and prevention. We finally conclude the study by identifying the scope of future research and will be a value addition for financial fraud researchers.},
   author = {Pallavi Sood and Chetan Sharma and Shivinder Nijjer and Sumit Sakhuja},
   doi = {10.1007/S13198-023-02043-7/METRICS},
   issn = {09764348},
   issue = {6},
   journal = {International Journal of System Assurance Engineering and Management},
   keywords = {Artificial intelligence,Automated techniques,Financial frauds,KNIME,Latent semantic analysis,Text mining,Topic modelling,Vosviewer},
   month = {12},
   pages = {2120-2135},
   publisher = {Springer},
   title = {Review the role of artificial intelligence in detecting and preventing financial fraud using natural language processing},
   volume = {14},
   url = {https://link.springer.com/article/10.1007/s13198-023-02043-7},
   year = {2023},
}
@article{Greco2023,
   abstract = {Transformer-based language models (TLMs) have widely been recognized to be a cutting-edge technology for the successful development of deep-learning-based solutions to problems and applications that require natural language processing and understanding. Like for other textual domains, TLMs have indeed pushed the state-of-the-art of AI approaches for many tasks of interest in the legal domain. Despite the first Transformer model being proposed about six years ago, there has been a rapid progress of this technology at an unprecedented rate, whereby BERT and related models represent a major reference, also in the legal domain. This article provides the first systematic overview of TLM-based methods for AI-driven problems and tasks in the legal sphere. A major goal is to highlight research advances in this field so as to understand, on the one hand, how the Transformers have contributed to the success of AI in supporting legal processes, and on the other hand, what are the current limitations and opportunities for further research development.},
   author = {Candida M. Greco and Andrea Tagarelli},
   doi = {10.1007/S10506-023-09374-7},
   isbn = {0123456789},
   issn = {1572-8382},
   journal = {Artificial Intelligence and Law 2023},
   keywords = {Artificial Intelligence,IT Law,Information Storage and Retrieval,Intellectual Property,Language models,Legal Aspects of Computing,Legal document review,Legal outcome prediction,Legal search,Media Law,Philosophy of Law,Retrieval,Statutory law data},
   month = {11},
   pages = {1-148},
   publisher = {Springer},
   title = {Bringing order into the realm of Transformer-based language models for artificial intelligence and law},
   url = {https://link.springer.com/article/10.1007/s10506-023-09374-7},
   year = {2023},
}
@article{Szoke2013,
   abstract = {Regulations affect every aspect of our lives. Compliance with the regulations impacts citizens and businesses similarly: they have to find their rights and obligations in the complex legal environment. The situation is more complex when languages and time versions of regulations should be considered. To propose a solution to these demands, we present a semantic enrichment approach which aims at (1) decreasing the ambiguousness of legal texts, (2) increasing the probability of finding the relevant legal materials, and (3) utilizing the application of legal reasoners. Our approach is also implemented both as a service for citizens and businesses and as a modeling environment for legal drafters. To evaluate the usefulness of the approach, a case study was carried out in a large organization and applied to corporate regulations and Hungarian laws. The results suggest this approach can support the previous aims. © 2013 Springer Science+Business Media Dordrecht.},
   author = {Ákos Szőke and András Förhécz and Gábor Kőrösi and György Strausz},
   doi = {10.1007/S10506-013-9145-Z/METRICS},
   issn = {15728382},
   issue = {4},
   journal = {Artificial Intelligence and Law},
   keywords = {Legal xml,Linked data,Ontology,Rdf,SWRL,Semantic enrichment},
   month = {11},
   pages = {485-519},
   publisher = {Kluwer Academic Publishers},
   title = {Versioned linking of semantic enrichment of legal documents: Emerald: An implementation of knowledge-based services in a semantic web approach},
   volume = {21},
   url = {https://link.springer.com/article/10.1007/s10506-013-9145-z},
   year = {2013},
}
@article{McLachlan2023,
   abstract = {Modelling that exploits visual elements and information visualisation are important areas that have contributed immensely to understanding and the computerisation advancements in many domains and yet remain unexplored for the benefit of the law and legal practice. This paper investigates the challenge of modelling and expressing structures and processes in legislation and the law by using visual modelling and information visualisation (InfoVis) to assist accessibility of legal knowledge, practice and knowledge formalisation as a basis for legal AI. The paper uses a subset of the well-defined Unified Modelling Language (UML) to visually express the structure and process of the legislation and the law to create visual flow diagrams called lawmaps, which form the basis of further formalisation. A lawmap development methodology is presented and evaluated by creating a set of lawmaps for the practice of conveyancing and the Landlords and Tenants Act 1954 of the United Kingdom. This paper is the first of a new breed of preliminary solutions capable of application across all aspects, from legislation to practice; and capable of accelerating development of legal AI.},
   author = {Scott McLachlan and Evangelia Kyrimi and Kudakwashe Dube and Norman Fenton and Lisa C. Webley},
   doi = {10.1007/S10506-021-09298-0/FIGURES/14},
   issn = {15728382},
   issue = {1},
   journal = {Artificial Intelligence and Law},
   keywords = {Flowcharts,Lawmaps,Legal process,Legislation,Process visualisation},
   month = {3},
   pages = {169-194},
   publisher = {Institute for Ionics},
   title = {Lawmaps: enabling legal AI development through visualisation of the implicit structure of legislation and lawyerly process},
   volume = {31},
   url = {https://link.springer.com/article/10.1007/s10506-021-09298-0},
   year = {2023},
}
@article{Breuker2004,
   abstract = {In this article we describe two core ontologies of law that specify knowledge that is common to all domains of law. The first one, FOLaw describes and explains dependencies between types of knowledge in legal reasoning; the second one, LRI-Core ontology, captures the main concepts in legal information processing. Although FOLaw has shown to be of high practical value in various applied European ICT projects, its reuse is rather limited as it is rather concerned with the structure of legal reasoning than with legal knowledge itself: as many other "legal core ontologies", FOLaw is therefore rather an epistemological framework than an ontology. Therefore, we also developed LRI-Core. As we argue here that legal knowledge is based to a large extend on common-sense knowledge, LRI-Core is particularly inspired by research on abstract common-sense concepts. The main categories of LRI-Core are: physical, mental and abstract concepts. Roles cover in particular social worlds. Another special category are occurrences; terms that denote events and situations. We illustrate the use of LRI-Core with an ontology for Dutch criminal law, developed in the e-Court European project. © Springer 2006.},
   author = {Joost Breuker and André Valente and Radboud Winkels},
   doi = {10.1007/S10506-006-0002-1/METRICS},
   issn = {09248463},
   issue = {4},
   journal = {Artificial Intelligence and Law},
   keywords = {Common-sense ontology,Legal core ontologies,Legal reasoning},
   month = {12},
   pages = {241-277},
   publisher = {Springer},
   title = {Legal ontologies in knowledge engineering and information management},
   volume = {12},
   url = {https://link.springer.com/article/10.1007/s10506-006-0002-1},
   year = {2004},
}
@article{Sarrafzadeh2017,
   abstract = {In information retrieval and information visualization, hierarchies are a common tool to structure information into topics or facets, and network visualizations such as knowledge graphs link related concepts within a domain. In this paper, we explore a multi-layer extension to knowledge graphs, hierarchical knowledge graphs (HKGs), that combines hierarchical and network visualizations into a unified data representation. .rough interaction logs, we show that HKGs preserve the bene.ts of single-layer knowledge graphs at conveying domain knowledge while incorporating the sensemaking advantages of hierarchies for knowledge seeking tasks. Specially, this paper describes our algorithm to construct these visualizations, analyzes interaction logs to quantitatively demonstrate performance parity with networks and performance advantages over hierarchies, and synthesizes data from interaction logs, interviews, and thinkalouds on a testbed data set to demonstrate the utility of the unified hierarchy+network structure in our HKGs.},
   author = {Bahareh Sarrafzadeh and Edward Lank},
   doi = {10.1145/3077136.3080829},
   isbn = {9781450350228},
   journal = {SIGIR 2017 - Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
   keywords = {Exploratory Search,Hierarchies,Information Seeking,Knowledge Graphs,Representations of Search Results},
   month = {8},
   pages = {145-154},
   publisher = {Association for Computing Machinery, Inc},
   title = {Improving exploratory search experience through hierarchical knowledge graphs},
   year = {2017},
}
@article{Greco2023,
   abstract = {Transformer-based language models (TLMs) have widely been recognized to be a cutting-edge technology for the successful development of deep-learning-based solutions to problems and applications that require natural language processing and understanding. Like for other textual domains, TLMs have indeed pushed the state-of-the-art of AI approaches for many tasks of interest in the legal domain. Despite the first Transformer model being proposed about six years ago, there has been a rapid progress of this technology at an unprecedented rate, whereby BERT and related models represent a major reference, also in the legal domain. This article provides the first systematic overview of TLM-based methods for AI-driven problems and tasks in the legal sphere. A major goal is to highlight research advances in this field so as to understand, on the one hand, how the Transformers have contributed to the success of AI in supporting legal processes, and on the other hand, what are the current limitations and opportunities for further research development.},
   author = {Candida M. Greco and Andrea Tagarelli},
   doi = {10.1007/S10506-023-09374-7},
   isbn = {0123456789},
   issn = {1572-8382},
   journal = {Artificial Intelligence and Law 2023},
   keywords = {Artificial Intelligence,IT Law,Information Storage and Retrieval,Intellectual Property,Language models,Legal Aspects of Computing,Legal document review,Legal outcome prediction,Legal search,Media Law,Philosophy of Law,Retrieval,Statutory law data},
   month = {11},
   pages = {1-148},
   publisher = {Springer},
   title = {Bringing order into the realm of Transformer-based language models for artificial intelligence and law},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/article/10.1007/s10506-023-09374-7},
   year = {2023},
}
@article{Glogar2023,
   abstract = {Many legal theorists and linguists have addressed the notion of legal language from different perspectives. Despite that, the definitions of legal language vary. Almost all of the approaches conclude that legal language entails several types of communication. Nevertheless, not all of these categories are sufficiently researched. Some types of legal communication seem to be neglected. This lack of interest might be rooted in the uncertainty of whether these texts or utterances even fall under the scope of the concept of legal language. In order to avoid this superficiality in subsequent research, it is first necessary to come to a clear determination of which communicative acts can be considered a part of legal language and which cannot. Accordingly, in this search for the definition of legal language, we should not neglect the fact that language is executed in concrete communicative acts, and the only means to grasp the language is through communication. The aim of this article is therefore to clearly delineate the boundaries of this concept. Based on analysis of how the given term is currently defined, I draw out the common features and trace the characteristics in which they differ. Taking into account these findings, I propose a novel comprehensive demarcation of legal language. This concept argues that the ‘legal’ nature of language should be determined by the context and function of the particular statement or exchange, in connection with the role of participants in the communication. This means that a particular act may be considered a part of legal language not in accordance with a certain form or lexicon used, but mainly by extralinguistic circumstances in the context of which it is being performed.},
   author = {Ondřej Glogar},
   doi = {10.1007/S11196-023-10010-5/FIGURES/2},
   issn = {15728722},
   issue = {3},
   journal = {International Journal for the Semiotics of Law},
   keywords = {Communication,Conceptualization,Legal communication,Legal language,Pragmatics},
   month = {6},
   pages = {1081-1107},
   publisher = {Springer Science and Business Media B.V.},
   title = {The Concept of Legal Language: What Makes Legal Language ‘Legal‘?},
   volume = {36},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/article/10.1007/s11196-023-10010-5},
   year = {2023},
}
@article{Kriz2014,
   abstract = {We present a system that extracts a knowledge base from raw unstructured texts that is designed as a set of entities and their relations and represented in an ontological framework. The extraction pipeline processes input texts by linguistically-aware tools and extracts entities and relations from their syntactic representation. Consequently, the extracted data is represented according to the Linked Data principles. The system is designed both domain and language independent and provides users with data for more intelligent search than full-text search. We present our first case study on processing Czech legal texts.},
   author = {Vincent Kríž and Barbora Hladká and Martin Nečaský and Tomáš Knap},
   doi = {10.1007/978-3-319-13647-9_13},
   isbn = {9783319136462},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {113-124},
   publisher = {Springer Verlag},
   title = {Data extraction using NLP techniques and its transformation to linked data},
   volume = {8856},
   year = {2014},
}
@article{Monroy2009,
   abstract = {Previous work has shown that modeling relationships between articles of a regulation as vertices of a graph network works twice as better than traditional information retrieval systems for returning articles relevant to the question. In this work we experiment by using natural language techniques such as lemmatizing and using manual and automatic thesauri for improving question based document retrieval. For the construction of the graph, we follow the approach of representing the set of all the articles as a graph; the question is split in two parts, and each of them is added as part of the graph. Then several paths are constructed from part A of the question to part B, so that the shortest path contains the relevant articles to the question. We evaluate our method comparing the answers given by a traditional information retrieval system - vector space model adjusted for article retrieval, instead of document retrieval - and the answers to 21 questions given manually by the general lawyer of the National Polytechnic Institute, based on 25 different regulations (academy regulation, scholarships regulation, postgraduate studies regulation, etc.); with the answer of our system based on the same set of regulations. We found that lemmatizing increases performance in around 10%, while the use of thesaurus has a low impact. © Springer-Verlag Berlin Heidelberg 2009.},
   author = {Alfredo Monroy and Hiramand Calvo and Alexander Gelbukh},
   doi = {10.1007/978-3-642-00382-0_40/COVER},
   isbn = {3642003818},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {498-508},
   publisher = {Springer, Berlin, Heidelberg},
   title = {NLP for shallow question answering of legal documents using graphs},
   volume = {5449 LNCS},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-642-00382-0_40},
   year = {2009},
}
@article{Ganguly2023,
   abstract = {Artificial Intelligence (AI), Machine Learning (ML), Information Retrieval (IR) and Natural Language Processing (NLP) are transforming the way legal professionals and law firms approach their work. The significant potential for the application of AI to Law, for instance, by creating computational solutions for legal tasks, has intrigued researchers for decades. This appeal has only been amplified with the advent of Deep Learning (DL). It is worth noting that working with legal text is far more challenging as compared to the other subdomains of IR/NLP, mainly due to the typical characteristics of legal text, such as considerably longer documents, complex language and lack of large-scale annotated datasets. In this tutorial, we introduce the audience to these characteristics of legal text, and with it, the challenges associated with processing the legal documents. We touch upon the history of AI and Law research, and how it has evolved over the years from relatively simpler approaches to more complex ones, such as those involving DL. We organize the tutorial as follows. First, we provide a brief introduction to state-of-the-art research in the general domain of IR and NLP. We then discuss in more detail IR/NLP tasks specific to the legal domain. We outline the methodologies (both from an academic and industry perspective), and the available tools and datasets to evaluate the methodologies. This is then followed by a hands-on coding/demo session.},
   author = {Debasis Ganguly and Jack G. Conrad and Kripabandhu Ghosh and Saptarshi Ghosh and Pawan Goyal and Paheli Bhattacharya and Shubham Kumar Nigam and Shounak Paul},
   doi = {10.1007/978-3-031-28241-6_34/COVER},
   isbn = {9783031282409},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {AI & Law,Legal data analytics,Legal information retrieval,Natural language processing},
   pages = {331-340},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Legal IR and NLP: The History, Challenges, and State-of-the-Art},
   volume = {13982 LNCS},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-031-28241-6_34},
   year = {2023},
}
@article{Kaltenboeck2022,
   abstract = {This chapter provides insights about the work done and the results achieved by the Horizon 2020-funded Innovation Action “Lynx—Building the Legal Knowledge Graph for Smart Compliance Services in Multilingual Europe.” The main objective of Lynx is to...},
   author = {Martin Kaltenboeck and Pascual Boil and Pieter Verhoeven and Christian Sageder and Elena Montiel-Ponsoda and Pablo Calleja-Ibáñez},
   doi = {10.1007/978-3-030-78307-5_12},
   journal = {Technologies and Applications for Big Data Value},
   pages = {253-271},
   publisher = {Springer International Publishing},
   title = {Using a Legal Knowledge Graph for Multilingual Compliance Services in Labor Law, Contract Management, and Geothermal Energy},
   year = {2022},
}
@article{Constantin2016,
   abstract = {The availability in machine-readable form of descriptions of the structure of documents, as well as of the document discourse (e.g. the scientific discourse within scholarly articles), is crucial for facilitating semantic publishing and the overall comprehension of documents by both users and machines. In this paper we introduce DoCO, the Document Components Ontology, an OWL 2 DL ontology that provides a general-purpose structured vocabulary of document elements to describe both structural and rhetorical document components in RDF. In addition to giving a formal description of the ontology, this paper showcases its utility in practice in a variety of our own applications and other activities of the Semantic Publishing community that rely on DoCO to annotate and retrieve document components of scholarly articles.},
   author = {Alexandru Constantin and Silvio Peroni and Steve Pettifer and David Shotton and Fabio Vitali},
   doi = {10.3233/SW-150177},
   issn = {22104968},
   issue = {2},
   journal = {Semantic Web},
   keywords = {DEO,DoCO,PDFX,SPAR ontologies,Utopia Documents,document components,rhetoric,structural patterns},
   month = {2},
   pages = {167-181},
   publisher = {IOS Press},
   title = {The Document Components Ontology (DoCO)},
   volume = {7},
   year = {2016},
}
@article{Ngo2023,
   abstract = {Today, the legal document system is increasingly strict with different levels of influence and affects activities in many different fields. The increasing number of legal documents interwoven with each other also leads to difficulties in searching and applying in practice. The construction of knowledge maps that involve one or a group of legal documents is an effective approach to represent actual knowledge domains. A legal knowledge graph constructed from laws and legal documents can enable a number of applications, such as question answering, document similarity, and search. In this paper, we describe the process of building a system of knowledge maps for the Vietnamese legal system from the source of about 325,000 legal documents that span all fields of social life. This study also proposes an integrated ontology to represent the legal knowledge from legal documents. This model integrates the ontology of relational knowledge and the graph of key phrases and entities in the form of a concept graph. It can express the semantics of the content of a given legal document. In addition, this study also describes the process of building and exploiting natural language processing tools to build a VLegalKMaps system, which is a repository of Vietnamese legal knowledge maps. We also highlight open challenges in the realization of knowledge graphs in a technical legal system that enables this approach at scale.},
   author = {Hung Q. Ngo and Hien D. Nguyen and Nhien An Le-Khac},
   doi = {10.1007/978-3-031-36886-8_3/COVER},
   isbn = {9783031368851},
   issn = {23673389},
   journal = {Lecture Notes in Networks and Systems},
   keywords = {Knowledge Engineering,Knowledge Map,Legal AI,Legal Documents,Legal Linked Data},
   pages = {25-36},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Building Legal Knowledge Map Repository with NLP Toolkits},
   volume = {734 LNNS},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-031-36886-8_3},
   year = {2023},
}
@article{Lame2004,
   abstract = {A method to identify ontology components is presented in this article. The method relies on Natural Language Processing (NLP) techniques to extract concepts and relations among these concepts. This method is applied in the legal field to build an ontology dedicated to information retrieval. Legal texts on which the method is performed are carefully chosen as describing and conceptualizing the legal domain. We suggest that this method can help legal ontology designers and may be used while building ontologies dedicated to other tasks than information retrieval. © Springer 2006.},
   author = {Guiraude Lame},
   doi = {10.1007/S10506-005-4160-3/METRICS},
   issn = {09248463},
   issue = {4},
   journal = {Artificial Intelligence and Law},
   keywords = {Natural language processing techniques,Ontology},
   month = {12},
   pages = {379-396},
   publisher = {Springer},
   title = {Using NLP techniques to identify legal ontology components: Concepts and relations},
   volume = {12},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/article/10.1007/s10506-005-4160-3},
   year = {2004},
}
@article{Kapitsaki2017,
   abstract = {Open source software is nowadays widely used and any open source software must carry a prominent license. However, the legal, natural language text of open source licenses is not always easy to interpret and an extensive manual analysis of the text may be required, in order to fully understand its content. Existing approaches present license content based on such manual interpretation. In this paper, we propose an automated license term extraction system (FOSS-LTE) for the identification of the license terms from a specific license text and the creation of a representation of these terms divided into rights, obligations and additional conditions. We present the process employed for the creation of the license term extraction system using NLP techniques and we evaluate its accuracy on a set of sentences from available licenses collected for this purpose.},
   author = {Georgia M. Kapitsaki and Demetris Paschalides},
   doi = {10.1109/APSEC.2017.62},
   isbn = {9781538636817},
   issn = {15301362},
   journal = {Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
   keywords = {license terms,licensing,open source software,topic modeling},
   month = {7},
   pages = {540-545},
   publisher = {IEEE Computer Society},
   title = {Identifying Terms in Open Source Software License Texts},
   volume = {2017-December},
   year = {2017},
}
@article{Saha2017,
   abstract = {With increasing regulation of Big Data, it is becoming essential for organizations to ensure compliance with various data protection standards. The Federal Acquisition Regulations System (FARS) within the Code of Federal Regulations (CFR) includes facts and rules for individuals and organizations seeking to do business with the US Federal government. Parsing and gathering knowledge from such lengthy regulation documents is currently done manually and is time and human intensive. Hence, developing a cognitive assistant for automated analysis of such legal documents has become a necessity. We have developed semantically rich approach to automate the analysis of legal documents and have implemented a system to capture various facts and rules contributing towards building an efficient legal knowledge base that contains details of the relationships between various legal elements, semantically similar terminologies, deontic expressions and cross-referenced legal facts and rules. In this paper, we describe our framework along with the results of automating knowledge extraction from the FARS document (Title 48, CFR). Our approach can be used by Big Data Users to automate knowledge extraction from Large Legal documents.},
   author = {Srishty Saha and Karuna P. Joshi and Renee Frank and Michael Aebig and Jiayong Lin},
   doi = {10.1109/BIGDATA.2017.8258353},
   isbn = {9781538627143},
   journal = {Proceedings - 2017 IEEE International Conference on Big Data, Big Data 2017},
   keywords = {Code of Federal Regulations,Deep Learning,Information Retrieval,NLP},
   month = {7},
   pages = {3596-3603},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Automated knowledge extraction from the federal acquisition regulations system (FARS)},
   volume = {2018-January},
   year = {2017},
}
@article{Zhong2020,
   abstract = {Legal Artificial Intelligence (LegalAI) focuses on applying the technology of artificial intelligence, especially natural language processing, to benefit tasks in the legal domain. In recent years, LegalAI has drawn increasing attention rapidly from both AI researchers and legal professionals, as LegalAI is beneficial to the legal system for liberating legal professionals from a maze of paperwork. Legal professionals often think about how to solve tasks from rule-based and symbol-based methods, while NLP researchers concentrate more on data-driven and embedding methods. In this paper, we describe the history, the current state, and the future directions of research in LegalAI. We illustrate the tasks from the perspectives of legal professionals and NLP researchers and show several representative applications in LegalAI. We conduct experiments and provide an in-depth analysis of the advantages and disadvantages of existing works to explore possible future directions. You can find the implementation of our work from https://github.com/thunlp/CLAIM.},
   author = {Haoxi Zhong and Chaojun Xiao and Cunchao Tu and Tianyang Zhang and Zhiyuan Liu and Maosong Sun},
   doi = {10.18653/V1/2020.ACL-MAIN.466},
   isbn = {9781952148255},
   issn = {0736587X},
   journal = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
   pages = {5218-5230},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {How does NLP benefit legal system: A summary of legal artificial intelligence},
   year = {2020},
}
@article{Dan2023,
   abstract = {Legal question answering is a critical task in artificial intelligence. Since most legal data are presented in text, using natural language processing (NLP) to solve legal question answering is a current research direction. Compared with traditional question answering tasks, legal question answering often contains some potential information, such as legal events, crime process, litigants, and victims. This potential information suggests the legal question answering model reasoning's theme and can help the model improve its reasoning ability. In addition, the legal question answering task must answer based on relevant legal clauses, and the number of relevant legal clauses is usually a lot. Hence, the model needs to eliminate the influence of redundant and noisy clauses. Therefore, we propose a double-granularity-based graph neural network that can reason through potential legal events. Based on this research, we design an attention mechanism based on text interaction and calculate the attention by different window sizes score to decrease the influence of noise graph nodes. Finally, we evaluate the proposed model on the JEC-QA benchmark dataset to demonstrate our method's effectiveness. Experimental results show that the model performs well on the Chinese legal examination data and outperforms classical baselines. Out-performs the best baseline model by 7.08 in overall performance and outperforms the best baseline model in single-choice and multiple-choice questions; they are 7.52 and 6.41, respectively.},
   author = {Jingpei Dan and Tianyuan Zhang and Yuming Wang},
   doi = {10.1109/IJCNN54540.2023.10192015},
   isbn = {9781665488679},
   journal = {Proceedings of the International Joint Conference on Neural Networks},
   keywords = {Double-granularity,Graph neural network,Legal Question Answering},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Double Granularity Graph Network for Chinese Legal Question Answering},
   volume = {2023-June},
   year = {2023},
}
@article{Abualhaija2022,
   abstract = {Software systems are increasingly subject to regulatory compliance. Extracting compliance requirements from regulations is challenging. Ideally, locating compliance-related information in a regulation requires a joint effort from requirements engineers and legal experts, whose availability is limited. However, regulations are typically long documents spanning hundreds of pages, containing legal jargon, applying complicated natural language structures, and including cross-references, thus making their analysis effort-intensive. In this paper, we propose an automated question-answering (QA) approach that assists requirements engineers in finding the legal text passages relevant to compliance requirements. Our approach utilizes large-scale language models fine-tuned for QA, including BERT and three variants. We evaluate our approach on 107 question-answer pairs, manually curated by subject-matter experts, for four different European regulatory documents. Among these documents is the general data protection regulation (GDPR) - a major source for privacy-related requirements. Our empirical results show that, in $\approx 94$% of the cases, our approach finds the text passage containing the answer to a given question among the top five passages that our approach marks as most relevant. Further, our approach successfully demarcates, in the selected passage, the right answer with an average accuracy of $\approx$91%.},
   author = {Sallam Abualhaija and Chetan Arora and Amin Sleimi and Lionel C. Briand},
   doi = {10.1109/RE54965.2022.00011},
   isbn = {9781665470001},
   issn = {23326441},
   journal = {Proceedings of the IEEE International Conference on Requirements Engineering},
   keywords = {BERT,Language Models (LMs),Natural Language Processing (NLP),Question Answering,Regulatory Compliance,Requirements Engineering},
   pages = {39-50},
   publisher = {IEEE Computer Society},
   title = {Automated Question Answering for Improved Understanding of Compliance Requirements: A Multi-Document Study},
   volume = {2022-August},
   year = {2022},
}
@article{Tieu2021,
   abstract = {With robust development in NLP (Natural Language Processing) methods and Deep Learning, there are a variety of solutions to the problems in question answering systems that achieve extraordinary results. In this paper, we describe our approach using at the Automated Legal Question Answering Competition (ALQAC) 2021. In this competition, we achieved the first prize of all tasks with the scores of 88.07%, 71.02%, 69.89% in Task 1, Task 2 and Task 3 respectively.},
   author = {Truong Thinh Tieu and Chieu Nguyen Chau and Nguyen Minh Hoang Bui and Truong Son Nguyen and Le Minh Nguyen},
   doi = {10.1109/KSE53942.2021.9648727},
   isbn = {9781665499750},
   issn = {26944804},
   journal = {Proceedings - International Conference on Knowledge and Systems Engineering, KSE},
   keywords = {alqac 2021,answer selection,deep learning,language model,legal domain,natural language processing,question answering},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Apply Bert-based models and Domain knowledge for Automated Legal Question Answering tasks at ALQAC 2021},
   volume = {2021-November},
   year = {2021},
}
@article{Montelongo2020,
   abstract = {Deep Learning (DL) has become the state-of-the-art method for Natural Language Processing (NLP). During the last 5 years DL became the primary Artificial Intelligence (AI) method in the legal domain. In this work we provide a systematic bibliometric review of the publications that have utilized DL as the primary methodology. In particular we analyzed the performed objectives (performed tasks), the corpus utilized to train the models and promising areas of research. The sample includes a total of 137 works published between 1987 and 2020. This analysis starts with the first DL models (formerly Neural Networks) in the legal domain until the latest articles in the ongoing year. Our results show an increment of 300% on the total number of publications during the last 5 years, mainly on information extraction and classification tasks. Moreover, classification is the category with most publications with 39% of the total sample. Finally, we have identified that summarization and text generation as promising areas of research. These findings show that DL in the legal domain is currently in a growing stage, and hence it will be a promising topic of research in the coming years.},
   author = {Alfredo Montelongo and Joao Luiz Becker},
   doi = {10.1109/ICDMW51313.2020.00113},
   isbn = {9781728190129},
   issn = {23759259},
   journal = {IEEE International Conference on Data Mining Workshops, ICDMW},
   keywords = {Deep Learning,Legal Corpus,Neural Networks},
   month = {11},
   pages = {775-781},
   publisher = {IEEE Computer Society},
   title = {Tasks performed in the legal domain through Deep Learning: A bibliometric review (1987-2020)},
   volume = {2020-November},
   year = {2020},
}
@article{Vogel2018,
   abstract = {Law exists solely in and through language. Nonetheless, systematical empirical analysis of legal language has been rare. Yet, the tides are turning: After judges at various courts (including the US Supreme Court) have championed a method of analysis called corpus linguistics, the Michigan Supreme Court held in June 2016 that this method “is consistent with how courts have understood statutory interpretation.” The court illustrated how corpus analysis can benefit legal casework, thus sanctifying twenty years of previous research into the matter. The present article synthesizes this research and introduces computer-assisted legal linguistics (CAL2) as a novel approach to legal studies. Computer-supported analysis of carefully preprocessed collections of legal texts lets lawyers analyze legal semantics, language, and sociosemiotics in different working contexts (judiciary, legislature, legal academia). The article introduces the interdisciplinary CAL2 research group (www.cal2.eu), its Corpus of German Law, and other related projects that make law more transparent.},
   author = {Friedemann Vogel and Hanjo Hamann and Isabelle Gauer},
   doi = {10.1111/LSI.12305},
   issn = {17474469},
   issue = {4},
   journal = {Law and Social Inquiry},
   month = {9},
   pages = {1340-1363},
   publisher = {Blackwell Publishing Inc.},
   title = {Computer-Assisted Legal Linguistics: Corpus Analysis as a New Tool for Legal Studies},
   volume = {43},
   year = {2018},
}
@article{Veena2019,
   abstract = {This paper presents an ontology-driven question- answering system for Motor Vehicle Department(MVD). The MVD uses the motor vehicle act that regulates almost all the aspects of road transport vehicles. It defines a set of offenses and related penalties. At present all this information are unstructured, so in this work, we propose a Natural Language Processing(NLP) based structured representation for these offenses and its related penalties. We have used Semantic Role Labeling(SRL) to identify the roles of each word in the offenses. We use the result of SRL to create the ontology.},
   author = {G. Veena and Deepa Gupta and Akshay Anil and S. Akhil},
   doi = {10.1109/ICICICT46008.2019.8993168},
   isbn = {9781728102832},
   journal = {2019 2nd International Conference on Intelligent Computing, Instrumentation and Control Technologies, ICICICT 2019},
   keywords = {MVD,OWL,Offenses,Ontology,Penalties,SRL},
   month = {7},
   pages = {947-951},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {An Ontology Driven Question Answering System for Legal Documents},
   year = {2019},
}
@article{STAMPER1991,
   author = {RONALD K. STAMPER},
   doi = {10.1111/J.1467-9337.1991.TB00094.X},
   issn = {14679337},
   issue = {2},
   journal = {Ratio Juris},
   pages = {219-244},
   title = {The Role of Semantics in Legal Expert Systems and Legal Reasoning},
   volume = {4},
   year = {1991},
}
@article{McCarty1989,
   author = {L. Thorne McCarty},
   doi = {10.1145/74014.74037},
   isbn = {0897913221},
   journal = {Proceedings of the International Conference on Artificial Intelligence and Law},
   month = {5},
   pages = {180-189},
   publisher = {Association for Computing Machinery},
   title = {A language for legal discourse I. Basic features},
   volume = {Part F130177},
   year = {1989},
}
@article{Amato2008,
   abstract = {In this paper we describe the use of NLP techniques and ontologies as the core for building novel e-gov based information systems, and in particular we define the main characteristics of a document management system in the legal domain, that manages a variety of paper documents, automatically transforming them into RDF statements, for suitable indexing, retrieval and long term preservation. Although we describe a general architecture that can be used for several application domains, our system is particularly suitable for the Italian notary realm. © 2008 IEEE.},
   author = {Flora Amato and Antonino Mazzeo and Antonio Penta and Antonio Picariello},
   doi = {10.1109/DEXA.2008.86},
   isbn = {9780769532998},
   issn = {15294188},
   journal = {Proceedings - International Workshop on Database and Expert Systems Applications, DEXA},
   pages = {67-71},
   title = {Using NLP and ontologies for notary document management systems},
   year = {2008},
}
@article{Quevedo2023,
   author = {Ernesto Quevedo and Tomas Cerny and Alejandro Rodriguez and Pablo Rivas and Jorge Yero and Korn Sooksatra and Alibek Zhakubayev and Davide Taibi},
   doi = {10.1109/ACCESS.2023.3333946},
   issn = {21693536},
   journal = {IEEE Access},
   month = {11},
   pages = {1-1},
   publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
   title = {Legal Natural Language Processing from 2015-2022: A Comprehensive Systematic Mapping Study of Advances and Applications},
   year = {2023},
}
@article{Cejas2023,
   abstract = {When the entity processing personal data (the processor) differs from the one collecting personal data (the controller), processing personal data is regulated in Europe by the General Data Protection Regulation (GDPR) through data processing agreements (DPAs). Checking the compliance of DPAs contributes to the compliance verification of software systems as DPAs are an important source of requirements for software development involving the processing of personal data. However, manually checking whether a given DPA complies with GDPR is challenging as it requires significant time and effort for understanding and identifying DPA-relevant compliance requirements in GDPR and then verifying these requirements in the DPA. Legal texts introduce additional complexity due to convoluted language and inherent ambiguity leading to potential misunderstandings. In this paper, we propose an automated solution to check the compliance of a given DPA against GDPR. In close interaction with legal experts, we first built two artifacts: (i) the 'shall' requirements extracted from the GDPR provisions relevant to DPA compliance and (ii) a glossary table defining the legal concepts in the requirements. Then, we developed an automated solution that leverages natural language processing (NLP) technologies to check the compliance of a given DPA against these 'shall' requirements. Specifically, our approach automatically generates phrasal-level representations for the textual content of the DPA and compares them against predefined representations of the 'shall' requirements. By comparing these two representations, the approach not only assesses whether the DPA is GDPR compliant but it further provides recommendations about missing information in the DPA. Over a dataset of 30 actual DPAs, the approach correctly finds 618 out of 750 genuine violations while raising 76 false violations, and further correctly identifies 524 satisfied requirements. The approach has thus an average precision of 89.1%, a recall of 82.4%, and an accuracy of 84.6%. Compared to a baseline that relies on off-the-shelf NLP tools, our approach provides an average accuracy gain of ≈20 percentage points. The accuracy of our approach can be improved to ≈94% with limited manual verification effort.},
   author = {Orlando Amaral Cejas and Muhammad Ilyas Azeem and Sallam Abualhaija and Lionel C. Briand},
   doi = {10.1109/TSE.2023.3288901},
   issn = {19393520},
   issue = {9},
   journal = {IEEE Transactions on Software Engineering},
   keywords = {Requirements engineering (RE),data processing agreement (DPA),natural language processing (NLP),privacy,regulatory compliance,the general data protection regulation (GDPR)},
   month = {9},
   pages = {4282-4303},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {NLP-Based Automated Compliance Checking of Data Processing Agreements Against GDPR},
   volume = {49},
   year = {2023},
}
@article{Kanhaiya2023,
   abstract = {The relationships between the law and fact, and the use of the facts to support a legal conclusion requires practice. This study introduces a technique called the "legal analysis of data extraction"which helps students/law practitioners to quickly assess a law article on the basis of the extracted attributes, and then apply their understanding in order to support a legal conclusion. To ease the processing of the law text data and retrieval of important information (entities) like names of Judges, appellants, respondents, judgment dates, etc. by utilizing NLP processes and ML techniques. This study demonstrates a novel approach of advanced information retrieval in the law articles leveraging natural language processing and machine learning techniques which are productized and used on live data as well. In this custom NER, aggregate accuracy of 95% is achieved using text classification followed by a regex fallback and flagging mechanism. This study consists of the following sections- a) Introduction b) Related Work c) About Dataset d) Methodology and approach e) Fallback mechanisms f) Results and Evaluations g) Conclusions & Future Scope with h) References.},
   author = {Kishan Kanhaiya and Naveen and Arpit Kumar Sharma and Kamlesh Gautam and Pramod Singh Rathore},
   doi = {10.1109/ICAISS58487.2023.10250733},
   isbn = {9798350325799},
   journal = {Proceedings of the 2023 2nd International Conference on Augmented Intelligence and Sustainable Systems, ICAISS 2023},
   keywords = {AI-enabled- Information Retrival Engine (AI-IRE),Name Entity Recognition (NER),Natural Language Processing (NLP),Word Embeddings},
   pages = {206-210},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {AI Enabled- Information Retrival Engine (AI-IRE) in Legal Services: An Expert-Annotated NLP for Legal Judgements},
   year = {2023},
}
@article{Nayak2019,
   abstract = {Due to introduction of General Data Protection Regulation (GDPR) in EU; all the cloud hosted applications (span across multi geo location) wherein captures the personal data need to first identify data privacy protection (DPP) entities and handle it as per EU norms and regulations. The company's legal contracts or transactions in tie up with other parties (customers, partners, suppliers, etc.) are usually stored in to repository; on termination of the contracts either by agreement or mutual consent then the other parties' information are usually archived for historical reasons. The other parties are usually interested in knowing what all documents or transactions they were participated earlier and expects the data to be pruned on need basis. As these documents are unstructured in nature, this paper proposes a solution in identifying all DPP entities with in legal contract documents, index the corpus level accumulated knowledgebase, apply customized ranking algorithm for the retrieved legal contract documents based on DPP search query, derive DPP entities specific legal contract document dependency relation graph for which the parties are participating by using techniques from Information Retrieval, Information Extraction, Natural Language Processing (NLP) and Ontology.},
   author = {Shiva Prasad Nayak and Suresh Pasumarthi},
   doi = {10.1109/DDP.2019.00023},
   isbn = {9781728153636},
   journal = {Proceedings - 2019 1st International Conference on Digital Data Processing, DDP 2019},
   keywords = {DPP,GDPR,Information Extraction,Information Retrieval,Legal Contract Documents,Natural Language Processing,Ontology},
   month = {11},
   pages = {70-75},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Automatic Detection and Analysis of DPP Entities in Legal Contract Documents},
   year = {2019},
}
@article{Eidelman2019,
   abstract = {Automatic summarization methods have been studied on a variety of domains, including news and scientific articles. Yet, legislation has not previously been considered for this task, despite US Congress and state governments releasing tens of thousands of bills every year. In this paper, we introduce BillSum, the first dataset for summarization of US Congressional and California state bills (https://github.com/FiscalNote/BillSum). We explain the properties of the dataset that make it more challenging to process than other domains. Then, we benchmark extractive methods that consider neural sentence representations and traditional contextual features. Finally, we demonstrate that models built on Congressional bills can be used to summarize California bills, thus, showing that methods developed on this dataset can transfer to states without human-written summaries.},
   author = {Vladimir Eidelman},
   doi = {10.18653/v1/d19-5406},
   month = {11},
   pages = {48-56},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {BillSum: A Corpus for Automatic Summarization of US Legislation},
   year = {2019},
}
@article{Song2022,
   abstract = {We present the first comprehensive empirical evaluation of pre-trained language models (PLMs) for legal natural language processing (NLP) in order to examine their effectiveness in this domain. Our study covers eight representative and challenging legal datasets, ranging from 900 to 57K samples, across five NLP tasks: binary classification, multi-label classification, multiple choice question answering, summarization and information retrieval. We first run unsupervised, classical machine learning and/or non-PLM based deep learning methods on these datasets, and show that baseline systems' performance can be 4%35% lower than that of PLM-based methods. Next, we compare general-domain PLMs and those specifically pre-trained for the legal domain, and find that domain-specific PLMs demonstrate 1%5% higher performance than general-domain models, but only when the datasets are extremely close to the pre-training corpora. Finally, we evaluate six general-domain state-of-the-art systems, and show that they have limited generalizability to legal data, with performance gains from 0.1% to 1.2% over other PLM-based methods. Our experiments suggest that both general-domain and domain-specific PLM-based methods generally achieve better results than simpler methods on most tasks, with the exception of the retrieval task, where the best-performing baseline outperformed all PLM-based methods by at least 5%. Our findings can help legal NLP practitioners choose the appropriate methods for different tasks, and also shed light on potential future directions for legal NLP research.},
   author = {Dezhao Song and Sally Gao and Baosheng He and Frank Schilder},
   doi = {10.1109/ACCESS.2022.3190408},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Legal natural language processing,deep learning,machine learning,pre-trained language model},
   pages = {75835-75858},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {On the Effectiveness of Pre-Trained Language Models for Legal Natural Language Processing: An Empirical Study},
   volume = {10},
   year = {2022},
}
@article{Raj2022,
   abstract = {Open Information Extraction system (OIE) extracts textual tuples consisting of arguments and its relation within the input sentence. OIE can also be considered as an unrestricted variant of information extraction. Throughout the years these types of systems have become more and more popular. The main factor for considering such a system is that they are made for speed, then any other factor. Also, one of the major advantages of using such a system is that the relations that are extracted are human-readable. Relation extraction in the legal domain has always remained a challenge because there is no structured data available. hence making this process more challenging. Since we are working in the legal domain, extracting the relations that are present in the document is very important. This leads us to a system that is capable of assisting people in the domain. The introduction of such a system would be really important for professionals like judges, advocates and also for the common people if they want to have a quick glance at certain documents. The outcome of these systems can be really helpful for future downstream applications like question answering, sentence similarity etc.},
   author = {Nikhil Raj and Shiyon Thomas and G. Veena},
   doi = {10.1109/GCAT55367.2022.9971995},
   isbn = {9781665468534},
   journal = {2022 IEEE 3rd Global Conference for Advancement in Technology, GCAT 2022},
   keywords = {MinIE,NLP,Named Entity Recognition,Open information extraction system},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Open Information Extraction System For Extracting Relations in Legal Documents},
   year = {2022},
}
@article{Siena2009,
   abstract = {New laws, such as HIPAA and SOX, are increasingly impacting the design of software systems, as business organisations strive to comply. This paper studies the problem of generating a set of requirements for a new system which comply with a given law. Specifically, the paper proposes a systematic process for generating law-compliant requirements by using a taxonomy of legal concepts and a set of primitives to describe stakeholders and their strategic goals. Given a model of law and a model of stakeholders goals, legal alternatives are identified and explored. Strategic goals that can realise legal prescriptions are systematically analysed, and alternative ways of fulfilling a law are evaluated. The approach is demonstrated by means of a case study. This work is part of the Nomos framework, intended to support the design of law-compliant requirements models. © Springer-Verlag 2009.},
   author = {Alberto Siena and John Mylopoulos and Anna Perini and Angelo Susi},
   doi = {10.1007/978-3-642-04840-1_35},
   isbn = {3642048390},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {472-486},
   title = {Designing law-compliant software requirements},
   volume = {5829 LNCS},
   year = {2009},
}
@article{Sannier2017,
   abstract = {When identifying and elaborating compliance requirements, analysts need to follow the cross references in legal texts and consider the additional information in the cited provisions. Enabling easier navigation and handling of cross references requires automated support for the detection of the natural language expressions used in cross references, the interpretation of cross references in their context, and the linkage of cross references to the targeted provisions. In this article, we propose an approach and tool support for automated detection and resolution of cross references. The approach leverages the structure of legal texts, formalized into a schema, and a set of natural language patterns for legal cross reference expressions. These patterns were developed based on an investigation of Luxembourg’s legislation, written in French. To build confidence about their applicability beyond the context where they were observed, these patterns were validated against the Personal Health Information Protection Act (PHIPA) by the Government of Ontario, Canada, written in both French and English. We report on an empirical evaluation where we assess the accuracy and scalability of our framework over several Luxembourgish legislative texts as well as PHIPA.},
   author = {Nicolas Sannier and Morayo Adedjouma and Mehrdad Sabetzadeh and Lionel Briand},
   doi = {10.1007/S00766-015-0241-3},
   issn = {1432010X},
   issue = {2},
   journal = {Requirements Engineering},
   keywords = {Conceptual modeling,Cross references,Legal compliance,Natural language processing (NLP)},
   month = {6},
   pages = {215-237},
   publisher = {Springer London},
   title = {An automated framework for detection and resolution of cross references in legal texts},
   volume = {22},
   year = {2017},
}
@article{Zeni2015,
   abstract = {Ensuring compliance of software systems with government regulations, policies, and laws is a complex problem. Generally speaking, solutions to the problem first identify rights and obligations defined in the law and then treat these as requirements for the system under design. This work examines the challenge of developing tool support for extracting such requirements from legal documents. To address this challenge, we have developed a tool called GaiusT. The tool is founded on a framework for textual semantic annotation. It semiautomatically generates elements of requirements models, including actors, rights, and obligations. We present the complexities of annotating prescriptive text, the architecture of GaiusT, and the process by which annotation is accomplished. We also present experimental results from two case studies to illustrate the application of the tool and its effectiveness relative to manual efforts. The first case study is based on the US Health Insurance Portability and Accountability Act, while the second analyzes the Italian accessibility law for information technology instruments.},
   author = {Nicola Zeni and Nadzeya Kiyavitskaya and Luisa Mich and James R. Cordy and John Mylopoulos},
   doi = {10.1007/S00766-013-0181-8},
   issn = {1432010X},
   issue = {1},
   journal = {Requirements Engineering},
   keywords = {Legal documents,Legal requirements,Multilingual annotation,Regulation compliance problem,Requirements engineering,Semantic annotation},
   month = {3},
   pages = {1-22},
   publisher = {Springer London},
   title = {GaiusT: supporting the extraction of rights and obligations for regulatory compliance},
   volume = {20},
   year = {2015},
}
@article{Sleimi2018,
   abstract = {[Context] Semantic legal metadata provides information that helps with understanding and interpreting the meaning of legal provisions. Such metadata is important for the systematic analysis of legal requirements. [Objectives] Our work is motivated by two observations: (1) The existing requirements engineering (RE) literature does not provide a harmonized view on the semantic metadata types that are useful for legal requirements analysis. (2) Automated support for the extraction of semantic legal metadata is scarce, and further does not exploit the full potential of natural language processing (NLP). Our objective is to take steps toward addressing these limitations. [Methods] We review and reconcile the semantic legal metadata types proposed in RE. Subsequently, we conduct a qualitative study aimed at investigating how the identified metadata types can be extracted automatically. [Results and Conclusions] We propose (1) a harmonized conceptual model for the semantic metadata types pertinent to legal requirements analysis, and (2) automated extraction rules for these metadata types based on NLP. We evaluate the extraction rules through a case study. Our results indicate that the rules generate metadata annotations with high accuracy.},
   author = {Amin Sleimi and Nicolas Sannier and Mehrdad Sabetzadeh and Lionel Briand and John Dann},
   doi = {10.1109/RE.2018.00022},
   isbn = {9781538674185},
   journal = {Proceedings - 2018 IEEE 26th International Requirements Engineering Conference, RE 2018},
   keywords = {Legal requirements,Natural language processing,Semantic legal metadata},
   month = {10},
   pages = {124-135},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Automated extraction of semantic legal metadata using natural language processing},
   year = {2018},
}
@article{Mahabal2019,
   abstract = {Training data for text classification is often limited in practice, especially for applications with many output classes or involving many related classification problems. This means classifiers must generalize from limited evidence, but the manner and extent of generalization is task dependent. Current practice primarily relies on pre-trained word embeddings to map words unseen in training to similar seen ones. Unfortunately, this squishes many components of meaning into highly restricted capacity. Our alternative begins with sparse pre-trained representations derived from unlabeled parsed corpora; based on the available training data, we select features that offers the relevant generalizations. This produces task-specific semantic vectors; here, we show that a feed-forward network over these vectors is especially effective in low-data scenarios, compared to existing state-of-the-art methods. By further pairing this network with a convolutional neural network, we keep this edge in low data scenarios and remain competitive when using full training sets.},
   author = {Abhijit Mahabal and Jason Baldridge and Burcu Karagol Ayan and Vincent Perot and Dan Roth},
   doi = {10.18653/V1/N19-1319},
   isbn = {9781950737130},
   journal = {NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference},
   pages = {3158-3167},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {Text classification with few examples using controlled generalization},
   volume = {1},
   year = {2019},
}
@article{Bambroo2021,
   abstract = {Transformers have caused a paradigm shift in tasks related to natural language. From text summarization to classification, these models have established new state-of-the-art results on various general and closed domain tasks. Having said that, most of the popular transformer based models (BERT - Bidirectional Encoder Representations from Transformers, DistilBERT, and RoBERTa) face a limitation in terms of the content length they can accept. This is because the self-attention used by these models scales quadratically with the context sequence length. While this works well for short passages and questions, for longer documents this method fails to effectively capture both the local and global contexts. This shortcoming is underscored further for closed domain tasks like Legal Document classification, where the length of the documents can extend up to a couple of pages and which differ in their vocabulary from general English substantially. In this paper, a new architecture is proposed, where the concept of “long” attention is applied to a distilled BERT and then the model is pre-trained on legal-domain specific corpora. This helps combine a local windowed attention with task-motivated global attention, making the model contextually-aware for longer sequences. The proposed model is also able to outperform fine-tuned BERT and other transformer-based models at the task of legal document classification while also being faster.},
   author = {Purbid Bambroo and Aditi Awasthi},
   doi = {10.1109/ICAECT49130.2021.9392558},
   isbn = {9781728157900},
   journal = {Proceedings of the 2021 1st International Conference on Advances in Electrical, Computing, Communications and Sustainable Technologies, ICAECT 2021},
   keywords = {Contextual awareness,DistilBERT,Legal document classification,Longformer,Self attention},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {LegalDB: Long distilbert for legal document classification},
   year = {2021},
}
@article{Fujita2023,
   abstract = {We report our system architecture of COLIEE 2022 Task 4, which challenges to solve the textual entailment part of the Japanese legal bar examination problems. We successfully improved the correct answer ratio by an ensemble of a rule-based method and BERT-based method. Our proposed methods mainly consist of two parts: data augmentations of training dataset and an ensemble of the methods. Regarding training data augmentation, the civil law articles are segmented once and reconstructed again with all the combinations. Data expansion is then performed by replacing the data with negative forms and alphabetical symbols. Focusing on the characteristics that the rule-based method is high in its precision but low in its coverage, we employed a modular way in our ensemble. We integrated other proposed methods such as Sentence-BERT to select necessary data, person name inference to replace alphabetical anonymized symbols with the actual role name of the person. We confirmed that our suggested methods are effective by comparing with our baseline models, achieved 0.6789 correct answer ratio in accuracy on the formal run test dataset, which was the best score among the COLIEE 2022 Task 4 submissions.},
   author = {Masaki Fujita and Takaaki Onaga and Ayaka Ueyama and Yoshinobu Kano},
   doi = {10.1007/978-3-031-29168-5_10},
   isbn = {9783031291678},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {BERT,COLIEE,Legal Bar Exam,Legal information Extraction,Predicate Argument Structure Analysis,Question Answering},
   pages = {138-153},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Legal Textual Entailment Using Ensemble of Rule-Based and BERT-Based Method with Data Augmentation by Related Article Generation},
   volume = {13859 LNAI},
   year = {2023},
}
@article{Katz2023,
   abstract = {In this paper, we summarize the current state of the field of NLP & Law with a specific focus on recent technical and substantive developments. To support our analysis, we construct and analyze a nearly complete corpus of more than six hundred NLP & Law related papers published over the past decade. Our analysis highlights several major trends. Namely, we document an increasing number of papers written, tasks undertaken, and languages covered over the course of the past decade. We observe an increase in the sophistication of the methods which researchers deployed in this applied context. Slowly but surely, Legal NLP is beginning to match not only the methodological sophistication of general NLP but also the professional standards of data availability and code reproducibility observed within the broader scientific community. We believe all of these trends bode well for the future of the field, but many questions in both the academic and commercial sphere still remain open.},
   author = {Daniel Martin Katz and Dirk Hartung and Lauritz Gerlach and Abhik Jana and Michael James Bommarito},
   doi = {10.2139/SSRN.4336224},
   journal = {SSRN Electronic Journal},
   publisher = {Elsevier BV},
   title = {Natural Language Processing in the Legal Domain},
   year = {2023},
}
@article{Chalkidis2019,
   abstract = {Deep Learning has been widely used for tackling challenging natural language processing tasks over the recent years. Similarly, the application of Deep Neural Networks in legal analytics has increased significantly. In this survey, we study the early adaptation of Deep Learning in legal analytics focusing on three main fields; text classification, information extraction, and information retrieval. We focus on the semantic feature representations, a key instrument for the successful application of deep learning in natural language processing. Additionally, we share pre-trained legal word embeddings using the word2vec model over large corpora, comprised legislations from UK, EU, Canada, Australia, USA, and Japan among others.},
   author = {Ilias Chalkidis and Dimitrios Kampas},
   doi = {10.1007/S10506-018-9238-9},
   issn = {15728382},
   issue = {2},
   journal = {Artificial Intelligence and Law},
   keywords = {Deep learning,Legal word vectors,Natural language processing},
   month = {6},
   pages = {171-198},
   publisher = {Springer Netherlands},
   title = {Deep learning in law: early adaptation and legal word embeddings trained on large corpora},
   volume = {27},
   year = {2019},
}
@article{Anh2023,
   author = {Dang Hoang Anh and Dinh-Truong Do and Vu Tran and Nguyen Le Minh},
   doi = {10.1109/KSE59128.2023.10299488},
   isbn = {9798350329742},
   issn = {26944804},
   month = {11},
   pages = {1-7},
   publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
   title = {The Impact of Large Language Modeling on Natural Language Processing in Legal Texts: A Comprehensive Survey},
   year = {2023},
}
@article{Nasar2021,
   abstract = {With the advent of Web 2.0, there exist many online platforms that result in massive textual-data production. With ever-increasing textual data at hand, it is of immense importance to extract infor...},
   author = {Zara Nasar and Syed Waqar Jaffry and Muhammad Kamran Malik},
   doi = {10.1145/3445965},
   isbn = {10.1145/3445965},
   issn = {15577341},
   issue = {1},
   journal = {ACM Computing Surveys (CSUR)},
   keywords = {Information extraction,deep learning,joint modeling,named entity recognition,relation extraction},
   month = {2},
   pages = {39},
   publisher = {
		ACM
		PUB27
		New York, NY, USA
	},
   title = {Named Entity Recognition and Relation Extraction},
   volume = {54},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3445965},
   year = {2021},
}
@article{Aires2017,
   author = {João Paulo Aires and Felipe Meneguzzi},
   doi = {10.5555/3091125.3091326},
   keywords = {CCS Concepts •Computing methodologies → Natural language process-ing,Information extraction,Keywords norm conflict, deep learning, contract reasoning},
   title = {A Deep Learning Approach for Norm Conflict Identification},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.5555/3091125.3091326},
   year = {2017},
}
@article{Mumford2023,
   abstract = {Recent years have witnessed significant progress in the deployment of advanced Natural Language Processing (NLP) techniques based on transformer technology, across many domains and applications. However, in legal domains, due to the complexity, length, and sparsity of legal case documents, the use of these advanced NLP techniques has offered comparatively slight returns. Perhaps even more importantly, such methods are critically lacking in explain-ability and justification of outputs, which are essential for many legal applications. We propose that the direction of these NLP techniques should be aimed at ascription to a legal knowledge model, which can then provide the necessary and auditable justifications for the rationale of any case outcome. In this paper we investigate the effectiveness of using Hierarchical Bidirectional Encoder Representations from Transformers (H-BERT) models to ascribe to an Angelic Domain Model (ADM) that is able to represent the legal knowledge of a domain in a structured way, enabling justifications and improving performance. Our study involved an annotation task on a popular domain, cases from the European Court of Human Rights, to gain an understanding of the balance of complaints in the domain. The data set produced from this study enabled training of models for factor ascription using the classification targets derived from the annotations. We present results of experiments conducted to evaluate the performance of the ascription task at three different levels of abstraction within the structured model. CCS CONCEPTS • Applied computing → Law.},
   author = {Jack Mumford and Katie Atkinson and Trevor Bench-Capon},
   doi = {10.1145/3594536.3595158},
   isbn = {9798400701979},
   issue = {2023},
   keywords = {Transformers,case annotation,domain model,factor ascription},
   month = {6},
   pages = {167-176},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Combining a Legal Knowledge Model with Machine Learning for Reasoning with Legal Cases},
   volume = {10},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3594536.3595158},
   year = {2023},
}
@article{Behnke2023,
   abstract = {Legal language is considered to be a key obstacle to the comprehen-sibility of court decisions for laypeople. While differences between written 'standard' and legal language have already been analysed with regard to syntactic peculiarities, there is still a lack of findings on the influence of divergent word meanings on comprehensibility. We present the course and the preliminary results of a study elaborating such ambiguities on the basis of over half a million German court decisions. As these differences are highly language-dependent, our study consequentially relates (only) to German. CCS CONCEPTS • Applied Computing → Law.},
   author = {Gregor Behnke and Niklas Wais},
   doi = {10.1145/3594536.3595123},
   isbn = {9798400701979},
   keywords = {NLP,semantics of legal texts},
   month = {6},
   pages = {382-386},
   publisher = {Association for Computing Machinery (ACM)},
   title = {On the Semantic Difference of Judicial and Standard Language},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3594536.3595123},
   year = {2023},
}
@article{Chalkidis2020,
   abstract = {BERT has achieved impressive performance in several NLP tasks. However, there has been limited investigation on its adaptation guidelines in specialised domains. Here we focus on the legal domain, where we explore several approaches for applying BERT models to downstream legal tasks, evaluating on multiple datasets. Our findings indicate that the previous guidelines for pre-training and fine-tuning, often blindly followed, do not always generalize well in the legal domain. Thus we propose a systematic investigation of the available strategies when applying BERT in specialised domains. These are: (a) use the original BERT out of the box, (b) adapt BERT by additional pre-training on domain-specific corpora, and (c) pre-train BERT from scratch on domain-specific corpora. We also propose a broader hyper-parameter search space when fine-tuning for downstream tasks and we release LEGAL-BERT, a family of BERT models intended to assist legal NLP research, computational law, and legal technology applications.},
   author = {Ilias Chalkidis and Manos Fergadiotis and Prodromos Malakasiotis and Nikolaos Aletras and Ion Androutsopoulos},
   doi = {10.18653/v1/2020.findings-emnlp.261},
   isbn = {9781952148903},
   journal = {Findings of the Association for Computational Linguistics Findings of ACL: EMNLP 2020},
   pages = {2898-2904},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {LEGAL-BERT: The muppets straight out of law school},
   year = {2020},
}
@article{Jiang2023,
   abstract = {Legal syllogism is a form of deductive reasoning commonly used by legal professionals to analyze cases. In this paper, we propose legal syllogism prompting (LoT), a simple prompting method to teach large language models (LLMs) for legal judgment prediction. LoT teaches only that in the legal syllogism the major premise is law, the minor premise is the fact, and the conclusion is judgment. Then the models can produce a syllogism reasoning of the case and give the judgment without any learning, fine-tuning, or examples. On CAIL2018, a Chinese criminal case dataset, we performed zero-shot judgment prediction experiments with GPT-3 models. Our results show that LLMs with LoT achieve better performance than the baseline and chain of thought prompting, the state-of-art prompting method on diverse reasoning tasks. LoT enables the model to concentrate on the key information relevant to the judgment and to correctly understand the legal meaning of acts, as compared to other methods. Our method enables LLMs to predict judgment along with law articles and justification, which significantly enhances the explainability of models.},
   author = {Cong Jiang and Xiaolei Yang},
   doi = {10.1145/3594536.3595170},
   isbn = {9798400701979},
   keywords = {Artificial intelligence KEYWORDS large language models, legal syllogism, legal judgment prediction, chain of thought,CCS CONCEPTS • Applied computing → Law,• Computing methodologies → Natural language generation},
   month = {6},
   pages = {417-421},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Legal Syllogism Prompting},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3594536.3595170},
   year = {2023},
}
@article{Giaoui2023,
   abstract = {Motivated by the subjective decision making and lack of strict protocols in damages as a remedy for contract breach, this project uses natural legal language processing (NLLP) and artificial intelligence (AI) techniques to analyze patterns in contract law cases and reduce uncertainty in their outcome. A 'hybrid' model combining heuristics, NLLP & the results of an LSTM based model into an XGBoost regressor along with contextual information had the best performance for the classification of entity types from unstructured proceedings text. Linear regressors were developed to approximate the Recovery Rate and the Win Rate using a set of 6 engineered features likely to affect the outcome.},
   author = {Frank Giaoui and Luv Aggarwal and Diego Lobo and Joan Gondolo and Philippe Lachkeur and Satvik Jain},
   doi = {10.1145/3594536.3595119},
   isbn = {9798400701979},
   month = {6},
   pages = {468-469},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Applying NLLP and ML to Predict Damages as a Remedy for Contract Breach},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3594536.3595119},
   year = {2023},
}
@article{Goebel2023,
   abstract = {We summarize the 10th Competition on Legal Information Extraction and Entailment. In this edition, the competition included four tasks on case law and statute law. The case law …},
   author = {Randy Goebel and Yoshinobu Kano and Mi-Young Kim and Juliano Rabelo and Ken Satoh and Masaharu Yoshioka},
   doi = {10.1145/3594536.3595176},
   isbn = {9798400701979},
   keywords = {CCS CONCEPTS • Information systems → Content analysis and feature se-lection,Clustering and classification,Document topic models,Information extraction,Similarity measures,Specialized informa-tion retrieval KEYWORDS legal textual entailment, legal information retrieval, text classifica-tion, imbalanced datasets},
   month = {6},
   pages = {472-480},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Summary of the Competition on Legal Information, Extraction/Entailment (COLIEE) 2023},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3594536.3595176},
   year = {2023},
}
@article{Savelka2023,
   abstract = {We evaluated the capability of a state-of-the-art generative pre-trained transformer (GPT) model to perform semantic annotation of short text snippets (one to few sentences) coming from legal documents of various types. Discussions of potential uses (e.g., document drafting, summarization) of this emerging technology in legal domain have intensified, but to date there has not been a rigorous analysis of these large language models' (LLM) capacity in sentence-level semantic annotation of legal texts in zero-shot learning settings. Yet, this particular type of use could unlock many practical applications (e.g., in contract review) and research opportunities (e.g., in empirical legal studies). We fill the gap with this study. We examined if and how successfully the model can semantically annotate small batches of short text snippets (10-50) based exclusively on concise definitions of the semantic types. We found that the GPT model performs surprisingly well in zero-shot settings on diverse types of documents (F 1 = .73 on a task involving court opinions, .86 for contracts, and .54 for statutes and regulations). These findings can be leveraged by legal scholars and practicing lawyers alike to guide their decisions in integrating LLMs in wide range of workflows involving semantic annotation of legal texts.},
   author = {Jaromir Savelka},
   doi = {10.1145/3594536.3595161},
   isbn = {9798400701979},
   month = {6},
   pages = {447-451},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Unlocking Practical Applications in Legal Domain},
   year = {2023},
}
@article{Servantez2023,
   abstract = {The emergence of contract specific programming languages has struggled to translate into widespread adoption of computable contracts due largely to high conversion costs. In this work, we present the first system for converting natural language contracts into code through the extraction of key entities, relationships, and formulas into a graph representation called the Obligation Logic Graph (OLG). This approach allows the semantic meaning of contract obligations, including dependencies between obligations, to be captured through the OLG and mapped to code downstream. We also introduce OLG extraction as a new joint entity and relation prediction task for legal contracts, and present the Contract-OLG dataset, consisting of 1,876 contract provisions, 18,597 entities and 18,170 relationships. We perform detailed experiments to understand the capabilities of state-of-the-art Transformer and graph-based models at completing these tasks, and identify where there is currently a significant gap between human expert and machine performance, particularly for relation extraction.},
   author = {Sergio Servantez and Nedim Lipka and Alexa Siu and Milan Aggarwal and Balaji Krishnamurthy and Aparna Garimella and Kristian Hammond KristianHammond and Rajiv Jain and Balaji Krish-namurthy and Kristian Hammond},
   doi = {10.1145/3594536.3595152},
   isbn = {9798400701979},
   keywords = {CCS CONCEPTS • Computing methodologies → Natural language processing,Information extraction,Machine learning,• Applied com-puting → Law KEYWORDS natural language processing, information extraction, computable contracts, obligation logic graph},
   month = {6},
   pages = {371-380},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Argument Mining with Graph Representation Learning},
   volume = {10},
   url = {https://doi.org/10.1145/3594536.3595162},
   year = {2023},
}
@article{Xu2023,
   abstract = {Duties and powers are two fundamental notions in private law. In this work, we provide our conception of duties and powers and present a logic for reasoning about them. We treat duties as agents' obligations towards others to perform actions. We think that powers are agents' legal abilities, conferred by law, to change legal positions between agents. How to exercise powers is also specified by law. Many factors, including the exercise of powers, fulfillment of duties, violation of duties, and factual changes in the world, can change duties. The ontic level of the logic is a multi-agent dynamic logic, where agents have abilities to change atomic facts. At its normative level, agents have duties towards others to change atomic facts, and have powers to change duties by changing atomic facts. When agents behave, the ontic and normative aspects of the world change accordingly. The implications of the formalization are studied extensively. CCS CONCEPTS • Theory of computation → Modal and temporal logics.},
   author = {Tianwen Xu and Fengkui Ju},
   doi = {10.1145/3594536.3595133},
   isbn = {9798400701979},
   keywords = {abilities,actions,duties,exercise of powers,powers},
   month = {6},
   pages = {361-370},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Multi-agent logic for reasoning about duties and powers in private law},
   url = {https://doi.org/10.1145/3594536.3595133},
   year = {2023},
}
@article{Woerkom2023,
   abstract = {Duties and powers are two fundamental notions in private law. In this work, we provide our conception of duties and powers and present a logic for reasoning about them. We treat duties as agents' obligations towards others to perform actions. We think that powers are agents' legal abilities, conferred by law, to change legal positions between agents. How to exercise powers is also specified by law. Many factors, including the exercise of powers, fulfillment of duties, violation of duties, and factual changes in the world, can change duties. The ontic level of the logic is a multi-agent dynamic logic, where agents have abilities to change atomic facts. At its normative level, agents have duties towards others to change atomic facts, and have powers to change duties by changing atomic facts. When agents behave, the ontic and normative aspects of the world change accordingly. The implications of the formalization are studied extensively. CCS CONCEPTS • Theory of computation → Modal and temporal logics.},
   author = {Wijnand van Woerkom and Davide Grossi and Henry Prakken and Bart Verheij},
   doi = {10.1145/3594536.3595154},
   isbn = {9798400701979},
   keywords = {abilities,actions,duties,exercise of powers,powers},
   month = {6},
   pages = {333-342},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Hierarchical Precedential Constraint},
   url = {https://doi.org/10.1145/3594536.3595133},
   year = {2023},
}
@article{Servantez2023,
   abstract = {The emergence of contract specific programming languages has struggled to translate into widespread adoption of computable contracts due largely to high conversion costs. In this work, we present the first system for converting natural language contracts into code through the extraction of key entities, relationships, and formulas into a graph representation called the Obligation Logic Graph (OLG). This approach allows the semantic meaning of contract obligations, including dependencies between obligations, to be captured through the OLG and mapped to code downstream. We also introduce OLG extraction as a new joint entity and relation prediction task for legal contracts, and present the Contract-OLG dataset, consisting of 1,876 contract provisions, 18,597 entities and 18,170 relationships. We perform detailed experiments to understand the capabilities of state-of-the-art Transformer and graph-based models at completing these tasks, and identify where there is currently a significant gap between human expert and machine performance, particularly for relation extraction.},
   author = {Sergio Servantez and Nedim Lipka and Alexa Siu and Milan Aggarwal and Balaji Krishnamurthy and Aparna Garimella and Kristian Hammond KristianHammond and Rajiv Jain and Balaji Krish-namurthy and Kristian Hammond},
   doi = {10.1145/3594536.3595162},
   isbn = {9798400701979},
   keywords = {CCS CONCEPTS • Computing methodologies → Natural language processing,Information extraction,Machine learning,• Applied com-puting → Law KEYWORDS natural language processing, information extraction, computable contracts, obligation logic graph},
   month = {6},
   pages = {267-276},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Computable Contracts by Extracting Obligation Logic Graphs},
   volume = {10},
   url = {https://doi.org/10.1145/3594536.3595162},
   year = {2023},
}
@article{Lewis2023,
   abstract = {What happens if the way in which we handle a genuine deontic conflict -i.e., a deontic ambiguity-matters regarding the application of other norms that are not directly affected by that conflict? We argue that the law requires sometimes propagating the ambiguity to other norms and sometimes confining it to some norms only. We explore this issue and model different reasoning patterns. The problem is addressed in a new variant of Defeasible Deontic Logic. The contribution of this paper is threefold: (a) we extend the treatment of ambiguity blocking and propagation to Defeasible Deontic Logic; (b) we discuss reasoning patterns in the law, especially in criminal law, where we need to deal with both ambiguity blocking and ambiguity propagation in the same legal system and logic; (c) we devise an annotated variant of Defeasible Deontic Logic where we distinguish literals that must be obtained through an ambiguity-blocking mechanism from those that are derived using an ambiguity-propagating mechanism.},
   author = {David D Lewis and Lenora Gray and Mark Noel},
   doi = {10.1145/3594536.3595175},
   isbn = {9798400701979},
   keywords = {SAVI,conformal prediction,e-disclosure,e-discovery,eDisclosure,effectiveness metrics,martingales,safe anytime-valid inference},
   month = {6},
   pages = {91-100},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Deontic Ambiguities in Legal Reasoning},
   url = {https://doi.org/10.1145/3594536.3595167},
   year = {2023},
}
@article{Lewis2023,
   abstract = {Technology-assisted review (TAR) workflows are central to electronic discovery (eDiscovery). Researchers have proposed many methods for evaluating TAR workflows, but this research has had little impact on eDiscovery practice. We examine the operational constraints faced by eDiscovery reviewers and managers, and show how past evaluation proposals are inconsistent with their needs. We then present a new evaluation approach for one-phase TAR workflows based on confidence sequences. Our approach provides a review manager with complete control over the design and duration of the TAR workflow, as well as the amount and timing of review of evaluation documents. Evaluation documents can be reused for supervised learning while preserving valid frequentist confidence intervals on recall at all points during review. The method is expensive in terms of sample size but plausible for large scale reviews, and has many opportunities for improvement. CCS CONCEPTS • Information systems → Retrieval effectiveness; • Applied computing → Enterprise applications; • Human-centered computing → Interaction design process and methods; • Mathematics of computing → Stochastic processes.},
   author = {David D Lewis and Lenora Gray and Mark Noel},
   doi = {10.1145/3594536.3595148},
   isbn = {9798400701979},
   keywords = {SAVI,conformal prediction,e-disclosure,e-discovery,eDisclosure,effectiveness metrics,martingales,safe anytime-valid inference},
   month = {6},
   pages = {52-61},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Reasoning with hierarchies of open-textured predicates},
   url = {https://doi.org/10.1145/3594536.3595167},
   year = {2023},
}
@article{Hulstijn2023,
   abstract = {What happens if the way in which we handle a genuine deontic conflict -i.e., a deontic ambiguity-matters regarding the application of other norms that are not directly affected by that conflict? We argue that the law requires sometimes propagating the ambiguity to other norms and sometimes confining it to some norms only. We explore this issue and model different reasoning patterns. The problem is addressed in a new variant of Defeasible Deontic Logic. The contribution of this paper is threefold: (a) we extend the treatment of ambiguity blocking and propagation to Defeasible Deontic Logic; (b) we discuss reasoning patterns in the law, especially in criminal law, where we need to deal with both ambiguity blocking and ambiguity propagation in the same legal system and logic; (c) we devise an annotated variant of Defeasible Deontic Logic where we distinguish literals that must be obtained through an ambiguity-blocking mechanism from those that are derived using an ambiguity-propagating mechanism.},
   author = {Joris Hulstijn and Joris Hulstijn@uni Lu},
   doi = {10.1145/3594536.3595175},
   isbn = {9798400701979},
   keywords = {AI,computational accountability,ethics,internal controls},
   month = {6},
   pages = {91-100},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Deontic Ambiguities in Legal Reasoning},
   url = {https://doi.org/10.1145/3594536.3595122},
   year = {2023},
}
@article{Chen2023,
   abstract = {Legal Question Answering (LQA) is a promising artificial intelligence application with high practical value. A professional and effective legal question answering (QA) agent can assist in reducing the workload of lawyers and judges, and help to achieve judicial accessibility. However, the NLP community lacks a large-scale LQA dataset with high quality, making it difficult to develop practical data-driven LQA agents. To tackle this bottleneck, this work presents EQUALS, a well-annotated real-world dataset for lEgal QUestion Answering via reading Chinese LawS. EQUALS contains 6,914 \{question, article , answer\} triplets as well as a pool of articles of laws that covers 10 different collections of Chinese Laws. Questions and the corresponding answers in EQUALS are collected from a professional law consultation forum. More importantly, the exact spans of law articles are annotated by senior law students as the answers. In this way, we could assure the quality and professionalism of EQUALS. Furthermore, this work proposes a QA framework that encompasses a law article retrieval module and a machine reading comprehension module for extracting accurate answers from the law article. We conduct thorough experiments with representative baselines on EQUALS, and the results indicate that EQUALS is a challenging question answering task. To the best of our knowledge, EQUALS is the largest real-world LQA dataset which shall significantly promote the research of LQA tasks. The work has been open-sourced and is available at: https://github.com/andongBlue/EQUALS. * Corresponding Authors.},
   author = {Andong Chen and Feng Yao and Xinyan Zhao and Yating Zhang and Changlong Sun and Yun Liu and Weixing Shen},
   doi = {10.1145/3594536.3595121},
   isbn = {9798400701979},
   keywords = {CCS CONCEPTS • Information systems → Question answering,Recommender sys-tems,• Computing methodologies → Language resources KEYWORDS Legal Dataset, Legal Question Answering, Question Answering Framework},
   month = {6},
   pages = {32-41},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Analogical Reasoning, Generalization, and Rule Learning for Common Law Reasoning},
   url = {https://doi.org/10.1145/3594536.3595159},
   year = {2023},
}
@article{Blair-Stanek2023,
   abstract = {Statutory reasoning is the task of reasoning with facts and statutes, which are rules written in natural language by a legislature. It is a basic legal skill. In this paper we explore the capabilities of the most capable GPT-3 model, text-davinci-003, on an established statutory-reasoning dataset called SARA. We consider a variety of approaches, including dynamic few-shot prompting, chain-of-thought prompting, and zero-shot prompting. While we achieve results with GPT-3 that are better than the previous best published results, we also identify several types of clear errors it makes. In investigating why these happen, we discover that GPT-3 has imperfect prior knowledge of the actual U.S. statutes on which SARA is based. More importantly, GPT-3 performs poorly at answering straightforward questions about simple synthetic statutes. By also posing the same questions when the synthetic statutes are written in sentence form, we find that some of GPT-3's poor performance results from difficulty in parsing the typical structure of statutes, containing subsections and paragraphs.},
   author = {Andrew Blair-Stanek and Nils Holzenberger and Benjamin Van Durme},
   doi = {10.1145/3594536.3595163},
   isbn = {9798400701979},
   keywords = {AI,computational accountability,ethics,internal controls},
   month = {6},
   pages = {22-31},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Can GPT-3 Perform Statutory Reasoning?},
   url = {https://doi.org/10.1145/3594536.3595122},
   year = {2023},
}
@article{Kaur2023,
   abstract = {A number of datasets for Relation Extraction (RE) have been created to aide downstream tasks such as information retrieval, semantic search, question answering and textual entailment. However, these datasets fail to capture financial-domain specific challenges since most of these datasets are compiled using general knowledge sources, hindering real-life progress and adoption within the financial world. To address this limitation, we propose REFinD, the first large-scale annotated dataset of relations, with ∼29K instances and 22 relations amongst 8 types of entity pairs, generated entirely over financial documents. We also provide an empirical evaluation with various state-of-the-art models as benchmarks for the RE task and highlight the challenges posed by our dataset. We observed that various state-of-the-art deep learning models struggle with numeric inference, relational and directional ambiguity.},
   author = {Simerjot Kaur and Joy Sain and Charese Smiley and Dongsheng Wang and Akshat Gupta and Suchetha Siddagangappa and Toyin Aguda and Sameena Shah},
   doi = {10.1145/3539618.3591911},
   isbn = {9781450394086},
   journal = {SIGIR 2023 - Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
   keywords = {annotation datasets,benchmarking,finance,information retrieval,natural language processing,relation extraction},
   month = {7},
   pages = {3054-3063},
   publisher = {Association for Computing Machinery, Inc},
   title = {REFinD: Relation Extraction Financial Dataset},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3539618.3591911},
   year = {2023},
}
@article{Koshiyama2020,
   abstract = {This paper reviews Artificial Intelligence (AI), Machine Learning (ML) and associated algorithms in future Capital Markets. New AI algorithms are constantly emerging, with each 'strain' mimicking a new form of human learning, reasoning, knowledge, and decisionmaking. The current main disrupting forms of learning include Deep Learning, Adversarial Learning, Transfer and Meta Learning. Albeit these modes of learning have been in the AI/ML field more than a decade, they now are more applicable due to the availability of data, computing power and infrastructure. These forms of learning have produced new models (e.g., Long Short-Term Memory, Generative Adversarial Networks) and leverage important applications (e.g., Natural Language Processing, Adversarial Examples, Deep Fakes, etc.). These new models and applications will drive changes in future Capital Markets, so it is important to understand their computational strengths and weaknesses. Since ML algorithms effectively self-program and evolve dynamically, financial institutions and regulators are becoming increasingly concerned with ensuring there remains a modicum of human control, focusing on Algorithmic Interpretability/Explainability, Robustness and Legality. For example, the concern is that, in the future, an ecology of trading algorithms across different institutions may 'conspire' and become unintentionally fraudulent (cf. LIBOR) or subject to subversion through compromised datasets (e.g. Microsoft Tay). New and unique forms of systemic risks can emerge, potentially coming from excessive algorithmic complexity. The contribution of this paper is to review AI, ML and associated algorithms, their computational strengths and weaknesses, and discuss their future impact on the Capital Markets.},
   author = {Adriano Koshiyama and Nick Firoozye and Philip Treleaven},
   doi = {10.1145/3383455.3422539},
   isbn = {9781450375849},
   journal = {ICAIF 2020 - 1st ACM International Conference on AI in Finance},
   keywords = {Artificial intelligence,Deep learning,Finance,Generative adversarial networks,Machine learning,Transfer learning},
   month = {10},
   publisher = {Association for Computing Machinery, Inc},
   title = {Algorithms in future capital markets: A survey on AI, ML and associated algorithms in capital markets},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3383455.3422539},
   year = {2020},
}
@article{Takayanagi2023,
   abstract = {With the development of online platforms, people can share and obtain opinions quickly. It also makes individuals' preferences change dynamically and rapidly because they may change their minds when getting convincing opinions from other users. Unlike representative areas of recommendation research such as e-commerce platforms where items' features are fixed, in investment scenarios financial instruments' features such as stock price, also change dynamically over time. To capture these dynamic features and provide a better-personalized recommendation for amateur investors, this study proposes a Personalized Dynamic Recommender System for Investors, PDRSI. The proposed PDRSI considers two investor's personal features: dynamic preferences and historical interests, and two temporal environmental properties: recent discussions on the social media platform and the latest market information. The experimental results support the usefulness of the proposed PDRSI, and the ablation studies show the effect of each module. For reproduction, we follow Twitter's developer policy to share our dataset for future work.},
   author = {Takehiro Takayanagi and Chung Chi Chen and Kiyoshi Izumi},
   doi = {10.1145/3539618.3592035},
   isbn = {9781450394086},
   journal = {SIGIR 2023 - Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
   keywords = {financial data mining,investor modeling,stock recommendation},
   month = {7},
   pages = {2246-2250},
   publisher = {Association for Computing Machinery, Inc},
   title = {Personalized Dynamic Recommender System for Investors},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3539618.3592035},
   year = {2023},
}
@article{Tian2021,
   abstract = {Both authors contributed equally to this research. This paper presents the method that we tackled the FinSBD-3 shared task (structure boundary detection) to extract the boundaries of sentences, lists, and items, including structure elements like footer, header, tables from noisy unstructured English and French financial texts. The deep attention model based on word embedding using data augmentation and BERT model named as hybrid deep learning model to detect the sentence, list-item, footer, header, tables boundaries in noisy English and French texts and classify the list-item sentences into list & different item types using deep attention model. The experiment is shown that the proposed method could be an effective solution to deal with the FinSBD-3 shared task. The submitted result ranks first based on the task metrics in the final leader board.},
   author = {Ke Tian and Hua Chen},
   doi = {10.1145/3442442.3451380},
   isbn = {9781450383134},
   journal = {The Web Conference 2021 - Companion of the World Wide Web Conference, WWW 2021},
   keywords = {Attention,BERT,Data Augmentation,FinSBD-3 shared task,Financial Texts,LSTM,Structure Boundary Detec-tion},
   month = {4},
   pages = {283-287},
   publisher = {Association for Computing Machinery, Inc},
   title = {Aiai at the FinSBD-3 task: Structure Boundary Detection of Noisy Financial Texts in English and French Using Data Augmentation and Hybrid Deep Learning Model},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3442442.3451380},
   year = {2021},
}
@article{Zhang2023,
   abstract = {Financial sentiment analysis is critical for valuation and investment decision-making. Traditional NLP models, however, are limited by their parameter size and the scope of their training datasets, which hampers their generalization capabilities and effectiveness in this field. Recently, Large Language Models (LLMs) pre-trained on extensive corpora have demonstrated superior performance across various NLP tasks due to their commendable zero-shot abilities. Yet, directly applying LLMs to financial sentiment analysis presents challenges: The discrepancy between the pre-training objective of LLMs and predicting the sentiment label can compromise their predictive performance. Furthermore, the succinct nature of financial news, often devoid of sufficient context, can significantly diminish the reliability of LLMs' sentiment analysis. To address these challenges, we introduce a retrieval-augmented LLMs framework for financial sentiment analysis. This framework includes an instruction-tuned LLMs module, which ensures LLMs behave as predictors of sentiment labels, and a retrieval-augmentation module which retrieves additional context from reliable external sources. Benchmarked against traditional models and LLMs like ChatGPT and LLaMA, our approach achieves 15% to 48% performance gain in accuracy and F1 score. CCS CONCEPTS • Computing methodologies → Natural language processing.},
   author = {Boyu Zhang and Australia Hongyang Yang and Tianyu Zhou and Ali Babar and Australia Xiao-Yang Liu and Hongyang Yang and Xiao-Yang Liu},
   city = {New York, NY, USA},
   doi = {10.1145/3604237.3626866},
   isbn = {9798400702402},
   journal = {4th ACM International Conference on AI in Finance},
   keywords = {Instruction Tuning,Large Language Models,Retrieval Augmented Generation * Authors contributed equally to this research † Corresponding author,Sentiment Analysis},
   month = {11},
   pages = {349-356},
   publisher = {ACM},
   title = {Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3604237.3626866},
   year = {2023},
}
@article{Ghosh2023,
   abstract = {Dealing with money has always been one of the basic skills one needs to live a comfortable life. However, financial literacy rates across the nations are extremely low. Furthermore, over the years the returns from traditional investment avenues like bank fixed deposits (FD), real estate, etc. have been diminishing. This entices new-age investors to trade and reap profits from the ever-growing stock markets. Nevertheless, in reality, only a handful of active traders are able to earn more than the FD rates. This is due to the lack of financial knowledge. The presence of complex concepts and jargons further reduces comprehensibility. In this paper, we present how financial texts can be demystified using Natural Language Processing (NLP). It consists of neural-based readability assessment and hypernym extraction tools to improve the readability of financial texts. Other modules include financial domain specific systems for automated claim detection, sustainability assessment, etc.},
   author = {Sohom Ghosh and Sudip Kumar Naskar},
   doi = {10.1145/3570991.3571051},
   isbn = {9781450397988},
   journal = {ACM International Conference Proceeding Series},
   keywords = {claim detection,financial text processing,hypernym detection,natural language processing,readability,• Computing methodologies → Language resources,• Information systems → Clustering and classification},
   month = {1},
   pages = {301-302},
   publisher = {Association for Computing Machinery},
   title = {Using Natural Language Processing to Enhance Understandability of Financial Texts},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3570991.3571051},
   year = {2023},
}
@article{Nath2023,
   abstract = {Dynamic Bipartite graph is naturally suited for modeling temporally evolving interaction in several domains, including digital payment and social media. Though dynamic graphs are widely studied, their focus remains on homogeneous graphs. This paper proposes a novel framework for representation learning in temporally evolving bi-partite graphs. It introduces a bipartite graph transformer layer, a temporal bipartite graph encoder based on an attention mechanism for learning node representations. It further extends the information maximization objective based on noise contrastive learning to temporal bipartite graphs. This combination of bipartite encoder layer and noise contrastive loss ensures each node-set in the temporal bipartite graph is represented uniquely and disentangled from other node-set. We use four public datasets with temporal bipar-tite characteristics in experimentation. The proposed model shows promising results on the transductive and inductive dynamic link prediction task and on the temporal recommendation task.},
   author = {Pritam Kumar Nath and Mastercard AI Garage India Govind Waghmare and Nikhil Tumbde and Mastercard AI Garage India Nitish Srivasatava and Mastercard AI Garage India Siddhartha Asthana},
   city = {New York, NY, USA},
   doi = {10.1145/3604237.3626908},
   isbn = {9798400702402},
   journal = {4th ACM International Conference on AI in Finance},
   keywords = {bipartite graphs,dynamic graph representation learning,graph neural network},
   month = {11},
   pages = {73-81},
   publisher = {ACM},
   title = {FlowMind: Automatic Workflow Generation with LLMs},
   url = {https://dl.acm.org/doi/10.1145/3604237.3626908},
   year = {2023},
}
@article{Stillman2023,
   abstract = {The ability to construct a realistic simulator of financial exchanges, including reproducing the dynamics of the limit order book, can give insight into many counterfactual scenarios, such as a flash crash, a margin call, or changes in macroeconomic outlook. In recent years, agent-based models have been developed that reproduce many features of an exchange, as summarised by a set of stylised facts and statistics. However, the ability to calibrate simulators to a specific period of trading remains an open challenge. In this work, we develop a novel approach to the calibration of market simulators by leveraging recent advances in deep learning, specifically using neural density estimators and embedding networks. We demonstrate that our approach is able to correctly identify high probability parameter sets, both when applied to synthetic and historical data, and without reliance on manually selected or weighted ensembles of stylised facts.},
   author = {Namid R Stillman and Rory Baggott and Justin Lyon and Jianfei Zhang and Hong Kong and Clearing Limited and Dingqiu Zhu and Tao Chen and Perukrishnen Vytelingum},
   city = {New York, NY, USA},
   doi = {10.1145/3604237.3626867},
   isbn = {9798400702402},
   journal = {4th ACM International Conference on AI in Finance},
   keywords = {Agent-based Models,Embedding networks,Market simulator,Neural density estimators,Simulation-based inference},
   month = {11},
   pages = {100-107},
   publisher = {ACM},
   title = {LLMs for Financial Advisement: A Fairness and Efficacy Study in Personal Decision Making},
   url = {https://dl.acm.org/doi/10.1145/3604237.3626867},
   year = {2023},
}
@article{Kim2023,
   author = {Seonmi Kim and Seyoung Kim and Yejin Kim and Junpyo Park and Seongjin Kim and Moolkyeol Kim and Chang Hwan Sung and Joohwan Hong and Yongjae Lee},
   city = {New York, NY, USA},
   doi = {10.1145/3604237.3627721},
   isbn = {9798400702402},
   journal = {4th ACM International Conference on AI in Finance},
   month = {11},
   pages = {383-391},
   publisher = {ACM},
   title = {LLMs Analyzing the Analysts: Do BERT and GPT Extract More Value from Financial Analyst Reports?},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3604237.3627721},
   year = {2023},
}
@article{Lee2021,
   abstract = {The Federal Reserve System (the Fed) plays a significant role in affecting monetary policy and financial conditions worldwide. Although it is important to analyse the Fed's communications to extract useful information, it is generally long-form and complex due to the ambiguous and esoteric nature of content. In this paper, we present FedNLP, an interpretable multi-component Natural Language Processing (NLP) system to decode Federal Reserve communications. This system is designed for end-users to explore how NLP techniques can assist their holistic understanding of the Fed's communications with NO coding. Behind the scenes, FedNLP uses multiple NLP models from traditional machine learning algorithms to deep neural network architectures in each downstream task. The demonstration shows multiple results at once including sentiment analysis, summary of the document, prediction of the Federal Funds Rate movement and visualization for interpreting the prediction model's result. Our application system and demonstration are available at https://fednlp.net.},
   author = {Jean Lee and Hoyoul Luis Youn and Nicholas Stevens and Josiah Poon and Soyeon Caren Han},
   doi = {10.1145/3404835.3462785},
   isbn = {9781450380379},
   journal = {SIGIR 2021 - Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
   keywords = {AI application,federal funds rate forecasting,federal reserve,interpretable machine learning,text analysis},
   month = {7},
   pages = {2560-2564},
   publisher = {Association for Computing Machinery, Inc},
   title = {FedNLP: An Interpretable NLP System to Decode Federal Reserve Communications},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3404835.3462785},
   year = {2021},
}
@article{Ghosh2023,
   abstract = {Over the years, promising returns have enticed the masses to invest in the stock markets. However, most people do not have the financial knowledge needed for making investment decisions. Even seasoned investors find it difficult to grasp all the available information. This is primarily due to the ever-changing market dynamics and information overload. Natural Language Processing based automated systems are the rescue to such problems. In this paper, we present the Financial Language Understandability Enhancement Toolkit (FLUEnT) for processing financial text. It consists of eight different tools for tasks like hypernym detection, numeral claim analysis, readability assessment, sustainability assessment, etc. The objective of the toolkit is to empower the masses and enable investors in making data-driven decisions. It is open-source under MIT license and is openly accessible from Colab and HuggingFace.1,},
   author = {Sohom Ghosh and Sudip Kumar Naskar},
   doi = {10.1145/3570991.3571067},
   isbn = {9781450397988},
   journal = {ACM International Conference Proceeding Series},
   keywords = {financial text processing,natural language processing,toolkit},
   month = {1},
   pages = {258-262},
   publisher = {Association for Computing Machinery},
   title = {FLUEnT: Financial Language Understandability Enhancement Toolkit},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3570991.3571067},
   year = {2023},
}
@article{Arslan2021,
   abstract = {Neural networks for language modeling have been proven effective on several sub-Tasks of natural language processing. Training deep language models, however, is time-consuming and computationally intensive. Pre-Trained language models such as BERT are thus appealing since (1) they yielded state-of-The-Art performance, and (2) they offload practitioners from the burden of preparing the adequate resources (time, hardware, and data) to train models. Nevertheless, because pre-Trained models are generic, they may underperform on specific domains. In this study, we investigate the case of multi-class text classification, a task that is relatively less studied in the literature evaluating pre-Trained language models. Our work is further placed under the industrial settings of the financial domain. We thus leverage generic benchmark datasets from the literature and two proprietary datasets from our partners in the financial technological industry. After highlighting a challenge for generic pre-Trained models (BERT, DistilBERT, RoBERTa, XLNet, XLM) to classify a portion of the financial document dataset, we investigate the intuition that a specialized pre-Trained model for financial documents, such as FinBERT, should be leveraged. Nevertheless, our experiments show that the FinBERT model, even with an adapted vocabulary, does not lead to improvements compared to the generic BERT models.},
   author = {Yusuf Arslan and Kevin Allix and Lisa Veiber and Cedric Lothritz and Tegawendé F. Bissyandé and Jacques Klein and Anne Goujon},
   doi = {10.1145/3442442.3451375},
   isbn = {9781450383134},
   journal = {The Web Conference 2021 - Companion of the World Wide Web Conference, WWW 2021},
   keywords = {BERT,FinBERT,financial text classification},
   month = {4},
   pages = {260-268},
   publisher = {Association for Computing Machinery, Inc},
   title = {A Comparison of Pre-Trained Language Models for Multi-Class Text Classification in the Financial Domain},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3442442.3451375},
   year = {2021},
}
@article{Chersoni2021,
   abstract = {In this contribution, we describe the systems presented by the PolyU CBS Team at the second Shared Task on Learning Semantic Similarities for the Financial Domain (FinSim-2), where participating teams had to identify the right hypernyms for a list of target terms from the financial domain. For this task, we ran our classification experiments with several distributional, string-based, and Transformer features. Our results show that a simple logistic regression classifier, when trained on a combination of word embeddings, semantic and string similarity metrics and BERT-derived probabilities, achieves a strong performance (above 90%) in financial hypernymy detection.},
   author = {Emmanuele Chersoni and Chu Ren Huang},
   doi = {10.1145/3442442.3451387},
   isbn = {9781450383134},
   journal = {The Web Conference 2021 - Companion of the World Wide Web Conference, WWW 2021},
   keywords = {Distributional Models,Financial NLP,Hypernymy Detection,Nat-ural language processing},
   month = {4},
   pages = {316-319},
   publisher = {Association for Computing Machinery, Inc},
   title = {PolyU-CBS at the FinSim-2 Task: Combining Distributional, String-Based and Transformers-Based Features for Hypernymy Detection in the Financial Domain},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3442442.3451387},
   year = {2021},
}
@article{Goel2021,
   abstract = {Recent advancement in neural network architectures has provided several opportunities to develop systems to automatically extract and represent information from domain specific unstructured text sources. The Finsim-2021 shared task, collocated with the FinNLP workshop, offered the challenge to automatically learn effective and precise semantic models of financial domain concepts. Building such semantic representations of domain concepts requires knowledge about the specific domain. Such a thorough knowledge can be obtained through the contextual information available in raw text documents on those domains. In this paper, we proposed a transformer-based BERT architecture that captures such contextual information from a set of domain specific raw documents and then perform a classification task to segregate domain terms into fixed number of class labels. The proposed model not only considers the contextual BERT embeddings but also incorporates a TF-IDF vectorizer that gives a word level importance to the model. The performance of the model has been evaluated against several baseline architectures.},
   author = {Tushar Goel and Vipul Chauhan and Ishan Verma and Tirthankar Dasgupta and Lipika Dey},
   doi = {10.1145/3442442.3451386},
   isbn = {9781450383134},
   journal = {The Web Conference 2021 - Companion of the World Wide Web Conference, WWW 2021},
   keywords = {Automatic Classification of Financial Term,Ontology,TFIDF Vectors,Text Classification,Transformers},
   month = {4},
   pages = {311-315},
   publisher = {Association for Computing Machinery, Inc},
   title = {TCS-WITM-2021 @FinSim-2: Transformer based Models for Automatic Classification of Financial Terms},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3442442.3451386},
   year = {2021},
}
@article{Devlin2019,
   abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
   author = {Jacob Devlin and Ming Wei Chang and Kenton Lee and Kristina Toutanova},
   isbn = {9781950737130},
   journal = {NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference},
   keywords = {Attention,BERT,Catboost,FinSim-2 task,LSTM,Ontology,RoBERTa,Word2vec},
   month = {4},
   pages = {4171-4186},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {BERT: Pre-training of deep bidirectional transformers for language understanding},
   volume = {1},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3442442.3451388},
   year = {2019},
}
@article{Nguyen2021,
   abstract = {In this paper, we present the different methods proposed for the FinSIM-2 Shared Task 2021 on Learning Semantic Similarities for the Financial domain. The main focus of this task is to evaluate the classification of financial terms into corresponding top-level concepts (also known as hypernyms) that were extracted from an external ontology. We approached the task as a semantic textual similarity problem. By relying on a siamese network with pre-Trained language model encoders, we derived semantically meaningful term embeddings and computed similarity scores between them in a ranked manner. Additionally, we exhibit the results of different baselines in which the task is tackled as a multi-class classification problem. The proposed methods outperformed our baselines and proved the robustness of the models based on textual similarity siamese network.},
   author = {Nhu Khoa Nguyen and Emanuela Boros and Gaël Lejeune and Antoine Doucet and Thierry Delahaut},
   doi = {10.1145/3442442.3451384},
   isbn = {9781450383134},
   journal = {The Web Conference 2021 - Companion of the World Wide Web Conference, WWW 2021},
   keywords = {Hypernym detection,semantic similarities,siamese networks},
   month = {4},
   pages = {302-306},
   publisher = {Association for Computing Machinery, Inc},
   title = {L3i_LBPAM at the FinSim-2 task: Learning Financial Semantic Similarities with Siamese Transformers},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3442442.3451384},
   year = {2021},
}
@article{Perdih2021,
   abstract = {Ontologies are increasingly used for machine reasoning over the last few years. They can provide explanations of concepts or be used for concept classification if there exists a mapping from the desired labels to the relevant ontology. Another advantage of using ontologies is that they do not need a learning process, meaning that we do not need the train data or time before using them. This paper presents a practical use of an ontology for a classification problem from the financial domain. It first transforms a given ontology to a graph and proceeds with generalization with the aim to find common semantic descriptions of the input sets of financial concepts. We present a solution to the shared task on Learning Semantic Similarities for the Financial Domain (FinSim-2 task). The task is to design a system that can automatically classify concepts from the Financial domain into the most relevant hypernym concept in an external ontology-the Financial Industry Business Ontology. We propose a method that maps given concepts to the mentioned ontology and performs a graph search for the most relevant hypernyms. We also employ a word vectorization method and a machine learning classifier to supplement the method with a ranked list of labels for each concept.},
   author = {Timen Stepišnik Perdih and Senja Pollak and Blaå3/4 Škrlj},
   doi = {10.1145/3442442.3451383},
   isbn = {9781450383134},
   journal = {The Web Conference 2021 - Companion of the World Wide Web Conference, WWW 2021},
   keywords = {FIBO,concept classification,financial vocabulary,generalization,hypernym discovery,ontology},
   month = {4},
   pages = {298-301},
   publisher = {Association for Computing Machinery, Inc},
   title = {JSI at the FinSim-2 task: Ontology-Augmented Financial Concept Classification},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3442442.3451383},
   year = {2021},
}
@article{Mansar2021,
   abstract = {The FinSim-2 is a second edition of FinSim Shared Task on Learning Semantic Similarities for the Financial Domain, colocated with the FinWeb workshop. FinSim-2 proposed the challenge to automatically learn effective and precise semantic models for the financial domain. The second edition of the FinSim offered an enriched dataset in terms of volume and quality, and interested in systems which make creative use of relevant resources such as ontologies and lexica, as well as systems which make use of contextual word embeddings such as BERT[4]. Going beyond the mere representation of words is a key step to industrial applications that make use of Natural Language Processing (NLP). This is typically addressed using either unsupervised corpus-derived representations like word embeddings, which are typically opaque to human understanding but very useful in NLP applications or manually created resources such as taxonomies and ontologies, which typically have low coverage and contain inconsistencies, but provide a deeper understanding of the target domain. Finsim is inspired from previous endeavours in the Semeval community, which organized several competitions on semantic/lexical relation extraction between concepts/words. This year, 18 system runs were submitted by 7 teams and systems were ranked according to 2 metrics, Accuracy and Mean rank. All the systems beat our baseline 1 model by over 15 points and the best systems beat the baseline 2 by over 1 ~3 points in accuracy.},
   author = {Youness Mansar and Juyeon Kang and Ismail El Maarouf},
   doi = {10.1145/3442442.3451381},
   isbn = {9781450383134},
   journal = {The Web Conference 2021 - Companion of the World Wide Web Conference, WWW 2021},
   keywords = {Domain specific ontology,Financial documents processing,Hypernym-hyponym relation extraction,Natural Language Processing,Word embeddings},
   month = {4},
   pages = {288-292},
   publisher = {Association for Computing Machinery, Inc},
   title = {The FinSim-2 2021 Shared Task: Learning Semantic Similarities for the Financial Domain},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3442442.3451381},
   year = {2021},
}
@article{Pei2021,
   abstract = {In this paper, we present our approaches for the FinSim 2021 Shared Task on Learning Semantic Similarities for the Financial Domain. The aim of the FinSim shared task is to automatically classify a given list of terms from the financial domain into the most relevant hypernym (or top-level) concept in an external ontology. Two different word representations have been compared in our study, i.e., customized word2vec provided by the shared task and FinBERT. We first create a customized corpus from the given prospectuses and relevant articles from Investopedia. Then we train the domain-specific word2vec embeddings using the customized data with customized word2vec and FinBERT as the initialized embeddings respectively. Our experimental results demonstrate that these customized word embeddings can effectively improve the classification performance and achieve better results than the direct utilization of the provided word embeddings. The class imbalance issue of the given data is also explored. We empirically study the classification performance by employing several different strategies for imbalanced classification problems. Our system ranks 2nd on both Average Accuracy and Mean Rank metrics.},
   author = {Yulong Pei and Qian Zhang},
   doi = {10.1145/3442442.3451385},
   isbn = {9781450383134},
   journal = {The Web Conference 2021 - Companion of the World Wide Web Conference, WWW 2021},
   keywords = {BERT,Word representations,imbalance classification,word2vec},
   month = {4},
   pages = {307-310},
   publisher = {Association for Computing Machinery, Inc},
   title = {GOAT at the FinSim-2 task: Learning Word Representations of Financial Data with Customized Corpus},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3442442.3451385},
   year = {2021},
}
@article{Portisch2021,
   abstract = {This paper presents the FinMatcher system and its results for the FinSim 2021 shared task which is co-located with the Workshop on Financial Technology on the Web (FinWeb) in conjunction with The Web Conference. The FinSim-2 shared task consists of a set of concept labels from the financial services domain. The goal is to find the most relevant top-level concept from a given set of concepts. The FinMatcher system exploits three publicly available knowledge graphs, namely WordNet, Wikidata, and WebIsALOD. The graphs are used to generate explicit features as well as latent features which are fed into a neural classifier to predict the closest hypernym.},
   author = {Jan Portisch and Michael Hladik and Heiko Paulheim},
   doi = {10.1145/3442442.3451382},
   isbn = {9781450383134},
   journal = {The Web Conference 2021 - Companion of the World Wide Web Conference, WWW 2021},
   keywords = {RDF2vec,financial services,hypernymy detection,knowledge graph embeddings,knowledge graphs,wikidata},
   month = {4},
   pages = {293-297},
   publisher = {Association for Computing Machinery, Inc},
   title = {FinMatcher at FinSim-2: Hypernym Detection in the Financial Services Domain using Knowledge Graphs},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3442442.3451382},
   year = {2021},
}
@article{Zhang2023,
   abstract = {The use of Large Language Models (LLMs) is revolutionizing the legal industry. In this technical talk, we would like to explore the various use cases of LLMs in legal tasks, discuss the best practices, investigate the available resources, examine the ethical concerns, and suggest promising research directions. CCS CONCEPTS • Information systems → Information retrieval; • Computing methodologies → Natural language processing; Neural networks; • Applied computing → Law. KEYWORDS legal data mining, legal information retrieval, legal natural language processing, legal knowledge management, large language models ACM Reference Format:},
   author = {Dell Zhang and Alina Petrova and Dietrich Trautmann and Frank Schilder},
   doi = {10.1145/3583780.3615993},
   isbn = {9798400701245},
   month = {10},
   pages = {5257-5258},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Unleashing the Power of Large Language Models for Legal Applications},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3583780.3615993},
   year = {2023},
}
@article{Huang2023,
   abstract = {In-context learning is the ability of a pretrained model to adapt to novel and diverse downstream tasks by conditioning on prompt examples, without optimizing any parameters. While large language models have demonstrated this ability, how in-context learning could be performed over graphs is unexplored. In this paper, we develop \textbf\{Pr\}etraining \textbf\{O\}ver \textbf\{D\}iverse \textbf\{I\}n-Context \textbf\{G\}raph S\textbf\{y\}stems (PRODIGY), the first pretraining framework that enables in-context learning over graphs. The key idea of our framework is to formulate in-context learning over graphs with a novel \emph\{prompt graph\} representation, which connects prompt examples and queries. We then propose a graph neural network architecture over the prompt graph and a corresponding family of in-context pretraining objectives. With PRODIGY, the pretrained model can directly perform novel downstream classification tasks on unseen graphs via in-context learning. We provide empirical evidence of the effectiveness of our framework by showcasing its strong in-context learning performance on tasks involving citation networks and knowledge graphs. Our approach outperforms the in-context learning accuracy of contrastive pretraining baselines with hard-coded adaptation by 18\% on average across all setups. Moreover, it also outperforms standard finetuning with limited data by 33\% on average with in-context learning.},
   author = {Qian Huang and Hongyu Ren and Peng Chen and Gregor Kržmanc and Daniel Zeng and Percy Liang and Jure Leskovec},
   month = {5},
   title = {PRODIGY: Enabling In-context Learning Over Graphs},
   url = {https://arxiv.org/abs/2305.12600v1},
   year = {2023},
}
@article{Kang2021,
   abstract = {With the recent rise in popularity of Transformer models in Natural Language Processing , research efforts have been dedicated to the development of domain-adapted versions of BERT-like architectures. In this study, we focus on FinBERT, a Transformer model trained on text from the financial domain. By comparing its performances with the original BERT on a wide variety of financial text processing tasks, we found continual pretraining from the original model to be the more beneficial option. Domain-specific pre-training from scratch, conversely, seems to be less effective.},
   author = {J Kang and I El Maarouf and S Bellato - Proceedings of the Third … and undefined 2021},
   journal = {aclanthology.org},
   pages = {37-44},
   title = {FinSim-3: The 3rd shared task on learning semantic similarities for the financial domain},
   url = {https://aclanthology.org/2021.finnlp-1.5.pdf},
}
@article{Boella2013,
   abstract = {Maintaining regulatory compliance is an increasing area of concern for business. Legal Knowledge Management systems that combine repositories of legislation with legal ontologies can support the work of in-house compliance managers. But there are challenges to overcome, of interpreting legal knowledge and mapping that knowledge onto business processes, and developing systems that can adequately handle the complexity with clarity and ease. In this paper we extend the Legal Knowledge Management system Eunomos to deal with alternative interpretations of norms connecting it with Business Process Management systems. Moreover, we propose a workflow involving the different roles in a company, which takes legal interpretation into account in mapping norms and processes, using Eunomos as a support. Copyright 2013 ACM.},
   author = {Guido Boella and Marijn Janssen and Joris Hulstijn and Llio Humphreys and Leendert Van Der Torre},
   doi = {10.1145/2514601.2514605},
   isbn = {9781450320801},
   journal = {Proceedings of the International Conference on Artificial Intelligence and Law},
   pages = {23-32},
   title = {Managing legal interpretation in regulatory compliance},
   year = {2013},
}
@article{Neill2017,
   abstract = {Texts expressed in legal language are often dicult and time consuming for lawyers to read through, particularly for the purpose of identifying relevant deontic modalities (obligations, prohibitions and permissions). By nature, the language of law is strict, hence the predominant use of modal logic as a substitute for the syntactical ambiguity in natural language, specically, deontic and alethic logic for the respective modalities. However, deontic modalities which express obligations,prohibitions and permissions, can have varying degree and preciseness to which they correspond to a matter, strict deontic logic does not allow for such quantitative measures. Therefore, this paper outlines a data-driven approach by classifying deontic modalities using ensembled Articial Neural Networks (ANN) that incorporate domain specic legal distributional semantic model (DSM) representations, in combination with, a general DSM representation. We propose to use well calibrated probability estimates from these classiers as an approximation to the degree which an obligation/prohibition or permission belongs to a given class based on SME annotated sentences. Best results show 82.33 % accuracy on a held-out test set.},
   author = {James O’ Neill and Cecile Robin and Paul Buitelaar and Leona O’ Brien},
   doi = {10.1145/3086512.3086528},
   isbn = {9781450348911},
   journal = {Proceedings of the International Conference on Artificial Intelligence and Law},
   keywords = {Deontic modality,Financial law,Sentence classication},
   month = {6},
   pages = {159-168},
   publisher = {Association for Computing Machinery},
   title = {Classifying sentential modality in legal language: A use case in financial regulations, acts and directives},
   url = {https://dl.acm.org/doi/10.1145/3086512.3086528},
   year = {2017},
}
@article{Al-Shabandar2019,
   abstract = {The global financial crisis of 2008 has led to the increased scrutiny of governance and conduct of financial services firms. A key component of monitoring conduct within this area is Financial Services Compliance Management. Financial institutions need to adhere to legislation such as the European MiFID II and GDPR through to anti-money laundering compliance. A recent report by Thomson Routers in 2018 has found through a survey with 800 financial firms that 66% of firms expect the cost of senior compliance staff to increase, up from 60% of firms in 2017, indicating a continuing growth in spending on compliance. Effective solutions need to be in place to mitigate these increasing costs while enhancing the compliance workflow. Doing this would provide a market edge. Artificial intelligence (AI) and machine learning (ML) have been gaining traction within the compliance management domain from both regulators and financial institutions in areas such as trade and market surveillance to regulatory compliance assurance. These areas share a commonality in terms of the volume of data to monitor often in real-time and from disparate sources both structured and unstructured with an emphasis on ensuring data quality and handling underlying bias in data. In this paper, an overview of the key use case areas of AI and ML in the compliance management domain will be provided. Detailed analysis on the application of specific AI solutions such as natural language processing, data discovery and generative modelling is introduced.},
   author = {Raghad Al-Shabandar and Gaye Lightbody and Fiona Browne and Jun Liu and Haiying Wang and Huiru Zheng},
   doi = {10.1145/3358331.3358339},
   isbn = {9781450372022},
   journal = {ACM International Conference Proceeding Series},
   keywords = {Anti-Money Laundering (AML),Artificial General Intelligence (AGI),Artificial Narrow Intelligence (ANI),Natural Language Processing (NLP),You’re your Customer (KYC)},
   month = {10},
   publisher = {Association for Computing Machinery},
   title = {The application of artificial intelligence in financial compliance management},
   url = {https://dl.acm.org/doi/10.1145/3358331.3358339},
   year = {2019},
}
@article{Rahat2022,
   abstract = {Since the General Data Protection Regulation (GDPR) came into force in May 2018, companies have worked on their data practices to comply with the requirements of GDPR. In particular, since the privacy policy is the essential communication channel for users to understand and control their privacy when using companies' services, many companies updated their privacy policies after GDPR was enforced. However, most privacy policies are verbose, full of jargon, and vaguely describe companies' data practices and users' rights. In addition, our study shows that more than 32% of end users find it difficult to understand the privacy policies explaining GDPR requirements. Therefore, it is challenging for the end users and law enforcement authorities to manually check if companies' privacy policies comply with the requirements enforced by GDPR. In this paper, we create a privacy policy dataset of 1,080 websites annotated by experts with 18 GDPR requirements and develop a Convolutional Neural Network (CNN) based model that can classify the privacy policies into GDPR requirements with an accuracy of 89.2%. We apply our model to automatically measure GDPR compliance in the privacy policies of 9,761 most visited websites. Our results show that, even after four years since GDPR went into effect, 68% of websites still fail to comply with at least one requirement of GDPR.},
   author = {Tamjid Al Rahat and Minjun Long and Yuan Tian},
   doi = {10.1145/3559613.3563195},
   isbn = {9781450398732},
   journal = {WPES 2022 - Proceedings of the 21st Workshop on Privacy in the Electronic Society, co-located with CCS 2022},
   keywords = {active learning,cnn,compliance check,deep learning,gdpr,privacy policy},
   month = {11},
   pages = {89-102},
   publisher = {Association for Computing Machinery, Inc},
   title = {Is Your Policy Compliant? A Deep Learning-based Empirical Study of Privacy Policies Compliance with GDPR},
   url = {https://dl.acm.org/doi/10.1145/3559613.3563195},
   year = {2022},
}
@article{Servantez2023,
   author = {Sergio Servantez and Nedim Lipka and Alexa Siu and Milan Aggarwal and Balaji Krishnamurthy and Aparna Garimella and Kristian Hammond and Rajiv Jain},
   doi = {10.1145/3594536.3595162},
   isbn = {9798400701979},
   month = {6},
   pages = {267-276},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Computable Contracts by Extracting Obligation Logic Graphs},
   url = {https://dl.acm.org/doi/10.1145/3594536.3595162},
   year = {2023},
}
@article{Sun2019,
   abstract = {SYNOPSIS: This paper aims to promote the application of deep learning to audit procedures by illustrating how the capabilities of deep learning for text understanding, speech recognition, visual recognition, and structured data analysis fit into the audit environment. Based on these four capabilities, deep learning serves two major functions in supporting audit decision making: information identification and judgment support. The paper proposes a framework for applying these two deep learning functions to a variety of audit procedures in different audit phases. An audit data warehouse of historical data can be used to construct prediction models, providing suggested actions for various audit procedures. The data warehouse will be updated and enriched with new data instances through the application of deep learning and a human auditor’s corrections. Finally, the paper discusses the challenges faced by the accounting profession, regulators, and educators when it comes to applying deep learning.},
   author = {Ting Sophia Sun},
   doi = {10.2308/ACCH-52455},
   issn = {15587975},
   issue = {3},
   journal = {Accounting Horizons},
   keywords = {Artificial intelligence,Audit procedure,Data analytics,Deep learning,Machine learning,Neural networks},
   pages = {89-109},
   publisher = {American Accounting Association},
   title = {Applying deep learning to audit procedures: An illustrative framework},
   volume = {33},
   year = {2019},
}
@article{Dinesh2008,
   abstract = {This paper considers the problem of checking whether an organization conforms to a body of regulation. Conformance is cast as a trace checking question - the regulation is represented in a logic that is evaluated against an abstract trace or run representing the operations of an organization. We focus on a problem in designing a logic to represent regulation. A common phenomenon in regulatory texts is for sentences to refer to others for conditions or exceptions. We motivate the need for a formal representation of regulation to accomodate such references between statements. We then extend linear temporal logic to allow statements to refer to others. The semantics of the resulting logic is defined via a combination of techniques from Reiter's default logic and Kripke's theory of truth. © 2008 Springer-Verlag.},
   author = {Nikhil Dinesh and Aravind Joshi and Insup Lee and Oleg Sokolsky},
   doi = {10.1007/978-3-540-70525-3_10/COVER},
   isbn = {3540705244},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {110-124},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Reasoning about conditions and exceptions to laws in regulatory conformance checking},
   volume = {5076 LNAI},
   url = {https://link.springer.com/chapter/10.1007/978-3-540-70525-3_10},
   year = {2008},
}
@article{Castiglione2023,
   abstract = {Legal language can be understood as the language typically used by those engaged in the legal profession and, as such, it may come both in spoken or written form. Recent legislation on cybersecurity obviously uses legal language in writing, thus inheriting all its interpretative complications due to the typical abundance of cases and sub-cases as well as to the general richness in detail. This paper faces the challenge of the essential interpretation of the legal language of cybersecurity, namely of the extraction of the essential Parts of Speech (POS) from the legal documents concerning cybersecurity. The challenge is overcome by our methodology for POS tagging of legal language. It leverages state-of-the-art open-source tools for Natural Language Processing (NLP) as well as manual analysis to validate the outcomes of the tools. As a result, the methodology is automated and, arguably, general for any legal language following minor tailoring of the preprocessing step. It is demonstrated over the most relevant EU legislation on cybersecurity, namely on the NIS 2 directive, producing the first, albeit essential, structured interpretation of such a relevant document. Moreover, our findings indicate that tools such as SpaCy and ClausIE reach their limits over the legal language of the NIS 2.},
   author = {Gianpietro Castiglione and Giampaolo Bella and Daniele Francesco Santamaria},
   doi = {10.1145/3600160.3605069},
   isbn = {9798400707728},
   journal = {ACM International Conference Proceeding Series},
   keywords = {Act,Data Protection,NLP,POS tagging,Privacy,Pronouncement},
   month = {8},
   publisher = {Association for Computing Machinery},
   title = {Towards Grammatical Tagging for the Legal Language of Cybersecurity},
   url = {https://dl.acm.org/doi/10.1145/3600160.3605069},
   year = {2023},
}
@article{Aires2017,
   abstract = {The exchange of goods and services between individuals is often formalised by a contract in which the parties establish norms to define what is expected of each one. Norms use deontic statements of obligation, prohibition, and permission, which may be in conflict. The task of manually detecting norm conflicts can be time–consuming and error-prone since contracts can be vast and complex. To automate such tasks, we develop an approach to identify potential conflicts between norms. We show the effectiveness of our approach and its individual components empirically using two publicly available corpora, and contribute with a new annotated test corpus for norm conflict identification.},
   author = {João Paulo Aires and Daniele Pinheiro and Vera Strubede Lima and Felipe Meneguzzi},
   doi = {10.1007/S10506-017-9205-X},
   issn = {15728382},
   issue = {4},
   journal = {Artificial Intelligence and Law},
   keywords = {Deontic logic,Natural language processing,Normative conflicts,Norms},
   month = {12},
   pages = {397-428},
   publisher = {Springer Netherlands},
   title = {Norm conflict identification in contracts},
   volume = {25},
   year = {2017},
}
@article{Pires2022,
   abstract = {Artificial Intelligence has proven to be effective in streamlining processes in several domains. The Brazilian judiciary, specifically, has a very large number of cases, above the work capacity of the courts, generating urgency in the creation of methods that mainly support the access and manipulation of unstructured data. This paper presents the construction of a Knowledge Graph of the Brazilian Legislation using Semantic Web standards that allows an understanding of how Brazilian laws interact with each other. The Knowledge Graph was quantitatively evaluated using complex network analysis and it was found to be useful to support experts in understanding the Brazilian legislation by detecting special nodes, namely the "bridge-like nodes", that play an important role in the structure of the graph.},
   author = {Rilder S. Pires and Henrique Santos and Ricardo Guedes and João A. Monteiro Neto and Carlos Caminha and Vasco Furtado},
   issn = {1613-0073},
   keywords = {Article},
   month = {11},
   publisher = {Joint Proceedings of the 3th International Workshop on Artificial Intelligence Technologies for Legal Documents (AI4LEGAL 2022) and the 1st International Workshop on Knowledge Graph Summarization (KGSum 2022)},
   title = {Building and Analyzing the Brazilian Legal Knowledge Graph},
   url = {https://dspace.rpi.edu/handle/20.500.13015/6362},
   year = {2022},
}
@article{Tan2023,
   abstract = {We present a reality check on large language models and inspect the promise of retrieval augmented language models in comparison. Such language models are semi-parametric, where models integrate model parameters and knowledge from external data sources to make their predictions, as opposed to the parametric nature of vanilla large language models. We give initial experimental findings that semi-parametric architectures can be enhanced with views, a query analyzer/planner, and provenance to make a significantly more powerful system for question answering in terms of accuracy and efficiency, and potentially for other NLP tasks},
   author = {Wang-Chiew Tan and Yuliang Li and Pedro Rodriguez and Richard James and Victoria Xi and Alon Lin and Scott Halevy and Meta Yih},
   month = {6},
   title = {Reimagining Retrieval Augmented Language Models for Answering Queries},
   url = {https://arxiv.org/abs/2306.01061v1},
   year = {2023},
}
@article{Junior2019,
   abstract = {This paper presents our experience on building RDF knowledge graphs for an industrial use case in the legal domain. The information contained in legal information systems are often accessed through simple keyword interfaces and presented as a simple list of hits. In order to improve search accuracy one may avail of knowledge graphs, where the semantics of the data can be made explicit. Significant research effort has been invested in the area of building knowledge graphs from semi-structured text documents, such as XML, with the prevailing approach being the use of mapping languages. In this paper, we present a semantic model for representing legal documents together with an industrial use case. We also present a set of use case requirements based on the proposed semantic model, which are used to compare and discuss the use of state-of-the-art mapping languages for building knowledge graphs for legal data.},
   author = {Ademar Crotti Junior and Fabrizio Orlandi and Declan O'Sullivan and Christian Dirschl and Quentin Reul},
   issn = {16130073},
   journal = {CEUR Workshop Proceedings},
   keywords = {Legal Knowledge Graphs,Legal se-mantic model,Legal semantic model,Mapping languages,Mapping languages·},
   month = {11},
   publisher = {CEUR-WS},
   title = {Using Mapping Languages for Building Legal Knowledge Graphs from XML Files},
   volume = {2599},
   url = {https://arxiv.org/abs/1911.07673v1},
   year = {2019},
}
@article{Alchourron1981,
   abstract = {We study some of the ways in which the imposition of a partial ordering on a code of laws or regulations can serve to overcome logical imperfections in the code itself. In particular, we first show how partial orderings of a code, and derivative orderings of its...},
   author = {Carlos E. Alchourrón and David Makinson},
   doi = {10.1007/978-94-009-8484-4_5},
   isbn = {978-94-009-8484-4},
   journal = {New Studies in Deontic Logic},
   pages = {125-148},
   publisher = {Springer, Dordrecht},
   title = {Hierarchies of Regulations and their Logic},
   url = {https://link.springer.com/chapter/10.1007/978-94-009-8484-4_5},
   year = {1981},
}
@article{Shelton2006,
   abstract = {Systems of law usually establish a hierarchy of norms based on the particular source from which the norms derive. In national legal systems, it is commonplace for the fundamental values of society to be given constitutional status and afforded precedence in the event of a conflict with norms enacted by legislation or adopted by administrative regulation; administrative rules themselves must conform to legislative mandates, while written law usually takes precedence over unwritten law and legal norms prevail over nonlegal (political or moral) rules. Norms of equal status must be balanced and reconciled to the extent possible. The mode of legal reasoning applied in practice is thus naturally hierarchical, establishing relationships and order between normative statements and levels of authority.},
   author = {Dinah Shelton},
   doi = {10.1017/S0002930000016675},
   issn = {0002-9300},
   issue = {2},
   journal = {American Journal of International Law},
   pages = {291-323},
   publisher = {Cambridge University Press},
   title = {Normative Hierarchy in International Law},
   volume = {100},
   url = {https://www.cambridge.org/core/journals/american-journal-of-international-law/article/abs/normative-hierarchy-in-international-law/B17B59F4D46511BE55786728214856BB},
   year = {2006},
}
@article{Li2023,
   abstract = {Graph plays a significant role in representing and analyzing complex relationships in real-world applications such as citation networks, social networks, and biological data. Recently, Large Language Models (LLMs), which have achieved tremendous success in various domains, have also been leveraged in graph-related tasks to surpass traditional Graph Neural Networks (GNNs) based methods and yield state-of-the-art performance. In this survey, we first present a comprehensive review and analysis of existing methods that integrate LLMs with graphs. First of all, we propose a new taxonomy, which organizes existing methods into three categories based on the role (i.e., enhancer, predictor, and alignment component) played by LLMs in graph-related tasks. Then we systematically survey the representative methods along the three categories of the taxonomy. Finally, we discuss the remaining limitations of existing studies and highlight promising avenues for future research. The relevant papers are summarized and will be consistently updated at: https://github.com/yhLeeee/Awesome-LLMs-in-Graph-tasks.},
   author = {Yuhan Li and Zhixun Li and Peisong Wang and Jia Li and Xiangguo Sun and Hong Cheng and Jeffrey Xu Yu},
   month = {11},
   title = {A Survey of Graph Meets Large Language Model: Progress and Future Directions},
   url = {https://arxiv.org/abs/2311.12399v2},
   year = {2023},
}
@article{ShaoYunqiu2023,
   abstract = {Legal case retrieval is a special Information Retrieval (IR) task focusing on legal case documents. Depending on the downstream tasks of the retrieved case documents, users’ information needs in le...},
   author = {ShaoYunqiu and LiHaitao and WuYueyue and LiuYiqun and AiQingyao and MaoJiaxin and MaYixiao and MaShaoping},
   doi = {10.1145/3626093},
   issn = {1046-8188},
   journal = {ACM Transactions on Information Systems},
   keywords = {legal case retrieval,search intent,taxonomy,user behavior,user satisfaction},
   month = {1},
   publisher = {
ACM
PUB27
New York, NY
},
   title = {An Intent Taxonomy of Legal Case Retrieval},
   url = {https://dl.acm.org/doi/10.1145/3626093},
   year = {2023},
}
@article{Pandya2019,
   abstract = {Manual Summarization of large bodies of text involves a lot of human effort and time, especially in the legal domain. Lawyers spend a lot of time preparing legal briefs of their clients' case files. Automatic Text summarization is a constantly evolving field of Natural Language Processing(NLP), which is a subdiscipline of the Artificial Intelligence Field. In this paper a hybrid method for automatic text summarization of legal cases using k-means clustering technique and tf-idf(term frequency-inverse document frequency) word vectorizer is proposed. The summary generated by the proposed method is compared using ROGUE evaluation parameters with the case summary as prepared by the lawyer for appeal in court. Further, suggestions for improving the proposed method are also presented.},
   author = {Varun Pandya},
   doi = {10.5121/csit.2019.91004},
   keywords = {Automatic Text Summarization,Legal domain,k-means clustering,tf-idf word vectors},
   month = {8},
   pages = {37-43},
   publisher = {Academy and Industry Research Collaboration Center (AIRCC)},
   title = {Automatic Text Summarization of Legal Cases: A Hybrid Approach},
   url = {http://arxiv.org/abs/1908.09119 http://dx.doi.org/10.5121/csit.2019.91004},
   year = {2019},
}
@article{Song2020,
   abstract = {In contract analysis and contract automation, a Knowledge Base (KB) of legal entities is fundamental for performing tasks such as contract verification, contract generation and contract analytic. However, such a knowledge base does not always exist nor can be produced in a short time. In this paper, we propose a clustering-based approach to automatically generate a reliable knowledge base of legal entities from given contracts without any supplemental references. The proposed method is robust to different types of errors produced by preprocessing such as Optical Character Recognition (OCR) and Named Entity Recognition (NER), as well as editing errors such as typos. We evaluate our method on a dataset that consists of 800 real contracts with various qualities from 15 clients. Compared to the collected ground-truth data, our method is able to recall 84% of the knowledge.},
   author = {Fuqi Song and Eric De La Clergerie},
   doi = {10.1109/BIGDATA50022.2020.9378166},
   isbn = {9781728162515},
   journal = {Proceedings - 2020 IEEE International Conference on Big Data, Big Data 2020},
   keywords = {clustering,contract analysis,contract automation,legal entity extraction,ontology population},
   month = {12},
   pages = {2149-2152},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Clustering-based Automatic Construction of Legal Entity Knowledge Base from Contracts},
   year = {2020},
}
@article{Ostling2023,
   abstract = {We introduce the Cambridge Law Corpus (CLC), a corpus for legal AI research. It consists of over 250 000 court cases from the UK. Most cases are from the 21st century, but the corpus includes cases as old as the 16th century. This paper presents the first release of the corpus, containing the raw text and meta-data. Together with the corpus, we provide annotations on case outcomes for 638 cases, done by legal experts. Using our annotated data, we have trained and evaluated case outcome extraction with GPT-3, GPT-4 and RoBERTa models to provide benchmarks. We include an extensive legal and ethical discussion to address the potentially sensitive nature of this material. As a consequence, the corpus will only be released for research purposes under certain restrictions.},
   author = {Andreas Östling and Holli Sargeant and Huiyuan Xie and Ludwig Bull and Alexander Terenin and Leif Jonsson and Måns Magnusson and Felix Steffek},
   doi = {10.17863/CAM.100221},
   month = {9},
   title = {The Cambridge Law Corpus: A Corpus for Legal AI Research},
   url = {https://arxiv.org/abs/2309.12269v3},
   year = {2023},
}
@article{Iyer2022,
   abstract = {The increasing focus on Web 3.0 is leading to automated creation and enrichment of ontologies and other linked datasets. Alongside automation, quality evaluation of enriched ontologies can impact software reliability and reuse. Current quality evaluation approaches oftentimes seek to evaluate ontologies in either syntactic (degree of following ontology development guidelines) or semantic (degree of semantic validity of enriched concepts/relations) aspects. This paper proposes an ontology quality evaluation framework consisting of: (a) SynEvaluator and (b) SemValidator for evaluating syntactic and semantic aspects of ontologies respectively. SynEvaluator allows dynamic task-specific creation and updation of syntactic rules at run-time without any need for programming. SemValidator uses Twitter-based expertise of validators for semantic evaluation. The efficacy and validity of the framework is shown empirically on multiple ontologies.},
   author = {Vivek Iyer and Lalit Mohan Sanagavarapu and Y. Raghu Reddy},
   doi = {10.1007/978-3-030-97532-6_5/COVER},
   isbn = {9783030975319},
   issn = {18650937},
   journal = {Communications in Computer and Information Science},
   keywords = {Crowdsourcing,Ontology quality evaluation,Semantic validation,Syntactic evaluation,Twitter-based expertise},
   pages = {73-93},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {A Framework for Syntactic and Semantic Quality Evaluation of Ontologies},
   volume = {1549 CCIS},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-97532-6_5},
   year = {2022},
}
@article{Ramesh2023,
   abstract = {Indian court legal texts and processes are essential towards the integrity of the judicial system and towards maintaining the social and political order of the nation. Due to the increase in number of pending court cases, there is an urgent need to develop tools to automate many of the legal processes with the knowledge of artificial intelligence. In this paper, we employ knowledge extraction techniques, specially the named entity extraction of legal entities within court case judgements. We evaluate several state of the art architectures in the realm of sequence labeling using models trained on a curated dataset of legal texts. We observe that a Bi-LSTM model trained on Flair Embeddings achieves the best results, and we also publish the BIO formatted dataset as part of this paper.},
   author = {Vinay N Ramesh and Rohan Eswara},
   isbn = {2306.02182v1},
   month = {6},
   title = {FlairNLP at SemEval-2023 Task 6b: Extraction of Legal Named Entities from Legal Texts using Contextual String Embeddings},
   url = {https://arxiv.org/abs/2306.02182v1},
   year = {2023},
}
@article{Carvalho2017,
   abstract = {In the context of the Competition on Legal Information Extraction/Entailment (COLIEE), we propose a method comprising the necessary steps for finding relevant documents to a legal question and deciding on textual entailment evidence to provide a correct answer. The proposed method is based on the combination of several lexical and morphological characteristics, to build a language model and a set of features for Machine Learning algorithms. We provide a detailed study on the proposed method performance and failure cases, indicating that it is competitive with state-of-the-art approaches on Legal Information Retrieval and Question Answering, while not needing extensive training data nor depending on expert produced knowledge. The proposed method achieved significant results in the competition, indicating a substantial level of adequacy for the tasks addressed.},
   author = {Danilo S. Carvalho and Minh Tien Nguyen and Chien Xuan Tran and Minh Le Nguyen},
   doi = {10.1007/978-3-319-50953-2_21/COVER},
   isbn = {9783319509525},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {295-311},
   publisher = {Springer Verlag},
   title = {Lexical-morphological modeling for legal text analysis},
   volume = {10091 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-319-50953-2_21},
   year = {2017},
}
@article{Cutting-Decelle2018,
   abstract = {Problems faced by international standardization bodies become more and more crucial as the number and the size of the standards they produce increase. Sometimes, also, the lack of coordination among the committees in charge of the development of standards may lead to overlaps, mistakes or incompatibilities in the documents. The aim of this study is to present a methodology enabling an automatic extraction of the technical concepts (terms) found in normative documents, through the use of semantic tools coming from the field of language processing. The first part of the paper provides a description of the standardization world, its structure, its way of working and the problems faced; we then introduce the concepts of semantic annotation, information extraction and the software tools available in this domain. The next section explains the concept of ontology and its potential use in the field of standardization. We propose here a methodology enabling the extraction of technical information from a given normative corpus, based on a semantic annotation process done according to reference ontologies. The application to the ISO 15531 MANDATE corpus provides a first use case of the methodology described in this paper. The paper ends with the description of the first experimental results produced by this approach, and with some issues and perspectives, notably its application to other standards and, or Technical Committees and the possibility offered to create pre-defined technical dictionaries of terms.},
   author = {A. F. Cutting-Decelle and A. Digeon and R. I. Young and J. L. Barraud and P. Lamboley},
   keywords = {ISO 15531 MANDATE,Key-words : international standardization,SDO,industrial systems,information extraction,ontologies,semantic annotation},
   month = {6},
   title = {Extraction Of Technical Information From Normative Documents Using Automated Methods Based On Ontologies : Application To The Iso 15531 Mandate Standard - Methodology And First Results},
   url = {https://arxiv.org/abs/1806.02242v2},
   year = {2018},
}
@article{Hassanzadeh2011,
   abstract = {The Semantic Web is an extension of the current web in which information is given well-defined meaning. The perspective of Semantic Web is to promote the quality and intelligence of the current web by changing its contents into machine understandable form. Therefore, semantic level information is one of the cornerstones of the Semantic Web. The process of adding semantic metadata to web resources is called Semantic Annotation. There are many obstacles against the Semantic Annotation, such as multilinguality, scalability, and issues which are related to diversity and inconsistency in content of different web pages. Due to the wide range of domains and the dynamic environments that the Semantic Annotation systems must be performed on, the problem of automating annotation process is one of the significant challenges in this domain. To overcome this problem, different machine learning approaches such as supervised learning, unsupervised learning and more recent ones like, semi-supervised learning and active learning have been utilized. In this paper we present an inclusive layered classification of Semantic Annotation challenges and discuss the most important issues in this field. Also, we review and analyze machine learning applications for solving semantic annotation problems. For this goal, the article tries to closely study and categorize related researches for better understanding and to reach a framework that can map machine learning techniques into the Semantic Annotation challenges and requirements.},
   author = {Hamed Hassanzadeh and MohammadReza Keyvanpour},
   doi = {10.5121/ijwest.2011.2203},
   issue = {2},
   journal = {International journal of Web & Semantic Technology},
   keywords = {Machine Learning,Semantic Annotation,Semantic Web},
   month = {4},
   pages = {27-38},
   publisher = {Academy and Industry Research Collaboration Center (AIRCC)},
   title = {A Machine Learning Based Analytical Framework for Semantic Annotation Requirements},
   volume = {2},
   url = {http://arxiv.org/abs/1104.4950 http://dx.doi.org/10.5121/ijwest.2011.2203},
   year = {2011},
}
@article{Eliot2020,
   abstract = {A framework is proposed that seeks to identify and establish a set of robust autonomous levels articulating the realm of Artificial Intelligence and Legal Reasoning (AILR). Doing so provides a sound and parsimonious basis for being able to assess progress in the application of AI to the law, and can be utilized by scholars in academic pursuits of AI legal reasoning, along with being used by law practitioners and legal professionals in gauging how advances in AI are aiding the practice of law and the realization of aspirational versus achieved results. A set of seven levels of autonomy for AI and Legal Reasoning are meticulously proffered and mindfully discussed.},
   author = {Lance Eliot},
   keywords = {AI,artificial intelligence,autonomous levels,autonomy,framework,legal reasoning,ontology,taxonomy,the law},
   month = {8},
   title = {An Ontological AI-and-Law Framework for the Autonomous Levels of AI Legal Reasoning},
   url = {https://arxiv.org/abs/2008.07328v1},
   year = {2020},
}
@article{Prasad2023,
   abstract = {Automatic legal judgment prediction and its explanation suffer from the problem of long case documents exceeding tens of thousands of words, in general, and having a non-uniform structure. Predicting judgments from such documents and extracting their explanation becomes a challenging task, more so on documents with no structural annotation. We define this problem as "scarce annotated legal documents" and explore their lack of structural information and their long lengths with a deep-learning-based classification framework which we call MESc; "Multi-stage Encoder-based Supervised with-clustering"; for judgment prediction. We explore the adaptability of LLMs with multi-billion parameters (GPT-Neo, and GPT-J) to legal texts and their intra-domain(legal) transfer learning capacity. Alongside this, we compare their performance and adaptability with MESc and the impact of combining embeddings from their last layers. For such hierarchical models, we also propose an explanation extraction algorithm named ORSE; Occlusion sensitivity-based Relevant Sentence Extractor; based on the input-occlusion sensitivity of the model, to explain the predictions with the most relevant sentences from the document. We explore these methods and test their effectiveness with extensive experiments and ablation studies on legal documents from India, the European Union, and the United States with the ILDC dataset and a subset of the LexGLUE dataset. MESc achieves a minimum total performance gain of approximately 2 points over previous state-of-the-art proposed methods, while ORSE applied on MESc achieves a total average gain of 50% over the baseline explainability scores.},
   author = {Nishchal Prasad and Mohand Boughanem and Taoufik Dkaki},
   journal = {Proceedings of ACM Conference (Conference'17)},
   keywords = {Extractive explanation,Legal judgment prediction,Long document classification,Multi-stage classification framework,Scarce annotated documents},
   month = {9},
   title = {A Hierarchical Neural Framework for Classification and its Explanation in Large Unstructured Legal Documents},
   volume = {1},
   url = {https://arxiv.org/abs/2309.10563v2},
   year = {2023},
}
@article{Kalamkar2022,
   abstract = {In populous countries, pending legal cases have been growing exponentially. There is a need for developing techniques for processing and organizing legal documents. In this paper, we introduce a new corpus for structuring legal documents. In particular, we introduce a corpus of legal judgment documents in English that are segmented into topical and coherent parts. Each of these parts is annotated with a label coming from a list of pre-defined Rhetorical Roles. We develop baseline models for automatically predicting rhetorical roles in a legal document based on the annotated corpus. Further, we show the application of rhetorical roles to improve performance on the tasks of summarization and legal judgment prediction. We release the corpus and baseline model code along with the paper.},
   author = {Prathamesh Kalamkar and Aman Tiwari and Astha Agarwal and Saurabh Karn and Smita Gupta and Vivek Raghavan and Ashutosh Modi},
   isbn = {9791095546726},
   journal = {2022 Language Resources and Evaluation Conference, LREC 2022},
   keywords = {Legal Document Segmentation,Legal NLP,Rhetorical Roles},
   month = {1},
   pages = {4420-4429},
   publisher = {European Language Resources Association (ELRA)},
   title = {Corpus for Automatic Structuring of Legal Documents},
   url = {https://arxiv.org/abs/2201.13125v2},
   year = {2022},
}
@article{Mahoney2019,
   abstract = {Companies regularly spend millions of dollars producing electronically-stored documents in legal matters. Over the past two decades, attorneys have been using a variety of technologies to conduct this exercise, and most recently, parties on both sides of the 'legal aisle' are accepting the use of machine learning techniques like text classification to cull massive volumes of data and to identify responsive documents for use in these matters. While text classification is regularly used to reduce the discovery costs in legal matters, text classification also faces a peculiar perception challenge: amongst lawyers, this technology is sometimes looked upon as a black box Put simply, very little information is provided for attorneys to understand why documents are classified as responsive. In recent years, a group of AI and Machine Learning researchers have been actively researching Explainable AI. In an explainable AI system, actions or decisions are human understandable. In legal 'document review' scenarios, a document can be identified as responsive, as long as one or more of the text snippets (small passages of text) in a document are deemed responsive. In these scenarios, if text classification can be used to locate these responsive snippets, then attorneys could easily evaluate the model's document classification decision. When deployed with defined and explainable results, text classification can drastically enhance the overall quality and speed of the document review process by reducing the time it takes to review documents. Moreover, explainable predictive coding provides lawyers with greater confidence in the results of that supervised learning task. This paper describes a framework for explainable text classification as a valuable tool in legal services: for enhancing the quality and efficiency of legal document review and for assisting in locating responsive snippets within responsive documents. This framework has been implemented in our legal analytics product, which has been used in hundreds of legal matters. We also report our experimental results using the data from an actual legal matter that used this type of document review.},
   author = {Christian J. Mahoney and Jianping Zhang and Nathaniel Huber-Fliflet and Peter Gronvall and Haozhen Zhao},
   doi = {10.1109/BIGDATA47090.2019.9005659},
   isbn = {9781728108582},
   journal = {Proceedings - 2019 IEEE International Conference on Big Data, Big Data 2019},
   keywords = {XAI,explainable AI,false negatives,legal document review,machine learning,predictive coding,text categorization,text classification},
   month = {12},
   pages = {1858-1867},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A Framework for Explainable Text Classification in Legal Document Review},
   year = {2019},
}
@article{Soman2023,
   abstract = {Large Language Models (LLMs) have been driving progress in AI at an unprecedented rate, yet still face challenges in knowledge-intensive domains like biomedicine. Solutions such as pre-training and domain-specific fine-tuning add substantial computational overhead, and the latter require domain-expertise. External knowledge infusion is task-specific and requires model training. Here, we introduce a task-agnostic Knowledge Graph-based Retrieval Augmented Generation (KG-RAG) framework by leveraging the massive biomedical KG SPOKE with LLMs such as Llama-2-13b, GPT-3.5-Turbo and GPT-4, to generate meaningful biomedical text rooted in established knowledge. KG-RAG consistently enhanced the performance of LLMs across various prompt types, including one-hop and two-hop prompts, drug repurposing queries, biomedical true/false questions, and multiple-choice questions (MCQ). Notably, KG-RAG provides a remarkable 71% boost in the performance of the Llama-2 model on the challenging MCQ dataset, demonstrating the framework's capacity to empower open-source models with fewer parameters for domain-specific questions. Furthermore, KG-RAG enhanced the performance of proprietary GPT models, such as GPT-3.5 which exhibited improvement over GPT-4 in context utilization on MCQ data. Our approach was also able to address drug repurposing questions, returning meaningful repurposing suggestions. In summary, the proposed framework combines explicit and implicit knowledge of KG and LLM, respectively, in an optimized fashion, thus enhancing the adaptability of general-purpose LLMs to tackle domain-specific questions in a unified framework.},
   author = {Karthik Soman and Peter W Rose and John H Morris and Rabia E Akbas and Brett Smith and Braian Peetoom and Catalina Villouta-Reyes and Gabriel Cerono and Yongmei Shi and Angela Rizk-Jackson and Sharat Israni and Charlotte A Nelson and Sui Huang and Sergio E Baranzini and Francisco San Francisco},
   month = {11},
   title = {Biomedical knowledge graph-enhanced prompt generation for large language models},
   url = {https://arxiv.org/abs/2311.17330v1},
   year = {2023},
}
@article{Sarica2021,
   abstract = {Engineers often need to discover and learn designs from unfamiliar domains for inspiration or other particular uses. However, the complexity of the technical design descriptions and the unfamiliarity to the domain make it hard for engineers to comprehend the function, behavior, and structure of a design. To help engineers quickly understand a complex technical design description new to them, one approach is to represent it as a network graph of the design-related entities and their relations as an abstract summary of the design. While graph or network visualizations are widely adopted in the engineering design literature, the challenge remains in retrieving the design entities and deriving their relations. In this paper, we propose a network mapping method that is powered by Technology Semantic Network (TechNet). Through a case study, we showcase how TechNet’s unique characteristic of being trained on a large technology-related data source advantages itself over common-sense knowledge bases, such as WordNet and ConceptNet, for design knowledge representation.},
   author = {Serhad Sarica and Jianxi Luo},
   doi = {10.1017/PDS.2021.104},
   issn = {2732-527X},
   journal = {Proceedings of the Design Society},
   keywords = {Design informatics,Knowledge Representation,Semantic data processing,Technology Semantic Network,Visualisation},
   pages = {1043-1052},
   publisher = {Cambridge University Press},
   title = {DESIGN KNOWLEDGE REPRESENTATION WITH TECHNOLOGY SEMANTIC NETWORK},
   volume = {1},
   url = {https://www.cambridge.org/core/journals/proceedings-of-the-design-society/article/design-knowledge-representation-with-technology-semantic-network/6BB0FB6F96756646EAE3F6B95A096F60},
   year = {2021},
}
@article{Zimmermann2012,
   abstract = {We describe a generic framework for representing and reasoning with annotated Semantic Web data, a task becoming more important with the recent increased amount of inconsistent and non-reliable meta-data on the Web. We formalise the annotated language, the corresponding deductive system and address the query answering problem. Previous contributions on specific RDF annotation domains are encompassed by our unified reasoning formalism as we show by instantiating it on (i) temporal, (ii) fuzzy, and (iii) provenance annotations. Moreover, we provide a generic method for combining multiple annotation domains allowing to represent, e.g., temporally-annotated fuzzy RDF. Furthermore, we address the development of a query language - AnQL - that is inspired by SPARQL, including several features of SPARQL 1.1 (subqueries, aggregates, assignment, solution modifiers) along with the formal definitions of their semantics. © 2011 Elsevier B.V. All rights reserved.},
   author = {Antoine Zimmermann and Nuno Lopes and Axel Polleres and Umberto Straccia},
   doi = {10.1016/J.WEBSEM.2011.08.006},
   issn = {1570-8268},
   journal = {Journal of Web Semantics},
   keywords = {Annotations,Query,RDF,RDFS,SPARQL},
   month = {3},
   pages = {72-95},
   publisher = {Elsevier},
   title = {A general framework for representing, reasoning and querying with annotated Semantic Web data},
   volume = {11},
   year = {2012},
}
@article{Bus2019,
   abstract = {Manually checking models for compliance against building regulation is a time-consuming task for architects and construction engineers. There is thus a need for algorithms that process information from construction projects and report non-compliant elements. Still automated code-compliance checking raises several obstacles. Building regulations are usually published as human readable texts and their content is often ambiguous or incomplete. Also, the vocabulary used for expressing such regulations is very different from the vocabularies used to express Building Information Models (BIM). Furthermore, the high level of details associated to BIM-contained geometries induces complex calculations. Finally, the level of complexity of the IFC standard also hinders the automation of IFC processing tasks. Model chart, formal rules and pre-processors approach allows translating construction regulations into semantic queries. We further demonstrate the usefulness of this approach through several use cases. We argue our approach is a step forward in bridging the gap between regulation texts and automated checking algorithms. Finally with the recent building ontology BOT recommended by the W3C Linked Building Data Community Group, we identify perspectives for standardizing and extending our approach.},
   author = {Nicolas Bus and Ana Roxin and Guillaume Picinbono and Muhammad Fahad},
   issn = {16130073},
   journal = {CEUR Workshop Proceedings},
   keywords = {Automated compliance checking,BIM,Construction regulations,IFC,IfcOWL,OWL,SWRL,Semantic rules},
   month = {10},
   pages = {6-15},
   publisher = {CEUR-WS},
   title = {Towards French Smart Building Code: Compliance Checking Based on Semantic Rules},
   volume = {2159},
   url = {https://arxiv.org/abs/1910.00334v1},
   year = {2019},
}
@article{Sleimi2021,
   abstract = {Semantic legal metadata provides information that helps with understanding and interpreting legal provisions. Such metadata is therefore important for the systematic analysis of legal requirements. However, manually enhancing a large legal corpus with semantic metadata is prohibitively expensive. Our work is motivated by two observations: (1) the existing requirements engineering (RE) literature does not provide a harmonized view on the semantic metadata types that are useful for legal requirements analysis; (2) automated support for the extraction of semantic legal metadata is scarce, and it does not exploit the full potential of artificial intelligence technologies, notably natural language processing (NLP) and machine learning (ML). Our objective is to take steps toward overcoming these limitations. To do so, we review and reconcile the semantic legal metadata types proposed in the RE literature. Subsequently, we devise an automated extraction approach for the identified metadata types using NLP and ML. We evaluate our approach through two case studies over the Luxembourgish legislation. Our results indicate a high accuracy in the generation of metadata annotations. In particular, in the two case studies, we were able to obtain precision scores of 97,2% and 82,4%, and recall scores of 94,9% and 92,4%.},
   author = {Amin Sleimi and Nicolas Sannier and Mehrdad Sabetzadeh and Lionel Briand and Marcello Ceci and John Dann},
   doi = {10.1007/S10664-020-09933-5/TABLES/11},
   issn = {15737616},
   issue = {3},
   journal = {Empirical Software Engineering},
   keywords = {Legal requirements,Natural language processing (NLP),Semantic legal metadata},
   month = {5},
   pages = {1-50},
   publisher = {Springer},
   title = {An automated framework for the extraction of semantic legal metadata from legal texts},
   volume = {26},
   url = {https://link.springer.com/article/10.1007/s10664-020-09933-5},
   year = {2021},
}
@article{Adhikary2023,
   abstract = {The escalating number of pending cases is a growing concern world-wide. Recent advancements in digitization have opened up possibilities for leveraging artificial intelligence (AI) tools in the processing of legal documents. Adopting a structured representation for legal documents, as opposed to a mere bag-of-words flat text representation, can significantly enhance processing capabilities. With the aim of achieving this objective, we put forward a set of diverse attributes for criminal case proceedings. We use a state-of-the-art sequence labeling framework to automatically extract attributes from the legal documents. Moreover, we demonstrate the efficacy of the extracted attributes in a downstream task, namely legal judgment prediction.},
   author = {Subinay Adhikary and Sagnik Das and Sagnik Saha and Procheta Sen and Dwaipayan Roy and Kripabandhu Ghosh},
   doi = {10.1016/j.jksuci},
   journal = {Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (Conference acronym 'XX)},
   keywords = {Attribute Extraction,Legal AI},
   month = {10},
   title = {Automated Attribute Extraction from Legal Proceedings},
   volume = {1},
   url = {https://arxiv.org/abs/2310.12131v1},
   year = {2023},
}
@article{Keshavarz2022,
   abstract = {Named entity recognition (NER) is a fundamental task for several important applications such as knowledge base construction and semantic search. So far, the focus has been on building machine learning models, which identify generic named entities (e.g., person, date). Such models can be used off-the-shelf without requiring ground truth labels for training. However, such models cannot generalize to specialized domains that have domain-specific named entities (e.g., the legal domain). In these cases, it is inevitable to generate ground truth data and experiment with a variety of models in order to achieve good performance. Motivated by a real use case from the financial sector, we discuss the approach and lessons learned when solving the NER problem in the legal domain. This task is particularly challenging because it requires extensive human expertise to produce high quality ground-truth labels. For solving the legal domain NER problem, we first crawl a large dataset of legal documents and then introduce a semi-automated process to generate high-quality labels for a set of eleven predefined named entities. We validate that the proposed approach achieves high quality labels that outperform popular out-of-the-box NER methods. On top of that, our method once followed, can generate ground truth labels for the pre-defined named entities for an unbounded number of documents. Next, we experiment with a set of models and training procedures and report their performance on the NER task. Our experimental evaluation confirms that most of the models can generalize very well, achieving F1-score between 86% and 98.9%. The dataset, the labels produced by human annotators and our semi-supervised approach, as well as our code are made available to the research community.},
   author = {Hossein Keshavarz and Zografoula Vagena and Pigi Kouki and Ilias Fountalis and Mehdi Mabrouki and Aziz Belaweid and Nikolaos Vasiloglou},
   doi = {10.1109/BIGDATA55660.2022.10020873},
   isbn = {9781665480451},
   journal = {Proceedings - 2022 IEEE International Conference on Big Data, Big Data 2022},
   pages = {2024-2033},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Named Entity Recognition in Long Documents: An End-to-end Case Study in the Legal Domain},
   year = {2022},
}
@article{Alshugran2016,
   abstract = {Some organizations use software applications to manage their customers' personal, medical, or financial information. In the United States, those software applications are obligated to preserve users' privacy and to comply with the United States federal privacy laws and regulations. To formally guarantee compliance with those regulations, it is essential to extract and model the privacy rules from the text of the law using a formal framework. In this work we propose a goal-oriented framework for modeling and extracting the privacy requirements from regulatory text using natural language processing techniques.},
   author = {Tariq Alshugran and Julius Dichter},
   doi = {10.5121/hiij.2016.5101},
   issue = {1},
   journal = {Health Informatics - An International Journal},
   keywords = {Data Modelling,Data privacy,Law Formalization,Privacy Policies,Role engineering},
   month = {3},
   pages = {1-10},
   publisher = {Academy and Industry Research Collaboration Center (AIRCC)},
   title = {A Framework for Extracting and Modeling HIPAA Privacy Rules for Healthcare Applications},
   volume = {5},
   url = {http://arxiv.org/abs/1603.02964 http://dx.doi.org/10.5121/hiij.2016.5101},
   year = {2016},
}
@article{Danenas2019,
   abstract = {Being among the best-selling and most advanced features of model-driven development, model-to-model transformation could help improving one of the most time- and resource-consuming efforts in the process of model-driven information systems engineering, namely, discovery and specification of business vocabularies and business rules within the problem domain. Nonetheless, despite the relatively high levels of automation throughout the whole systems’ model-driven development process, business modeling stage remains among the most under re-searched areas throughout the whole process. In this paper, we introduce a novel natural language processing (NLP) technique to one of our latest developments for the automatic extraction of SBVR business vocabularies and business rules from UML use case diagrams. This development remains arguably the most comprehensive development of this kind currently available in public. The experiment provided proof that the developed NLP enhancement delivered even better extraction results compared to the already satisfactory performance of the previous development. This work contributes to the research in the areas of model transformations and NLP within the model-driven development of information systems, and beyond.},
   author = {Paulius Danenas and Tomas Skersys and Rimantas Butleris},
   doi = {10.1145/3368640.3368641},
   isbn = {9781450372923},
   journal = {ACM International Conference Proceeding Series},
   keywords = {Business rules,Business vocabulary,Model transformation,Natural language processing,SBVR,UML,Use case diagram},
   month = {11},
   publisher = {Association for Computing Machinery},
   title = {Enhancing the extraction of SBVR business vocabularies and business rules from UML use case diagrams with natural language processing},
   url = {https://dl.acm.org/doi/10.1145/3368640.3368641},
   year = {2019},
}
@article{Wang2023,
   abstract = {Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model. In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text "Columbus is a city" is transformed to generate the text sequence "@@Columbus## is a city", where special tokens @@## marks the entity to extract. To efficiently address the "hallucination" issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a labeled entity tag. We conduct experiments on five widely adopted NER datasets, and GPT-NER achieves comparable performances to fully supervised baselines, which is the first time as far as we are concerned. More importantly, we find that GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, GPT-NER performs significantly better than supervised models. This demonstrates the capabilities of GPT-NER in real-world NER applications where the number of labeled examples is limited.},
   author = {Shuhe Wang and Xiaofei Sun and Xiaoya Li and Rongbin Ouyang and Fei Wu and Tianwei Zhang and Jiwei Li and Guoyin Wang},
   month = {4},
   title = {GPT-NER: Named Entity Recognition via Large Language Models},
   url = {https://arxiv.org/abs/2304.10428v4},
   year = {2023},
}
@article{Rodrigues2019,
   abstract = {Over the last 30 years, AI & Law has provided breakthroughs in studies involving case-based reasoning, rule-based reasoning, information retrieval and, most recently, conceptual models for knowledge representation and reasoning, known as Legal Ontologies. Ontologies have been widely used by legal practitioners, scholars, and lay people in a variety of situations, such as simulating legal actions, semantic search and indexing, and to keep up-to-date with the continual change of laws and regulations. Given the high number of legal ontologies produced, the need to summarize this research realm through a well-defined methodological procedure is urgent need. This study presents the results of a systematic mapping of the literature, aiming at categorizing legal ontologies along certain dimensions, such as purpose, level of generality, underlying legal theories, among other aspects. The reasons to carry out a systematic mapping are twofold: in addition to explaining the maturation of the area over recent decades, it helps to avoid the old problem of reinventing the wheel. Through organizing and classifying what has already been produced, it is possible to realize that the development of legal ontologies can rise to the level of reusability where prefabricated models might be coupled with new and more complex ontologies for practical law.},
   author = {Cleyton Mário de Oliveira Rodrigues and Frederico Luiz Gonçalves de Freitas and Emanoel Francisco Spósito Barreiros and Ryan Ribeiro de Azevedo and Adauto Trigueiro de Almeida Filho},
   doi = {10.1016/J.ESWA.2019.04.009},
   issn = {0957-4174},
   journal = {Expert Systems with Applications},
   keywords = {Legal expert system,Legal ontology,Legal theory,Semantic web,Systematic mapping study},
   month = {9},
   pages = {12-30},
   publisher = {Pergamon},
   title = {Legal ontologies over time: A systematic mapping study},
   volume = {130},
   year = {2019},
}
@article{Casellas2012,
   abstract = {The application of Linked Open Data (LOD) principles to legal information (URI naming of resources, assertions about named relationships between resources or between resources and data values, and the possibility to easily extend, update and modify these relationships and resources) could offer better access and understanding of regulatory information to individual citizens, businesses and government agencies and administrations, and allow its sharing and reuse across applications, organizations and jurisdictions. © 2012 Authors.},
   author = {Nuria Casellas and Thomas R. Bruce and Sara S. Frug and Sarah Bouwman and Dallas Dias and Jie Lin and Sharvari Marathe and Krithi Rai and Ankit Singh and Debraj Sinha and Sanjna Venkataraman},
   doi = {10.1145/2307729.2307785},
   isbn = {9781450314039},
   journal = {ACM International Conference Proceeding Series},
   keywords = {RDF,SKOS,information extraction,linked data},
   pages = {280-281},
   title = {Linked legal data: Improving access to regulations},
   url = {https://dl.acm.org/doi/10.1145/2307729.2307785},
   year = {2012},
}
@article{Gordon2010,
   abstract = {The Legal Knowledge Interchange Format (LKIF) is an XML Schema for representing theories and arguments (proofs) constructed from theories ESTRELLA Project (2008). A theory in LKIF consists of a set of axioms and defeasible inference rules. The language of individuals, predicate and function symbols used by the theory can be imported from an ontology represented in the Web Ontology Language (OWL). Importing an ontology also imports the axioms of the ontology. All symbols are represented using Universal Resource Identifiers (URIs). Other LKIF files may also be imported, enabling complex theories to modularized. © 2010 Springer-Verlag Berlin Heidelberg.},
   author = {Thomas F. Gordon},
   doi = {10.1007/978-3-642-15402-7_30/COVER},
   isbn = {3642154018},
   issn = {18651348},
   journal = {Lecture Notes in Business Information Processing},
   pages = {240-242},
   publisher = {Springer Verlag},
   title = {An overview of the legal knowledge interchange format},
   volume = {57 LNBIP},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-15402-7_30},
   year = {2010},
}
@article{Sharifi2020,
   abstract = {Legal contracts specify the terms and conditions (in essence, requirements) that apply to business transactions. Smart contracts are software systems that monitor and control the execution of contracts to ensure compliance. This paper proposes a formal specification language for contracts, called Symboleo, where contracts consist of collections of obligations and powers that define the legal contract's compliant executions. The formal semantics of Symboleo is based on an extension of an ontology for Law and is described in terms of logical axioms on statecharts that describe the lifetimes of contracts, obligations and powers. Our proposal includes a preliminary evaluation through the specification of a real life-inspired Sale-of-Goods contract, with a prototype execution engine. We envision this language to enable formally verifying contracts to detect requirements-level issues and to generate executable smart contracts (e.g., on blockchain technology).},
   author = {Sepehr Sharifi and Alireza Parvizimosaed and Daniel Amyot and Luigi Logrippo and John Mylopoulos},
   doi = {10.1109/RE48521.2020.00049},
   isbn = {9781728174389},
   issn = {23326441},
   journal = {Proceedings of the IEEE International Conference on Requirements Engineering},
   keywords = {Legal contracts,formal specification languages,smart contracts,software requirements specifications},
   month = {8},
   pages = {364-369},
   publisher = {IEEE Computer Society},
   title = {Symboleo: Towards a Specification Language for Legal Contracts},
   volume = {2020-August},
   year = {2020},
}
@article{Hohfeld1913,
   abstract = {PRINTED},
   author = {Wesley Newcomb Hohfeld},
   doi = {10.2307/1275798},
   issn = {00262234},
   issue = {8},
   journal = {Michigan Law Review},
   month = {6},
   pages = {537},
   publisher = {JSTOR},
   title = {The Relations between Equity and Law},
   volume = {11},
   year = {1913},
}
@article{Hohfeld1917,
   abstract = {The present discussion, while intended to be intrinsically complete so far as intelligent and convenient perusal is concerned, represents, as originally planned, a continuation of an article which appeared under the same title more than three years ago. It therefore seems desirable to indicate, in very general form, the scope and purpose of the latter. The main divisions were entitled: Legal Conceptions Contrasted with Non-legal Conceptions; Operative Facts Contrasted with Evidential Facts; and Fundamental Jural Relations Contrasted with One Another. The jural relations analyzed and discussed under the last subtitle were, at the outset, grouped in a convenient "scheme of opposites and correlatives"; and it will greatly facilitate the presentation of the matters to be hereafter considered if that scheme be reproduced at the present point:},
   author = {Wesley Newcomb Hohfeld},
   doi = {10.2307/786270},
   issn = {00440094},
   issue = {8},
   journal = {The Yale Law Journal},
   month = {6},
   pages = {710},
   publisher = {JSTOR},
   title = {Fundamental Legal Conceptions as Applied in Judicial Reasoning},
   volume = {26},
   year = {1917},
}
@article{Hogan2021,
   abstract = {In this article, we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting...},
   author = {Aidan Hogan and Eva Blomqvist and Michael Cochez and Claudia D'Amato and Gerard De Melo and Claudio Gutierrez and Sabrina Kirrane and José Emilio Labra Gayo and Roberto Navigli and Sebastian Neumaier and Axel Cyrille Ngonga Ngomo and Axel Polleres and Sabbir M. Rashid and Anisa Rula and Lukas Schmelzeisen and Juan Sequeda and Steffen Staab and Antoine Zimmermann},
   doi = {10.1145/3447772},
   issn = {15577341},
   issue = {4},
   journal = {ACM Computing Surveys (CSUR)},
   keywords = {Knowledge graphs,embeddings,graph algorithms,graph databases,graph neural networks,graph query languages,ontologies,rule mining,shapes},
   month = {7},
   publisher = {
		ACM
		PUB27
		New York, NY, USA
	},
   title = {Knowledge Graphs},
   volume = {54},
   url = {https://dl.acm.org/doi/10.1145/3447772},
   year = {2021},
}
@article{Yao2023,
   abstract = {With the widespread use of large language models (LLMs) in NLP tasks, researchers have discovered the potential of Chain-of-thought (CoT) to assist LLMs in accomplishing complex reasoning tasks by generating intermediate steps. However, human thought processes are often non-linear, rather than simply sequential chains of thoughts. Therefore, we propose Graph-of-Thought (GoT) reasoning, which models human thought processes not only as a chain but also as a graph. By representing thought units as nodes and connections between them as edges, our approach captures the non-sequential nature of human thinking and allows for a more realistic modeling of thought processes. Similar to Multimodal-CoT, we modeled GoT reasoning as a two-stage framework, generating rationales first and then producing the final answer. Specifically, we employ an additional graph-of-thoughts encoder for GoT representation learning and fuse the GoT representation with the original input representation through a gated fusion mechanism. We implement a GoT reasoning model on the T5 pre-trained model and evaluate its performance on a text-only reasoning task (GSM8K) and a multimodal reasoning task (ScienceQA). Our model achieves significant improvement over the strong CoT baseline with 3.41% and 5.08% on the GSM8K test set with T5-base and T5-large architectures, respectively. Additionally, our model boosts accuracy from 84.91% to 91.54% using the T5-base model and from 91.68% to 92.77% using the T5-large model over the state-of-the-art Multimodal-CoT on the ScienceQA test set. Experiments have shown that GoT achieves comparable results to Multimodal-CoT(large) with over 700M parameters, despite having fewer than 250M backbone model parameters, demonstrating the effectiveness of GoT.},
   author = {Yao Yao and Zuchao Li and Hai Zhao},
   month = {5},
   title = {Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models},
   url = {https://arxiv.org/abs/2305.16582v1},
   year = {2023},
}
@article{Brachman1977,
   abstract = {Semantic networks constitute one of the many attempts to capture human knowledge in an abstraction suitable for processing by computer program. While semantic nets enjoy widespread popularity, they seem never to live up to their authors' expectations of expressive power and ease of construction. This paper examines the fundamentals of network notation, in order to understand why the “formalism” has not been the panacea it was once hoped to be. We focus here on “concepts”—what net-authors think they are, and how network nodes might represent them. The simplistic view of concept nodes as representing extensional sets is examined, and found wanting in several respects. In an effort to solve the foundational problems exposed, we emphasize the importance of considering an “epistemological foundation” on which to consistently build representations for complex concepts. A level of representation above that of completely uniform nodes and links, but below the level of conceptual knowledge itself, seems to be the key to using previously learned concepts to interpret and structure new ones. A particular foundation is proposed here, based on the notion of a set of functional roles bound together by a structuring interrelationship. Procedures for using this foundation to automatically build instances and conceptual modifications are presented. In addition, the intensional nature of such a representation and its implications are discussed. © 1977, All rights reserved.},
   author = {Ronald J. Brachman},
   doi = {10.1016/S0020-7373(77)80017-5},
   issn = {0020-7373},
   issue = {2},
   journal = {International Journal of Man-Machine Studies},
   month = {3},
   pages = {127-152},
   publisher = {Academic Press},
   title = {What's in a concept: structural foundations for semantic networks},
   volume = {9},
   year = {1977},
}
@article{Agrawal2023,
   abstract = {The contemporary LLMs are prone to producing hallucinations, stemming mainly from the knowledge gaps within the models. To address this critical limitation, researchers employ diverse strategies to augment the LLMs by incorporating external knowledge, aiming to reduce hallucinations and enhance reasoning accuracy. Among these strategies, leveraging knowledge graphs as a source of external information has demonstrated promising results. In this survey, we conduct a comprehensive review of these knowledge-graph-based knowledge augmentation techniques in LLMs, focusing on their efficacy in mitigating hallucinations. We systematically categorize these methods into three overarching groups, offering both methodological comparisons and empirical evaluations of their performance. Lastly, the paper explores the challenges associated with these techniques and outlines potential avenues for future research in this emerging field.},
   author = {Garima Agrawal and Tharindu Kumarage and Zeyad Alghami and Huan Liu},
   month = {11},
   title = {Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey},
   url = {https://arxiv.org/abs/2311.07914v1},
   year = {2023},
}
@article{Abu-Salih2021,
   abstract = {Knowledge Graphs (KGs) have made a qualitative leap and effected a real revolution in knowledge representation. This is leveraged by the underlying structure of the KG which underpins a better comprehension, reasoning and interpretation of knowledge for both human and machine. Therefore, KGs continue to be used as the main means of tackling a plethora of real-life problems in various domains. However, there is no consensus in regard to a plausible and inclusive definition of a domain-specific KG. Further, in conjunction with several limitations and deficiencies, various domain-specific KG construction approaches are far from perfect. This survey is the first to offer a comprehensive definition of a domain-specific KG. Also, the paper presents a thorough review of the state-of-the-art approaches drawn from academic works relevant to seven domains of knowledge. An examination of current approaches reveals a range of limitations and deficiencies. At the same time, uncharted territories on the research map are highlighted to tackle extant issues in the literature and point to directions for future research.},
   author = {Bilal Abu-Salih},
   doi = {10.1016/J.JNCA.2021.103076},
   issn = {1084-8045},
   journal = {Journal of Network and Computer Applications},
   keywords = {Domain ontology,Domain-specific knowledge graph,Knowledge graph,Knowledge graph construction,Knowledge graph embeddings,Knowledge graph evaluation,Survey},
   month = {7},
   pages = {103076},
   publisher = {Academic Press},
   title = {Domain-specific knowledge graphs: A survey},
   volume = {185},
   year = {2021},
}
@article{Sequeda2023,
   abstract = {Enterprise applications of Large Language Models (LLMs) hold promise for question answering on enterprise SQL databases. However, the extent to which LLMs can accurately respond to enterprise questions in such databases remains unclear, given the absence of suitable Text-to-SQL benchmarks tailored to enterprise settings. Additionally, the potential of Knowledge Graphs (KGs) to enhance LLM-based question answering by providing business context is not well understood. This study aims to evaluate the accuracy of LLM-powered question answering systems in the context of enterprise questions and SQL databases, while also exploring the role of knowledge graphs in improving accuracy. To achieve this, we introduce a benchmark comprising an enterprise SQL schema in the insurance domain, a range of enterprise queries encompassing reporting to metrics, and a contextual layer incorporating an ontology and mappings that define a knowledge graph. Our primary finding reveals that question answering using GPT-4, with zero-shot prompts directly on SQL databases, achieves an accuracy of 16%. Notably, this accuracy increases to 54% when questions are posed over a Knowledge Graph representation of the enterprise SQL database. Therefore, investing in Knowledge Graph provides higher accuracy for LLM powered question answering systems.},
   author = {Juan Sequeda and Dean Allemang and Bryon Jacob},
   keywords = {Answering · SQL,Augmented,Benchmark ·,Databases ·,Generation (RAG),Graphs ·,Knowledge,Language,Large,Models ·,Question,Retrieval},
   month = {11},
   title = {A Benchmark to Understand the Role of Knowledge Graphs on Large Language Model's Accuracy for Question Answering on Enterprise SQL Databases},
   url = {https://arxiv.org/abs/2311.07509v1},
   year = {2023},
}
@article{Joshi2020,
   abstract = {Federal government agencies and organizations doing business with them have to adhere to the Code of Federal Regulations (CFR). The CFRs are currently available as large text documents that are not...},
   author = {Karuna Pande Joshi and Srishty Saha},
   doi = {10.1145/3425192},
   issn = {26390175},
   issue = {3},
   journal = {Digital Government: Research and Practice},
   keywords = {Deep learning,Semantic Web,compliance,legal text analytics},
   month = {11},
   publisher = {
		ACM
		PUB27
		New York, NY, USA
	},
   title = {A Semantically Rich Framework for Knowledge Representation of Code of Federal Regulations},
   volume = {1},
   url = {https://dl.acm.org/doi/10.1145/3425192},
   year = {2020},
}
@article{Casellas2011,
   abstract = {The online publication of legal materials by governments and administrations is not recent, nevertheless, nowadays some of these materials are being made available in machine readable formats, and follow the Linked Open Data principles: URI naming of resources, assertions about named relationships between resources or between resources and data values, and the possibility to easily extend, update and modify these relationships and resources.The combination of the publication of legislation, regulations, judicial cases, or administrative decisions as Linked Open Legal Data together with the availability of legal OWL/RDF ontologies and SKOS thesauri, could offer new possibilities to enhance not only access but also understanding of legal information.  This paper outlines the possibilities offered by the use Open Legal Data, and gives a preliminary description of the ongoing work on the reuse of the Code of Federal Regulations and one of its finding aids, the Thesaurus of Indexing Terms.  An improved version of this paper has been accepted for presentation at the AAAI Fall Symposium on Open Government Knowledge: AI Opportunities and Challenges, November 5-6, Arlington, VA. It will be published as part of the Technical Reports.},
   author = {Núria Casellas and Joan-Josep Vallbé and Thomas Robert Bruce},
   doi = {10.2139/SSRN.1959931},
   journal = {SSRN Electronic Journal},
   keywords = {CFR,From Legal Information to Open Legal Data: A Case Study in U.S. Federal Legal Information,Joan-Josep Vallbé,Núria Casellas,RDF,SKOS,SSRN,Thomas Robert Bruce,legal information,legal ontologies,open legal data,thesaurus},
   month = {9},
   publisher = {Elsevier BV},
   title = {From Legal Information to Open Legal Data: A Case Study in U.S. Federal Legal Information},
   url = {https://papers.ssrn.com/abstract=1959931},
   year = {2011},
}
@article{Wang2023,
   abstract = {Graph embedding methods such as Graph Neural Networks (GNNs) and Graph Transformers have contributed to the development of graph reasoning algorithms for various tasks on knowledge graphs. However, the lack of interpretability and explainability of graph embedding methods has limited their applicability in scenarios requiring explicit reasoning. In this paper, we introduce the Graph Agent (GA), an intelligent agent methodology of leveraging large language models (LLMs), inductive-deductive reasoning modules, and long-term memory for knowledge graph reasoning tasks. GA integrates aspects of symbolic reasoning and existing graph embedding methods to provide an innovative approach for complex graph reasoning tasks. By converting graph structures into textual data, GA enables LLMs to process, reason, and provide predictions alongside human-interpretable explanations. The effectiveness of the GA was evaluated on node classification and link prediction tasks. Results showed that GA reached state-of-the-art performance, demonstrating accuracy of 90.65%, 95.48%, and 89.32% on Cora, PubMed, and PrimeKG datasets, respectively. Compared to existing GNN and transformer models, GA offered advantages of explicit reasoning ability, free-of-training, easy adaption to various graph reasoning tasks},
   author = {Qinyong Wang and Zhenxiang Gao and Rong Xu},
   month = {10},
   title = {Graph Agent: Explicit Reasoning Agent for Graphs},
   url = {https://arxiv.org/abs/2310.16421v1},
   year = {2023},
}
@article{Ding2018,
   abstract = {Embedding knowledge graphs (KGs) into continuous vector spaces is a focus of current research. Early works performed this task via simple models developed over KG triples. Recent attempts focused on either designing more complicated triple scoring models, or incorporating extra information beyond triples. This paper, by contrast, investigates the potential of using very simple constraints to improve KG embedding. We examine non-negativity constraints on entity representations and approximate entailment constraints on relation representations. The former help to learn compact and interpretable representations for entities. The latter further encode regularities of logical entailment between relations into their distributed representations. These constraints impose prior beliefs upon the structure of the embedding space, without negative impacts on efficiency or scalability. Evaluation on WordNet, Freebase, and DBpedia shows that our approach is simple yet surprisingly effective, significantly and consistently outperforming competitive baselines. The constraints imposed indeed improve model interpretability, leading to a substantially increased structuring of the embedding space. Code and data are available at https://github.com/iieir-km/ComplEx-NNE_AER.},
   author = {Boyang Ding and Quan Wang and Bin Wang and Li Guo},
   doi = {10.18653/v1/p18-1011},
   isbn = {9781948087322},
   journal = {ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
   month = {5},
   pages = {110-121},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {Improving Knowledge Graph Embedding Using Simple Constraints},
   volume = {1},
   url = {https://arxiv.org/abs/1805.02408v2},
   year = {2018},
}
@article{Tunstall2023,
   abstract = {We aim to produce a smaller language model that is aligned to user intent. Previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, we experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access RLHF-based model. Code, models, data, and tutorials for the system are available at https://github.com/huggingface/alignment-handbook.},
   author = {Lewis Tunstall and Edward Beeching and Nathan Lambert and Nazneen Rajani and Kashif Rasul and Younes Belkada and Shengyi Huang and Leandro von Werra and Clémentine Fourrier and Nathan Habib and Nathan Sarrazin and Omar Sanseviero and Alexander M. Rush and Thomas Wolf},
   month = {10},
   title = {Zephyr: Direct Distillation of LM Alignment},
   url = {https://arxiv.org/abs/2310.16944v1},
   year = {2023},
}
@article{Galkin2023,
   abstract = {Foundation models in language and vision have the ability to run inference on any textual and visual inputs thanks to the transferable representations such as a vocabulary of tokens in language. Knowledge graphs (KGs) have different entity and relation vocabularies that generally do not overlap. The key challenge of designing foundation models on KGs is to learn such transferable representations that enable inference on any graph with arbitrary entity and relation vocabularies. In this work, we make a step towards such foundation models and present ULTRA, an approach for learning universal and transferable graph representations. ULTRA builds relational representations as a function conditioned on their interactions. Such a conditioning strategy allows a pre-trained ULTRA model to inductively generalize to any unseen KG with any relation vocabulary and to be fine-tuned on any graph. Conducting link prediction experiments on 57 different KGs, we find that the zero-shot inductive inference performance of a single pre-trained ULTRA model on unseen graphs of various sizes is often on par or better than strong baselines trained on specific graphs. Fine-tuning further boosts the performance.},
   author = {Mikhail Galkin and Xinyu Yuan and Hesham Mostafa and Jian Tang and Zhaocheng Zhu},
   month = {10},
   title = {Towards Foundation Models for Knowledge Graph Reasoning},
   url = {https://arxiv.org/abs/2310.04562v1},
   year = {2023},
}
@article{Akehurst2006,
   abstract = {A number of different Model Transformation Frameworks (MTF) are being developed, each of them requiring a user to learn a different language and each possessing its own specific language peculiarities, even if they are based on the QVT standard. To write even a simple transformation, these MTFs require a large amount of learning time. We describe in this paper a minimal, Java based, library that can be used to support the implementation of many practical transformations. Use of this library enables simple transformations to be implemented simply, whilst still providing some support for more complex transformations. © Springer-Verlag Berlin Heidelberg 2006.},
   author = {D. H. Akehurst and B. Bordbar and M. J. Evans and W. G.J. Howells and K. D. McDonald-Maier},
   doi = {10.1007/11880240_25/COVER},
   isbn = {3540457720},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {351-364},
   publisher = {Springer Verlag},
   title = {SiTra: Simple transformations in Java},
   volume = {4199 LNCS},
   url = {https://link.springer.com/chapter/10.1007/11880240_25},
   year = {2006},
}
@article{Su2022,
   abstract = {We introduce INSTRUCTOR, a new method for computing text embeddings given task instructions: every text input is embedded together with instructions explaining the use case (e.g., task and domain descriptions). Unlike encoders from prior work that are more specialized, INSTRUCTOR is a single embedder that can generate text embeddings tailored to different downstream tasks and domains, without any further training. We first annotate instructions for 330 diverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive loss. We evaluate INSTRUCTOR on 70 embedding evaluation tasks (66 of which are unseen during training), ranging from classification and information retrieval to semantic textual similarity and text generation evaluation. INSTRUCTOR, while having an order of magnitude fewer parameters than the previous best model, achieves state-of-the-art performance, with an average improvement of 3.4% compared to the previous best results on the 70 diverse datasets. Our analysis suggests that INSTRUCTOR is robust to changes in instructions, and that instruction finetuning mitigates the challenge of training a single model on diverse datasets. Our model, code, and data are available at https://instructor-embedding.github.io.},
   author = {Hongjin Su and Weijia Shi and Jungo Kasai and Yizhong Wang and Yushi Hu and Mari Ostendorf and Wen-tau Yih and Noah A. Smith and Luke Zettlemoyer and Tao Yu},
   doi = {10.18653/v1/2023.findings-acl.71},
   month = {12},
   pages = {1102-1121},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {One Embedder, Any Task: Instruction-Finetuned Text Embeddings},
   url = {https://arxiv.org/abs/2212.09741v3},
   year = {2022},
}
@article{Reimers2019,
   abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
   author = {Nils Reimers and Iryna Gurevych},
   doi = {10.18653/v1/d19-1410},
   isbn = {9781950737901},
   journal = {EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference},
   month = {8},
   pages = {3982-3992},
   publisher = {Association for Computational Linguistics},
   title = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
   url = {https://arxiv.org/abs/1908.10084v1},
   year = {2019},
}
@article{Akl2021,
   abstract = {In this paper, we present our approaches for the FinSim-3 Shared Task 2021: Learning Semantic Similarities for the Financial Domain. The aim of this shared task is to correctly classify a list of given terms from the financial domain into the most relevant hypernym (or top-level) concept in an external ontology. For our system submission, we evaluate two methods: a Sentence-RoBERTa (SRoBERTa) embeddings model pre-trained on a custom corpus, and a dual word-sentence embeddings model that builds on the first method by improving the proposed baseline word embeddings construction using the FastText model to boost the classification performance. Our system ranks 2nd overall on both metrics, scoring 0.917 on Average Accuracy and 1.141 on Mean Rank.},
   author = {Hanna Abi Akl and Dominique Mariko and Hugues De and Mazancourt Yseop},
   month = {8},
   title = {Yseop at FinSim-3 Shared Task 2021: Specializing Financial Domain Learning with Phrase Representations},
   url = {https://arxiv.org/abs/2108.09485v1},
   year = {2021},
}
@article{Corro2013,
   abstract = {We propose ClausIE, a novel, clause-based approach to open information extraction, which extracts relations and their arguments from natural language text. ClausIE fundamentally differs from previous approaches in that it separates the detection of "useful" pieces of information expressed in a sentence from their representation in terms of extractions. In more detail, ClausIE exploits linguistic knowledge about the grammar of the English language to first detect clauses in an input sentence and to subsequently identify the type of each clause according to the grammatical function of its constituents. Based on this information, ClausIE is able to generate high-precision extractions; the representation of these extractions can be flexibly customized to the underlying application. ClausIE is based on dependency parsing and a small set of domain-independent lexica, operates sentence by sentence without any post-processing, and requires no training data (whether labeled or unlabeled). Our experimental study on various real-world datasets suggests that ClausIE obtains higher recall and higher precision than existing approaches, both on high-quality text as well as on noisy text as found in the web.},
   author = {Luciano Del Corro and Rainer Gemulla},
   doi = {10.1145/2488388.2488420},
   month = {5},
   pages = {355-366},
   publisher = {Association for Computing Machinery (ACM)},
   title = {ClausIE},
   url = {https://dl.acm.org/doi/10.1145/2488388.2488420},
   year = {2013},
}
@article{Fatemi2023,
   abstract = {Graphs are a powerful tool for representing and analyzing complex relationships in real-world applications such as social networks, recommender systems, and computational finance. Reasoning on graphs is essential for drawing inferences about the relationships between entities in a complex system, and to identify hidden patterns and trends. Despite the remarkable progress in automated reasoning with natural text, reasoning on graphs with large language models (LLMs) remains an understudied problem. In this work, we perform the first comprehensive study of encoding graph-structured data as text for consumption by LLMs. We show that LLM performance on graph reasoning tasks varies on three fundamental levels: (1) the graph encoding method, (2) the nature of the graph task itself, and (3) interestingly, the very structure of the graph considered. These novel results provide valuable insight on strategies for encoding graphs as text. Using these insights we illustrate how the correct choice of encoders can boost performance on graph reasoning tasks inside LLMs by 4.8% to 61.8%, depending on the task.},
   author = {Bahare Fatemi and Jonathan Halcrow and Bryan Perozzi and Google Research},
   month = {10},
   title = {Talk like a Graph: Encoding Graphs for Large Language Models},
   url = {https://arxiv.org/abs/2310.04560v1},
   year = {2023},
}
@article{Bouzidi2013,
   abstract = {In this paper, we present a semantic web approach for modelling the process of creating new technical and regulatory documents related to the Building sector. This industry, among other industries, is currently experiencing a phenomenal growth in its technical and regulatory texts. Therefore, it is urgent and crucial to improve the process of creating regulations by automating it as much as possible. We focus on the creation of particular technical documents issued by the French Scientific and Technical Centre for Building (CSTB), called Technical Assessments, and we propose services based on Semantic Web models and techniques for modelling the process of their creation.},
   author = {Khalil Riad Bouzidi and Bruno Fies and Marc Bourdeau and Catherine Faron-Zucker and Nhan Le-Thanh},
   keywords = {Building Industry,Knowledge Management,Ontology,Semantic Web,e-regulations},
   month = {2},
   pages = {26-28},
   title = {An Ontology for Modelling and Supporting the Process of Authoring Technical Assessments},
   url = {https://arxiv.org/abs/1302.4726v1},
   year = {2013},
}
@article{Lewis2020,
   abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
   author = {Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen Tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
   isbn = {2005.11401v4},
   issn = {10495258},
   journal = {Advances in Neural Information Processing Systems},
   month = {5},
   publisher = {Neural information processing systems foundation},
   title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
   volume = {2020-December},
   url = {https://arxiv.org/abs/2005.11401v4},
   year = {2020},
}
@article{Gao2012,
   abstract = {A contract is a legally binding agreement between real-world business entities whom we treat as providing services to one another. We focus on business rather than technical services. We think of a business contract as specifying the functional and nonfunctional behaviors of and interactions among the services. In current practice, contracts are produced as text documents. Thus the relevant service capabilities, requirements, qualities, and risks are hidden and difficult to access and reason about. We describe a simple but effective unsupervised information extraction approach and tool, Contract Miner, for discovering service exceptions at the phrase level from a large contract repository. Our approach involves preprocessing followed by an application of linguistic patterns and parsing to extract the service exception phrases. Identifying such (noun) phrases can help build service exception vocabularies that support the development of a taxonomy of business terms, and also facilitate modeling and analyzing service engagements. A lightweight online tool that comes with Contract Miner highlights the relevant text in service contracts and thereby assists users in reviewing contracts. Contract Miner produces promising results in terms of precision and recall when evaluated over a corpus of manually annotated contracts. © 2012 IEEE.},
   author = {Xibin Gao and Munindar P. Singh and Pankaj Mehra},
   doi = {10.1109/TSC.2011.1},
   issn = {19391374},
   issue = {3},
   journal = {IEEE Transactions on Services Computing},
   keywords = {Contract analysis,service exceptions,text mining},
   pages = {333-344},
   title = {Mining business contracts for service exceptions},
   volume = {5},
   year = {2012},
}
@article{Asai2023,
   abstract = {Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.},
   author = {Akari Asai and Zeqiu Wu and Yizhong Wang and Avirup Sil and Hannaneh Hajishirzi},
   month = {10},
   title = {Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection},
   url = {https://arxiv.org/abs/2310.11511v1},
   year = {2023},
}
@article{Saad-Falcon2023,
   abstract = {Large Language Models (LLMs) have issues with document question answering (QA) in situations where the document is unable to fit in the small context length of an LLM. To overcome this issue, most existing works focus on retrieving the relevant context from the document, representing them as plain text. However, documents such as PDFs, web pages, and presentations are naturally structured with different pages, tables, sections, and so on. Representing such structured documents as plain text is incongruous with the user's mental model of these documents with rich structure. When a system has to query the document for context, this incongruity is brought to the fore, and seemingly trivial questions can trip up the QA system. To bridge this fundamental gap in handling structured documents, we propose an approach called PDFTriage that enables models to retrieve the context based on either structure or content. Our experiments demonstrate the effectiveness of the proposed PDFTriage-augmented models across several classes of questions where existing retrieval-augmented LLMs fail. To facilitate further research on this fundamental problem, we release our benchmark dataset consisting of 900+ human-generated questions over 80 structured documents from 10 different categories of question types for document QA.},
   author = {Jon Saad-Falcon and Joe Barrow and Alexa Siu and Ani Nenkova and Ryan A Rossi and Adobe Research and Franck Dernoncourt},
   month = {9},
   title = {PDFTriage: Question Answering over Long, Structured Documents},
   url = {https://arxiv.org/abs/2309.08872v1},
   year = {2023},
}
@article{Koreeda2021,
   abstract = {While many NLP pipelines assume raw, clean texts, many texts we encounter in
the wild, including a vast majority of legal documents, are not so clean, with
many of them being visually structured documents (VSDs) such as PDFs.
Conventional preprocessing tools for VSDs mainly focused on word segmentation
and coarse layout analysis, whereas fine-grained logical structure analysis
(such as identifying paragraph boundaries and their hierarchies) of VSDs is
underexplored. To that end, we proposed to formulate the task as prediction of
"transition labels" between text fragments that maps the fragments to a tree,
and developed a feature-based machine learning system that fuses visual,
textual and semantic cues.Our system is easily customizable to different types
of VSDs and it significantly outperformed baselines in identifying different
structures in VSDs. For example, our system obtained a paragraph boundary
detection F1 score of 0.953 which is significantly better than a popular
PDF-to-text tool with an F1 score of 0.739.},
   author = {Yuta Koreeda and Christopher D. Manning},
   doi = {10.18653/v1/2021.nllp-1.15},
   isbn = {9781954085985},
   journal = {Natural Legal Language Processing, NLLP 2021 - Proceedings of the 2021 Workshop},
   month = {5},
   pages = {144-154},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {Capturing Logical Structure of Visually Structured Documents with Multimodal Transition Parser},
   url = {https://arxiv.org/abs/2105.00150v2},
   year = {2021},
}
@article{Liu2023,
   abstract = {While recent language models have the ability to take long contexts as input,
relatively little is known about how well they use longer context. We analyze
language model performance on two tasks that require identifying relevant
information within their input contexts: multi-document question answering and
key-value retrieval. We find that performance is often highest when relevant
information occurs at the beginning or end of the input context, and
significantly degrades when models must access relevant information in the
middle of long contexts. Furthermore, performance substantially decreases as
the input context grows longer, even for explicitly long-context models. Our
analysis provides a better understanding of how language models use their input
context and provides new evaluation protocols for future long-context models.},
   author = {Nelson F. Liu and Kevin Lin and John Hewitt and Ashwin Paranjape and Michele Bevilacqua and Fabio Petroni and Percy Liang},
   month = {7},
   title = {Lost in the Middle: How Language Models Use Long Contexts},
   url = {https://arxiv.org/abs/2307.03172v2},
   year = {2023},
}
@article{Demuth2007,
   abstract = {Business rules should improve the human communication inside of an enterprise or between business partners and must be therefore independent of implementations in IT systems. As a long-term goal, business rules should be guaranteed by all IT applications of an enterprise. A first step to define and to standardize what business rules are is an OMG initiative to specify a metamodel for business rules and the vocabulary on which business rules are defined. The result of OMG's effort is the SBVR (Semantics of Business Vocabulary and Business Rules) specification that we took as starting point of our investigations to automate business rules. There are multiple ways for transforming business rules. In this paper we show how SBVR based vocabulary and rules can be translated by model transformation chains into Semantic Web Languages. In our approach we use OWL and R2ML (REWERSE Rule Markup Language). Both are languages with a high potential for a broad usage in future rule-based applications. © Springer-Verlag Berlin Heidelberg 2007.},
   author = {Birgit Demuth and Hans Bernhard Liebau},
   doi = {10.1007/978-3-540-75975-1_10/COVER},
   isbn = {9783540759744},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {119-133},
   publisher = {Springer Verlag},
   title = {An approach for bridging the gap between business rules and the semantic Web},
   volume = {4824 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-540-75975-1_10},
   year = {2007},
}
@article{Linehan2008,
   abstract = {Semantics of Business Vocabulary and Rules (SBVR) is a new standard from the OMG that combines aspects of ontologies and of rule systems. This paper summarizes SBVR, reviews some possible use cases for SBVR, and discusses ways that vocabularies and rules given in SBVR could relate to established ontology standards, to rules technologies, and to other IT implementation technologies. It also describes experience with an SBVR prototype that transforms a subset of SBVR rules types to several types of runtime implementations. © 2008 Springer Berlin Heidelberg.},
   author = {Mark H. Linehan},
   doi = {10.1007/978-3-540-88808-6_20/COVER},
   isbn = {3540888071},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Business rules,Ontology,Rules,SBVR,Semantics,Vocabulary},
   pages = {182-196},
   publisher = {Springer Verlag},
   title = {SBVR use cases},
   volume = {5321 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-540-88808-6_20},
   year = {2008},
}
@article{Boley2007,
   abstract = {Four principal Web rule issues constitute our starting points: I1) Formal knowledge representation can act as content in its own right and/or as metadata for content. 12) Knowledge on the open Web is typically inconsistent but closed 'intranet' reasoning can exploit local consistency. 13) Scalability of reasoning calls for representation layering on top of quite inexpressive languages. 14) Rule representation should stay compatible with relevant Web standards. To address these, four corresponding essentials of Web rules have emerged: El) Combine formal and informal knowledge in a Rule Wiki, where the formal parts can be taken as code (or as metadata) for the informal parts, and the informal parts as documentation (or as content) for the formal parts. This can be supported by tools for Controlled Natural Language: mapping a subset of, e.g., English into rules and back. E2) Represent distributed knowledge via a module construct, supporting local consistency, permitting scoped negation as failure, and reducing the search space of scoped queries. Modules are embedded into an 'Entails' element: prove whether a query is entailed by a module. E3) Develop a dual layering of assertional and terminological knowledge as well as their blends. To permit the specification of terminologies independent of assertions, the CARIN principle is adopted: a terminological predicate is not permitted in the head of a rule. E4) Differentiate the Web notion of URIs as in URLs, for access, vs. URNs, for naming. A URI can then be used: URL-like, for module import, where it is an error if dereferencing does not yield a valid knowledge document; URN-like, as an identifier, where dereferencing is not intended; or, as a name whose dereferencing can access its (partial) definition. © Springer-Verlag Berlin Heidelberg 2007.},
   author = {Harold Boley},
   doi = {10.1007/978-3-540-75975-1_2/COVER},
   isbn = {9783540759744},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {7-24},
   publisher = {Springer Verlag},
   title = {Are your rules online? Four Web rule essentials},
   volume = {4824 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-540-75975-1_2},
   year = {2007},
}
@article{Aiello2014,
   abstract = {This paper proposes a mapping technique for automatically translating rules expressed in a format based on natural language, i.e. Semantics of Business Vocabulary and Business Rules (SBVR) standard, into production rules that can be executed by a computer (i.e. Rule engine). The proposed approach achieves a twofold purpose: on the one hand non IT skilled people (i.e. Domain expert) can effectively focus on business rules definition by using statements in natural language, and on the other hand the IT staff will have to manage business rules in a format ready to be executed by a rule engine. The main goal is to overcome some weaknesses in the software development process that could produce inconsistencies between the domain requirements identification and the implemented software functionalities. An exhaustive analysis of the mapping technique is provided and a real case study is presented in order to prove the validity of our work.},
   author = {Giovanni Aiello and Roberto Di Bernardo and Martino Maggio and Daniele Di Bona and Giuseppe Lo Re},
   doi = {10.1109/SOCA.2014.39},
   isbn = {9781479968336},
   journal = {Proceedings - IEEE 7th International Conference on Service-Oriented Computing and Applications, SOCA 2014},
   keywords = {Business Rules,Drools,Natural Language,SBVR},
   month = {12},
   pages = {131-136},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Inferring business rules from natural language expressions},
   year = {2014},
}
@article{Bollen2008,
   abstract = {In this paper we will give an introduction to the recently established OMG SBVR standard on business rules. This standard is a major step forward in improving the productivity of business rule- modelers and analysts. The paper furthermore, illustrates how the mature fact-oriented approaches, e.g. ORM and CogNiam, are related to this new standard and how they can contribute to deliver high quality SBVR models.},
   author = {Peter Bollen},
   doi = {10.1007/978-3-540-88875-8_96/COVER},
   isbn = {9783540888741},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Business rules,CogNIAM,Fact-Orientation,NIAM,OMG,ORM,SBVR,Semantics of business vocabulary and business rules},
   pages = {718-727},
   publisher = {Springer Verlag},
   title = {SBVR: A fact-oriented OMG standard},
   volume = {5333},
   url = {https://link.springer.com/chapter/10.1007/978-3-540-88875-8_96},
   year = {2008},
}
@misc{SIFMA2023,
   author = {SIFMA},
   title = {Capital Markets Fact Book, 2023 - SIFMA - Capital Markets Fact Book, 2023 - SIFMA},
   url = {https://www.sifma.org/resources/research/fact-book/},
   year = {2023},
}
@article{Gruzitis2010,
   abstract = {Computational semantics and logic-based controlled natural languages (CNL) do not address systematically the word sense disambiguation problem of content words, i.e., they tend to interpret only some functional words that are crucial for construction of discourse representation structures. We show that micro-ontologies and multi-word units allow integration of the rich and polysemous multi-domain background knowledge into CNL thus providing interpretation for the content words. The proposed approach is demonstrated by extending the Attempto Controlled English (ACE) with polysemous and procedural constructs resulting in a more natural CNL named PAO covering narrative multi-domain texts. © 2010 Springer-Verlag Berlin Heidelberg.},
   author = {Normunds Gruzitis and Guntis Barzdins},
   doi = {10.1007/978-3-642-14418-9_7/COVER},
   isbn = {3642144179},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {102-120},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Polysemy in controlled natural language texts},
   volume = {5972 LNAI},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-14418-9_7},
   year = {2010},
}
@article{Hough2019,
   abstract = {Understanding is at the core of higher-level information processing and has a long history in the cognitive sciences. It is often described as a complex phenomenon with many dimensions, which makes it difficult to define with precision. Many researchers have noted that understanding is often ill defined, indirectly addressed, or avoided altogether. This is particularly disappointing considering that understanding has been a topic of interest since the ancient Greeks. In order to address this problem with our understanding of understanding, we reviewed literature from philosophy, psychology , education, neuroscience, and computer science. Here we summarize insights from that review, focusing on similarities and differences across those domains, as well as implications for the nature, measurement, and modeling of understanding.},
   author = {AR Hough and KA Gluck - Advances in Cognitive Systems and undefined 2019},
   journal = {cogsys.orgAR Hough, KA GluckAdvances in Cognitive Systems, 2019•cogsys.org},
   pages = {13-32},
   title = {The understanding problem in cognitive science},
   volume = {8},
   url = {http://www.cogsys.org/journal/volume8/article-8-3.pdf},
   year = {2019},
}
@article{Jackson2020,
   abstract = {How can we define and understand the nature of understanding itself? This paper discusses cognitive processes for understanding the world in general and for understanding natural language. The discussion considers whether and how an artificial cognitive system could use a 'natural language of thought', and whether the ambiguities of natural language would be a theoretical barrier or could be a theoretical advantage for such a system, in a research approach toward human-level artificial intelligence.},
   author = {Philip Jackson},
   doi = {10.1016/J.PROCS.2020.02.138},
   issn = {1877-0509},
   journal = {Procedia Computer Science},
   keywords = {TalaMind,ambiguity,language of thought,mentalese,natural language,understanding},
   month = {1},
   pages = {209-225},
   publisher = {Elsevier},
   title = {Understanding understanding and ambiguity in natural language},
   volume = {169},
   year = {2020},
}
@article{Csaki2022,
   abstract = {Over the last three decades legal service providers as well as legal departments of various firms have embraced the opportunity to apply the latest digital technology to improve the efficiency and effectiveness of their work. Since language is central to both law-making and during the application of the law, Natural Language Processing solutions have found their way to this profession. One particular research area relates to the issue of small languages. The problem is rooted in the size of the population speaking a given language: in a small market, it is not economically feasible to develop NLP technologies as they require considerable time and effort to develop a sufficient language corpus. This paper reviews the challenges countries and jurisdictions of small languages face in light of increasing NLP applications in legal contexts, while also examining the role of the public sector in relation to addressing such issues.},
   author = {Csaba Csáki and Péter Homoki and György Görög and Pál Vadász},
   keywords = {AI regulations,Legal Tech,Natural Language Processing,small languages},
   title = {NLP in the Legal Profession: How about Small Languages?},
   url = {https://ceur-ws.org/Vol-3399/paper19.pdf},
   year = {2022},
}
@book{Janssen2022,
   author = {Marijn Janssen and Csaba Csáki and Marius Rohde Johannessen and Robert Krimmer and Thomas Lampoltshammer and Ida Lindgren and Euripidis Loukis and Ulf Melin and Peter Parycek and Gabriela Viale Pereira and Manuel Pedro and Rodríguez Bolívar and Gerhard Schwabe and Efthimios Tambouris and Jolien Ubacht},
   journal = {dgsociety.org},
   title = {NLP in the Legal Profession: How about Small Languages?},
   url = {https://dgsociety.org/wp-content/uploads/2022/09/CEUR-proceedings-2022.pdf#page=217},
   year = {2022},
}
@article{Allahyari2017,
   abstract = {In recent years, there has been a explosion in the amount of text data from a variety of sources. This volume of text is an invaluable source of information and knowledge which needs to be effectively summarized to be useful. In this review, the main approaches to automatic text summarization are described. We review the different processes for summarization and describe the effectiveness and shortcomings of the different methods.},
   author = {Mehdi Allahyari and Seyedamin Pouriyeh and Mehdi Assefi and Saeid Safaei and Elizabeth D. and Juan B. and Krys Kochut},
   doi = {10.14569/ijacsa.2017.081052},
   issn = {2158107X},
   issue = {10},
   journal = {International Journal of Advanced Computer Science and Applications},
   publisher = {The Science and Information Organization},
   title = {Text Summarization Techniques: A Brief Survey},
   volume = {8},
   year = {2017},
}
@article{Eidelman2019,
   abstract = {Automatic summarization methods have been studied on a variety of domains, including news and scientific articles. Yet, legislation has not previously been considered for this task, despite US Congress and state governments releasing tens of thousands of bills every year. In this paper, we introduce BillSum, the first dataset for summarization of US Congressional and California state bills (https://github.com/FiscalNote/BillSum). We explain the properties of the dataset that make it more challenging to process than other domains. Then, we benchmark extractive methods that consider neural sentence representations and traditional contextual features. Finally, we demonstrate that models built on Congressional bills can be used to summarize California bills, thus, showing that methods developed on this dataset can transfer to states without human-written summaries.},
   author = {Vladimir Eidelman},
   doi = {10.18653/v1/d19-5406},
   month = {11},
   pages = {48-56},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {BillSum: A Corpus for Automatic Summarization of US Legislation},
   year = {2019},
}
@article{See2017,
   abstract = {Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.},
   author = {Abigail See and Peter J. Liu and Christopher D. Manning},
   doi = {10.18653/v1/P17-1099},
   isbn = {9781945626753},
   journal = {ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
   pages = {1073-1083},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {Get to the point: Summarization with pointer-generator networks},
   volume = {1},
   year = {2017},
}
@article{Merchant2018,
   abstract = {It is very essential for lawyers and ordinary citizens to do an exhaustive research related to their case before they answer questions in court. For quite some time they have had to read extremely long judgements and try to pick out the useful information from them or hire legal editors to create summaries. We propose an automated text summarization system that generates short and useful summaries from lengthy judgements. We make use of a natural language processing technique called latent semantic analysis (LSA) to capture concepts within a single document. We use two approaches-a single document untrained approach and a multi-document trained approach depending on the type of input case (criminal or civil). Our data was collected from official government sites that included Supreme Court, high court and district court cases and our model achieved an average ROGUE-1 score of 0.58. Finally, our system was approved by professional lawyers. In the future we aim to provide better continuity within our generated summaries and evaluate our system more accurately.},
   author = {Kaiz Merchant and Yash Pande},
   doi = {10.1109/ICACCI.2018.8554831},
   isbn = {9781538653142},
   journal = {2018 International Conference on Advances in Computing, Communications and Informatics, ICACCI 2018},
   keywords = {Latent semantic analysis,Legal,Natural language processing,Text summarization},
   month = {11},
   pages = {1803-1807},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {NLP Based Latent Semantic Analysis for Legal Text Summarization},
   year = {2018},
}
@article{Kanapala2019,
   abstract = {Enormous amount of online information, available in legal domain, has made legal text processing an important area of research. In this paper, we attempt to survey different text summarization techniques that have taken place in the recent past. We put special emphasis on the issue of legal text summarization, as it is one of the most important areas in legal domain. We start with general introduction to text summarization, briefly touch the recent advances in single and multi-document summarization, and then delve into extraction based legal text summarization. We discuss different datasets and metrics used in summarization and compare performances of different approaches, first in general and then focused to legal text. we also mention highlights of different summarization techniques. We briefly cover a few software tools used in legal text summarization. We finally conclude with some future research directions.},
   author = {Ambedkar Kanapala and Sukomal Pal and Rajendra Pamula},
   doi = {10.1007/S10462-017-9566-2/METRICS},
   issn = {15737462},
   issue = {3},
   journal = {Artificial Intelligence Review},
   keywords = {Legal domain,Multi-document summarization,Single-document summarization,Summarization from legal documents},
   month = {3},
   pages = {371-402},
   publisher = {Springer Netherlands},
   title = {Text summarization from legal documents: a survey},
   volume = {51},
   url = {https://link.springer.com/article/10.1007/s10462-017-9566-2},
   year = {2019},
}
@article{Katz2023,
   abstract = {In this paper, we summarize the current state of the field of NLP & Law with a specific focus on recent technical and substantive developments. To support our analysis, we construct and analyze a nearly complete corpus of more than six hundred NLP & Law related papers published over the past decade. Our analysis highlights several major trends. Namely, we document an increasing number of papers written, tasks undertaken, and languages covered over the course of the past decade. We observe an increase in the sophistication of the methods which researchers deployed in this applied context. Slowly but surely, Legal NLP is beginning to match not only the methodological sophistication of general NLP but also the professional standards of data availability and code reproducibility observed within the broader scientific community. We believe all of these trends bode well for the future of the field, but many questions in both the academic and commercial sphere still remain open.},
   author = {Daniel Martin Katz and Dirk Hartung and Lauritz Gerlach and Abhik Jana and Michael J. Bommarito II},
   doi = {10.2139/ssrn.4336224},
   journal = {SSRN Electronic Journal},
   month = {2},
   publisher = {Elsevier BV},
   title = {Natural Language Processing in the Legal Domain},
   url = {https://arxiv.org/abs/2302.12039v1},
   year = {2023},
}
@article{Hendrycks2021,
   abstract = {Many specialized domains remain untouched by deep learning, as large labeled datasets require expensive expert annotators. We address this bottleneck within the legal domain by introducing the Contract Understanding Atticus Dataset (CUAD), a new dataset for legal contract review. CUAD was created with dozens of legal experts from The Atticus Project and consists of over 13,000 annotations. The task is to highlight salient portions of a contract that are important for a human to review. We find that Transformer models have nascent performance, but that this performance is strongly influenced by model design and training dataset size. Despite these promising results, there is still substantial room for improvement. As one of the only large, specialized NLP benchmarks annotated by experts, CUAD can serve as a challenging research benchmark for the broader NLP community.},
   author = {Dan Hendrycks and Collin Burns and Anya Chen and Spencer Ball},
   month = {3},
   title = {CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review},
   url = {https://arxiv.org/abs/2103.06268v2},
   year = {2021},
}
@article{Hupkes2022,
   abstract = {The ability to generalise well is one of the primary desiderata of natural language processing (NLP). Yet, what 'good generalisation' entails and how it should be evaluated is not well understood, nor are there any evaluation standards for generalisation. In this paper, we lay the groundwork to address both of these issues. We present a taxonomy for characterising and understanding generalisation research in NLP. Our taxonomy is based on an extensive literature review of generalisation research, and contains five axes along which studies can differ: their main motivation, the type of generalisation they investigate, the type of data shift they consider, the source of this data shift, and the locus of the shift within the modelling pipeline. We use our taxonomy to classify over 400 papers that test generalisation, for a total of more than 600 individual experiments. Considering the results of this review, we present an in-depth analysis that maps out the current state of generalisation research in NLP, and we make recommendations for which areas might deserve attention in the future. Along with this paper, we release a webpage where the results of our review can be dynamically explored, and which we intend to update as new NLP generalisation studies are published. With this work, we aim to take steps towards making state-of-the-art generalisation testing the new status quo in NLP.},
   author = {Dieuwke Hupkes and Mario Giulianelli and Verna Dankers and Mikel Artetxe and Yanai Elazar and Tiago Pimentel and Christos Christodoulopoulos and Karim Lasri and Naomi Saphra and Arabella Sinclair and Dennis Ulmer and Florian Schottmann and Khuyagbaatar Batsuren and Kaiser Sun and Koustuv Sinha and Leila Khalatbari and Maria Ryskina and Rita Frieske and Ryan Cotterell and Zhijing Jin},
   month = {10},
   title = {State-of-the-art generalisation research in NLP: A taxonomy and review},
   url = {https://arxiv.org/abs/2210.03050v3},
   year = {2022},
}
@article{Hupkes2023,
   abstract = {The ability to generalize well is one of the primary desiderata for models of natural language processing (NLP), but what ‘good generalization’ entails and how it should be evaluated is not well understood. In this Analysis we present a taxonomy for characterizing and understanding generalization research in NLP. The proposed taxonomy is based on an extensive literature review and contains five axes along which generalization studies can differ: their main motivation, the type of generalization they aim to solve, the type of data shift they consider, the source by which this data shift originated, and the locus of the shift within the NLP modelling pipeline. We use our taxonomy to classify over 700 experiments, and we use the results to present an in-depth analysis that maps out the current state of generalization research in NLP and make recommendations for which areas deserve attention in the future. With the rapid development of natural language processing (NLP) models in the last decade came the realization that high performance levels on test sets do not imply that a model robustly generalizes to a wide range of scenarios. Hupkes et al. review generalization approaches in the NLP literature and propose a taxonomy based on five axes to analyse such studies: motivation, type of generalization, type of data shift, the source of this data shift, and the locus of the shift within the modelling pipeline.},
   author = {Dieuwke Hupkes and Mario Giulianelli and Verna Dankers and Mikel Artetxe and Yanai Elazar and Tiago Pimentel and Christos Christodoulopoulos and Karim Lasri and Naomi Saphra and Arabella Sinclair and Dennis Ulmer and Florian Schottmann and Khuyagbaatar Batsuren and Kaiser Sun and Koustuv Sinha and Leila Khalatbari and Maria Ryskina and Rita Frieske and Ryan Cotterell and Zhijing Jin and Hong Kong},
   doi = {10.1038/s42256-023-00729-y},
   issn = {2522-5839},
   issue = {10},
   journal = {Nature Machine Intelligence 2023 5:10},
   keywords = {Computer science,Language and linguistics},
   month = {10},
   pages = {1161-1174},
   publisher = {Nature Publishing Group},
   title = {A taxonomy and review of generalization research in NLP},
   volume = {5},
   url = {https://www.nature.com/articles/s42256-023-00729-y},
   year = {2023},
}
@article{Zhu2023,
   abstract = {When prompted with a few examples and intermediate steps, large language models (LLMs) have demonstrated impressive performance in various reasoning tasks. However, prompting methods that rely on implicit knowledge in an LLM often hallucinate incorrect answers when the implicit knowledge is wrong or inconsistent with the task. To tackle this problem, we present Hypotheses-to-Theories (HtT), a framework that learns a rule library for reasoning with LLMs. HtT contains two stages, an induction stage and a deduction stage. In the induction stage, an LLM is first asked to generate and verify rules over a set of training examples. Rules that appear and lead to correct answers sufficiently often are collected to form a rule library. In the deduction stage, the LLM is then prompted to employ the learned rule library to perform reasoning to answer test questions. Experiments on both numerical reasoning and relational reasoning problems show that HtT improves existing prompting methods, with an absolute gain of 11-27% in accuracy. The learned rules are also transferable to different models and to different forms of the same problem.},
   author = {Zhaocheng Zhu and Yuan Xue and Xinyun Chen and Denny Zhou and Jian Tang and Dale Schuurmans and Hanjun Dai},
   month = {10},
   title = {Large Language Models can Learn Rules},
   url = {https://arxiv.org/abs/2310.07064v1},
   year = {2023},
}
@article{Zheng2023,
   abstract = {We present Step-Back Prompting, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide the reasoning steps, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of Step-Back Prompting with PaLM-2L models and observe substantial performance gains on a wide range of challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting improves PaLM-2L performance on MMLU Physics and Chemistry by 7% and 11%, TimeQA by 27%, and MuSiQue by 7%.},
   author = {Huaixiu Steven Zheng and Swaroop Mishra and Xinyun Chen and Heng-Tze Cheng and Ed H Chi Quoc and V Le and Denny Zhou and Google Deepmind},
   month = {10},
   title = {Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models},
   url = {https://arxiv.org/abs/2310.06117v1},
   year = {2023},
}
@article{Showkat2022,
   abstract = {The rise of automated text processing systems has led to the development of tools designed for a wide variety of application domains. These technologies are often developed to support non-technical users such as domain experts and are often developed in isolation of the tools primary user. While such developments are exciting, less attention has been paid to domain experts' expectations about the values embedded in these automated systems. As a step toward addressing that gap, we examined values expectations of journalists and legal experts. Both these domains involve extensive text processing and place high importance on values in professional practice. We engaged participants from two non-profit organizations in two separate co-speculation design workshops centered around several speculative automated text processing systems. This study makes three interrelated contributions. First, we provide a detailed investigation of domain experts' values expectations around future NLP systems. Second, the speculative design fiction concepts, which we specifically crafted for these investigative journalists and legal experts, illuminated a series of tensions around the technical implementation details of automation. Third, our findings highlight the utility of design fiction in eliciting not-to-design implications, not only about automated NLP but also about technology more broadly. Overall, our study findings provide groundwork for the inclusion of domain experts values whose expertise lies outside of the field of computing into the design of automated NLP systems.},
   author = {Dilruba Showkat and Eric P.S. Baumer},
   doi = {10.1145/3532106.3533483},
   isbn = {9781450393584},
   journal = {DIS 2022 - Proceedings of the 2022 ACM Designing Interactive Systems Conference: Digital Wellbeing},
   keywords = {Design Fiction,Natural Language Processing (NLP) Automation,Participatory Design Fiction Workshop,Values in Automated NLP},
   month = {6},
   pages = {100-122},
   publisher = {Association for Computing Machinery, Inc},
   title = {"It's Like the Value System in the Loop": Domain Experts' Values Expectations for NLP Automation},
   year = {2022},
}
@article{Mok2019,
   author = {Wai Yin Mok and Jonathan R. Mok},
   doi = {10.1145/3322640.3326737},
   isbn = {9781450367547},
   journal = {Proceedings of the 17th International Conference on Artificial Intelligence and Law, ICAIL 2019},
   month = {6},
   pages = {266-267},
   publisher = {Association for Computing Machinery, Inc},
   title = {Legal machine-learning analysis: First steps towards A.I. Assisted legal research},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3322640.3326737},
   year = {2019},
}
@article{Shea-Blymyer2022,
   abstract = {This work gives a logical characterization of the (ethical and social) obligations of an agent trained with Reinforcement Learning (RL). An RL agent takes actions by following a utility-maximizing policy. We maintain that the choice of utility function embeds ethical and social values implicitly, and that it is necessary to make these values explicit. This work provides a basis for doing so. First, we propose a probabilistic deontic logic that is suited for formally specifying the obligations of a stochastic system, including its ethical obligations. We prove some useful validities about this logic, and how its semantics are compatible with those of Markov Decision Processes (MDPs). Second, we show that model checking allows us to prove that an agent has a given obligation to bring about some state of affairs-meaning that by acting optimally, it is seeking to reach that state of affairs. We develop a model checker for our logic against MDPs. Third, we observe that it is useful for a system designer to obtain a logical characterization of her system's obligations, which is potentially more interpretable and helpful in debugging than the expression of a utility function. Enumerating all the obligations of an agent is impractical, so we propose a Bayesian optimization routine that learns to generate a system's obligations that the system designer deems interesting. We implement the model checking and Bayesian optimization routines, and demonstrate their effectiveness with an initial pilot study. This work provides a rigorous method to characterize utility-maximizing agents in terms of the (ethical and social) obligations that they implicitly seek to satisfy.},
   author = {Colin Shea-Blymyer and Houssam Abbas},
   doi = {10.1145/3514094.3534163},
   isbn = {9781450392471},
   journal = {AIES 2022 - Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
   keywords = {deontic logic,explainability,machine ethics,model checking,normative systems},
   month = {7},
   pages = {653-663},
   publisher = {Association for Computing Machinery, Inc},
   title = {Generating Deontic Obligations from Utility-Maximizing Systems},
   url = {https://doi.org/10.1145/3514094.3534163},
   year = {2022},
}
@article{Jiang2023,
   abstract = {Legal syllogism is a form of deductive reasoning commonly used by legal professionals to analyze cases. In this paper, we propose legal syllogism prompting (LoT), a simple prompting method to teach large language models (LLMs) for legal judgment prediction. LoT teaches only that in the legal syllogism the major premise is law, the minor premise is the fact, and the conclusion is judgment. Then the models can produce a syllogism reasoning of the case and give the judgment without any learning, fine-tuning, or examples. On CAIL2018, a Chinese criminal case dataset, we performed zero-shot judgment prediction experiments with GPT-3 models. Our results show that LLMs with LoT achieve better performance than the baseline and chain of thought prompting, the state-of-art prompting method on diverse reasoning tasks. LoT enables the model to concentrate on the key information relevant to the judgment and to correctly understand the legal meaning of acts, as compared to other methods. Our method enables LLMs to predict judgment along with law articles and justification, which significantly enhances the explainability of models.},
   author = {Cong Jiang and Xiaolei Yang},
   doi = {10.1145/3594536.3595170},
   isbn = {9798400701979},
   keywords = {Artificial intelligence KEYWORDS large language models, legal syllogism, legal judgment prediction, chain of thought,CCS CONCEPTS • Applied computing → Law,• Computing methodologies → Natural language generation},
   month = {6},
   pages = {417-421},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Legal Syllogism Prompting},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3594536.3595170},
   year = {2023},
}
@article{Wright2020,
   abstract = {A "rightful machine" is an explicitly moral, autonomous machine agent whose behavior conforms to principles of justice and the positive public law of a legitimate state. In this paper, I set out some basic elements of a deontic logic appropriate for capturing conflicting legal obligations for purposes of programming rightful machines. Justice demands that the prescriptive system of enforceable public laws be consistent, yet statutes or case holdings may often describe legal obligations that contradict; moreover, even fundamental constitutional rights may come into conflict. I argue that a deontic logic of the law should not try to work around such conflicts but, instead, identify and expose them so that the rights and duties that generate inconsistencies in public law can be explicitly qualified and the conflicts resolved. I then argue that a credulous, non-monotonic deontic logic can describe inconsistent legal obligations while meeting the normative demand for consistency in the prescriptive system of public law. I propose an implementation of this logic via a modified form of "answer set programming," which I demonstrate with some simple examples.},
   author = {Ava Thomas Wright},
   doi = {10.1145/3375627.3375867},
   isbn = {9781450371100},
   keywords = {Answer Set Programming,Conflicts,Deontic Logic,Justice,Law,Logic Programming,Machine Ethics,Rightful Machines},
   month = {2},
   pages = {392-392},
   publisher = {Association for Computing Machinery (ACM)},
   title = {A Deontic Logic for Programming Rightful Machines},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3375627.3375867},
   year = {2020},
}
@article{Rotolo2015,
   abstract = {This paper offers a new logical machinery for reasoning about interpretive canons. We identify some options for modeling reasoning about interpretations and show that interpretative argumentation has a distinctive structure where the claim that a legal text ought or may be interpreted in a certain way can be supported or attacked by arguments, whose conicts may have to be assessed according to further arguments.},
   author = {Antonino Rotolo and Guido Governatori and Giovanni Sartor},
   doi = {10.1145/2746090.2746100},
   isbn = {9781450335225},
   journal = {Proceedings of the International Conference on Artificial Intelligence and Law},
   keywords = {Argumentation,Defeasible Logic,Legal Interpretation},
   month = {6},
   pages = {99-108},
   publisher = {Association for Computing Machinery},
   title = {Deontic defeasible reasoning in legal interpretation: Two options for modelling interpretive arguments},
   volume = {08-12-June-2015},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/2746090.2746100},
   year = {2015},
}
@article{Joshi2020,
   abstract = {Federal government agencies and organizations doing business with them have to adhere to the Code of Federal Regulations (CFR). The CFRs are currently available as large text documents that are not...},
   author = {Karuna Pande Joshi and Srishty Saha},
   doi = {10.1145/3425192},
   issn = {26390175},
   issue = {3},
   journal = {Digital Government: Research and Practice},
   keywords = {Deep learning,Semantic Web,compliance,legal text analytics},
   month = {11},
   publisher = {
		ACM
		PUB27
		New York, NY, USA
	},
   title = {A Semantically Rich Framework for Knowledge Representation of Code of Federal Regulations},
   volume = {1},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3425192},
   year = {2020},
}
@article{Libal2019,
   abstract = {In this article we introduce a logical structure for normative reasoning, called Normative Detachment Structure with Ideal Conditions, that can be used to represent the content of certain legal texts in a normalized way. The structure exploits the deductive properties of a system of bimodal logic able to distinguish between ideal and actual normative statements, as well as a novel formalization of conditional normative statements able to capture interesting cases of contrary-to-duty reasoning and to avoid deontic paradoxes. Furthermore, we illustrate how the theoretical framework proposed can be mechanized to get an automated procedure of query-answering on an example of legal text.},
   author = {Tomer Libal and Matteo Pascucci},
   city = {New York, NY, USA},
   doi = {10.1145/3322640.3326707},
   isbn = {9781450367547},
   journal = {Proceedings of the 17th International Conference on Artificial Intelligence and Law, ICAIL 2019},
   keywords = {Deontic logic,Legal reasoning,Normative ideality},
   month = {6},
   pages = {63-72},
   publisher = {Association for Computing Machinery, Inc},
   title = {Automated reasoning in normative detachment structures with ideal conditions},
   url = {https://doi.org/10.1145/3322640.3326721},
   year = {2019},
}
@article{Araszkiewicz2023,
   abstract = {We present an approach designed to support the process of legislative drafting by helping to detect errors in a normative text. It is based on a framework allowing for representation and comparison of structure and semantic content of legal provisions. Such comparison serves as a starting point for detection of (potential) legislative errors. The approach provides in particular criteria to select provisions to be compared, related to the phenomenon of provisions overlapping. We show that specific cases of such an overlap may amount to legislative errors. The presented framework enables a precise and transparent account of these errors. We also acknowledge that textual provisions enable various interpretations, while the error methodology detection assumes that the semantic representation of provisions is a result of a specific interpretation. We introduce the notion of Constraining Interpretive Rules which are used to evaluate the acceptability of specific interpretations of legal provisions. We discuss the features of the model on a real example and we present an implementation of the approach by using semantic technologies. CCS CONCEPTS • Computing methodologies → Reasoning about belief and knowledge ; • Applied computing → Law.},
   author = {Michał Araszkiewicz and Enrico Francesconi and European Parliament and Tomasz Zurek},
   doi = {10.1145/3594536.3595172},
   isbn = {9798400701979},
   keywords = {Enrico Francesconi,Interpretation of statutory law ACM Reference Format: Michał Araszkiewicz,Legal knowledge representation,Legislative drafting support,Legislative errors,Semantic tech-nologies,and Tomasz Zurek 2023 Iden-tification of Legislative},
   month = {6},
   pages = {2-11},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Identification of Legislative Errors},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3594536.3595172},
   year = {2023},
}
@article{Witt2021,
   abstract = {A critical challenge in "Rules as Code"("RaC") initiatives is enhancing legal accuracy. In this paper, we present the preliminary results of a two-week, first of its kind experiment that aims to shed light on how different legally trained people interpret and convert Australian Commonwealth legislation into machine-executable code. We find that coders collaboratively agreeing on key legal terms, or atoms, before commencing independent coding work can significantly increase the similarity of their encoded rules. Participants nonetheless made a range of divergent interpretive choices, which we argue are most likely due to: (1) the complexity of statutory interpretation, (2) encoded provisions having varying levels of granularity, and (3) the functionality of our coding language. Based on these findings, we draw an important distinction between processes for technical validation of encoded rules, which focus on ensuring rules adhere to select coding languages and conventions, and processes of legal alignment, which we conceptualise as enhancing congruence between the encoded provisions and the true meaning of the statutory text in line with the modern approach to statutory interpretation. We argue that these processes are distinct but both critically important in enhancing the accuracy of encoded rules. We conclude by emphasising the need in RaC initiatives for multi-disciplinary expertise across specific legal subject matters, statutory interpretation and technical programming.},
   author = {Alice Witt and Anna Huggins and Guido Governatori and Joshua Buckley},
   doi = {10.1145/3462757.3466083},
   isbn = {9781450385268},
   journal = {Proceedings of the 18th International Conference on Artificial Intelligence and Law, ICAIL 2021},
   keywords = {digitising legislation,legal alignment,machine-executable code,rules as code,statutory interpretation,technical processes},
   month = {6},
   pages = {139-148},
   publisher = {Association for Computing Machinery, Inc},
   title = {Converting copyright legislation into machine-executable code: Interpretation, coding validation and legal alignment},
   volume = {21},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3462757.3466083},
   year = {2021},
}
@article{Rotolo2021,
   abstract = {A legal procedure in court proceedings is a sequence of actions in which the last action is (the creation of) a(n individual) norm, where the court settles that it is obligatory in the interest of some agents that other agents bring about a certain state of affairs. This paper models legal procedures by using a variant of Propositional Dynamic Logic (PDL) enriched with a preference operator for prioritising procedural actions. The key reason towards the usage of PDL is that, in procedural law, claims and resolutions resemble programs to be executed. Requests are organised in a preference order and resolutions have their own dynamics of execution (either spontaneously by the one obliged and/or by force of law).},
   author = {Antonino Rotolo and Clara Smith},
   doi = {10.1145/3462757.3466089},
   isbn = {9781450385268},
   journal = {Proceedings of the 18th International Conference on Artificial Intelligence and Law, ICAIL 2021},
   keywords = {PDL,legal procedures,preferences},
   month = {6},
   pages = {220-224},
   publisher = {Association for Computing Machinery, Inc},
   title = {Modelling legal procedures},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3462757.3466089},
   year = {2021},
}
@article{Maranhao2019,
   abstract = {The research aims at a formal definition of constructive interpretation in law as the dynamic of revision of theories about the normative system, embedding a model of balancing values [13] into an architecture of i/o logics representing conceptual, deontological and axiological rules [11]. We also introduce new revision operators which are relevant in the context of value assessments.},
   author = {Juliano Maranhão and Giovanni Sartor},
   doi = {10.1145/3322640.3326709},
   isbn = {9781450367547},
   journal = {Proceedings of the 17th International Conference on Artificial Intelligence and Law, ICAIL 2019},
   keywords = {Balancing values,Input/output logics,Normative systems},
   month = {6},
   pages = {219-223},
   publisher = {Association for Computing Machinery, Inc},
   title = {Value assessment and revision in legal interpretation},
   volume = {5},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3322640.3326709},
   year = {2019},
}
@article{Parvizimosaed2022,
   abstract = {Legal contracts specify requirements for business transactions. As any other requirements specification, contracts may contain errors and violate properties expected by contracting parties. Symboleo was recently proposed as a formal specification language for legal contracts. This paper presents SymboleoPC, a tool for analyzing Symboleo contracts using model checking. It highlights the architecture, implementation and testing of the tool, as well as a scalability evaluation with respect to the size of contracts and properties to be checked through a series of experiments. The results suggest that SymboleoPC can be usefully applied to the analysis of formal specifications of contracts with real-life sizes and structures.},
   author = {Alireza Parvizimosaed and Marco Roveri and Aidin Rasti and Daniel Amyot and Luigi Logrippo and John Mylopoulos},
   doi = {10.1145/3550355.3552449},
   isbn = {9781450394666},
   journal = {Proceedings - 25th ACM/IEEE International Conference on Model Driven Engineering Languages and Systems, MODELS 2022},
   keywords = {formal specification languages,legal contracts,model checking,nuXmv,performance analysis,smart contracts,software requirements specifications},
   month = {10},
   pages = {278-288},
   publisher = {Association for Computing Machinery, Inc},
   title = {Model-checking legal contracts with SymboleoPC},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3550355.3552449},
   year = {2022},
}
@article{Shea-Blymyer2020,
   abstract = {We consider the pressing question of how to model, verify, and ensure that autonomous systems meet certain obligations (like the obligation to respect traffic laws), and refrain from impermissible behavior (like recklessly changing lanes). Temporal logics are heavily used in autonomous system design; however, as we illustrate here, temporal (alethic) logics alone are inappropriate for reasoning about obligations of autonomous systems. This paper proposes the use of Dominance Act Utilitarianism (DAU), a deontic logic of agency, to encode and reason about obligations of autonomous systems. We use DAU to analyze Intel's Responsibility-Sensitive Safety (RSS) proposal as a real-world case study. We demonstrate that DAU can express well-posed RSS rules, formally derive undesirable consequences of these rules, illustrate how DAU could help design systems that have specific obligations, and how to model-check DAU obligations.},
   author = {Colin Shea-Blymyer and Houssam Abbas},
   doi = {10.1145/3365365.3382203},
   isbn = {9781450370189},
   journal = {HSCC 2020 - Proceedings of the 23rd International Conference on Hybrid Systems: Computation and Control ,part of CPS-IoT Week},
   month = {4},
   publisher = {Association for Computing Machinery, Inc},
   title = {A deontic logic analysis of autonomous systems' safety},
   volume = {11},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3365365.3382203},
   year = {2020},
}
@article{Joshi2021,
   abstract = {Contracts are agreements between parties engaging in economic transactions. They specify deontic modalities that the signatories should be held responsible for and state the penalties or actions to be taken if the stated agreements are not met. Additionally, contracts have also been known to be source of Software Engineering (SE) requirements. Identifying the deontic modalities in contracts can therefore add value to the Requirements Engineering (RE) phase of SE. The complex and ambiguous language of contracts make it difficult and time-consuming to identify the deontic modalities (obligations, permissions, prohibitions), embedded in the text. State-of-art neural network models are effective for text classification; however, they require substantial amounts of training data. The availability of contracts data is sparse owing to the confidentiality concerns of customers. In this paper, we leverage the linguistic and taxonomical similarities between regulations (available abundantly in the public domain) and contracts to demonstrate that it is possible to use regulations as training data for classifying deontic modalities in real-life contracts. We discuss the results of a range of experiments from the use of rule-based approach to Bidirectional Encoder Representations from Transformers (BERT) for automating the classification of deontic modalities. With BERT, we obtained an average precision and recall of 90% and 89.66% respectively.},
   author = {Vivek Joshi and Preethu Rose Anish and Smita Ghaisas},
   doi = {10.1145/3468264.3473921},
   isbn = {9781450385626},
   journal = {ESEC/FSE 2021 - Proceedings of the 29th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
   keywords = {BERT,BiLSTM,Business Contract,Deep Learning Models,Deontic Modality,Domain Adaptation,Regulation},
   month = {8},
   pages = {1275-1280},
   publisher = {Association for Computing Machinery, Inc},
   title = {Domain adaptation for an automated classification of deontic modalities in software engineering contracts},
   volume = {21},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3468264.3473921},
   year = {2021},
}
@article{Areces2023,
   abstract = {We introduce a logic for representing the deontic notion of knowingly complying-associated to an agent's conciousness of taking a normative course of action for achieving a certain goal. Our logic features an operator for describing normative courses of actions, and another operator for describing what each agent knowingly complies with. We provide a sound and complete axiom system for our logic, and study the computational complexity of its satis-fiability problem. Finally, we extend our logic with an additional operator for capturing the general abilities of the agents. This operator enables us to distinguish 'what agents can do' and 'what agents do according to norms'. For this extension, we also provide a sound and complete axiom system.},
   author = {Carlos Areces and Valentin Cassano and Pablo F Castro and Raul Fervari and Andrés R Saravia},
   doi = {10.5555/3545946.3598659},
   journal = {IFAAMAS},
   keywords = {Deontic Logic,Knowingly Complying,Multiagent},
   title = {A Deontic Logic of Knowingly Complying},
   volume = {9},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.5555/3545946.3598659},
   year = {2023},
}
@article{Palmirani2011,
   abstract = {Legal reasoning involves multiple temporal dimensions but the existing state of the art of legal representation languages does not allow us to easily combine expressiveness, performance and legal reasoning requirements. Moreover we also aim at the combination of legal temporal reasoning with the defeasible logic approach, maintaining a computable complexity. The contribution of this work is to extend LKIF-rules with temporal dimensions and defeasible tools, extending our previous work [17]. © 2011 Authors.},
   author = {Monica Palmirani and Guido Governatori and Giuseppe Contissa},
   doi = {10.1145/2018358.2018378},
   isbn = {9781450307550},
   journal = {Proceedings of the International Conference on Artificial Intelligence and Law},
   keywords = {LKIF-rule,rule modelling,temporal dimension},
   pages = {131-135},
   title = {Modelling temporal legal rules},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/2018358.2018378},
   year = {2011},
}
@article{Cheng2006,
   abstract = {This paper discusses why classical mathematical logic, its various classical conservatives extensions, or its non-classical alternatives are not suitable candidates for the right fundamental logic to underlie legal information systems, and show that deontic relevant logic is a more hopeful candidate for the right fundamental logic. Copyright 2006 ACM.},
   author = {Jingde Cheng},
   doi = {10.1145/1141277.1141353},
   isbn = {1595931082},
   journal = {Proceedings of the ACM Symposium on Applied Computing},
   keywords = {Legal knowledge representation and reasoning,Relevant logic},
   pages = {319-320},
   publisher = {Association for Computing Machinery},
   title = {Deontic relevant logic as the logical basis for legal information systems},
   volume = {1},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/1141277.1141353},
   year = {2006},
}
@article{Governatori2021,
   abstract = {Legal documents often contain references to either other documents, or other parts (of the same document). The use of references is meant to reduce the complexity of the documents; however, they pose serious concerns for the formal (logical) representation of the norms stipulated in the document itself. We propose an approach to directly model the references in a logic language and to resolve them during the computation of the legal effects in force in a case. The approach is proved to be computationally feasible and to have an efficient algorithmic implementation.},
   author = {Guido Governatori and Francesco Olivieri},
   city = {New York, NY, USA},
   doi = {10.1145/3462757},
   isbn = {9781450385268},
   journal = {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Law},
   keywords = {Automated reasoning KEYWORDS Defeasible Deontic Logic, legal references,CCS CONCEPTS • Computing methodologies → Artificial intelligence,• Ap-plied computing → Law,• Theory of computation → Proof theory},
   pages = {10},
   publisher = {ACM},
   title = {Unravel Legal References in Defeasible Deontic Logic},
   url = {https://doi.org/10.1145/3462757.3466080},
   year = {2021},
}
@article{Allen1999,
   author = {Layman E Allen and Charles S Saxon Abstract},
   doi = {10.1145/323706.323718},
   month = {6},
   pages = {80-89},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Application of enriched deontic legal relations},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/323706.323718},
   year = {1999},
}
@article{Governatori2023,
   abstract = {What happens if the way in which we handle a genuine deontic conflict -i.e., a deontic ambiguity-matters regarding the application of other norms that are not directly affected by that conflict? We argue that the law requires sometimes propagating the ambiguity to other norms and sometimes confining it to some norms only. We explore this issue and model different reasoning patterns. The problem is addressed in a new variant of Defeasible Deontic Logic. The contribution of this paper is threefold: (a) we extend the treatment of ambiguity blocking and propagation to Defeasible Deontic Logic; (b) we discuss reasoning patterns in the law, especially in criminal law, where we need to deal with both ambiguity blocking and ambiguity propagation in the same legal system and logic; (c) we devise an annotated variant of Defeasible Deontic Logic where we distinguish literals that must be obtained through an ambiguity-blocking mechanism from those that are derived using an ambiguity-propagating mechanism.},
   author = {Guido Governatori and Antonino Rotolo},
   doi = {10.1145/3594536.3595175},
   isbn = {9798400701979},
   keywords = {Automated reason-ing,CCS CONCEPTS • Theory of computation → Proof theory,• Applied computing → Law KEYWORDS Defeasible Deontic Logic, Deontic Ambiguities, Ambiguity Block-ing, Ambiguity Propagation},
   month = {6},
   pages = {91-100},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Deontic Ambiguities in Legal Reasoning},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3594536.3595175},
   year = {2023},
}
@article{Savelka2023,
   abstract = {We evaluated the capability of a state-of-the-art generative pre-trained transformer (GPT) model to perform semantic annotation of short text snippets (one to few sentences) coming from legal documents of various types. Discussions of potential uses (e.g., document drafting, summarization) of this emerging technology in legal domain have intensified, but to date there has not been a rigorous analysis of these large language models' (LLM) capacity in sentence-level semantic annotation of legal texts in zero-shot learning settings. Yet, this particular type of use could unlock many practical applications (e.g., in contract review) and research opportunities (e.g., in empirical legal studies). We fill the gap with this study. We examined if and how successfully the model can semantically annotate small batches of short text snippets (10-50) based exclusively on concise definitions of the semantic types. We found that the GPT model performs surprisingly well in zero-shot settings on diverse types of documents (F 1 = .73 on a task involving court opinions, .86 for contracts, and .54 for statutes and regulations). These findings can be leveraged by legal scholars and practicing lawyers alike to guide their decisions in integrating LLMs in wide range of workflows involving semantic annotation of legal texts.},
   author = {Jaromir Savelka},
   doi = {10.1145/3594536.3595161},
   isbn = {9798400701979},
   keywords = {CCS CONCEPTS • Applied computing → Law; Annotation KEYWORDS Semantic legal annotation,GPT,adjudicatory decisions,contracts,generative pre-trained transformers,statutory and regulatory provisions,transfer learning,zero-shot},
   month = {6},
   pages = {447-451},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Unlocking Practical Applications in Legal Domain},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3594536.3595161},
   year = {2023},
}
@article{Stavropoulou2020,
   abstract = {One of the main objectives of the paper is to present a big open legal data (BOLD) platform that will enable easy and advanced access to open legal information across the European Union. We envision facilitating public access for all types of users to open legal data accessed and served via a Legal Web platform in a customizable, structured, intuitive and easy-To-handle way. The user-centric suite of services to be offered include: i) research through legal corpora, analyzing the alignment of national legislation with EU legislation, ii) comparing national laws which target the same life events, iii) analyzing the references to European legislation by national laws, iv) analyzing related laws within the same Member State, v) timeline analysis for all legal acts, vi) visualization of the progress and current status of a specific national or European piece of legislation and vii) sentiment analysis towards new legislation. To implement these, the Manylaws platform builds upon data analytics, text mining, semantic analysis and interactive visualization approaches, for multilingual resources, that will allow users to pose advanced queries, upon which the system will semantically correlate them with annotated resources, to allow the user to retrieve relevant and detailed results, visualized in intuitive ways, and thus allowing better understanding of legal information or building innovative data-centric services. The paper thus describes the novel ManyLaws platform architecture and design specification that will drive the implementation and operation of the ManyLaws portal service infrastructure. The ManyLaws architecture and design specification have been devised following co-design paradigms with legal and parliamentary stakeholders, and software engineering practices for elaboration of its components and their connectors.},
   author = {Stefania Stavropoulou and Ilias Romas and Sofia Tsekeridou and Michalis Avgerinos Loutsaris and Thomas Lampoltshammer and LÅ'rinc Thurnay and Shefali Virkar and Guenther Schefbeck and Nektarios Kyriakou and Zoi Lachana and Charalampos Alexopoulos and Yannis Charalambidis},
   doi = {10.1145/3428502.3428610},
   isbn = {9781450376747},
   journal = {ACM International Conference Proceeding Series},
   keywords = {European legislation,legal informatics,legal ontologies,national legal framework,open legal data,semantic legal annotation,semantics,text mining,visualization},
   month = {9},
   pages = {723-730},
   publisher = {Association for Computing Machinery},
   title = {Architecting an innovative big open legal data analytics, search and retrieval platform},
   volume = {8},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3428502.3428610},
   year = {2020},
}
@article{Lachana2020,
   abstract = {One of the most promising developments comes with the use of innovative technologies and thus with the availability of novel services. The combination of text mining with legal elements may contrib...},
   author = {Zoi Lachana and Michalis Avgerinos Loutsaris and Charalampos Alexopoulos and Yannis Charalabidis},
   doi = {10.4018/IJESMA.2020040105},
   issn = {19416288},
   issue = {2},
   journal = {International Journal of E-Services and Mobile Applications (IJESMA)},
   keywords = {Automated Codification,Automated Interrelation,Laws/Connecting Graphs,Legal Elements Intercon-Nections,Legal Text Mining,Parsing Legal Texts},
   month = {4},
   pages = {79-96},
   publisher = {IGI Global},
   title = {Automated Analysis and Interrelation of Legal Elements Based on Text Mining},
   volume = {12},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.4018/IJESMA.2020040105},
   year = {2020},
}
@article{Casanovas2016,
   abstract = {Ontology-driven systems with reasoning capabilities in the legal field are now better understood. Legal concepts are not discrete, but make up a dynamic continuum between common sense terms, specific technical use, and professional knowledge, in an evolving institutional reality. Thus, the tension between a plural understanding of regulations and a more general understanding of law is bringing into view a new landscape in which general legal frameworks -grounded in well-known legal theories stemming from 20th-century c. legal positivism or sociological jurisprudence -are made compatible with specific forms of rights management on the Web. In this sense, Semantic Web tools are not only being designed for information retrieval, classification, clustering, and knowledge management. They can also be understood as regulatory tools, i.e. as components of the contemporary legal architecture, to be used by multiple stakeholders -front-line practitioners, policymakers, legal drafters, companies, market agents, and citizens. That is the issue broadly addressed in this Special Issue on the Semantic Web for the Legal Domain, overviewing the work carried out over the last fifteen years, and seeking to foster new research in this field, beyond the state of the art.},
   author = {Pompeu Casanovas and Monica Palmirani and Silvio Peroni and Tom Van Engers and Fabio Vitali},
   doi = {10.3233/SW-160224},
   issn = {22104968},
   issue = {3},
   journal = {Semantic Web},
   keywords = {Linked Data,Semantic Web and its applications,Semantic Web for the Legal Domain,ontologies,regulations,rights},
   month = {3},
   pages = {213-227},
   publisher = {IOS Press},
   title = {Semantic Web for the Legal Domain: The'next step},
   volume = {7},
   year = {2016},
}
@article{Loutsaris2021,
   abstract = {The globalization of communication networks and the possibilities offered by the information and communication technologies (ICTs) significantly change the public sector's operation and services. Digital Governance is now integrated into administrations' policies and programs at all levels: local, regional, national, European. At the national level, there is a requirement to provide electronic public services according to citizens' needs while, in the sense of globalization, at the European level, there are many programs (e.g., the Europe 2005 and i2010 program) emphasizing the Digital Governance world (or better Digital Governance community) that indicates rapid changes not only in the sense of the change in the public sector's systems but also in the mentality that the public sector operates. On the other hand, Digital Governance's evolution affects societies intensively, emphasizing the importance of cross-border interaction and information sharing between them. [6]. Concerning the legal informatics domain, this can result in changing governments' operations in many ways [2]. By now, the massive amount of each country's legal information currently remains fragmented across multiple national databases and systems or even better legal databases. Most of these legal databases result from the significant advancements in the "legal informatics"research field that observed since governments have started to promote the development of legal information systems [9]. This research contributes to this purpose by developing an open and automated legal system capable of providing any EU country's legal information based on the existing ontologies.},
   author = {Michalis Avgerinos Loutsaris and Zoi Lachana and Charalampos Alexopoulos and Yannis Charalabidis},
   doi = {10.1145/3463677.3463730},
   isbn = {9781450384926},
   journal = {ACM International Conference Proceeding Series},
   keywords = {big open legal data,legal information systems,legal ontologies},
   month = {6},
   pages = {522-532},
   publisher = {Association for Computing Machinery},
   title = {Legal Text Processing: Combing two legal ontological approaches through text mining},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3463677.3463730},
   year = {2021},
}
@article{Gopalan2012,
   abstract = {Developing an access control system that satisfies the requirements expressed in regulations, such as the Health Insurance Portability and Accountability Act (HIPAA), can help ensure regulatory compliance in software systems. A usage control model that specifies the rules governing information access and usage, as expressed in law, is an important step towards achieving such compliance. Software systems that handle health records must comply with regulations in the HIPAA Privacy and Security Rules. Herein, we analyze the HIPAA Privacy Rule using a grounded theory methodology coupled with an inquiry driven approach to determine the components that must be supported by a usage control model to achieve regulatorycompliant health records usage. In this paper, we propose a usage control model, UCON LEGAL, which extends UCON ABC with components to model purposes, cross-references, exceptions, conditions, and logs. We also employ UCON LEGAL to show how to express the access and usage rules we identified in the HIPAA Privacy Rule. Our analysis yielded seven types of conditions specific to HIPAA that we include in UCON LEGAL; these conditions were previously unsupported by existing usage control models. Copyright © 2012 ACM.},
   author = {Ramya Gopalan and Annie Antön and Jon Doyle},
   doi = {10.1145/2110363.2110391},
   isbn = {9781450307819},
   journal = {IHI'12 - Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium},
   keywords = {Access control,Electronic health records,HIPAA-compliance,Legal compliance,Personal health information,Regulatory compliance,Software requirements,Usage control},
   pages = {227-236},
   title = {UCON LEGAL: A usage control model for HIPAA},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/2110363.2110391},
   year = {2012},
}
@article{Flouris2023,
   abstract = {Robust Usage Control (UC) mechanisms are necessary to protect sensitive data and resources, especially when these are distributed across multiple nodes or users. Existing solutions have limitations in expressing and enforcing usage control policies due to difficulties in capturing complex requirements and the lack of formal semantics necessary for automated compliance checking. To address these challenges, we propose GUCON, a generic policy framework that allows for the expression of and reasoning over granular UC policies. This is achieved by leveraging the expressiveness and semantics of graph pattern expressions, as well as the flexibility of deontic concepts. Additionally , GUCON incorporates algorithms for conflict detection, resolution, compliance and requirements checking, ensuring active policy enforcement. We demonstrate the effectiveness of our framework by proposing instantiations using SHACL, OWL and ODRL. We show how instantiations provide a bridge between abstract formalism and concrete implementations, thus allowing existing reasoners and implementations to be leveraged.},
   author = {I Akaichi and G Flouris and I Fundulaki and S Kirrane},
   journal = {penni.wu.ac.atI Akaichi, G Flouris, I Fundulaki, S Kirranepenni.wu.ac.at},
   keywords = {Control ·,Deontic,Enforcement,Policy ·,Reasoning ·,Rules ·,Usage},
   title = {GUCON: A Generic Graph Pattern based Policy Framework for Usage Control Enforcement},
   url = {https://penni.wu.ac.at/papers/RuleML%20RR%202023%20GUCON-A%20Generic%20Graph%20Pattern%20based%20Policy%20Framework%20for%20Usage%20Control%20Enforcement.pdf},
}
@article{Bench-Capon2022,
   abstract = {We present argumentation schemes to model reasoning with legal cases. We provide schemes for each of the three stages that take place after the facts are established: factor ascription, issue resolution and outcome determination. The schemes are illustrated with examples from a specific legal domain, US Trade Secrets law, and the wider applicability of these schemes is discussed.},
   author = {Trevor Bench-Capon and Katie Atkinson},
   month = {10},
   title = {Using Argumentation Schemes to Model Legal Reasoning},
   url = {https://arxiv.org/abs/2210.00315v1},
   year = {2022},
}
@inbook{Dimishkovska2017,
   author = {Ana Dimishkovska},
   city = {Dordrecht},
   doi = {10.1007/978-94-007-6730-0_228-1},
   journal = {Encyclopedia of the Philosophy of Law and Social Philosophy},
   pages = {1-7},
   publisher = {Springer Netherlands},
   title = {Deontic Logic and Legal Rules},
   url = {https://link.springer.com/10.1007/978-94-007-6730-0_228-1},
   year = {2017},
}
@article{Wyner2011,
   abstract = {Rules in regulations such as found in the US Federal Code of Regulations can be expressed using conditional and deontic rules. Identifying and extracting such rules from the language of the source material would be useful for automating rulebook management and translating into an executable logic. The paper presents a linguistically-oriented, rule-based approach, which is in contrast to a machine learning approach. It outlines use cases, discusses the source materials, reviews the methodology, then provides initial results and future steps. © 2011 The authors and IOS Press. All rights reserved.},
   author = {Adam Wyner and Wim Peters},
   doi = {10.3233/978-1-60750-981-3-113},
   isbn = {9781607509806},
   issn = {09226389},
   journal = {Frontiers in Artificial Intelligence and Applications},
   keywords = {Conditionals,Deontic Logic,Regulation,Text analysis},
   pages = {113-122},
   publisher = {IOS Press},
   title = {On Rule Extraction from Regulations},
   volume = {235},
   url = {https://ebooks.iospress.nl/doi/10.3233/978-1-60750-981-3-113},
   year = {2011},
}
@article{Zhong2020,
   abstract = {Legal Artificial Intelligence (LegalAI) focuses on applying the technology of artificial intelligence, especially natural language processing, to benefit tasks in the legal domain. In recent years, LegalAI has drawn increasing attention rapidly from both AI researchers and legal professionals, as LegalAI is beneficial to the legal system for liberating legal professionals from a maze of paperwork. Legal professionals often think about how to solve tasks from rule-based and symbol-based methods, while NLP researchers concentrate more on data-driven and embedding methods. In this paper, we introduce the history, the current state, and the future directions of research in LegalAI. We illustrate the tasks from the perspectives of legal professionals and NLP researchers and show several representative applications in LegalAI. We conduct experiments and provide an in-depth analysis of the advantages and disadvantages of existing works to explore possible future directions. You can find the implementation of our work from https://github.com/thunlp/CLAIM.},
   author = {Haoxi Zhong and Chaojun Xiao and Cunchao Tu and Tianyang Zhang and Zhiyuan Liu and Maosong Sun},
   doi = {10.18653/v1/2020.acl-main.466},
   isbn = {9781952148255},
   issn = {0736587X},
   journal = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
   month = {4},
   pages = {5218-5230},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {How Does NLP Benefit Legal System: A Summary of Legal Artificial Intelligence},
   url = {https://arxiv.org/abs/2004.12158v5},
   year = {2020},
}
@article{Boella2013,
   abstract = {Legal ontology is one of the most researched areas of Artificial Intelligence & Law, but is less applied in the commercial world. This is mainly due to a historical focus on general purpose legal ontologies that do not capture the variety of definitions and interpretations that apply in different contexts, and a focus on automated extraction over manual verification in a domain where accuracy is of utmost importance. In this paper, we show how the use of a domain-specific ontology within a sophisticated legal monitoring software managed by legal experts can help compliance officers in banks and insurance companies comply with strict regulatory duties in a highly complex and constantly evolving area of law.},
   author = {Guido Boella and Llio Humphreys and Marco Martin and Piercarlo Rossi and Leendert Van Der Torre and Andrea Violato},
   doi = {10.1007/978-3-7908-2789-7_62/COVER},
   isbn = {9783790827897},
   journal = {Information Systems: Crossroads for Organization, Management, Accounting and Engineering: ItAIS: The Italian Association for Information Systems},
   month = {9},
   pages = {571-578},
   publisher = {Physica-Verlag},
   title = {Eunomos, A legal document and knowledge management system for regulatory compliance},
   volume = {9783790827897},
   url = {https://link.springer.com/chapter/10.1007/978-3-7908-2789-7_62},
   year = {2013},
}
@article{Winkels2005,
   abstract = {The Dutch Tax and Customs Administration (DTCA) is one of many organizations that deal with a multitude of electronic legal data, from various sources and in different formats. In this paper, we describe the results of a study aimed at better access to these sources by having a supplier and format independent knowledge store that describes the sources and their interrelations in a semantic network. Furthermore we developed parsers to automatically detect the identity of sources and typed references within the sources to other legal documents. These parsers can be used to fill and update the semantic network as new documents are added. Copyright 2005 ACM.},
   author = {Radboud Winkels and Alexander Boer and Emile De Maat and Tom Van Engers and Matthijs Breebaart and Henri Melger},
   doi = {10.1145/1165485.1165505},
   isbn = {1595930817},
   journal = {Proceedings of the International Conference on Artificial Intelligence and Law},
   keywords = {Legal CMS,Reference parsing,Semantic web,Standardisation},
   pages = {125-132},
   title = {Constructing a semantic network for legal content},
   url = {https://dl.acm.org/doi/10.1145/1165485.1165505},
   year = {2005},
}
@article{Junior2019,
   abstract = {This paper presents our experience on building RDF knowledge graphs for an industrial use case in the legal domain. The information contained in legal information systems are often accessed through simple keyword interfaces and presented as a simple list of hits. In order to improve search accuracy one may avail of knowledge graphs, where the semantics of the data can be made explicit. Significant research effort has been invested in the area of building knowledge graphs from semi-structured text documents, such as XML, with the prevailing approach being the use of mapping languages. In this paper, we present a semantic model for representing legal documents together with an industrial use case. We also present a set of use case requirements based on the proposed semantic model, which are used to compare and discuss the use of state-of-the-art mapping languages for building knowledge graphs for legal data.},
   author = {Ademar Crotti Junior and Fabrizio Orlandi and Declan O'Sullivan and Christian Dirschl and Quentin Reul},
   issn = {16130073},
   journal = {CEUR Workshop Proceedings},
   keywords = {Legal Knowledge Graphs,Legal se-mantic model,Legal semantic model,Mapping languages,Mapping languages·},
   month = {11},
   publisher = {CEUR-WS},
   title = {Using Mapping Languages for Building Legal Knowledge Graphs from XML Files},
   volume = {2599},
   url = {https://arxiv.org/abs/1911.07673v1},
   year = {2019},
}
@article{Tang2020,
   abstract = {Knowledge graph has become an essential tool for semantic analysis with the development of natural language processing and deep learning. A high-quality knowledge graph is handy for building a high-performance knowledge-driven application. Despite recent advances in information extraction (IE) techniques, no suitable automated methods can be applied to constructing a domain-specific, comprehensive, and high-quality knowledge graph. However, a semi-automatic strategy, which can ensure the basic quality requirements of a knowledge graph, has been successfully implemented in the elementary science domain. This paper presents a semantic annotation system developed for building a high-quality legal knowledge graph (SALKG) using the semi-automatic strategy. We introduce its system design, architecture, algorithms, functions, and implementation. To investigate the effectiveness of SALKG, we conduct a preliminary annotation experiment with 280 legal texts which were collected from the Harvard Caselaw Access Project. The user evaluation from 32 graduate students demonstrates the high usability of SALKG in semantic annotation and the potential for building a high-quality legal knowledge graph. The system can also be adapted to other fields for constructing domain-specific knowledge graphs.},
   author = {Mingwei Tang and Cui Su and Haihua Chen and Jingye Qu and Junhua Ding},
   doi = {10.1109/BIGDATA50022.2020.9378107},
   isbn = {9781728162515},
   journal = {Proceedings - 2020 IEEE International Conference on Big Data, Big Data 2020},
   keywords = {Annotation System,Knowledge Graph,Legal Text,Semantic Annotation},
   month = {12},
   pages = {2153-2159},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {SALKG: A Semantic Annotation System for Building a High-quality Legal Knowledge Graph},
   year = {2020},
}
@article{Casellas2011,
   author = {Núria Casellas},
   city = {Dordrecht},
   doi = {10.1007/978-94-007-1497-7},
   isbn = {978-94-007-1496-0},
   publisher = {Springer Netherlands},
   title = {Legal Ontology Engineering},
   volume = {3},
   url = {https://link.springer.com/10.1007/978-94-007-1497-7},
   year = {2011},
}
@article{Leone2018,
   author = {V Leone and L Di Caro and S Villata},
   abstract = {Legal ontologies aim to provide a structured representation of legal concepts and their interconnections. These ontologies are then exploited to support information extraction and question answering in the legal domain. In addition, given the increasing importance of the Web of Data in public administration and in companies, being able to provide machine-readable legal information is becoming a valuable and desired contribution. The problem is that these ontologies are not always very transparent to end users, in particular if they lack legal knowledge. In this context, we present the InvestigatiOnt tool which aims to ease the interaction of end users with legal ontologies in order to spread the use of machine-processable legal information as well as its understanding.}
   journal = {scholar.archive.org},
   title = {Legal Ontologies and How to Choose Them: the InvestigatiOnt Tool},
   url = {https://scholar.archive.org/work/aua6jz2zlfbh3ffesziwmr6ihm/access/wayback/http://ceur-ws.org/Vol-2180/paper-36.pdf},
}
@article{Bos2008,
   abstract = {Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorial Grammar (CCG) and Discourse Representation Theory (DRT). Used together with the C&C tools, Boxer reaches more than 95% coverage on newswire texts. The semantic representations produced by Boxer, known as Discourse Representation Structures (DRSs), incorporate a neo-Davidsonian representations for events, using the VerbNet inventory of thematic roles. The resulting DRSs can be translated to ordinary first-order logic formulas and be processing by standard theorem provers for first-order logic. Boxer's performance on the shared task for comparing semantic represtations was promising. It was able to produce complete DRSs for all seven texts. Manually inspecting the output revealed that: (a) the computed predicate argument structure was generally of high quality, in particular dealing with hard constructions involving control or coordination; (b) discourse structure triggered by conditionals, negation or discourse adverbs was overall correctly computed; (c) some measure and time expressions are correctly analysed, others aren't; (d) several shallow analyses are given for lexical phrases that require deep analysis; (e) bridging references and pronouns are not resolved in most cases. Boxer is distributed with the C&C tools and freely available for research purposes.},
   author = {Johan Bos},
   doi = {10.3115/1626481.1626503},
   journal = {Semantics in Text Processing, STEP 2008 - Conference Proceedings},
   pages = {277-286},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {Wide-coverage semantic analysis with Boxer},
   url = {https://www.researchgate.net/publication/254502455_Wide-Coverage_Semantic_Analysis_with_Boxer},
   year = {2008},
}
@article{Ngo2010,
   abstract = {Traditional information retrieval systems represent documents and queries by keyword sets. However, the content of a document or a query is mainly defined by both keywords and named entities occurring in it. Named entities have ontological features, namely, their aliases, classes, and identifiers, which are hidden from their textual appearance. Besides, the meaning of a query may imply latent named entities that are related to the apparent ones in the query. We propose an ontology-based generalized vector space model to semantic text search. It exploits ontological features of named entities and their latently related ones to reveal the semantics of documents and queries. We also propose a framework to combine different ontologies to take their complementary advantages for semantic annotation and searching. Experiments on a benchmark dataset show better search quality of our model to other ones. © 2010 Springer-Verlag Berlin Heidelberg.},
   author = {Vuong M. Ngo and Tru H. Cao},
   doi = {10.1007/978-3-642-12090-9_4/COVER},
   isbn = {9783642120893},
   issn = {1860949X},
   journal = {Studies in Computational Intelligence},
   pages = {41-52},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Ontology-based query expansion with latently related named entities for semantic text searchfs},
   volume = {283},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-12090-9_4},
   year = {2010},
}
@article{Erekhinskaya2020,
   abstract = {In the last years, Artificial Intelligence and Deep Learning have matured from a facinating research area to real-word applications across multiple domains. Enterprises adopt data-driven approaches for various use cases. With the increased adoption, such issues as governance of the models, deployment, scalability, reusablity and maintenance are widely addressed on the engineering side, but not so much on the knowledge side. In this paper, we demonstrate 10 ways of leveraging ontology for Natural Language Processing. Specifically, we explore the usage of ontologies and related standards for labeling schema, configuration, providing lexical data, powering rule engine and automated generation of rules, as well as providing a standard output format. Additionally, we discuss three NLP-based applications: semantic search, question answering and natural language querying and show how they can benefit from ontology usage. The paper summarizes our experience of using ontology in a number of projects for medical, enterprise, financial, legal and security domains.},
   author = {Tatiana Erekhinskaya and Dmitriy Strebkov and Sujal Patel and Mithun Balakrishna and Marta Tatu and Dan Moldovan},
   doi = {10.1145/3391274.3393639},
   isbn = {9781450379748},
   journal = {Proceedings of the International Workshop on on Semantic Big Data, SBD 2020 - In conjunction with the 2020 ACM SIGMOD/PODS Conference},
   keywords = {domain-specific knowledge,labeling,natural language processing,natural language querying,ontologies,semantic graph},
   month = {6},
   publisher = {Association for Computing Machinery, Inc},
   title = {Ten ways of leveraging ontologies for natural language processing and its enterprise applications},
   url = {https://dl.acm.org/doi/10.1145/3391274.3393639},
   year = {2020},
}
@article{Vallet2007,
   abstract = {Personalized content retrieval aims at improving the retrieval process by taking into account the particular interests of individual users. However, not all user preferences are relevant in all situations. It is well known that human preferences are complex, multiple, heterogeneous, changing, even contradictory, and should be understood in context with the user goals and tasks at hand. In this paper, we propose a method to build a dynamic representation of the semantic context of ongoing retrieval tasks, which is used to activate different subsets of user interests at runtime, in a way that out-of-context preferences are discarded. Our approach is based on an ontology-driven representation of the domain of discourse, providing enriched descriptions of the semantics involved in retrieval actions and preferences, and enabling the definition of effective means to relate preferences and context. © 2007 IEEE.},
   author = {David Vallet and Pablo Castells and Miriam Fernández and Phivos Mylonas and Yannis Avrithis},
   doi = {10.1109/TCSVT.2007.890633},
   issn = {10518215},
   issue = {3},
   journal = {IEEE Transactions on Circuits and Systems for Video Technology},
   keywords = {Content search and retrieval,Context modeling,Ontology,Personalization},
   month = {3},
   pages = {336-345},
   title = {Personalized content retrieval in context using ontological knowledge},
   volume = {17},
   year = {2007},
}
@article{Fernandez2011,
   abstract = {Currently, techniques for content description and query processing in Information Retrieval (IR) are based on keywords, and therefore provide limited capabilities to capture the conceptualizations associated with user needs and contents. Aiming to solve the limitations of keyword-based models, the idea of conceptual search, understood as searching by meanings rather than literal strings, has been the focus of a wide body of research in the IR field. More recently, it has been used as a prototypical scenario (or even envisioned as a potential "killer app") in the Semantic Web (SW) vision, since its emergence in the late nineties. However, current approaches to semantic search developed in the SW area have not yet taken full advantage of the acquired knowledge, accumulated experience, and technological sophistication achieved through several decades of work in the IR field. Starting from this position, this work investigates the definition of an ontology-based IR model, oriented to the exploitation of domain Knowledge Bases to support semantic search capabilities in large document repositories, stressing on the one hand the use of fully fledged ontologies in the semantic-based perspective, and on the other hand the consideration of unstructured content as the target search space. The major contribution of this work is an innovative, comprehensive semantic search model, which extends the classic IR model, addresses the challenges of the massive and heterogeneous Web environment, and integrates the benefits of both keyword and semantic-based search. Additional contributions include: an innovative rank fusion technique that minimizes the undesired effects of knowledge sparseness on the yet juvenile SW, and the creation of a large-scale evaluation benchmark, based on TREC IR evaluation standards, which allows a rigorous comparison between IR and SW approaches. Conducted experiments show that our semantic search model obtained comparable and better performance results (in terms of MAP and P@10 values) than the best TREC automatic system. © 2010 Elsevier B.V. All rights reserved.},
   author = {Miriam Fernández and Iván Cantador and Vanesa López and David Vallet and Pablo Castells and Enrico Motta},
   doi = {10.1016/J.WEBSEM.2010.11.003},
   issn = {1570-8268},
   issue = {4},
   journal = {Journal of Web Semantics},
   keywords = {Information Retrieval,Semantic Web,Semantic search},
   month = {12},
   pages = {434-452},
   publisher = {Elsevier},
   title = {Semantically enhanced Information Retrieval: An ontology-based approach},
   volume = {9},
   year = {2011},
}
@article{Marie2011,
   abstract = {The objective of ONTORULE is to enable users, from business executives over business analysts to IT developers, to interact in their own way with the part of a business application that is relevant to them. This extended abstract describes the approach the ONTORULE project proposes to business rule application development, and it introduces the architecture and the semantic technologies that we develop for that purpose and that are validated and demonstrated in two pilot applications. © 2011 Springer-Verlag.},
   author = {Christian De Sainte Marie and Miguel Iglesias Escudero and Peter Rosina},
   doi = {10.1007/978-3-642-23580-1_3/COVER},
   isbn = {9783642235795},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Business rule management systems,ONTORULE,knowledge management,ontology,rules},
   pages = {24-29},
   publisher = {Springer, Berlin, Heidelberg},
   title = {The ONTORULE project: Where ontology meets business rules},
   volume = {6902 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-23580-1_3},
   year = {2011},
}
@article{Kholkar2017,
   abstract = {Enterprises today face the problem of complying with ever-increasing regulation. Use of rule engines for implementing compliance is widespread, however, the rule base needs to be encoded manually. We present a method using model-driven architecture (MDA) to automate generation of rules in a rule language, from a platform-independent model derived from a specification given by domain experts. We demonstrate how a Semantics of Business Vocabulary and Rules (SBVR) model of regulation rules can serve as the common source model for generating rules on various categories of rule engine platforms. The approach is illustrated using a real-life case study from the MiFID-2 financial regulation.},
   author = {Deepali Kholkar and Sagar Sunkle and Vinay Kulkarni},
   doi = {10.5220/0006216406170628},
   isbn = {9789897582103},
   journal = {MODELSWARD 2017 - Proceedings of the 5th International Conference on Model-Driven Engineering and Software Development},
   keywords = {Business Rule Management Systems,CIM,Defeasible Logic,Fact-oriented Model,Formal Compliance Checking,Model Driven Architecture,Model Transformation,PIM,PSM,Production Rule Systems,Rule Engines,SBVR},
   pages = {617-628},
   publisher = {SciTePress},
   title = {Towards automated generation of regulation rule bases using MDA},
   volume = {2017-January},
   url = {https://dl.acm.org/doi/10.5220/0006216406170628},
   year = {2017},
}
@article{Njonko2012,
   abstract = {This paper presents a methodology for transforming business rules (BR) written in natural language (NL) such as English into a set of executable models as Unified Modeling Language (UML), Structured Query language (SQL), etc. As the direct automatic transformation of NL specifications to executable models is very difficult due to the inherent ambiguities of NL, this methodology aims at using the Semantics of Business Vocabulary and Business Rules (SBVR) as an intermediate model front-ended by Micro-Systemic Linguistic Analysis (MSLA) because of their mathematical underpinnings. SBVR is a Semantic Metamodel (SMM) introduced by the Object Management Group (OMG) for specifying semantic models of business using NL. SBVR is not only easy to process by machine since it is grounded in formal logic, but it is also easy to understand both by software developers and other stakeholders. Given that SBVR is fully integrated in OMG's Model Driven Architecture (MDA) and behaves as a Computational Independent Model (CIM), our approach advocates model transformation which is the key constituent of the MDA standard. © 2012 IEEE.},
   author = {Paul Brillant Feuto Njonko and Walid El Abed},
   doi = {10.1109/ICSAI.2012.6223550},
   isbn = {9781467301992},
   journal = {2012 International Conference on Systems and Informatics, ICSAI 2012},
   keywords = {Business Rules,Executable Models,MDA,MSLA,Natural Language,SBVR,Semantic Metamodel},
   pages = {2453-2457},
   title = {From natural language business requirements to executable models via SBVR},
   year = {2012},
}
@article{Ghali2012,
   abstract = {Ontologies are known to be suitable to represent business knowledge. However, in the Business Rules community the business models are usually represented using object models (OM). Many of the existing Business Rules Management Systems (BRMS) allow the Business Users to represent Business Object Models in their own proprietary languages. Some work has been done in the last years to bridge the gap between the ontologies and the Business Rules. A pragmatic approach consist in projecting ontologies into the Object Models used by the BRMS, to ease the use of ontologies by the Business Users. The main issue with this approach is that the expressive power of the targeted Object Model is not enough to cope with the content of the ontology. Hence, the translation looses some of the information contained in the ontology, such as axioms. The aim of this paper, is to go a step further using this approach by translating some of the axioms defined in an OWL ontology into Business Rules. This translation brings at least two benefits: (i) it allow the Business Users to understand better the content of the Ontology by having some of its axioms in the rule language they are used to. (ii) at the run-time level, the translated axioms will be handled by the rule engine. We explain the basic mechanism of this translation and detail its implementation in the JRules BRMS system. © 2012 Springer-Verlag.},
   author = {Adil El Ghali and Amina Chniti and Hugues Citeau},
   doi = {10.1007/978-3-642-32689-9_6/COVER},
   isbn = {9783642326882},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Business Rules,OWL,Ontologies,RIF-PRD},
   pages = {62-76},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Bringing OWL ontologies to the business rules users},
   volume = {7438 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-32689-9_6},
   year = {2012},
}
@article{Skersys2018,
   abstract = {In model-driven information systems engineering, model transformations reside at the very core of this paradigm. Indeed, model transformations (in particular, model-to-model, or M2M) are a must-have feature of any modern model-driven approach supported by CASE technology. Model transformations are intended to raise quality of the models under development, and also speed-up the modeling itself by bringing in certain level of automation into the development process. Nevertheless, due to certain objective reasons, the level of such automation is spread unevenly throughout the development process – in this respect, Business Modeling and System Analysis are, arguably, the most underdeveloped phases of the model-driven information systems development life cycle. In this paper, we show how M2M transformation technology was used to extract well-structured business vocabularies and business rules from formal use case models represented through a set of use case diagrams; Object Management Group's (OMG) standards Semantics for Business Vocabulary and Rules (SBVR) and Unified Modeling Language (UML) were used for this purpose. The proposed solution consists of two concurrent approaches, namely, automatic and semi-automatic, which may be used selectively to achieve the best expected result. Basic implementation aspects of the solution integrating both approaches are also briefly presented in the paper. While UML use case models is the main subject in this research, the proposed solution may be adopted for other UML and MOF-based models as well.},
   author = {Tomas Skersys and Paulius Danenas and Rimantas Butleris},
   doi = {10.1016/J.JSS.2018.03.061},
   issn = {0164-1212},
   journal = {Journal of Systems and Software},
   keywords = {Model-to-model transformation,SBVR business rules,SBVR business vocabulary,UML use case diagram},
   month = {7},
   pages = {111-130},
   publisher = {Elsevier},
   title = {Extracting SBVR business vocabularies and business rules from UML use case diagrams},
   volume = {141},
   year = {2018},
}
@article{Omrane2011,
   abstract = {This paper describes a platform that helps industrial domain experts to preserve the connection between textual sources and formalized business rules by using lexicalized ontologies both for links and for storage of the conceptual knowledge. Business Rules Management Systems (BRMSs) are used to update and query business rules of an automotive use case. They rely strongly on domain ontologies, which model the business knowledge and provide a conceptual vocabulary for the formalization of the rules that are expressed in written policies. We show that lexicalized ontologies are a key component of such BRMSs and how such knowledge can be encoded. Our proposed solution supports domain experts in the automotive industry in understanding and maintaining their business rules by presenting the relevant source documents that were used to create the ontological concepts. The use case is based on a car development scenario that models the connection between car testing scenarios, e.g., safety tests, and the methods and tools used to analyze and prepare these tests. The intended solution has been developed in the ONTORULE project and is still work in progress. © 2011 Springer-Verlag.},
   author = {Nouha Omrane and Adeline Nazarenko and Peter Rosina and Sylvie Szulman and Christoph Westphal},
   doi = {10.1007/978-3-642-24908-2_21/COVER},
   isbn = {9783642249075},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Business rules,domain ontology,semantic annotation},
   pages = {179-192},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Lexicalized ontology for a business rules management platform: An automotive use case},
   volume = {7018 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-24908-2_21},
   year = {2011},
}
@article{Sullivan2023,
   abstract = {Decision trees remain one of the most popular machine learning models today, largely due to their out-of-the-box performance and interpretability. In this work, we present a Bayesian approach to decision tree induction via maximum a posteriori inference of a posterior distribution over trees. We first demonstrate a connection between maximum a posteriori inference of decision trees and AND/OR search. Using this connection, we propose an AND/OR search algorithm, dubbed MAPTree, which is able to recover the maximum a posteriori tree. Lastly, we demonstrate the empirical performance of the maximum a posteriori tree both on synthetic data and in real world settings. On 16 real world datasets, MAPTree either outperforms baselines or demonstrates comparable performance but with much smaller trees. On a synthetic dataset, MAPTree also demonstrates greater robustness to noise and better generalization than existing approaches. Finally, MAPTree recovers the maxiumum a posteriori tree faster than existing sampling approaches and, in contrast with those algorithms, is able to provide a certificate of optimality. The code for our experiments is available at https://github.com/ThrunGroup/maptree.},
   author = {Colin Sullivan and Mo Tiwari and Sebastian Thrun},
   month = {9},
   title = {MAPTree: Beating "Optimal" Decision Trees with Bayesian Decision Trees},
   url = {https://arxiv.org/abs/2309.15312v1},
   year = {2023},
}
@article{Ye2023,
   abstract = {The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scalable prompts based on natural language instructions, and use natural language to describe the geometric structure and node features of the graph for instruction tuning an LLM to perform learning and inference on graphs in a generative manner. Our method exceeds all competitive GNN baselines on ogbn-arxiv, Cora and PubMed datasets, which demonstrates the effectiveness of our method and sheds light on generative large language models as the foundation model for graph machine learning.},
   author = {Ruosong Ye and Caiqi Zhang and Runhui Wang and Shuyuan Xu and Yongfeng Zhang},
   month = {8},
   title = {Natural Language is All a Graph Needs},
   url = {https://arxiv.org/abs/2308.07134v3},
   year = {2023},
}
@article{Guo2023,
   abstract = {Large language models~(LLM) like ChatGPT have become indispensable to artificial general intelligence~(AGI), demonstrating excellent performance in various natural language processing tasks. In the real world, graph data is ubiquitous and an essential part of AGI and prevails in domains like social network analysis, bioinformatics and recommender systems. The training corpus of large language models often includes some algorithmic components, which allows them to achieve certain effects on some graph data-related problems. However, there is still little research on their performance on a broader range of graph-structured data. In this study, we conduct an extensive investigation to assess the proficiency of LLMs in comprehending graph data, employing a diverse range of structural and semantic-related tasks. Our analysis encompasses 10 distinct tasks that evaluate the LLMs' capabilities in graph understanding. Through our study, we not only uncover the current limitations of language models in comprehending graph structures and performing associated reasoning tasks but also emphasize the necessity for further advancements and novel approaches to enhance their graph processing capabilities. Our findings contribute valuable insights towards bridging the gap between language models and graph understanding, paving the way for more effective graph mining and knowledge extraction.},
   author = {Jiayan Guo and Lun Du and Hengyu Liu and Mengyu Zhou and Xinyi He and Shi Han},
   month = {5},
   title = {GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking},
   url = {https://arxiv.org/abs/2305.15066v2},
   year = {2023},
}
@article{Ye2023,
   abstract = {The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scalable prompts based on natural language instructions, and use natural language to describe the geometric structure and node features of the graph for instruction tuning an LLM to perform learning and inference on graphs in a generative manner. Our method exceeds all competitive GNN baselines on ogbn-arxiv, Cora and PubMed datasets, which demonstrates the effectiveness of our method and sheds light on generative large language models as the foundation model for graph machine learning.},
   author = {Ruosong Ye and Caiqi Zhang and Runhui Wang and Shuyuan Xu and Yongfeng Zhang},
   month = {8},
   title = {Natural Language is All a Graph Needs},
   url = {https://arxiv.org/abs/2308.07134v3},
   year = {2023},
}
@article{Moon2023,
   abstract = {We present Any-Modality Augmented Language Model (AnyMAL), a unified model that reasons over diverse input modality signals (i.e. text, image, video, audio, IMU motion sensor), and generates textual responses. AnyMAL inherits the powerful text-based reasoning abilities of the state-of-the-art LLMs including LLaMA-2 (70B), and converts modality-specific signals to the joint textual space through a pre-trained aligner module. To further strengthen the multimodal LLM's capabilities, we fine-tune the model with a multimodal instruction set manually collected to cover diverse topics and tasks beyond simple QAs. We conduct comprehensive empirical analysis comprising both human and automatic evaluations, and demonstrate state-of-the-art performance on various multimodal tasks.},
   author = {Seungwhan Moon and Andrea Madotto and Zhaojiang Lin and Tushar Nagarajan and Matt Smith and Shashank Jain and Chun-Fu Yeh and Prakash Murugesan and Peyman Heidari and Yue Liu and Kavya Srinet and Babak Damavandi and Anuj Kumar},
   month = {9},
   title = {AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model},
   url = {https://arxiv.org/abs/2309.16058v1},
   year = {2023},
}
@article{Nazarenko2011,
   abstract = {Knowledge acquisition is a key issue in the business rule methodology. As Natural Language (NL) policies and regulations are often important or even contractual sources of knowledge, we propose a framework for acquisition and maintenance of business rules based on NL texts. It enables business experts to author the specification of rule applications without the help of knowledge engineers. This framework has been created as part of the ONTORULE project, which is defining an integrated platform for acquisition, maintenance and execution of business-oriented knowledge bases combining ontologies and rules. Our framework relies on a data structure, called "index", encompassing and connecting the source text, the ontology and a textual representation of rules. Textual rules are as close to the Structured English representation of SBVR as possible for business users in charge of rule elicitation. The index relies on W3C technologies, which makes the tools interoperable and enable semantic search. We show that such an index structure supports the parallel maintenance of policy documents and knowledge bases (acquisition, consistency check and update). Two detailed examples with preliminary results are provided, one from air travel and the other from the automotive industry. © Springer-Verlag Berlin Heidelberg 2011.},
   author = {Adeline Nazarenko and Abdoulaye Guissé and François Lévy and Nouha Omrane and Sylvie Szulman},
   doi = {10.1007/978-3-642-22546-8_9/COVER},
   isbn = {9783642225451},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {99-113},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Integrating written policies in business rule management systems},
   volume = {6826 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-22546-8_9},
   year = {2011},
}
@article{Anand2018,
   abstract = {Business rules control and constrain the behavior and structure of the business system in terms of its policies and principles. Business rules are restructured frequently as per the internal or external circumstances based on market opportunities, statutory regulations, and business focus. The current practice in industry, of detecting inconsistencies manually, is error prone, due to the size, complexity and ambiguity in representation using natural language. Our work detects inconsistencies in business rules based on model checking that exploits the FOL basis of SBVR specification. We aim to reduce the burden on solvers and obtain effective system level test data, leading to the development of a novel inconsistency rule checker based on extracting the unsatisfiable cores using solvers like Z3, CVC4, etc. We introduce the concept of graphical clusters, to partition SBVR vocabularies and represent the former exploiting the many-sorted logic and graph reachability algorithm, thus reducing the domain of quantification and the number of uninterpreted functions. The translation of SBVR to SMT-LIBv2 is implemented as part of our tool BuRRiTo. Experimental results are shown on industrial level rule sets.},
   author = {Kritika Anand and Pavan Kumar Chittimalli and Ravindra Naik},
   doi = {10.1007/978-3-319-73305-0_6/COVER},
   isbn = {9783319733043},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Business rules,First order logic,SBVR,SMT solvers},
   pages = {80-96},
   publisher = {Springer Verlag},
   title = {An automated detection of inconsistencies in SBVR-based business rules using many-sorted logic},
   volume = {10702 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-319-73305-0_6},
   year = {2018},
}
@article{Ceci2016,
   abstract = {The user has requested enhancement of the downloaded file.},
   author = {Marcello Ceci and Firas Al Khalil and Leona O'brien},
   journal = {researchgate.net},
   title = {Making Sense of Regulations with SBVR.},
   url = {https://www.researchgate.net/profile/Firas-Al-Khalil-2/publication/305333061_Making_Sense_of_Regulations_with_SBVR/links/5788f3b708ae7a588ee853ec/Making-Sense-of-Regulations-with-SBVR.pdf},
   year = {2016},
}
@article{Emani2019,
   abstract = {Many standards exist to formalize legal texts and rules. The same is true for legal ontologies. However, there is no proof theory to draw conclusions for these ontologically modeled rules. We address this gap by the proposal of a new modeling of deontic statements, and then we use this modeling to propose reasoning mechanisms to answer deontic questions i.e., questions like “Is it mandatory/permitted/prohibited to..”. We also show that using this modeling, it is possible to check the consistency of a deontic rule base. This work stands as a first important step towards a proof theory over a deontic rule base.},
   author = {Cheikh Kacfah Emani and Yannis Haralambous},
   doi = {10.1007/978-3-030-21348-0_14/COVER},
   isbn = {9783030213473},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {209-224},
   publisher = {Springer Verlag},
   title = {Deontic reasoning for legal ontologies},
   volume = {11503 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-21348-0_14},
   year = {2019},
}
@article{Šukys2012,
   abstract = {The goal of the paper is to present question patterns in structured natural language and their transformations into ontology query language SPARQL for allowing business participants to communicate with business software services and data in more flexible and friendly way. The structured language is based on Semantics of Business Vocabulary and Business Rules (SBVR) metamodel, which allows creating and managing business vocabularies and business rules in specific domains. The current paper is focused on transforming question patterns, including usage of synonyms and synonymous forms; projecting formulations constrained by atomic formulations based on facts and fact types; projections on several variables; restricting query results by auxiliary variables constrained by various logical formulations; supplementing questions with derivation rules from SBVR vocabulary of business rules, etc. Patterns are followed by examples, proved by implemented SBVR query editor and SBVR to SPARQL transformations. © 2012 Springer-Verlag.},
   author = {Algirdas Šukys and Lina Nemuraite and Bronius Paradauskas},
   doi = {10.1007/978-3-642-33308-8_36},
   journal = {Communications in Computer and Information Science},
   keywords = {OWL2,SBVR,SBVR question,SPARQL,SWRL,business rule,business vocabulary,ontology},
   pages = {436-451},
   title = {Representing and Transforming SBVR Question Patterns into SPARQL},
   volume = {319 CCIS},
   url = {https://www.researchgate.net/profile/Algirdas-Sukys/publication/283555035_Representing_and_transforming_SBVR_question_patterns_into_SPARQL/links/56a492ad08ae1b65113252df/Representing-and-transforming-SBVR-question-patterns-into-SPARQL.pdf},
   year = {2012},
}
@article{Bouzidi2011,
   abstract = {This paper gives an overview of a formal semantic-based approach of modeling some regulations in the photovoltaic field to help the delivering of technical assessments at the French scientific center on Building Industry (CSTB). Starting from regulatory texts, we first explicit SBVR rules and then formalize them into ontology-based rules in the SPARQL language. These are exploited in the modeling of the compliance checking process required for the delivering of technical assessments.},
   author = {Khalil Bouzidi and Catherine Faron-Zucker and Bruno Fiés and Nhan Le Thanh and Khalil Riad Bouzidi and Bruno Fies and Nhan Le Thanh An and Nhan Le Than},
   doi = {10.1007/978-3-642-23580-1_19},
   journal = {researchgate.net},
   keywords = {Building Industry,E-Government,E-regulations,Knowledge Management,Ontology,Semantic Web},
   pages = {244-249},
   title = {An ontological approach for modeling technical standards for compliance checking},
   volume = {6902 LNCS},
   url = {https://www.researchgate.net/profile/Catherine-Faron-Zucker/publication/221211655_An_Ontological_Approach_for_Modeling_Technical_Standards_for_Compliance_Checking/links/5832f60208aef19cb81c885e/An-Ontological-Approach-for-Modeling-Technical-Standards-for-Compliance-Checking.pdf},
   year = {2011},
}
@article{Sukys2012,
   abstract = {The paper presents transformation framework from questions in structured language based on Semantics of Business Vocabulary and Rules (SBVR) into SPARQL queries over ontologies defined in Web Ontology Language OWL 2 and, possibly, supplemented with Semantic Web rules SWRL. Such transformation depends on OWL 2 ontology related with corresponding SBVR vocabulary and rules. The current work considers a family of transformations and metamodels required for relating ontologies, rules, SPARQL queries and real business data supported by computerised information systems, as well as establishes requirements for harmonizing the coexistence and preserving semantics of these different representations.},
   author = {Algirdas Sukys and Lina Nemuraite and Bronius Paradauskas and Edvinas Sinkevicius},
   isbn = {9781612082233},
   journal = {Citeseer},
   keywords = {SBVR,SBVR question,SPARQL,business rule,business vocabulary,ontology},
   title = {Transformation Framework for SBVR based Semantic Queries in Business Information Systems},
   url = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=4dbd014ad97245712e8e64291034b9f1aef175c7},
   year = {2012}
}
@article{Leopold2012,
   abstract = {Process Modeling is a widely used concept for understanding, documenting and also redesigning the operations of organizations. The validation and usage of process models is however affected by the fact that only business analysts fully understand them in detail. This is in particular a problem because they are typically not domain experts. In this paper, we investigate in how far the concept of verbalization can be adapted from object-role modeling to process models. To this end, we define an approach which automatically transforms BPMN process models into natural language texts and combines different techniques from linguistics and graph decomposition in a flexible and accurate manner. The evaluation of the technique is based on a prototypical implementation and involves a test set of 53 BPMN process models showing that natural language texts can be generated in a reliable fashion. © 2012 Springer-Verlag Berlin Heidelberg.},
   author = {Henrik Leopold and Jan Mendling and Artem Polyvyanyy},
   doi = {10.1007/978-3-642-31095-9_5},
   isbn = {9783642310942},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Business Process Models,Natural Language Generation,Verbalization},
   pages = {64-79},
   title = {Generating natural language texts from business process models},
   volume = {7328 LNCS},
   year = {2012},
}
@article{Skersys2022,
   abstract = {The Object Management Group (OMG) has put considerable effort into the standardization of various business modeling aspects within the context of model-driven systems development. Indeed, the Business Process Model and Notation (BPMN) is now arguably the most popular process modeling language. At the same time, the Semantics of Business Vocabulary and Business Rules (SBVR), which is a novel and formally sound standard for the specification of virtually any kind of knowledge using controlled natural language, is also gaining its grounds. Nonetheless, the integration between these two very much related standards remains weak. In this paper, we present one such integration effort, namely an approach for the extraction of SBVR process rules from BPMN processes. To accomplish this, we utilized model-to-model transformation technology, which is one of the core features of Model-Driven Architecture. At the core of the presented solution stands a set of model transformation rules and two algorithms specifying the formation of formally defined process rules from process models. Basic implementation aspects, together with the source code of the solution, are also presented in the paper. The experimental results acquired from the automatic model transformation have shown full compliance with the benchmark results and cover the entirety of the specified flow of work defined in the experimental process models. Following this, it is safe to conclude that the set of specified transformation rules and algorithms was sufficient for the given scope of the experiment, providing a solid background for the practical application and future developments of the solution.},
   author = {Tomas Skersys and Paulius Danenas and Egle Mickeviciute and Rimantas Butleris},
   doi = {10.3390/APP12188976},
   issn = {2076-3417},
   issue = {18},
   journal = {Applied Sciences 2022, Vol. 12, Page 8976},
   keywords = {BPMN,SBVR,business process model,model transformation,process rules},
   month = {9},
   pages = {8976},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {Transforming BPMN Processes to SBVR Process Rules with Deontic Modalities},
   volume = {12},
   url = {https://www.mdpi.com/2076-3417/12/18/8976/htm https://www.mdpi.com/2076-3417/12/18/8976},
   year = {2022},
}
@article{Racz2010,
   abstract = {Governance, Risk and Compliance (GRC) is an emerging topic in the business and information technology world. However to this day the concept behind the acronym has neither been adequately researched, nor is there a common understanding among professionals. The research at hand provides a frame of reference for research of integrated GRC that was derived from the first scientifically grounded definition of the term. By means of a literature review the authors merge observations, an analysis of existing definitions and results from prior surveys in the derivation of a single-phrase definition. The definition is evaluated and improved through a survey among GRC professionals. Finally a frame of reference for GRC research is constructed. © 2010 Springer-Verlag.},
   author = {Nicolas Racz and Edgar Weippl and Andreas Seufert},
   doi = {10.1007/978-3-642-13241-4_11/COVER},
   isbn = {3642132405},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {GRC,compliance,definition,governance,integrated,risk},
   pages = {106-117},
   publisher = {Springer, Berlin, Heidelberg},
   title = {A frame of reference for research of integrated Governance, Risk and Compliance (GRC)},
   volume = {6109 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-13241-4_11},
   year = {2010},
}
@article{Douzon2023,
   abstract = {Transformer-based Language Models are widely used in Natural Language Processing related tasks. Thanks to their pre-training, they have been successfully adapted to Information Extraction in business documents. However, most pre-training tasks proposed in the literature for business documents are too generic and not sufficient to learn more complex structures. In this paper, we use LayoutLM, a language model pre-trained on a collection of business documents, and introduce two new pre-training tasks that further improve its capacity to extract relevant information. The first is aimed at better understanding the complex layout of documents, and the second focuses on numeric values and their order of magnitude. These tasks force the model to learn better-contextualized representations of the scanned documents. We further introduce a new post-processing algorithm to decode BIESO tags in Information Extraction that performs better with complex entities. Our method significantly improves extraction performance on both public (from 93.88 to 95.50 F1 score) and private (from 84.35 to 84.84 F1 score) datasets composed of expense receipts, invoices, and purchase orders.},
   author = {Thibault Douzon and Stefan Duffner and Christophe Garcia and Jérémy Espinas},
   doi = {10.1007/978-3-031-06555-2_8},
   isbn = {9783031065545},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {BIESO Decoding,Business documents,Document understanding,Information extraction,Pre-,Pre-training,Training · BIESO,Transformer,Understanding ·},
   month = {9},
   pages = {111-125},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Improving Information Extraction on Business Documents with Specific Pre-Training Tasks},
   volume = {13237 LNCS},
   url = {https://arxiv.org/abs/2309.05429v1},
   year = {2023},
}
@article{Dragoni2016,
   abstract = {Legal texts express conditions in natural language describing what is permitted, forbidden or mandatory in the context they regulate. Despite the numerous approaches tackling the problem of moving from a natural language legal text to the respective set of machine-readable conditions, results are still unsatisfiable and it remains a major open challenge. In this paper, we propose a preliminary approach which combines different Natural Language Processing techniques towards the extraction of rules from legal documents. More precisely, we combine the linguistic information provided by WordNet together with a syntax-based extraction of rules from legal texts, and a logic-based extraction of dependencies between chunks of such texts. Such a combined approach leads to a powerful solution towards the extraction of machine-readable rules from legal documents. We evaluate the proposed approach over the Australian " Telecommunications consumer protections code " .},
   author = {Mauro Dragoni and Serena Villata and Williams Rizzi and Guido Governatori},
   title = {Combining NLP Approaches for Rule Extraction from Legal Documents},
   url = {https://hal.science/hal-01572443 https://hal.science/hal-01572443/document},
   year = {2016},
}
@article{Kotis2022,
   abstract = {Semantics of Business Vocabulary and Rules (SBVR) is a standard that is applied in describing business knowledge in the form of controlled natural language. Business process designers develop SBVR from formal documents and later translate it into business process models. In many immature companies, these documents are often unavailable and could hinder resource efficiency efforts. This study introduced a novel approach called informal document to SBVR (ID2SBVR). This approach is used to extract operational rules of SBVR from informal documents. ID2SBVR mines fact type candidates using word patterns or extracting triplets (actor, action, and object) from sentences. A candidate fact type can be a complex, compound, or complex-compound sentence. ID2SBVR extracts fact types from candidate fact types and transforms them into a set of SBVR operational rules. The experimental results show that our approach can be used to generate the operational rules of SBVR from informal documents with an accuracy of 0.91. Moreover, ID2SBVR can also be used to extract fact types with an accuracy of 0.96. The unstructured data is successfully converted into semi-structured data for use in pre-processing. ID2SBVR allows the designer to automatically generate business process models from informal documents.},
   author = {Konstantinos Kotis and Dimitris Spiliotopoulos and Irene Tangkawarow and Riyanarto Sarno and Daniel Siahaan},
   doi = {10.3390/BDCC6040119},
   issn = {2504-2289},
   issue = {4},
   journal = {Big Data and Cognitive Computing 2022, Vol. 6, Page 119},
   keywords = {SBVR,fact type,informal document to SBVR,natural language,operational rules,resource efficiency},
   month = {10},
   pages = {119},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {ID2SBVR: A Method for Extracting Business Vocabulary and Rules from an Informal Document},
   volume = {6},
   url = {https://www.mdpi.com/2504-2289/6/4/119/htm https://www.mdpi.com/2504-2289/6/4/119},
   year = {2022},
}
@article{Amaral2022,
   abstract = {Finance and economics are wide domains, where ontologies are useful instruments for dealing with semantic interoperability and information integration problems, as well as improving communication and problem solving among people. In particular, reference ontologies have been widely recognized as powerful tools for representing a model of consensus within a community to support communication, meaning negotiation, consensus establishment, as well as semantic interoperability and information integration. In domains like economics and finance, which are too large and complex to be represented as a single, large and monolithic ontology, it is necessary to create an ontological framework, built incrementally and in an integrated way, as a network. Therefore, in this paper we introduce OntoFINE, an Ontology Network in Finance and Economics that organizes and integrates knowledge in the realm on finance and economics, serving as a basis to several applications. We discuss the development of OntoFINE and present some of its applications.},
   author = {Glenda Amaral and Tiago Prince Sales and Giancarlo Guizzardi},
   doi = {10.1007/978-3-031-11520-2_4/COVER},
   isbn = {9783031115196},
   issn = {18651356},
   journal = {Lecture Notes in Business Information Processing},
   keywords = {Economic Exchanges,Money,Ontology Network,Risk,Trust,Value},
   pages = {42-57},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Towards an Ontology Network in Finance and Economics},
   volume = {441 LNBIP},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-11520-2_4},
   year = {2022},
}
@article{Fodor2013,
   author = {Paul Fodor and Dumitru Roman and Darko Anicic and Adam Wyner and Monica Palmirani and Davide Sottara and François Lévy},
   title = {Interpreting Regulations with SBVR.},
   url = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=0928e1773796b9979aa999b6d28db1fe7d9cb9fe#page=66},
   year = {2013},
}
@article{Roychoudhury2018,
   abstract = {Modern enterprises operate in an unprecedented regulatory environment where increasing regulation and heavy penalties on non-compliance have placed regulatory compliance among the topmost concerns of enterprises worldwide. Previous research in the field of compliance has established that the manual specification/tagging of the regulations not only fails to ensure their proper coverage but also negatively affects the turnaround time both in proving and maintaining the compliance. Our contribution in this paper is a case study using a subset of European Union Regulation in the financial markets, namely, Money Market Statistical Reporting (MMSR) and that we validated it in the context of our model-driven semi-automated compliance framework. The novelty of the framework is the key participation of domain experts to author regulatory rules in a controlled natural language to enable compliance checking. We demonstrate transformation of regulations present in legal natural language text (English) to a model form via authoring of Structured English rules in the context of MMSR regulations for a large European bank. This generated regulatory model is eventually translated to formal logic that enables formal compliance checking contrary to current industry practice, that provides content management-based, document-driven and expert-dependent ways of managing regulatory compliance.},
   author = {Suman Roychoudhury and Sagar Sunkle and Namrata Choudhary and Deepali Kholkar and Vinay Kulkarni},
   doi = {10.1007/978-3-030-02302-7_18/FIGURES/8},
   isbn = {9783030023010},
   issn = {18651348},
   journal = {Lecture Notes in Business Information Processing},
   keywords = {Compliance checking,Money market statistical reporting,Regulatory compliance,Structured English},
   pages = {288-302},
   publisher = {Springer Verlag},
   title = {A case study on modeling and validating financial regulations using (semi-) automated compliance framework},
   volume = {335},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-02302-7_18},
   year = {2018},
}
@article{Sunkle2016,
   abstract = {Enterprises today face an unprecedented regulatory regime and are increasingly looking to technology to ease their regulatory compliance concerns. Formal approaches in research focus on checking compliance of business processes against rules, and assume usage of matching terminology on both sides. We focus on run-time compliance of enterprise data, and the specific problem of identifying enterprise data relevant to a regulation, in an automated manner. We present a knowledge representation approach and semi-automated solution using models and model transformations to extract the same from distributed enterprise databases. We use a Semantics of Business Vocabulary and Rules (SBVR) model of regulation rules as the basis to arrive at the necessary and sufficient model of enterprise data. The approach is illustrated using a real-life case study of the MiFID-II financial regulation.},
   author = {Sagar Sunkle and Vinay Kulkarni and Deepali Kholkar},
   doi = {10.5220/0006002600600071},
   journal = {researchgate.net},
   keywords = {Defeasible Logic,Enterprise Data Integration,Fact-oriented Model,Formal Compliance Checking,Knowledge Base,Knowledge Representation,Model Transformation,Reasoning,SBVR},
   title = {From Natural-language Regulations to Enterprise Data using Knowledge Representation and Model Transformations},
   url = {https://www.researchgate.net/profile/Sagar-Sunkle/publication/305720703_From_Natural-language_Regulations_to_Enterprise_Data_using_Knowledge_Representation_and_Model_Transformations/links/5b0783c5aca2725783e25575/From-Natural-language-Regulations-to-Enterprise-Data-using-Knowledge-Representation-and-Model-Transformations.pdf},
   year = {2016},
}
@article{Frohlich2019,
   author = {By Marcel Fröhlich},
   doi = {10.1002/9781119362197.CH16},
   journal = {The RegTech Book},
   keywords = {RegTech functionality,artificial intelligence approaches,banks,linked data,readable regulation,regulatory compliance process,resource description framewok,unambiguous machine},
   month = {7},
   publisher = {John Wiley & Sons, Ltd},
   title = {Enabling RegTech Up Front: Unambiguous Machine-readable Regulation},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1002/9781119362197.ch16 https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119362197.ch16 https://onlinelibrary.wiley.com/doi/10.1002/9781119362197.ch16},
   year = {2019},
}
@misc{,
   abstract = {This paper presents a visualization technique to assist legal experts in formalising their interpretation of legal texts in terms of regulatory requirements. (Semi-)automation of compliance processes requires a machine-readable version of legal requirements in a format that enables effective compliance assessment. The use of a semi-structured controlled natural language as an intermediate step of the translation from a human-readable text to a machine-readable and understandable format ensures that the process of interpretation of those requirements is as simple as possible. However, it does not ensure that the formal representation resulting from the interpretation faithfully represents the intended semantics provided by the legal expert. Visualization techniques such as property graphs in Neo4j could fill this gap, allowing legal experts to understand and control the formal representation of the result of their act of interpretation.},
   author = {Selja Seppälä and Marcello Ceci and Hai Huang and Leona O'Brien and Tom Butler},
   issn = {1613-0073},
   keywords = {Controlled natural languages,Neo4j,RegTech,SBVR},
   month = {1},
   pages = {73-85},
   publisher = {CEUR Workshop Proceedings},
   title = {SmaRT visualisation of legal rules for compliance},
   volume = {2049},
   url = {https://hdl.handle.net/10468/11829},
   year = {2018},
}
@article{Sunkle2015,
   abstract = {Modern enterprises increasingly face the challenge of keeping pace with regulatory compliances. Semantic disparity between regulation texts, their interpretations, and operational specifics of enterprise often leads enterprises to situations where it becomes difficult for them to establish what compliance means, how they are supposed to affect it in the operational practices, and how to prove that they comply when asked for explanations of (non-)compliance. We take a step toward reducing the semantic disparity by using semantic vocabularies to map regulations with available operational details of enterprise and utilize them in enacting compliance. We also propose to provide explanations of proofs of (non-)compliance.We report our ongoing work in this regard using the design science research (DSR) paradigm. Initial iterations of design cycle from DSR have been useful to us in identifying and matching stakeholderspecific goals in solving these problems.},
   author = {Sagar Sunkle and Deepali Kholkar and Vinay Kulkarni},
   doi = {10.1007/978-3-319-19237-6_21/COVER},
   isbn = {9783319192369},
   issn = {18651348},
   journal = {Lecture Notes in Business Information Processing},
   keywords = {Business process,Design cycle,Design science research,Enterprise data,GRC,Regulatory compliance,SBVR},
   pages = {326-341},
   publisher = {Springer Verlag},
   title = {Solving semantic disparity and explanation problems in regulatory compliance-a research-in-progress report with design science research perspective},
   volume = {214},
   url = {https://link.springer.com/chapter/10.1007/978-3-319-19237-6_21},
   year = {2015},
}
@article{Johnsen2010,
   abstract = {The paper reports on how the two separate worlds of legislator and IT-technologist can be bridged through the formalization of legal rules with SBVR. The legislator can use SBVR to transform legal rules expressed in natural language to legal rules expressed in controlled natural language. During the transformation, impreciseness and inconsistence in the law formulation may be revealed and entail improved quality of the law formulation. The institutions implementing the legal rules can use SBVR documents published by the legislator to save time in their analyzing phase and even to automate the transformation from vocabulary and rules in SBVR to vocabulary and rules in a business rules management system (BRMS). The more institutions that are affected by the same legislation the more time and effort will be saved.},
   author = {Åshild Johnsen and Arne-Jørgen Berre#},
   journal = {Citeseer},
   keywords = {SBVR,interpreting,legal rules,legislator,semantic ontology},
   title = {A bridge between legislator and technologist-Formalization in SBVR for improved quality and understanding of legal rules},
   url = {https://citeseer.ist.psu.edu/document?repid=rep1&type=pdf&doi=71e5ceca50474b7945bc44ba0739a6aec8d206d8},
   year = {2010}
}
@article{Sunkle2015,
   abstract = {With recent regulatory advances, modern enterprises have to not only comply with regulations but have to be prepared to provide explanation of proof of (non-)compliance. On top of compliance checking, this necessitates modeling concepts from regulations and enterprise operations so that stakeholder-specific and close to natural language explanations could be generated. We take a step in this direction by using Semantics of Business Vocabulary and Rules to model and map vocabularies of regulations and operations of enterprise. Using these vocabularies and leveraging proof generation abilities of an existing compliance engine, we show how such explanations can be created. Basic natural language explanations that we generate can be easily enriched by adding requisite domain knowledge to the vocabularies.},
   author = {Sagar Sunkle and Deepali Kholkar and Vinay Kulkarni},
   doi = {10.1007/978-3-319-21542-6_25/COVER},
   isbn = {9783319215419},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Explanation of proof,Proof of compliance,Regulatory compliance,SBVR},
   pages = {388-403},
   publisher = {Springer Verlag},
   title = {Explanation of proofs of regulatory (Non-)compliance using semantic vocabularies},
   volume = {9202},
   url = {https://link.springer.com/chapter/10.1007/978-3-319-21542-6_25},
   year = {2015},
}
@article{Elgammal2015,
   abstract = {Following the crisis in 2008, the financial industry has faced growing numbers of laws and regulations globally. The number and complexity of these regulations is creating significant issues for governance, risk and compliance management in almost all industrial sectors; however some of these sectors are characterized by being heavily-regulated including the financial industry. This paper proposes a semantically-enabled compliance management framework. In the heart of the framework is an integrated semantic repository incorporating regulatory, business and compliance knowledge; i.e., CMKB. The approach is underpinned by legal Subject Matter Experts (SMEs) interpreting financial regulations and encoding them in the Semantics of Business vocabulary and Business Rule (SBVR) standard. As a proof-of-concept, we have integrated the SBVR and CMKB repositories with a validated compliance solution for design-time compliance verification. However, the approach could be integrated with other compliance solutions at different phases of the business process lifecycle.},
   author = {Amal Elgammal and Tom Butler},
   doi = {10.1007/978-3-319-22885-3_15/COVER},
   isbn = {9783319228846},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Business process compliance management,Compliance patterns,Financial services,SBVR,Semantic compliance management},
   pages = {171-184},
   publisher = {Springer Verlag},
   title = {Towards a framework for semantically-enabled compliance management in financial services},
   volume = {8954},
   url = {https://link.springer.com/chapter/10.1007/978-3-319-22885-3_15},
   year = {2015},
}
@article{Kumar2020,
   abstract = {Enterprises model the behavior of their business to prepare a communication standard for business analysts and to specify requirements to Information Technology (IT) people. The communication gap between IT group and business analysts, who lie on the opposite end of the business spectrum exists due to the different terminologies used in their respective fields regarding the same context. This gap has led to major software failures which prompted the OMG group has come up with a new standard - Semantic of Business Vocabulary and Business Rules (SBVR). Declarative models are provided by SBVR to represent Business Vocabulary and Business Rules which can be understood by everyone working throughout the business spectrum. Each business is governed by business rules which are constrained by the regulation policy set up by the policy guidelines of the organization and government regulations set up on the organization. Business rules are specified in documents like user guides, requirement documents, terms and conditions, do's and don'ts. Typically a Business Analyst interprets the document and manually extracts rules based on his understanding which leads to potential discrepancies, ambiguities and quality issues in the software system. To minimize such errors, in this paper we present an unsupervised approach to automatically extract SBVR vocabularies and rules from domain-specific business documents. We also present our initial results and comparative study with our earlier approach.},
   author = {Pavan Kumar and Chandan Prakash and Ravindra Naik and Abhidip Bhattacharyya},
   doi = {10.1145/3385032.3385046},
   isbn = {9781450375948},
   journal = {ACM International Conference Proceeding Series},
   keywords = {Business rules extraction,Natural language processing,Rule components,Rule document,Sbvr,Text mining},
   month = {2},
   publisher = {Association for Computing Machinery},
   title = {An Approach to Mine SBVR Vocabularies and Rules from Business Documents},
   url = {https://dl.acm.org/doi/10.1145/3385032.3385046},
   year = {2020},
}
@article{Khalil2016,
   abstract = {The Semantics of Business Vocabulary and Business Rules (SBVR) is a specification created by the Object Management Group (OMG) to provide a way to semantically describe business concepts and specify business rules. However, reasoning with SBVR is still an open subject, and current efforts to provide reasoning are done through the Web Ontology Language (OWL), by providing a mapping between SBVR and OWL. In this paper we focus on the problem of mapping SBVR vocabulary and rulebook to OWL 2, but unlike previous mappings described in the literature, we provide a novel and unorthodox mapping that allows to describe legal rules which have their own intricate anatomy.},
   author = {Firas Al Khalil and Marcello Ceci and Kosala Yapa and Leona O’Brien},
   doi = {10.1007/978-3-319-42019-6_17/COVER},
   isbn = {9783319420189},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Legal,OWL,Rule,SBVR},
   pages = {258-266},
   publisher = {Springer Verlag},
   title = {SBVR to OWL 2 mapping in the domain of legal rules},
   volume = {9718},
   url = {https://link.springer.com/chapter/10.1007/978-3-319-42019-6_17},
   year = {2016},
}
@article{Abi-Lahoud2014,
   abstract = {Regulatory compliance has proved to be difficult and time consuming across business domains. In Financial Services, the wide and complex spectrum of regulations calls for machine assistance in making sense of, and in consuming, the regulatory text. Semantic...},
   author = {Elie Abi-Lahoud and Leona O’Brien and Tom Butler},
   doi = {10.1007/978-3-662-45960-7_14},
   pages = {188-201},
   publisher = {Springer, Berlin, Heidelberg},
   title = {On the Road to Regulatory Ontologies},
   url = {https://link.springer.com/chapter/10.1007/978-3-662-45960-7_14},
   year = {2014},
}
@article{Chuang2023,
   abstract = {Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this Decoding by Contrasting Layers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA by 12-17% absolute points, demonstrating its potential in making LLMs reliably generate truthful facts.},
   author = {Yung-Sung Chuang and Yujia Xie and Hongyin Luo and Yoon Kim and James Glass and Pengcheng He},
   month = {9},
   title = {DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models},
   url = {https://arxiv.org/abs/2309.03883v1},
   year = {2023},
}
@article{Qian2023,
   abstract = {Large language models (LLMs) acquire extensive knowledge during pre-training, known as their parametric knowledge. However, in order to remain up-to-date and align with human instructions, LLMs inevitably require external knowledge during their interactions with users. This raises a crucial question: How will LLMs respond when external knowledge interferes with their parametric knowledge? To investigate this question, we propose a framework that systematically elicits LLM parametric knowledge and introduces external knowledge. Specifically, we uncover the impacts by constructing a parametric knowledge graph to reveal the different knowledge structures of LLMs, and introduce external knowledge through distractors of varying degrees, methods, positions, and formats. Our experiments on both black-box and open-source models demonstrate that LLMs tend to produce responses that deviate from their parametric knowledge, particularly when they encounter direct conflicts or confounding changes of information within detailed contexts. We also find that while LLMs are sensitive to the veracity of external knowledge, they can still be distracted by unrelated information. These findings highlight the risk of hallucination when integrating external knowledge, even indirectly, during interactions with current LLMs. All the data and results are publicly available.},
   author = {Cheng Qian and Xinran Zhao and Sherry Tongshuang Wu},
   month = {9},
   title = {"Merge Conflicts!" Exploring the Impacts of External Distractors to Parametric Knowledge Graphs},
   url = {https://arxiv.org/abs/2309.08594v1},
   year = {2023},
}
@article{Li2023,
   abstract = {Large language models (LLMs) often demonstrate inconsistencies with human
preferences. Previous research gathered human preference data and then aligned
the pre-trained models using reinforcement learning or instruction tuning, the
so-called finetuning step. In contrast, aligning frozen LLMs without any extra
data is more appealing. This work explores the potential of the latter setting.
We discover that by integrating self-evaluation and rewind mechanisms,
unaligned LLMs can directly produce responses consistent with human preferences
via self-boosting. We introduce a novel inference method, Rewindable
Auto-regressive INference (RAIN), that allows pre-trained LLMs to evaluate
their own generation and use the evaluation results to guide backward rewind
and forward generation for AI safety. Notably, RAIN operates without the need
of extra data for model alignment and abstains from any training, gradient
computation, or parameter updates; during the self-evaluation phase, the model
receives guidance on which human preference to align with through a
fixed-template prompt, eliminating the need to modify the initial prompt.
Experimental results evaluated by GPT-4 and humans demonstrate the
effectiveness of RAIN: on the HH dataset, RAIN improves the harmlessness rate
of LLaMA 30B over vanilla inference from 82% to 97%, while maintaining the
helpfulness rate. Under the leading adversarial attack llm-attacks on Vicuna
33B, RAIN establishes a new defense baseline by reducing the attack success
rate from 94% to 19%.},
   author = {Yuhui Li and Fangyun Wei and Jinjing Zhao and Chao Zhang and Hongyang Zhang},
   month = {9},
   title = {RAIN: Your Language Models Can Align Themselves without Finetuning},
   url = {https://arxiv.org/abs/2309.07124v1},
   year = {2023},
}
@article{Gupta2023,
   abstract = {Annual Reports of publicly listed companies contain vital information about
their financial health which can help assess the potential impact on Stock
price of the firm. These reports are comprehensive in nature, going up to, and
sometimes exceeding, 100 pages. Analysing these reports is cumbersome even for
a single firm, let alone the whole universe of firms that exist. Over the
years, financial experts have become proficient in extracting valuable
information from these documents relatively quickly. However, this requires
years of practice and experience. This paper aims to simplify the process of
assessing Annual Reports of all the firms by leveraging the capabilities of
Large Language Models (LLMs). The insights generated by the LLM are compiled in
a Quant styled dataset and augmented by historical stock price data. A Machine
Learning model is then trained with LLM outputs as features. The walkforward
test results show promising outperformance wrt S&P500 returns. This paper
intends to provide a framework for future work in this direction. To facilitate
this, the code has been released as open source.},
   author = {Udit Gupta},
   isbn = {2309.03079v1},
   keywords = {ChatGPT,Investing,LLM,Quantitative Finance * Github Repository,Stocks},
   month = {9},
   title = {GPT-InvestAR: Enhancing Stock Investment Strategies through Annual Report Analysis with Large Language Models},
   url = {https://arxiv.org/abs/2309.03079v1},
   year = {2023},
}
@article{Wang2023,
   abstract = {The 'pre-train, prompt, predict' paradigm of large language models (LLMs) has
achieved remarkable success in open-domain question answering (OD-QA). However,
few works explore this paradigm in the scenario of multi-document question
answering (MD-QA), a task demanding a thorough understanding of the logical
associations among the contents and structures of different documents. To fill
this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to
formulate the right context in prompting LLMs for MD-QA, which consists of a
graph construction module and a graph traversal module. For graph construction,
we create a knowledge graph (KG) over multiple documents with nodes symbolizing
passages or document structures (e.g., pages/tables), and edges denoting the
semantic/lexical similarity between passages or intra-document structural
relations. For graph traversal, we design an LM-guided graph traverser that
navigates across nodes and gathers supporting passages assisting LLMs in MD-QA.
The constructed graph serves as the global ruler that regulates the
transitional space among passages and reduces retrieval latency. Concurrently,
the LM-guided traverser acts as a local navigator that gathers pertinent
context to progressively approach the question and guarantee retrieval quality.
Extensive experiments underscore the efficacy of KGP for MD-QA, signifying the
potential of leveraging graphs in enhancing the prompt design for LLMs. Our
code is at https://github.com/YuWVandy/KG-LLM-MDQA.},
   author = {Yu Wang and Nedim Lipka and Ryan A. Rossi and Alexa Siu and Ruiyi Zhang and Tyler Derr},
   month = {8},
   title = {Knowledge Graph Prompting for Multi-Document Question Answering},
   url = {https://arxiv.org/abs/2308.11730v1},
   year = {2023},
}
@article{Sunkle2015,
   abstract = {Industry governance, risk, and compliance (GRC) solutions stand to gain from various analyses offered by formal compliance checking approaches. Such adoption is made difficult by the fact that most formal approaches assume that a mapping between concepts of regulations and models of operational specifics exists. Industry solutions offer tagging mechanisms to map regulations to operational specifics; however, they are mostly semi-formal in nature and tend to rely extensively on experts. We propose to use Semantics of Business Vocabularies and Rules along with similarity measures to create an explicit mapping between concepts of regulations and models of operational specifics of the enterprise. We believe that our work-in-progress takes a step toward adapting and leveraging formal compliance checking approaches in industry GRC solutions.},
   author = {Sagar Sunkle and Deepali Kholkar and Vinay Kulkarni},
   doi = {10.7250/CSIMQ.2015-5.04},
   issn = {2255-9922},
   issue = {5},
   journal = {Complex Systems Informatics and Modeling Quarterly},
   keywords = {Regulatory compliance,SBVR,business process models,operations,semantic similarity},
   month = {12},
   pages = {39-60},
   publisher = {CEUR-WS},
   title = {Toward Better Mapping between Regulations and Operations of Enterprises Using Vocabularies and Semantic Similarity},
   volume = {0},
   url = {https://csimq-journals.rtu.lv/article/view/csimq.2015-5.04},
   year = {2015},
}
@article{Mitra2018,
   abstract = {In the modern age, the need for automation has led to Business Organizations representing their functionality as structured Business Rules. SBVR has come up as an universally popular format for representation of Business Rules. The presence of different Business Organizations working in a particular real life domain results in generation of different rules for each of the organization. Due to the varying business practices, like mergers & acquisitions, upgrades, incorporation of a new application, etc., it becomes necessary to compare a set of Business Rules of a particular organization with the rules of a reference model, to get a measure of similarity among the business functionality of the two. Presently, this comparison is carried out manually by business experts or by executing the rules of one organization with the data of another and checking if they are compliant. Both the approaches are extremely tedious and expensive as modern organizations have huge rule sets and data sets.We present MatGap, a tool which performs a systematic Match and Gap Analysis between two sets of SBVR-based Business Rules applicable to a specific domain, using Global Vectors(GloVe) model and SMT-LIBv2. The analysis report gives a measure of Match among the rules and entities, thus providing the best alignment and aids to identify the representational Gaps(if any) among the rules and entities. The tool also checks whether the embedded logic in the reference Business Rule set is covered by the other Rule set, thus highlighting the business functionality gap that is present in the latter.},
   author = {Sayandeep Mitra and Chandan Prakash and Shayak Chakraborty and Pavan Kumar Chittimalli},
   doi = {10.1109/APSEC.2018.00070},
   isbn = {9781728119700},
   issn = {15301362},
   journal = {Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
   keywords = {Business Rules,Clustering,Gap Analysis,Matching,Natural Language Processing,SBVR,SMT,Verification},
   month = {7},
   pages = {551-560},
   publisher = {IEEE Computer Society},
   title = {MatGap: A Systematic Approach to Perform Match and Gap Analysis among SBVR-Based Domain Specific Business Rules},
   volume = {2018-December},
   year = {2018},
}
@article{Haj2019,
   abstract = {Business rules are generally captured in a natural language. The inherit ambiguity of the latter is often seen as a cause for project failure, which makes it necessary to translate natural language business rules statements to another language sufficiently formal. However, business experts are generally not familiar with formal languages, which can complicate the communication between stakeholders. For this reason, Object Management Group (OMG) had proposed SBVR Standard (2008) for modeling complex organizations in a natural language but in a formal and detailed way. As a result, several studies have succeeded to increase the accuracy of their approaches by transforming their models from/to SBVR standard. Clearly then, the success of these approaches depends on the quality of the SBVR based statements used or generated. This paper presents an approach for checking conformance of both lexicon and syntax of Business Rules (BR) expressed with Semantic of Business Vocabulary and Rules (SBVR), to SBVR Structured English notation (SBVR-SE) using Natural Language Processing (NLP).},
   author = {Abdellatif Haj and Youssef Balouki and Taoufiq Gadi},
   doi = {10.1007/978-3-030-11928-7_63/COVER},
   isbn = {9783030119270},
   issn = {21945365},
   journal = {Advances in Intelligent Systems and Computing},
   keywords = {BR,Business rules,NLP,Natural language processing,SBVR,SBVR structured english,Semantic of business vocabulary and rules},
   pages = {697-706},
   publisher = {Springer Verlag},
   title = {Automated checking of conformance to SBVR structured english notation},
   volume = {915},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-11928-7_63},
   year = {2019},
}
@article{Chittimalli2019,
   abstract = {An enterprise system operates business by providing various services that are guided by set of certain business rules (BR) and constraints. These BR are usually written using plain Natural Language in operating procedures, terms and conditions, and other documents or in source code of legacy enterprise systems. For implementing the BR in a software system, expressing them as UML use-case specifications, or preparing for Merger & Acquisition (M&A) activity, analysts manually interpret the documents or try to identify constraints from the source code, leading to potential discrepancies and ambiguities. These issues in the software system can be resolved only after testing, which is a very tedious and expensive activity. To minimize such errors and efforts, we propose BuRRiTo framework consisting of automatic extraction of BR by mining documents and source code, ability to clean them of various anomalies like inconsistency, redundancies, conflicts, etc. and able to analyze the functional gaps present and performing semantic querying and searching.},
   author = {Pavan Kumar Chittimalli and Kritika Anand and Shrishti Pradhan and Sayandeep Mitra and Chandan Prakash and Rohit Shere and Ravindra Naik},
   doi = {10.1109/ASE.2019.00134},
   isbn = {9781728125084},
   journal = {Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019},
   keywords = {Business Rules Extraction,Graphs,Match and Gap,Natural Language Processing,Rule Components,Rule Document,SBVR,Search and Query,Source Code,Text Mining},
   month = {11},
   pages = {1190-1193},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {BuRRiTo: A framework to extract, specify, verify and analyze business rules},
   year = {2019},
}
@article{Meyer2023,
   abstract = {Knowledge Graphs (KG) provide us with a structured, flexible, transparent,
cross-system, and collaborative way of organizing our knowledge and data across
various domains in society and industrial as well as scientific disciplines.
KGs surpass any other form of representation in terms of effectiveness.
However, Knowledge Graph Engineering (KGE) requires in-depth experiences of
graph structures, web technologies, existing models and vocabularies, rule
sets, logic, as well as best practices. It also demands a significant amount of
work. Considering the advancements in large language models (LLMs) and their
interfaces and applications in recent years, we have conducted comprehensive
experiments with ChatGPT to explore its potential in supporting KGE. In this
paper, we present a selection of these experiments and their results to
demonstrate how ChatGPT can assist us in the development and management of KGs.},
   author = {Lars-Peter Meyer and Claus Stadler and Johannes Frey and Norman Radtke and Kurt Junghanns and Roy Meissner and Gordian Dziwis and Kirill Bulert and Michael Martin},
   month = {7},
   title = {LLM-assisted Knowledge Graph Engineering: Experiments with ChatGPT},
   url = {https://arxiv.org/abs/2307.06917v1},
   year = {2023},
}
@article{Lewis2020,
   abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG)-models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
   author = {Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-Tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
   doi = {10.5555/3495724.3496517},
   journal = {NIPS'20: Proceedings of the 34th International Conference on Neural Information Processing Systems},
   pages = {9459-9474},
   title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
   url = {https://dl.acm.org/doi/10.5555/3495724.3496517},
   year = {2020},
}
@article{Wolczyk2023,
   abstract = {Recently, foundation models have achieved remarkable results in fields such as computer vision and language processing. Although there has been a significant push to introduce similar approaches in reinforcement learning, these have not yet succeeded on a comparable scale. In this paper, we take a step towards understanding and closing this gap by highlighting one of the problems specific to foundation RL models, namely the data shift occurring during fine-tuning. We show that fine-tuning on compositional tasks, where parts of the environment might only be available after a long training period, is inherently prone to catastrophic forgetting. In such a scenario, a pre-trained model might forget useful knowledge before even seeing parts of the state space it can solve. We provide examples of both a grid world and realistic robotic scenarios where catastrophic forgetting occurs. Finally, we show how this problem can be mitigated by using tools from continual learning. We discuss the potential impact of this finding and propose further research directions.},
   author = {Maciej Wołczyk and Bartłomiej Cupiał and Jagiellonian Unviersity and Michał Zaj and Razvan Pascanu Deepmind and Łukasz Kucí and Piotr Miło´s Miło´s},
   doi = {10.18653/v1/w18-5413},
   isbn = {9781948087711},
   journal = {EMNLP 2018 - 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, Proceedings of the 1st Workshop},
   month = {3},
   pages = {108-114},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {On The Role of Forgetting in Fine-Tuning Reinforcement Learning Models},
   year = {2023},
}
@misc{YAO2023,
   author = {JESSICA YAO},
   journal = {on-line},
   title = {Why You (Probably) Don't Need to Fine-tune an LLM - Tidepool by Aquarium},
   url = {https://www.tidepool.so/2023/08/17/why-you-probably-dont-need-to-fine-tune-an-llm/?utm_source=tldrnewsletter},
   year = {2023},
}
@article{Ahmad2022,
   abstract = {In the area of natural language processing, measuring sentence similarity is an essential problem. Searching for semantic meaning in natural language is a related issue. The task of measuring sentence similarity is to find semantic symmetry in two sentences, not matter how they are arranged. It is important to measure the similarity of sentences accurately. To compute the similarity between sentences, existing methods have been constructed from approaches for large texts. Since these methods work in very high-dimensional spaces, they are inefficient, require human input, and are not flexible enough for some applications. In this study, we propose a hybrid method (HydMethod) which considers not only semantic information including lexical databases, word embeddings, and corpus statistics, but also implied word order information. With lexical databases, our method models human common sense knowledge, and that knowledge can then be adapted to be used in different domains with the incorporation of corpus statistics. Therefore, the methodology is applicable across several domains. As part of our experiments, we used two standard datasets - Pilot Short Text Semantic Similarity Benchmark and MS paraphrase - in order to demonstrate the efficacy of our proposed method. As a result, the proposed method outperforms the existing approaches when tested on these two datasets, giving the highest correlation value for both word and sentence similarity. Moreover, it achieves a maximum of 32% higher increase than only using word vector or WorldNet based methodology. With Rubenstein and Goodenough word & sentence pairs, our algorithm's similarity measure shows a high Pearson correlation coefficient of 0.8953.},
   author = {Farooq Ahmad and Dr Mohammad Faisal},
   doi = {10.1016/J.IJCCE.2022.02.001},
   issn = {2666-3074},
   journal = {International Journal of Cognitive Computing in Engineering},
   keywords = {Corpus,Lexical database,Natural language processing,Semantic search,Semantic similarity,Word embedding,Word overlap,WordNet},
   month = {6},
   pages = {58-77},
   publisher = {Elsevier},
   title = {A novel hybrid methodology for computing semantic similarity between sentences through various word senses},
   volume = {3},
   year = {2022},
}
@article{Ahmad2023,
   abstract = {The majority of projects fail to achieve their intended objectives, according to research. This could arise for a number of reasons, such as ensuring requirements are managed, excessive documentation of the code, or the difficulty in delivering software that includes all the requested features on time. An effort could be made to overcome such failure rates by establishing a proper management of requirements and concept of reusability. The correct requirements can be identified by checking similarity between the requirements received from the various stakeholders. A reusable software component can result in substantial savings in both time and money. It can be challenging to make a choice regarding the reuse of certain software components. A comparison of the requirements of a new project with those of previous projects prior to starting a new project or even at a later stage during development is useful for identifying reusable components. This paper proposes a framework (ReSim) for identifying software requirements' similarities, in an attempt to improve reusability and identify the correct requirements. A crucial component of ReSim is to measure similarity between software requirements. Different well-known similarity measurement techniques used by the researchers to evaluate the similarity between the software requirements. Some of the methods used to measure this include dice, jaccard, and cosine coefficients, but in this paper, we have used recently developed hybrid method which considers not only semantic information including lexical databases, word embeddings, and corpus statistics, but also implied word order information and produced significant improvements in the results related to the measurement of semantic similarity between words and sentences. As part of the experiments, the study used PURE dataset-in order to demonstrate the efficacy of the proposed framework. As a result, recently developed hybrid method of measuring the requirements similarity is more accurate than Dice, Jaccard, and Cosine, while Cosine is a better choice than Dice, and Jaccard is more accurate than Dice. Thus, ReSim outperforms existing approaches when tested on the PURE dataset, providing the most accurate results for both functional and non-functional requirements.},
   author = {Farooq Ahmad and Mohammad Faisal},
   doi = {10.5815/ijieeb.2023.02.05},
   keywords = {Framework,Index Terms: Measurement,Requirements,Reusability,Semantic,Similarity},
   pages = {38-53},
   title = {Assessing Similarity between Software Requirements: A Semantic Approach},
   volume = {2},
   url = {https://orcid.org/0000-0002-6120-5259},
   year = {2023},
}
@article{Ferrari2017,
   abstract = {This paper presents PURE (PUblic REquirements dataset), a dataset of 79 publicly available natural language requirements documents collected from the Web. The dataset includes 34,268 sentences and can be used for natural language processing tasks that are typical in requirements engineering, such as model synthesis, abstraction identification and document structure assessment. It can be further annotated to work as a benchmark for other tasks, such as ambiguity detection, requirements categorisation and identification of equivalent re-quirements. In the paper, we present the dataset and we compare its language with generic English texts, showing the peculiarities of the requirements jargon, made of a restricted vocabulary of domain-specific acronyms and words, and long sentences. We also present the common XML format to which we have manually ported a subset of the documents, with the goal of facilitating replication of NLP experiments.},
   author = {Alessio Ferrari and Giorgio Oronzo Spagnolo and Stefania Gnesi},
   doi = {10.1109/RE.2017.29},
   isbn = {9781538631911},
   journal = {Proceedings - 2017 IEEE 25th International Requirements Engineering Conference, RE 2017},
   keywords = {Empirical Software Engineering,Empirical Studies,Model Synthesis,NLP,NLP Tasks,Natural Language Requirements,PURE,Public Requirements,Requirements Abstraction,Requirements Ambiguity Detection,Requirements Categorisation,Requirements Dataset,XML},
   month = {9},
   pages = {502-505},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {PURE: A Dataset of Public Requirements Documents},
   year = {2017},
}
@article{Yao2019,
   abstract = {Knowledge graphs are important resources for many artificial intelligence
tasks but often suffer from incompleteness. In this work, we propose to use
pre-trained language models for knowledge graph completion. We treat triples in
knowledge graphs as textual sequences and propose a novel framework named
Knowledge Graph Bidirectional Encoder Representations from Transformer
(KG-BERT) to model these triples. Our method takes entity and relation
descriptions of a triple as input and computes scoring function of the triple
with the KG-BERT language model. Experimental results on multiple benchmark
knowledge graphs show that our method can achieve state-of-the-art performance
in triple classification, link prediction and relation prediction tasks.},
   author = {Liang Yao and Chengsheng Mao and Yuan Luo},
   month = {9},
   title = {KG-BERT: BERT for Knowledge Graph Completion},
   url = {https://arxiv.org/abs/1909.03193v2},
   year = {2019},
}
@article{Pan2023,
   abstract = {Large language models (LLMs), such as ChatGPT and GPT4, are making new waves
in the field of natural language processing and artificial intelligence, due to
their emergent ability and generalizability. However, LLMs are black-box
models, which often fall short of capturing and accessing factual knowledge. In
contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are
structured knowledge models that explicitly store rich factual knowledge. KGs
can enhance LLMs by providing external knowledge for inference and
interpretability. Meanwhile, KGs are difficult to construct and evolving by
nature, which challenges the existing methods in KGs to generate new facts and
represent unseen knowledge. Therefore, it is complementary to unify LLMs and
KGs together and simultaneously leverage their advantages. In this article, we
present a forward-looking roadmap for the unification of LLMs and KGs. Our
roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs,
which incorporate KGs during the pre-training and inference phases of LLMs, or
for the purpose of enhancing understanding of the knowledge learned by LLMs; 2)
LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding,
completion, construction, graph-to-text generation, and question answering; and
3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a
mutually beneficial way to enhance both LLMs and KGs for bidirectional
reasoning driven by both data and knowledge. We review and summarize existing
efforts within these three frameworks in our roadmap and pinpoint their future
research directions.},
   author = {Shirui Pan and Senior Member and Linhao Luo and Yufei Wang and Chen Chen and Jiapu Wang and Xindong Wu},
   isbn = {00000000/00$00.0},
   keywords = {Bidirectional Reasoning,Generative Pre-Training,Index Terms-Natural Language Processing,Knowledge Graphs,Large Language Models,Roadmap},
   month = {6},
   title = {Unifying Large Language Models and Knowledge Graphs: A Roadmap},
   url = {https://arxiv.org/abs/2306.08302v2},
   year = {2023},
}
@article{Trajanoska2023,
   abstract = {The growing trend of Large Language Models (LLM) development has attracted
significant attention, with models for various applications emerging
consistently. However, the combined application of Large Language Models with
semantic technologies for reasoning and inference is still a challenging task.
This paper analyzes how the current advances in foundational LLM, like ChatGPT,
can be compared with the specialized pretrained models, like REBEL, for joint
entity and relation extraction. To evaluate this approach, we conducted several
experiments using sustainability-related text as our use case. We created
pipelines for the automatic creation of Knowledge Graphs from raw texts, and
our findings indicate that using advanced LLM models can improve the accuracy
of the process of creating these graphs from unstructured text. Furthermore, we
explored the potential of automatic ontology creation using foundation LLM
models, which resulted in even more relevant and accurate knowledge graphs.},
   author = {Milena Trajanoska and Riste Stojanov and Dimitar Trajanov},
   keywords = {Index Terms-ChatGPT,LLMs,NLP,REBEL,Relation-extraction,Sustainability},
   month = {5},
   title = {Enhancing Knowledge Graph Construction Using Large Language Models},
   url = {https://arxiv.org/abs/2305.04676v1},
   year = {2023},
}
@article{Mihindukulasooriya2023,
   abstract = {The recent advances in large language models (LLM) and foundation models with
emergent capabilities have been shown to improve the performance of many NLP
tasks. LLMs and Knowledge Graphs (KG) can complement each other such that LLMs
can be used for KG construction or completion while existing KGs can be used
for different tasks such as making LLM outputs explainable or fact-checking in
Neuro-Symbolic manner. In this paper, we present Text2KGBench, a benchmark to
evaluate the capabilities of language models to generate KGs from natural
language text guided by an ontology. Given an input ontology and a set of
sentences, the task is to extract facts from the text while complying with the
given ontology (concepts, relations, domain/range constraints) and being
faithful to the input sentences. We provide two datasets (i) Wikidata-TekGen
with 10 ontologies and 13,474 sentences and (ii) DBpedia-WebNLG with 19
ontologies and 4,860 sentences. We define seven evaluation metrics to measure
fact extraction performance, ontology conformance, and hallucinations by LLMs.
Furthermore, we provide results for two baseline models, Vicuna-13B and
Alpaca-LoRA-13B using automatic prompt generation from test cases. The baseline
results show that there is room for improvement using both Semantic Web and
Natural Language Processing techniques.},
   author = {Nandana Mihindukulasooriya and Sanju Tiwari and Carlos F. Enguix and Kusum Lata},
   doi = {10.5281/zenodo.7916716},
   keywords = {Benchmark ·,Extraction ·,Generation ·,Graph,Graph ·,Knowledge,Language,Large,Models,Relation},
   month = {8},
   title = {Text2KGBench: A Benchmark for Ontology-Driven Knowledge Graph Generation from Text},
   url = {https://arxiv.org/abs/2308.02357v1},
   year = {2023},
}
@article{Butler2019,
   abstract = {The world’s first digital computer, Electronic Numerical Integrator and Computer (ENIAC), would have turned 73 this year. Since ENIAC’s birth in 1946, we have used computers to create a digital version of our analogue world. Human intelligence (and natural stupidity) evolved for the analogue world; however, human cognitive capabilities are limited when it comes to the complexity of understanding and decision-making in the digital world. This paper explores the capability of artificial intelligence (AI) to transform the financial industry. Banks and insurance companies have effectively digitised their businesses, with financial institutions reportedly spending more than any others on data; however, they find themselves caught between the Scylla of big regulation and the Charybdis of big data, particularly where financial compliance and risk management is concerned. Supervisory authorities are equally challenged. It is no surprise, then, to discover that AI is shaping the FinTech, RegTech and SupTech landscapes, in addition to related activities in the legal and professional services sectors. In the face of unbridled enthusiasm and unquestioning acceptance of many of the claims made for AI, this paper takes a balanced, critical stance in explaining the what, why and how of AI in the financial industry, with a particular focus on the art of the possible in regulatory compliance.},
   author = {Tom Butler and Leona O’Brien and Tom Butler and Leona O’Brien},
   issue = {1},
   journal = {Journal of Financial Compliance},
   keywords = {RegTech,SupTech,artificial intelligence (AI),digital technologies,finance,financial regulators,regulatory compliance,semantic technologies},
   pages = {44-59},
   publisher = {Henry Stewart Publications},
   title = {Artificial intelligence for regulatory compliance: Are we there yet?},
   volume = {3},
   url = {https://EconPapers.repec.org/RePEc:aza:jfc000:y:2019:v:3:i:1:p:44-59},
   year = {2019},
}
@article{Palagin2023,
   abstract = {This research presents a comprehensive methodology for utilizing an ontology-driven structured prompts system in interplay with ChatGPT, a widely used large language model (LLM). The study develops formal models, both information and functional, and establishes the methodological foundations for integrating ontology-driven prompts with ChatGPT's meta-learning capabilities. The resulting productive triad comprises the methodological foundations, advanced information technology, and the OntoChatGPT system, which collectively enhance the effectiveness and performance of chatbot systems. The implementation of this technology is demonstrated using the Ukrainian language within the domain of rehabilitation. By applying the proposed methodology, the OntoChatGPT system effectively extracts entities from contexts, classifies them, and generates relevant responses. The study highlights the versatility of the methodology, emphasizing its applicability not only to ChatGPT but also to other chatbot systems based on LLMs, such as Google's Bard utilizing the PaLM 2 LLM. The underlying principles of meta-learning, structured prompts, and ontology-driven information retrieval form the core of the proposed methodology, enabling their adaptation and utilization in various LLM-based systems. This versatile approach opens up new possibilities for NLP and dialogue systems, empowering developers to enhance the performance and functionality of chatbot systems across different domains and languages.},
   author = {Oleksandr Palagin and Vladislav Kaverinskiy and Anna Litvin and Kyrylo Malakhov},
   doi = {10.47839/ijc.22.2.3086},
   issue = {2},
   journal = {International Journal of Computing},
   keywords = {ChatGPT,OntoChatGPT,chatbot,composite service,meta-learning,ontology engineering,ontology-driven information system,prompt engineering,prompt-based learning,transdisciplinary research},
   month = {7},
   pages = {170-183},
   publisher = {Research Institute for Intelligent Computer Systems},
   title = {OntoChatGPT Information System: Ontology-Driven Structured Prompts for ChatGPT Meta-Learning},
   volume = {170},
   url = {http://arxiv.org/abs/2307.05082 http://dx.doi.org/10.47839/ijc.22.2.3086},
   year = {2023},
}
@article{Schlag2023,
   abstract = {In recent years, large pre-trained language models (LLMs) have demonstrated
the ability to follow instructions and perform novel tasks from a few examples.
The possibility to parameterise an LLM through such in-context examples widens
their capability at a much lower cost than finetuning. We extend this line of
reasoning and present a method which further expands the capabilities of an LLM
by embedding it within an algorithm or program. To demonstrate the benefits of
this approach, we present an illustrative example of evidence-supported
question-answering. We obtain a 6.4\% improvement over the chain of thought
baseline through a more algorithmic approach without any finetuning.
Furthermore, we highlight recent work from this perspective and discuss the
advantages and disadvantages in comparison to the standard approaches.},
   author = {Imanol Schlag and Sainbayar Sukhbaatar and Asli Celikyilmaz and Wen-Tau Yih and Jason Weston and J ¨ Urgen Schmidhuber and Xian Li},
   month = {5},
   title = {Large Language Model Programs},
   url = {https://arxiv.org/abs/2305.05364v1},
   year = {2023},
}
@article{Chen2023,
   abstract = {There is a rapidly growing number of large language models (LLMs) that users
can query for a fee. We review the cost associated with querying popular LLM
APIs, e.g. GPT-4, ChatGPT, J1-Jumbo, and find that these models have
heterogeneous pricing structures, with fees that can differ by two orders of
magnitude. In particular, using LLMs on large collections of queries and text
can be expensive. Motivated by this, we outline and discuss three types of
strategies that users can exploit to reduce the inference cost associated with
using LLMs: 1) prompt adaptation, 2) LLM approximation, and 3) LLM cascade. As
an example, we propose FrugalGPT, a simple yet flexible instantiation of LLM
cascade which learns which combinations of LLMs to use for different queries in
order to reduce cost and improve accuracy. Our experiments show that FrugalGPT
can match the performance of the best individual LLM (e.g. GPT-4) with up to
98% cost reduction or improve the accuracy over GPT-4 by 4% with the same cost.
The ideas and findings presented here lay a foundation for using LLMs
sustainably and efficiently.},
   author = {Lingjiao Chen and Matei Zaharia and James Zou},
   month = {5},
   title = {FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance},
   url = {https://arxiv.org/abs/2305.05176v1},
   year = {2023},
}
@article{Chern2023,
   abstract = {The emergence of generative pre-trained models has facilitated the synthesis
of high-quality text, but it has also posed challenges in identifying factual
errors in the generated text. In particular: (1) A wider range of tasks now
face an increasing risk of containing factual errors when handled by generative
models. (2) Generated texts tend to be lengthy and lack a clearly defined
granularity for individual facts. (3) There is a scarcity of explicit evidence
available during the process of fact checking. With the above challenges in
mind, in this paper, we propose FacTool, a task and domain agnostic framework
for detecting factual errors of texts generated by large language models (e.g.,
ChatGPT). Experiments on four different tasks (knowledge-based QA, code
generation, mathematical reasoning, and scientific literature review) show the
efficacy of the proposed method. We release the code of FacTool associated with
ChatGPT plugin interface at https://github.com/GAIR-NLP/factool .},
   author = {I-Chun Chern and Steffi Chern and Shiqi Chen and Weizhe Yuan and Kehua Feng and Chunting Zhou and Junxian He and Graham Neubig and Pengfei Liu and Shanghai Jiao},
   month = {7},
   title = {FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios},
   url = {https://arxiv.org/abs/2307.13528v2},
   year = {2023},
}
@article{Gunasekar2023,
   abstract = {We introduce phi-1, a new large language model for code, with significantly
smaller size than competing models: phi-1 is a Transformer-based model with
1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook
quality" data from the web (6B tokens) and synthetically generated textbooks
and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains
pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays
surprising emergent properties compared to phi-1-base, our model before our
finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller
model with 350M parameters trained with the same pipeline as phi-1 that still
achieves 45% on HumanEval.},
   author = {Suriya Gunasekar and Yi Zhang and Jyoti Aneja and Caio César and Teodoro Mendes and Allie Del Giorno and Sivakanth Gopi and Mojan Javaheripi and Piero Kauffmann and Gustavo De and Rosa Olli and Saarikivi Adil and Salim Shital and Shah Harkirat and Singh Behl and Xin Wang and Sébastien Bubeck and Ronen Eldan and Adam Tauman and Kalai Yin and Tat Lee and Yuanzhi Li},
   month = {6},
   title = {Textbooks Are All You Need},
   url = {https://arxiv.org/abs/2306.11644v1},
   year = {2023},
}
@article{Johannessen2023,
   abstract = {Current anti-money laundering (AML) systems, predominantly rule-based,
exhibit notable shortcomings in efficiently and precisely detecting instances
of money laundering. As a result, there has been a recent surge toward
exploring alternative approaches, particularly those utilizing machine
learning. Since criminals often collaborate in their money laundering
endeavors, accounting for diverse types of customer relations and links becomes
crucial. In line with this, the present paper introduces a graph neural network
(GNN) approach to identify money laundering activities within a large
heterogeneous network constructed from real-world bank transactions and
business role data belonging to DNB, Norway's largest bank. Specifically, we
extend the homogeneous GNN method known as the Message Passing Neural Network
(MPNN) to operate effectively on a heterogeneous graph. As part of this
procedure, we propose a novel method for aggregating messages across different
edges of the graph. Our findings highlight the importance of using an
appropriate GNN architecture when combining information in heterogeneous
graphs. The performance results of our model demonstrate great potential in
enhancing the quality of electronic surveillance systems employed by banks to
detect instances of money laundering. To the best of our knowledge, this is the
first published work applying GNN on a large real-world heterogeneous network
for anti-money laundering purposes.},
   author = {Fredrik Johannessen and Martin Jullum},
   keywords = {PyTorch Geometric,anti-money laundering,graph neural networks,heterogeneous graphs,supervised learning},
   month = {7},
   title = {Finding Money Launderers Using Heterogeneous Graph Neural Networks},
   url = {https://arxiv.org/abs/2307.13499v1},
   year = {2023},
}
@article{Tran2019,
   abstract = {We present our method for tackling the legal case retrieval task of the Competition on Legal Information Extraction/Entailment 2019. Our approach is based on the idea that summarization is important for retrieval. On one hand, we adopt a summarization based model called encoded summarization which encodes a given document into continuous vector space which embeds the summary properties of the document. We utilize the resource of COLIEE 2018 on which we train the document representation model. On the other hand, we extract lexical features on different parts of a given query and its candidates. We observe that by comparing different parts of the query and its candidates, we can achieve better performance. Furthermore, the combination of the lexical features with latent features by the summarization-based method achieves even better performance. We have achieved the state-of-the-art result for the task on the benchmark of the competition.},
   author = {Vu Tran and Minh Le Nguyen and Ken Satoh},
   doi = {10.1145/3322640.3326740},
   isbn = {9781450367547},
   journal = {Proceedings of the 17th International Conference on Artificial Intelligence and Law, ICAIL 2019},
   keywords = {Deep learning,Document representation,Information retrieval,Legal texts,Structure analysis},
   month = {6},
   pages = {275-282},
   publisher = {Association for Computing Machinery, Inc},
   title = {Building legal case retrieval systems with lexical matching and summarization using a pre-trained phrase scoring model},
   url = {https://dl.acm.org/doi/10.1145/3322640.3326740},
   year = {2019},
}
@article{Opijnen2017,
   abstract = {The concept of `relevance' is crucial to legal information retrieval, but because of its intuitive understanding it goes undefined too easily and unexplored too often. We discuss a conceptual frame...},
   author = {Marc van Opijnen and Cristiana Santos},
   doi = {10.1007/S10506-017-9195-8},
   issn = {15728382},
   issue = {1},
   journal = {Artificial Intelligence and Law},
   keywords = {Legal information retrieval,Legal information seeking behaviour,Relevance},
   month = {3},
   pages = {65-87},
   publisher = {
		Kluwer Academic Publishers
		PUB879
		Norwell, MA, USA
	},
   title = {On the concept of relevance in legal information retrieval},
   volume = {25},
   url = {https://dl.acm.org/doi/10.1007/s10506-017-9195-8},
   year = {2017},
}
@misc{Yu2023,
   abstract = {The use of large language models (LLMs) for zero-or few-shot prompting in natural language processing has given rise to a new research area known as prompt engineering, which shows promising improvement in tasks such as arithmetic and common-sense reasoning. This paper explores the use of such approaches in legal reasoning tasks by conducting experiments on the COLIEE entailment task, which is based on the Japanese Bar exam. We further evaluate zero-shot/few-shot and fine-tuning approaches with and without explanations, alongside various prompting strategies. Our results indicate that while these techniques can improve general performance, the best results are achieved with prompts derived from specific legal reasoning techniques, such as IRAC (Issue, Rule, Application , Conclusion). In addition, we observe that few-shot learning with demonstrations derived from clustering past training data consistently yields high performance on the most recent COLIEE entailment tasks. Through our experiments , we improve the previous best result on the 2021 COLIEE task from 0.7037 to 0.8025 and surpass the best system from 2022 with an accuracy of 0.789.},
   author = {Fangyi Yu and Lee Quartey and Frank Schilder},
   pages = {13582-13596},
   title = {Exploring the Effectiveness of Prompt Engineering for Legal Reasoning Tasks},
   url = {https://aclanthology.org/2023.findings-acl.858},
   year = {2023},
}
@article{Shao2020,
   abstract = {Legal case retrieval is a specialized IR task that involves retrieving supporting cases given a query case. Compared with traditional ad-hoc text retrieval , the legal case retrieval task is more challenging since the query case is much longer and more complex than common keyword queries. Besides that, the definition of relevance between a query case and a supporting case is beyond general topical relevance and it is therefore difficult to construct a large-scale case retrieval dataset, especially one with accurate relevance judgments. To address these challenges, we propose BERT-PLI, a novel model that utilizes BERT to capture the semantic relationships at the paragraph-level and then infers the relevance between two cases by ag-gregating paragraph-level interactions. We fine-tune the BERT model with a relatively small-scale case law entailment dataset to adapt it to the legal scenario and employ a cascade framework to reduce the computational cost. We conduct extensive experiments on the benchmark of the relevant case retrieval task in COLIEE 2019. Experimental results demonstrate that our proposed method out-performs existing solutions.},
   author = {Yunqiu Shao and Jiaxin Mao and Yiqun Liu and Weizhi Ma and Ken Satoh and Min Zhang and Shaoping Ma},
   doi = {10.5555/3491440.3491924},
   keywords = {Machine Learning Applications: Other,Multidisciplinary Topics and Applications: Information Retrieval,Natural Language Processing: Information Retrieval},
   title = {BERT-PLI: Modeling Paragraph-Level Interactions for Legal Case Retrieval},
   url = {https://dl.acm.org/doi/10.5555/3491440.3491924},
   year = {2020},
}
@article{Song2022,
   abstract = {We present the first comprehensive empirical evaluation of pre-trained language models (PLMs) for legal natural language processing (NLP) in order to examine their effectiveness in this domain. Our study covers eight representative and challenging legal datasets, ranging from 900 to 57K samples, across five NLP tasks: binary classification, multi-label classification, multiple choice question answering, summarization and information retrieval. We first run unsupervised, classical machine learning and/or non-PLM based deep learning methods on these datasets, and show that baseline systems' performance can be 4%35% lower than that of PLM-based methods. Next, we compare general-domain PLMs and those specifically pre-trained for the legal domain, and find that domain-specific PLMs demonstrate 1%5% higher performance than general-domain models, but only when the datasets are extremely close to the pre-training corpora. Finally, we evaluate six general-domain state-of-the-art systems, and show that they have limited generalizability to legal data, with performance gains from 0.1% to 1.2% over other PLM-based methods. Our experiments suggest that both general-domain and domain-specific PLM-based methods generally achieve better results than simpler methods on most tasks, with the exception of the retrieval task, where the best-performing baseline outperformed all PLM-based methods by at least 5%. Our findings can help legal NLP practitioners choose the appropriate methods for different tasks, and also shed light on potential future directions for legal NLP research.},
   author = {Dezhao Song and Sally Gao and Baosheng He and Frank Schilder},
   doi = {10.1109/ACCESS.2022.3190408},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Legal natural language processing,deep learning,machine learning,pre-trained language model},
   pages = {75835-75858},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {On the Effectiveness of Pre-Trained Language Models for Legal Natural Language Processing: An Empirical Study},
   volume = {10},
   year = {2022},
}
@article{Kia2022,
   abstract = {In closed-domain Question Answering (QA), the goal is to retrieve answers to questions within a specific domain. The main challenge of closed-domain QA is to develop a model that only requires small datasets for training since large-scale corpora may not be available. One approach is a flexible QA model that can adapt to different closed domains and train on their corpora. In this paper, we present a novel versatile reading comprehension style approach for closed-domain QA (called CA-AcdQA). The approach is based on pre-trained contextualized language models, Convolutional Neural Network (CNN), and a self-attention mechanism. The model captures the relevance between the question and context sentences at different levels of granularity by exploring the dependencies between the features extracted by the CNN. Moreover, we include candidate answer identification and question expansion techniques for context reduction and rewriting ambiguous questions. The model can be tuned to different domains with a small training dataset for sentence-level QA. The approach is tested on four publicly-available closed-domain QA datasets: Tesla (person), California (region), EU-law (system), and COVID-QA (biomedical) against nine other QA approaches. Results show that the ALBERT model variant outperforms all approaches on all datasets with a significant increase in Exact Match and F1 score. Furthermore, for the Covid-19 QA in which the text is complicated and specialized, the model is improved considerably with additional biomedical training resources (an F1 increase of 15.9 over the next highest baseline).},
   author = {Mahsa Abazari Kia and Aygul Garifullina and Mathias Kern and Jon Chamberlain and Shoaib Jameel},
   doi = {10.1109/ACCESS.2022.3170466},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Closed-domain question answering,convolutional neural network,question expansion,self-attention},
   pages = {45080-45092},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Adaptable Closed-Domain Question Answering Using Contextualized CNN-Attention Models and Question Expansion},
   volume = {10},
   year = {2022},
}
@article{Sun2019,
   abstract = {We consider open-domain queston answering (QA) where answers are drawn from
either a corpus, a knowledge base (KB), or a combination of both of these. We
focus on a setting in which a corpus is supplemented with a large but
incomplete KB, and on questions that require non-trivial (e.g., ``multi-hop'')
reasoning. We describe PullNet, an integrated framework for (1) learning what
to retrieve (from the KB and/or corpus) and (2) reasoning with this
heterogeneous information to find the best answer. PullNet uses an \{iterative\}
process to construct a question-specific subgraph that contains information
relevant to the question. In each iteration, a graph convolutional network
(graph CNN) is used to identify subgraph nodes that should be expanded using
retrieval (or ``pull'') operations on the corpus and/or KB. After the subgraph
is complete, a similar graph CNN is used to extract the answer from the
subgraph. This retrieve-and-reason process allows us to answer multi-hop
questions using large KBs and corpora. PullNet is weakly supervised, requiring
question-answer pairs but not gold inference paths. Experimentally PullNet
improves over the prior state-of-the art, and in the setting where a corpus is
used with incomplete KB these improvements are often dramatic. PullNet is also
often superior to prior systems in a KB-only setting or a text-only setting.},
   author = {Haitian Sun and Tania Bedrax-Weiss and William W. Cohen},
   doi = {10.18653/v1/d19-1242},
   isbn = {9781950737901},
   journal = {EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference},
   month = {4},
   pages = {2380-2390},
   publisher = {Association for Computational Linguistics},
   title = {PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text},
   url = {https://arxiv.org/abs/1904.09537v1},
   year = {2019},
}
@article{Wang2023,
   abstract = {Large language models (LLMs) have demonstrated their ability to learn
in-context, allowing them to perform various tasks based on a few input-output
examples. However, the effectiveness of in-context learning is heavily reliant
on the quality of the selected examples. In this paper, we propose a novel
framework to iteratively train dense retrievers that can identify high-quality
in-context examples for LLMs. Our framework initially trains a reward model
based on LLM feedback to evaluate the quality of candidate examples, followed
by knowledge distillation to train a bi-encoder based dense retriever. Our
experiments on a suite of 30 tasks demonstrate that our framework significantly
enhances in-context learning performance. Furthermore, we show the
generalization ability of our framework to unseen tasks during training. An
in-depth analysis reveals that our model improves performance by retrieving
examples with similar patterns, and the gains are consistent across LLMs of
varying sizes.},
   author = {Liang Wang and Nan Yang and Furu Wei},
   month = {7},
   title = {Learning to Retrieve In-Context Examples for Large Language Models},
   url = {https://arxiv.org/abs/2307.07164v1},
   year = {2023},
}
@article{Hao2023,
   abstract = {Augmenting large language models (LLMs) with external tools has emerged as a
promising approach to solving complex problems. However, traditional methods,
which finetune LLMs with tool demonstration data, can be both costly and
restricted to a predefined set of tools. Recent in-context learning paradigm
alleviates these issues, but the limited context length only allows for a few
shots of demonstrations, leading to suboptimal understandings of the tools.
Moreover, when there are numerous tools to choose from, in-context learning
could completely fail to work. In this paper, we propose an alternative
approach, $\textbf\{ToolkenGPT\}$, which combines the benefits of both sides. Our
approach represents each $\underline\{tool\}$ as a to$\underline\{ken\}$
($\textit\{toolken\}$) and learns an embedding for it, enabling tool calls in the
same way as generating a regular word token. Once a toolken is triggered, the
LLM is prompted to complete arguments for the tool to execute. ToolkenGPT
offers the flexibility to plug in an arbitrary number of tools by expanding the
set of toolkens on the fly. In addition, it improves tool use by allowing
extensive demonstration data for learning the toolken embeddings. In diverse
domains, including numerical reasoning, knowledge-based question answering, and
embodied plan generation, our approach effectively augments LLMs with tools and
substantially outperforms various latest baselines. ToolkenGPT demonstrates the
promising ability to use relevant tools from a large tool set in complex
scenarios.},
   author = {Shibo Hao and Tianyang Liu and Zhen Wang and Zhiting Hu},
   month = {5},
   title = {ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings},
   url = {https://arxiv.org/abs/2305.11554v2},
   year = {2023},
}
@article{Diao2023,
   abstract = {The increasing scale of large language models (LLMs) brings emergent
abilities to various complex tasks requiring reasoning, such as arithmetic and
commonsense reasoning. It is known that the effective design of task-specific
prompts is critical for LLMs' ability to produce high-quality answers. In
particular, an effective approach for complex question-and-answer tasks is
example-based prompting with chain-of-thought (CoT) reasoning, which
significantly improves the performance of LLMs. However, current CoT methods
rely on a fixed set of human-annotated exemplars, which are not necessarily the
most effective examples for different tasks. This paper proposes a new method,
Active-Prompt, to adapt LLMs to different tasks with task-specific example
prompts (annotated with human-designed CoT reasoning). For this purpose, we
propose a solution to the key problem of determining which questions are the
most important and helpful ones to annotate from a pool of task-specific
queries. By borrowing ideas from the related problem of uncertainty-based
active learning, we introduce several metrics to characterize the uncertainty
so as to select the most uncertain questions for annotation. Experimental
results demonstrate the superiority of our proposed method, achieving
state-of-the-art on eight complex reasoning tasks. Further analyses of
different uncertainty metrics, pool sizes, zero-shot learning, and
accuracy-uncertainty relationship demonstrate the effectiveness of our method.
Our code will be available at https://github.com/shizhediao/active-prompt.},
   author = {Shizhe Diao and Pengcheng Wang and Yong Lin and Rui Pan and Xiang Liu and Tong Zhang},
   month = {2},
   title = {Active Prompting with Chain-of-Thought for Large Language Models},
   url = {https://arxiv.org/abs/2302.12246v3},
   year = {2023},
}
@article{Chang2023,
   abstract = {Large language models (LLMs) are gaining increasing popularity in both
academia and industry, owing to their unprecedented performance in various
applications. As LLMs continue to play a vital role in both research and daily
use, their evaluation becomes increasingly critical, not only at the task
level, but also at the society level for better understanding of their
potential risks. Over the past years, significant efforts have been made to
examine LLMs from various perspectives. This paper presents a comprehensive
review of these evaluation methods for LLMs, focusing on three key dimensions:
what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide
an overview from the perspective of evaluation tasks, encompassing general
natural language processing tasks, reasoning, medical usage, ethics,
educations, natural and social sciences, agent applications, and other areas.
Secondly, we answer the `where' and `how' questions by diving into the
evaluation methods and benchmarks, which serve as crucial components in
assessing performance of LLMs. Then, we summarize the success and failure cases
of LLMs in different tasks. Finally, we shed light on several future challenges
that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to
researchers in the realm of LLMs evaluation, thereby aiding the development of
more proficient LLMs. Our key point is that evaluation should be treated as an
essential discipline to better assist the development of LLMs. We consistently
maintain the related open-source materials at:
https://github.com/MLGroupJLU/LLM-eval-survey.},
   author = {Yupeng Chang and Xu Wang and Jindong Wang and Yuan Wu and Kaijie Zhu and Hao Chen and Linyi Yang and Xiaoyuan Yi and Cunxiang Wang and Yidong Wang and Wei Ye and Yue Zhang and Yi Chang and Senior Member and Philip S Yu and Qiang Yang and Xing Xie},
   keywords = {Index Terms-Large language models,benchmark ✦,evaluation,model assessment},
   month = {7},
   title = {A Survey on Evaluation of Large Language Models},
   url = {https://arxiv.org/abs/2307.03109v1},
   year = {2023},
}
@article{Saxena2020,
   abstract = {Knowledge Graphs (KG) are multi-relational graphs consisting of entities as nodes and relations among them as typed edges. Goal of the Question Answering over KG (KGQA) task is to answer natural language queries posed over the KG. Multi-hop KGQA requires reasoning over multiple edges of the KG to arrive at the right answer. KGs are often incomplete with many missing links, posing additional challenges for KGQA, especially for multi-hop KGQA. Recent research on multi-hop KGQA has attempted to handle KG sparsity using relevant external text, which isn't always readily available. In a separate line of research, KG embedding methods have been proposed to reduce KG sparsity by performing missing link prediction. Such KG embedding methods, even though highly relevant, have not been explored for multi-hop KGQA so far. We fill this gap in this paper and propose EmbedKGQA. EmbedKGQA is particularly effective in performing multi-hop KGQA over sparse KGs. EmbedKGQA also relaxes the requirement of answer selection from a pre-specified neighborhood, a sub-optimal constraint enforced by previous multi-hop KGQA methods. Through extensive experiments on multiple benchmark datasets, we demonstrate EmbedKGQA's effectiveness over other state-of-the-art baselines.},
   author = {Apoorv Saxena and Aditay Tripathi and Partha Talukdar},
   doi = {10.18653/V1/2020.ACL-MAIN.412},
   isbn = {9781952148255},
   issn = {0736587X},
   journal = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
   pages = {4498-4507},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings},
   url = {https://aclanthology.org/2020.acl-main.412},
   year = {2020},
}
@misc{,
   author = {Singh Shivani and Das Nishtha and Michael Rachel and Tanwar Poonam},
   issn = {2229-5518},
   journal = {International Journal of Scientific & Engineering Research Volume 7, Issue 12, December-2016 ISSN },
   month = {12},
   pages = {55-60},
   title = {The Question Answering System Using NLP and AI - IJSER Journal Publication},
   url = {https://www.ijser.org/onlineResearchPaperViewer.aspx?The-Question-Answering-System-Using-NLP-and-AI.pdf},
   year = {2016},
}
@article{Haj2020,
   abstract = {Abstract: Business Rules (BR) are usually written by different stakeholders, which makes them vulnerable to contain different designations for a same concept. Such problem can be the source of a not well orchestrated behaviors. Whereas identification of synonyms is manual or totally neglected in most approaches dealing with natural language Business Rules. In this paper, we present an automated approach to identify semantic similarity between terms in textual BR using Natural Language Processing and knowledge-based algorithm refined using heuristics. Our method is unique in that it also identifies abbreviations/expansions (as a special case of synonym) which is not possible using a dictionary. Then, results are saved in a standard format (SBVR) for reusability purposes. Our approach was applied on more than 160 BR statements divided on three cases with an accuracy between 69% and 87% which suggests it to be an indispensable enhancement for other methods dealing with textual BR.},
   author = {Abdellatif Haj and Youssef Balouki and Taoufiq Gadi},
   doi = {10.22266/IJIES2021.0228.15},
   issn = {21853118},
   issue = {1},
   journal = {International Journal of Intelligent Engineering and Systems},
   keywords = {Abbreviation identification,Business rules,NLP.,Semantic similarity,Synonym extraction},
   pages = {147-156},
   publisher = {Intelligent Network and Systems Society},
   title = {Automated Identification of Semantic Similarity between Concepts of Textual Business Rules},
   volume = {14},
   year = {2020},
}
@article{Montecchiari2022,
   abstract = {This research demonstrates the feasibility of using enterprise ontologies as representation formalism for automated validation of Enterprise Architecture (EA) principles. EA Principles are usually described in natural language, while the Enterprise Architecture is represented with graphical models. As both are intended for human interpretation, it requires humans to validate whether EA Principles are satisfied by an Enterprise Architecture. This is a complex and time-consuming process. We describe a step-by-step procedure for transforming the knowledge of Enterprise Architecture models and the vocabulary of EA Principles into the ArchiMEO ontology, which allows for automated validation. The approach is evaluated in two case studies. They confirm that an ontology-based validation of EA Principles is applicable in reality.},
   author = {Devid Montecchiari and Knut Hinkelmann},
   doi = {10.1007/978-3-031-21488-2_5/COVER},
   isbn = {9783031214875},
   issn = {18651356},
   journal = {Lecture Notes in Business Information Processing},
   keywords = {EA principles,Enterprise architecture,Enterprise ontology,Knowledge engineering,SBVR},
   pages = {66-81},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Towards Ontology-Based Validation of EA Principles},
   volume = {456 LNBIP},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-21488-2_5},
   year = {2022},
}
@article{Abrahamsson2017,
   abstract = {Agile systems development methods emerged as a response to the inability of previous plan-driven approaches to handle rapidly changing environments (Highsmith, 2002). Originating from the so-called...},
   author = {Pekka Abrahamsson and Kieran Conboy and Xiaofeng Wang},
   doi = {10.1057/EJIS.2009.27},
   issn = {14769344},
   issue = {4},
   journal = {https://doi.org/10.1057/ejis.2009.27},
   pages = {281-284},
   publisher = {Taylor & Francis},
   title = {‘Lots done, more to do’: the current state of agile systems development research},
   volume = {18},
   url = {https://www.tandfonline.com/doi/abs/10.1057/ejis.2009.27},
   year = {2017},
}
@article{Agrawal2011,
   abstract = {Semantics of Business Vocabulary and Business Rules (SBVR), an OMG standard, provides a meta-model for the semantic and declarative models of business vocabulary and business rules. Logical formulation of SBVR facilitates IT people to interpret these models generated by business people. However, an important aspect of a business is process model, and it is outside the scope of SBVR. Knowledge intensive and dynamic nature of business processes require such a declarative meta-model for process modeling in order to provide flexibility and adaptability. In this work, an approach for process modeling using SBVR's methodology is proposed. We have made an initial attempt to define a declarative meta-model, Semantics of Business Process Vocabulary and Process Rules (SBPVR). SBPVR divides knowledge of business processes into three parts; process concept types, process fact types and process rules. Process concept types and fact types represent structure of processes and process rules provide guidance over the structure and flow of processes at execution time. In the paper, these types are further categorized to represent the various elements of process models. SBPVR provides flexibility and adaptability to the process model through its declarative nature and fact oriented approach.},
   author = {Ashish Agrawal},
   doi = {10.1145/1953355.1953363},
   isbn = {9781450305594},
   journal = {Proceedings of the 4th India Software Engineering Conference 2011, ISEC'11},
   keywords = {Fact oriented modeling,Process design methods and methodologies,SBVR},
   pages = {61-68},
   title = {Semantics of business process vocabulary and process rules},
   year = {2011},
}
@article{Athan2013,
   abstract = {In this paper we present the motivation, use cases, design principles, abstract syntax, and initial core of LegalRuleML. The LegalRuleML-core is sufficiently rich for expressing legal sources, time, defeasibil-ity, and deontic operators. An example is provided. LegalRuleMLis compared to related work. Copyright 2013 ACM.},
   author = {Tara Athan and Harold Boley and Guido Governatori and Monica Palmirani and Adrian Paschke and Adam Wyner},
   doi = {10.1145/2514601.2514603},
   isbn = {9781450320801},
   journal = {Proceedings of the International Conference on Artificial Intelligence and Law},
   keywords = {LegalRuleML},
   pages = {3-12},
   title = {OASIS LegalRuleML},
   year = {2013},
}
@article{Goedertier2009,
   abstract = {A business process model is called rule-based if the logic of its control flow, data flow and resource allocation is declaratively expressed by means of business rules. Business rules are recognised as powerful representation forms that can potentially define the semantics of business process models and business vocabulary. To date, however, there is little consensus and fragmentary knowledge about the precise relationship between these elements of business modelling. In this article, we develop a first-version metamodel that is to be used as a foundation in integrating and developing existing and new forms of rule-based business process modelling. In addition, we show how rule-based process models can be brought to execution in the context of service-oriented architecture.},
   author = {Stijn Goedertier and K U Leuven and Jan Vanthienen and Raf Haesen},
   doi = {10.1504/IJBPIM.2008.023219},
   issue = {3},
   journal = {researchgate.net},
   keywords = {business rules,ontology,process enactment,process modelling,service-oriented architecture},
   pages = {194-207},
   publisher = {Inderscience Publishers},
   title = {Rule-based business process modelling and enactment},
   volume = {3},
   url = {https://www.researchgate.net/profile/Stijn-Goedertier/publication/46430331_Rule-based_business_process_modelling_and_enactment/links/004635188cc8aa90db000000/Rule-based-business-process-modelling-and-enactment.pdf?_sg%5B0%5D=started_experiment_milestone&origin=journalDetail},
   year = {2009},
}
@article{Boella2013,
   abstract = {Maintaining regulatory compliance is an increasing area of concern for business. Legal Knowledge Management systems that combine repositories of legislation with legal on-tologies, can support the work of in-house compliance managers. But there are challenges to overcome, of interpreting legal knowledge and mapping that knowledge onto business processes, and developing systems that can adequately handle the complexity with clarity and ease. In this paper we extend the Legal Knowledge Management system Eunomos to deal with alternative interpretations of norms connecting it with Business Process Management systems. Moreover, we propose a workflow involving the different roles in a company , which takes legal interpretation into account in mapping norms and processes, and uses Eunomos as a support.},
   author = {Guido Boella and Livio Robaldo and Luigi Di Caro and Marijn Janssen and Joris Hulstijn and Llio Humphreys and Leendert Van Der Torre},
   journal = {Citeseer},
   title = {Managing Legal Interpretation in Regulatory Compliance},
   url = {https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.726.4864&rep=rep1&type=pdf},
   year = {2013}
}
@article{Jarzebowicz2021,
   abstract = {Agile Software Development methods have become a widespread approach used by the software industry. Non-functional requirements (NFRs) are often reported to be a problematic issue for such methods. We aimed to identify (within the context of Agile projects): (1) the issues (challenges and problems) reported as affecting the implementation of NFRs; and (2) practices that facilitate the successful implementation of NFRs. We conducted a systematic literature review and processed its results to obtain a comprehensive summary. We were able to present two lists, dedicated to issues and practices, respectively. Most items from both lists, but not all, are related to the requirements engineering area. We found out that the issues reported are mostly related to the common themes of: NFR documentation techniques, NFR traceability, elicitation and communication activities. The facilitating practices mostly cover similar topics and the recommendation is to start focusing on NFRs early in the project.},
   author = {A Jarzębowicz and P Weichbroth - Conference on Lean and Agile Software … and undefined 2021},
   doi = {10.1007/978-3-030-67084-9_6},
   journal = {Springer},
   keywords = {Ag-ile Software Development,Agile Requirements Engineering,Non-functional Requirements,Quality Requirements,Systematic Literature Review},
   pages = {91-110},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {A systematic literature review on implementing non-functional requirements in agile software development: Issues and facilitating practices},
   volume = {408},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-67084-9_6},
   year = {2021},
}
@article{Moschoyiannis2019,
   abstract = {The service choreography approach has been proposed for describing the global ordering constraints on the observable message exchanges between participant services in service oriented architectures. Recent work advocates the use of structured natural language, in the form of Semantics of Business Vocabulary and Rules (SBVR), for specifying and validating choreographies. This paper addresses the verification of choreographies - whether the local behaviours of the individual participants conform to the global protocol prescribed by the choreography. We describe how declarative specifications of service choreographies can be verified using a trace-based model, namely an adaptation of Shields' vector languages. We also use the so-called blackboard rules, which draw upon the Bach coordination language, as a middleware that adds reactiveness to this declarative setting. Vector languages are to trace languages what matrices are to linear transformations; they afford a more concrete representation which has advantages when it comes to computation or manipulation.},
   author = {Sotiris Moschoyiannis and Leandros Maglaras and Nurulhuda A. Manaf},
   doi = {10.1109/SOCA.2018.00034},
   isbn = {9781538691335},
   journal = {Proceedings - IEEE 11th International Conference on Service-Oriented Computing and Applications, SOCA 2018},
   keywords = {Complex systems,Concurrency,Global behaviour,SBVR,Service interactions verification,Vector languages},
   month = {1},
   pages = {185-193},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Trace-Based Verification of Rule-Based Service Choreographies},
   year = {2019},
}
@article{Chittimalli2019,
   abstract = {Functionality of a software system that implements business operations can be captured using business processes and rules. To understand the 'as-is' processes and rules, the source-code is arguably the best source of knowledge. We present a novel method that combines program analysis and domain knowledge to create the descriptions for "IT rules", as a critical step towards extracting business rules automatically. We introduce and use the concept of 'variable provenance' to propagate the domain descriptions into the source code to create Semantics of Business Vocabularies and Rules (SBVR) rules. In our experiments on sample, near-real-life systems, we could successfully annotate very large percentage (> 90%) of IT rules and enable to create SBVR rules. We present and describe the ProgAnnotator tool which is based on variable prove-nance and generates descriptions for IT rules in the source code and subsequently create SBVR rules automatically.},
   author = {Pavan Kumar Chittimalli and Abhidip Bhattacharyya},
   city = {New York, NY, USA},
   doi = {10.1145/3299771},
   isbn = {9781450362153},
   journal = {Proceedings of the 12th Innovations on Software Engineering Conference (formerly known as India Software Engineering Conference)},
   keywords = {Business Rule Extraction,Rule Annotation,SBVR,Static Program Analysis,Variable Provenance},
   publisher = {ACM},
   title = {SBVR-based Business Rule Creation for Legacy Programs using Variable Provenance},
   url = {https://doi.org/10.1145/3299771.3299786},
   year = {2019},
}
@article{Heeager2018,
   abstract = {Context: Safety-critical software systems are increasingly being used in new application areas, such as personal medical devices, traffic control, and detection of pathogens. A current research debate is regarding whether safety-critical systems are better developed with traditional waterfall processes or agile processes that are purportedly faster and promise to lead to better products. Objective: To identify the issues and disputes in agile development of safety-critical software and the key qualities as found in the extant research literature. Method: We conducted a systematic literature review as an interpretive study following a research design to search, assess, extract, group, and understand the results of the found studies. Results: There are key issues and propositions that we elicit from the literature and combine into a conceptual model for understanding the foundational challenges of agile software development of safety-critical systems. The conceptual model consists of four problematic practice areas and five relationships, which we find to be even more important than the problematic areas. From this review, we suggest that there are important research gaps that need to be investigated. Conclusions: We suggest that future research should have a primary focus on the relationships in the resulting conceptual model and specifically on the dynamics of the field as a whole, on incremental versus iterative development, and on how to create value with minimal but sufficient effort.},
   author = {Lise Tordrup Heeager and Peter Axel Nielsen},
   doi = {10.1016/J.INFSOF.2018.06.004},
   issn = {0950-5849},
   journal = {Information and Software Technology},
   keywords = {Agile processes,Agile software development,Interpretive literature review,Safety-critical software systems,Software development,Systematic literature review},
   month = {11},
   pages = {22-39},
   publisher = {Elsevier},
   title = {A conceptual model of agile software development in a safety-critical context: A systematic literature review},
   volume = {103},
   year = {2018},
}
@article{Martins2016,
   abstract = {Context Safety-Critical Systems (SCS) are becoming increasingly present in our society. A considerable amount of research effort has been invested into improving the SCS requirements engineering process as it is critical to the successful development of SCS and, in particular, the engineering of safety aspects. Objective This article aims to investigate which approaches have been proposed to elicit, model, specify and validate safety requirements in the context of SCS, as well as to what extent such approaches have been validated in industrial settings. The paper will also investigate how the usability and usefulness of the reported approaches have been explored, and to what extent they enable requirements communication among the development project/team actors in the development of SCS. Method We conducted a systematic literature review by selecting 151 papers published between 1983 and 2014. The research methodology to conduct the SLR was based on the guidelines proposed by Kitchenham and Biolchini. Results The results of this systematic review should encourage further research into the design of studies to improve the requirements engineering for SCS, particularly to enable the communication of the safety requirements among the project team actors, and the adoption of other models for hazard and accident models. The presented results point to the need for more industry-oriented studies, particularly with more participation of practitioners in the validation of new approaches. Conclusion The most relevant findings from this review and their implications for further research are as follows: integration between requirements engineering and safety engineering areas; dominance of the traditional approaches; early mortality of new approaches; need for industry validation; lack of evidence for the usefulness and usability of most approaches; and the lack of studies that investigate how to improve the communication process throughout the lifecycle. Based on the findings, we suggest a research agenda to the community of researchers and advices to SCS practitioners.},
   author = {Luiz Eduardo G. Martins and Tony Gorschek},
   doi = {10.1016/J.INFSOF.2016.04.002},
   issn = {0950-5849},
   journal = {Information and Software Technology},
   keywords = {Accident,Hazard,Requirements engineering,Safety requirements,Safety-critical systems,Systematic literature review},
   month = {7},
   pages = {71-89},
   publisher = {Elsevier},
   title = {Requirements engineering for safety-critical systems: A systematic literature review},
   volume = {75},
   year = {2016},
}
@article{Amorndettawin2019,
   abstract = {Following the agile principles, agile software development is popular among software developing organizations. The attractive characteristics of agile development are that it embraces frequent changes and that it gives high priority to users. Due to such characteristics, non-functional requirements are usually not identified in the requirements elicitation process. They are often neglected in an early phase, and even overlooked in the later stages of software development. This results in poor quality software. To enhance non-functional requirements identification in agile development, this paper proposes a development of a set of non-functional requirement patterns in the form of requirement templates for agile development. The set is composed of 10 security requirement patterns and 13 fault tolerance requirement patterns, which are derived from an analysis of security and fault tolerance design patterns. The proposed non-functional requirement patterns can facilitate non-functional requirements gathering and help agile team members in writing the requirements. In an experiment on a Scrum team, the team members took less time to write security and fault tolerance requirements for a number of given problems when using the proposed patterns, compared with the case of not using the patterns. In addition, the non-functional requirement patterns helped the team members to write more complete security and fault tolerance requirements. Despite the effort they had to spend in learning the proposed requirement patterns, the team also agreed that the patterns are useful in practice. CCS Concepts • Software and its engineering➝Requirements analysis • Software and its engineering➝Agile software development • Software and its engineering➝Software fault tolerance • Software and its engineering➝Software reliability • Software and its engineering➝Design patterns • Security and privacy➝Software security engineering.},
   author = {M Amorndettawin and T Senivongse - on Software and e-Business and undefined 2019},
   doi = {10.1145/3374549.3374561},
   isbn = {9781450376495},
   journal = {dl.acm.org},
   keywords = {Fault Tolerance Patterns,Non-functional Requirements,Requirement Patterns,Scrum,Security Patterns},
   month = {12},
   pages = {66-74},
   publisher = {Association for Computing Machinery},
   title = {Non-functional requirement patterns for agile software development},
   url = {https://dl.acm.org/doi/abs/10.1145/3374549.3374561?casa_token=0OY5Th4yYhUAAAAA:fcY5v3W47C1028J8K9hCFIau82RUDFE3anzcJYkvGxOe2IZk9AkfO_RE55IUXUky4O2ane56NyRcuA},
   year = {2019},
}
@article{Aalst2011,
   abstract = {The independent verification of the right applications of business rules in an information system is a task for auditors. The increasing complexity of information systems, and the high risks associated with violations of business rules, have created the need for Online Auditing Tools. In this paper we sketch a conceptual design for such a tool. The components of the tool are described briefly. The focus is on the database and the conformance checker, which are described in detail. The approach is illustrated with an example and some preliminary case studies from industry. © 2010 Elsevier B.V. All rights reserved.},
   author = {Wil Van Der Aalst and Kees Van Hee and Jan Martijn Van Der Werf and Akhil Kumar and Marc Verdonk},
   doi = {10.1016/J.DSS.2010.08.014},
   issn = {0167-9236},
   issue = {3},
   journal = {Decision Support Systems},
   keywords = {Architecture,Auditing,Business rules,Conceptual model,Conformance checking,Constraints,Information assurance},
   month = {2},
   pages = {636-647},
   publisher = {North-Holland},
   title = {Conceptual model for online auditing},
   volume = {50},
   year = {2011},
}
@inproceedings{Livshitz2016,
   abstract = {The application of Integrated Management Systems (IMS) is now attracting the attention of TOP management of a variety of organizations: refineries, machinery, instrument-making, aviation, defense, etc. However, now the major problem is still the performance of IMS audits as full implementation of complex checks from different ISO standards with a substantial limitatio...},
   author = {Ilya I. Livshitz and Kseniya A. Nikiforova and Pavel A. Lontsikh and Elena Y. Drolova and Natalia P. Lontsikh},
   journal = {2016 IEEE Conference on Quality Management, Transport and Information Security, Information Technologies (IT&MQ&IS)},
   publisher = {IEEE},
   title = {The optimization of the integrated management system audit program},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/7751919/},
   year = {2016},
}
@article{Balachandar2019,
   abstract = {The number of connected things or devices will be 125 billion by 2030 as per IHS market survey. It indirectly translates that large number of users will be accessing these devices for collecting, controlling, monitoring and building edge analytics. The device management policy and data policy on accessing data from different resources needs proper governance and control. The data security will help these users to assure regulatory compliance, security, privacy and protection. This paper is a centralized data security management across different layers of Internet of Things along with rule builder. Mainstream of this study focusses on compliance standard, identity management, data management and policy engine and audit reports. The specific regulatory compliance, security standards or firmware or hardware security and vulnerabilities at IoT layer has not been captured in this paper.},
   author = {S. Balachandar and R. Chinnaiyan},
   doi = {10.1007/978-981-10-8681-6_19/COVER},
   issn = {23674520},
   journal = {Lecture Notes on Data Engineering and Communications Technologies},
   keywords = {Audit engine,Business rules,Data audit,HTTP,Identity management,IoT message broker,LoraWan,MQTT (message queuing telemetry transport),Metadata management,NFC (near field communication),Privacy,REST,Zig Bee},
   pages = {193-201},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Centralized Reliability and Security Management of Data in Internet of Things (IoT) with Rule Builder},
   volume = {15},
   url = {https://link.springer.com/chapter/10.1007/978-981-10-8681-6_19 https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-981-10-8681-6_19},
   year = {2019},
}
@article{Sirisom2017,
   abstract = {This paper presents a system design using the design and linking semantic technology of ontologies by mapping the structure base and finding identical meanings of each text. The Wu and Palmer method and WordNet database were used for this purpose. The accuracy of the results of the concept are measured by using Recall, Precision, and F-Measure. Then, the proposed designed can be used to developed tools to qualify the security system for communications security domain under the standards of information security management for ISO 27001:2013. However, the cost of certification to organisations to meet international standards is considerable. Our intention was to demonstrate the ontology-based concept for organisations to be able to reduce their certification costs by waiving the requirement for an external consultant to evaluate their standards and policies.},
   author = {Pongsak Sirisom and Janjira Payakpate and Winai Wongthai},
   doi = {10.1007/978-981-10-4154-9_30/COVER},
   isbn = {9789811041532},
   issn = {18761119},
   journal = {Lecture Notes in Electrical Engineering},
   keywords = {ISO 27001:2013,Ontology mapping,Similarity comparison on ontology},
   pages = {257-265},
   publisher = {Springer Verlag},
   title = {A system design for the measurement and evaluation of the communications security domain in ISO 27001:2013 using an ontology},
   volume = {424},
   url = {https://link.springer.com/chapter/10.1007/978-981-10-4154-9_30},
   year = {2017},
}
@article{Turetken2012,
   abstract = {In today's IT-centric business environment, managing compliance with regulations, laws, and other imperatives has become critical for success. Directives govern almost every aspect of running a business, requiring organizations to provide assurances to regulators, stakeholders, customers, and business partners. Assuring compliance across an enterprise necessitates a h...},
   author = {Oktay Turetken and Amal Elgammal and Willem-Jan van den Heuvel and Michael P. Papazoglou},
   issue = {3},
   journal = {IEEE Software},
   publisher = {IEEE},
   title = {Capturing Compliance Requirements: A Pattern-Based Approach},
   volume = {29},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/6158632/},
   year = {2012},
}
@inproceedings{Livshitz2020,
   abstract = {Nowadays, the attention has increased significantly on the IT-components security, including the assessment of multiple types of objects using cloud infrastructure. The article discusses the opinions of Russian and foreign experts on a wide range of issues of IT-security as stand-alone IT-components and holistic cloud services. The article proposes a practical approach based on t...},
   author = {Ilya I. Livshitz and Pawel A. Lontsikh and Elena Y. Golovina and Egor P. Kunakov and Valentina V. Kozhukhova},
   journal = {2020 International Conference Quality Management, Transport and Information Security, Information Technologies (IT&QM&IS)},
   publisher = {IEEE},
   title = {Security Assessment Process of IT-components for Cloud Infrastructure},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/9322976/},
   year = {2020},
}
@inproceedings{Haralambiev2011,
   abstract = {Source code analysis has been and still is extensively researched topic with various applications to the modern software industry. In this paper we share our experience in applying various source code analysis techniques for assessing the quality of and detecting potential defects in a large mission-critical software system. The case study is about the maintenance of a software system of a B...},
   author = {Haralambi Haralambiev and Stanimir Boychev and Delyan Lilov and Kraicho Kraichev},
   journal = {2011 IEEE EUROCON - International Conference on Computer as a Tool},
   publisher = {IEEE},
   title = {Applying source code analysis techniques: A case study for a large mission-critical software system},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/5929241/},
   year = {2011},
}
@article{Carlin2007,
   abstract = {This paper discussed an IT audit that involves examining all IT business processes and data that integrate with an organization's financial systems. The examination reviews controls and compliance with policies. An IT audit might also require a review of the organization's use of IT in supporting business from efficiency, effectiveness, and economic ...},
   author = {Anna Carlin and Frederick Gallegos},
   issue = {7},
   journal = {Computer},
   publisher = {IEEE},
   title = {IT Audit: A Critical Business Process},
   volume = {40},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/4287254/},
   year = {2007},
}
@article{Thirumaran2010,
   author = {M Thirumaran and P Dhavachelvan and S Subhashree},
   journal = {academia.edu},
   title = {Business Rule Management Framework for N-Tier E-Business Applications},
   url = {https://www.academia.edu/download/57053837/0910ijmpict01.pdf},
   year = {2010},
}
@article{Jagadeesan2009,
   abstract = {Accountability mechanisms, which rely on after-the-fact verification, are an attractive means to enforce authorization policies. In this paper, we describe an operational model of accountability-based distributed systems. We describe analyses which support both the design of accountability systems and the validation of auditors for finitary accountability systems. Our study provides formal foundations to explore the tradeoffs underlying the design of accountability systems including: the power of the auditor, the efficiency of the audit protocol, the requirements placed on the agents, and the requirements placed on the communication infrastructure. © 2009 Springer Berlin Heidelberg.},
   author = {Radha Jagadeesan and Alan Jeffrey and Corin Pitcher and James Riely},
   doi = {10.1007/978-3-642-04444-1_10/COVER},
   isbn = {3642044433},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {152-167},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Towards a theory of accountability and audit},
   volume = {5789 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-04444-1_10},
   year = {2009},
}
@article{Schonherr2021,
   abstract = {Purpose: While corporate social responsibility (CSR) standards are amongst the most widely adopted instruments for supporting firms in becoming more accountable, firms who adopt them frequently fail to comply. In this context, the purpose of this study is to explore to what extent CSR standards are designed for accountability. In the analysis, this paper investigates design characteristics related to accountability across different standard types, namely, principle-based, reporting, certification and process standards. Design/methodology/approach: This study reviews the design characteristics of 50 CSR standards in a systematic and comparative fashion. This paper combines qualitative deductive coding with exploratory quantitative analyses methods to elucidate structural variance and patterns of accountability-related design characteristics across the sample. Findings: This study finds that the prevalence of design characteristics aimed at fostering accountability varies significantly between different types of standards. This paper identifies three factors related to the specific purpose of any given standard that explain this structural variation in design characteristics, namely, implementability, comparability and measurability. Practical implications: Non-compliance limits the effectiveness and legitimacy of CSR standards. The systematic exploration of patterns and structural variation in design characteristics that promote accountability may provide valuable clues for the design of more effective CSR standards in the future. Social implications: Better understanding the role of design characteristics of CSR standards is critical to ensure they contribute to greater corporate accountability. Originality/value: This study strives to expand the current understanding of the design characteristics of CSR standards beyond individual cases through a systematic exploration of accountability-related design characteristics across a larger sample.},
   author = {Norma Schönherr and Heike Vogel-Pöschl and Florian Findler and André Martinuzzi},
   doi = {10.1108/SAMPJ-07-2020-0257/FULL/PDF},
   issn = {2040803X},
   issue = {1},
   journal = {Sustainability Accounting, Management and Policy Journal},
   keywords = {Accountability mechanisms,CSR standards,Corporate social responsibility,Mixed-method design,Standard design},
   pages = {1-29},
   publisher = {Emerald Group Holdings Ltd.},
   title = {Accountability by design? Exploring design characteristics of corporate social responsibility standards},
   volume = {13},
   year = {2021},
}
@article{Ko2011,
   abstract = {Cloud computing signifies a paradigm shift from owning computing systems to buying computing services. As a result of this paradigm shift, many key concerns such as the transparency of data transfer and access within the cloud, and the lack of clarity in data ownership were surfaced. To address these concerns, we propose a new way of approaching traditional security and trust problems: To adopt a detective, data-centric thinking instead of the classical preventive, system-centric thinking. While classical preventive approaches are useful, they play a catch-up game; often do not address the problems (i.e. data accountability, data retention, etc) directly. In this paper, we propose a data-centric, detective approach to increase trust and security of data in the cloud. Our framework, known as TrustCloud, contains a suite of techniques that address cloud security, trust and accountability from a detective approach at all levels of granularity. TrustCloud also extends detective techniques to policies and regulations governing IT systems. © 2011 IEEE.},
   author = {Ryan K.L. Ko and Markus Kirchberg and Bu Sung Lee},
   doi = {10.1109/DSR.2011.6026885},
   isbn = {9781424492763},
   journal = {2011 Defense Science Research Conference and Expo, DSR 2011},
   keywords = {TrustCloud framework,accountability,cloud computing,cloud computing security,data-centric logging,trust},
   title = {From system-centric to data-centric logging - Accountability, trust & security in cloud computing},
   year = {2011},
}
@article{Borges2012,
   abstract = {The processing of business rules usually occurs through its encoding in software applications, causing impacts that hinder their maintenance and reuse. In the financial area, the centralization of business rules represented in technology XBRL Formula promotes reuse by multiple applications and reduces these impacts. However, its use is not widespread and solutions that enable the applications reusing business rules with flexibility are restricted to proprietary solutions. This article proposes a framework for flexible processing of business rules defined with XBRL Formula from the use of services.},
   author = {Fernando Cezar R. Borges and Paulo Caetano da Silva},
   journal = {Anais do Simpósio Brasileiro de Sistemas Multimídia e Web (WebMedia)},
   month = {10},
   pages = {47-50},
   publisher = {SBC},
   title = {A framework for processing business financial rules},
   url = {https://sol.sbc.org.br/index.php/webmedia/article/view/5523},
   year = {2012},
}
@article{Butin2013,
   abstract = {Accountability is a requirement to be included in the initial design phase of systems because of its strong impact on log architecture implementation. As an illustration, the logs we examine here record actions by data controllers handling personally identifiable information to deliver services to data subjects. The structures of those logs seldom consider requirements for accountability, preventing effective dispute resolution. We address the question of what information should be included in logs to make their a posteriori compliance analysis meaningful. Real-world scenarios are used to show that decisions about log architecture are nontrivial and should be made from the design stage on. Four categories of situations for which straightforward solutions are problematic are presented. Our contribution shows how log content choices and accountability definitions mutually affect each other and incites service providers to rethink up to what extent they can be held responsible. These different aspects are synthesized into key guidelines to avoid common pitfalls in accountable log design. This analysis is based on case studies performed on our implementation of the PPL policy language. © 2013 IEEE.},
   author = {Denis Butin and Marcos Chicote and Daniel Le Métayer},
   doi = {10.1109/SPW.2013.26},
   isbn = {9780769550176},
   journal = {Proceedings - IEEE CS Security and Privacy Workshops, SPW 2013},
   keywords = {Accountability,PPL,Privacy,Security Policy},
   pages = {1-7},
   title = {Log design for accountability},
   year = {2013},
}
@article{Wang2014,
   author = {Wei Wang and Marta Indulska and Shazia Sadiq},
   isbn = {978-1-927184-26-4},
   keywords = {Business Process Management,Business Rule Management,Digital Payment,Disruptive Technologies,IS centralization/decentralization,Integrated Modelling,Platform Design,Platforms},
   publisher = {ACIS},
   title = {Factors Affecting Business Process and Business Rule Integration},
   url = {https://openrepository.aut.ac.nz/handle/10292/8060},
   year = {2014},
}
@article{Muehlen2008,
   author = {Michael Zur Muehlen and Marta Indulska and Kai Kittel},
   journal = {ACIS 2008 Proceedings},
   month = {1},
   title = {Towards Integrated Modeling of Business Processes and Business Rules},
   url = {https://aisel.aisnet.org/acis2008/108/ https://aisel.aisnet.org/acis2008/108},
   year = {2008},
}
@article{Ahmad2019,
   abstract = {Audit logs serve as a critical component in enterprise business systems and are used for auditing, storing, and tracking changes made to the data. However, audit logs are vulnerable to a series of attacks enabling adversaries to tamper data and corresponding audit logs without getting detected. Among them, two well-known attacks are “the physical access attack,” which exploits root privileges, and “the remote vulnerability attack,” which compromises known vulnerabilities in database systems. In this paper, we present BlockAudit: a scalable and tamper-proof system that leverages the design properties of audit logs and security guarantees of blockchain to enable secure and trustworthy audit logs. Towards that, we construct the design schema of BlockAudit and outline its functional and operational procedures. We implement our design on a custom-built Practical Byzantine Fault Tolerance (PBFT) blockchain system and evaluate the performance in terms of latency, network size, payload size, and transaction rate. Our results show that conventional audit logs can seamlessly transition into BlockAudit to achieve higher security and defend against the known attacks on audit logs.},
   author = {A. Ahmad and Muhammad Saad and Aziz Mohaisen},
   journal = {Journal of Network and Computer Applications},
   keywords = {Audit logs,Blockchain,Distributed systems},
   month = {11},
   pages = {102406},
   publisher = {Academic Press},
   title = {Secure and transparent audit logs with BlockAudit},
   volume = {145},
   year = {2019},
}
@article{Silva2014,
   abstract = {This chapter provides the context for the book. It starts with an example of a verification problem for a business rules program that was inspired by real-life business cases. It then gives an overview of what a Business Rules Management System is. Before a short...},
   author = {Bruno Berstel-Da Silva},
   doi = {10.1007/978-3-642-40038-4_1},
   journal = {Verification of Business Rules Programs},
   pages = {3-17},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Introduction},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-40038-4_1},
   year = {2014},
}
@article{Raim2014,
   abstract = {Process mining is a discipline that aims at discovering, monitoring and improving real-life processes by extracting knowledge from event logs. Process discovery and conformance checking are the two main process mining tasks. Process discovery techniques can be used to learn a process model from example traces in an event log, whereas the goal of conformance checking is to compare the observed behavior in the event log with the modeled behavior. In this paper, we propose an approach based on temporal logic query checking, which is in the middle between process discovery and conformance checking. It can be used to discover those LTL-based business rules that are valid in the log, by checking against the log a (user-defined) class of rules. The proposed approach is not limited to provide a boolean answer about the validity of a business rule in the log, but it rather provides valuable diagnostics in terms of traces in which the rule is satisfied (witnesses) and traces in which the rule is violated (counterexamples). We have implemented our approach as a proof of concept and conducted a wide experimentation using both synthetic and real-life logs.},
   author = {Margus Räim and Claudio Di Ciccio and Fabrizio Maria Maggi and Massimo Mecella and Jan Mendling},
   doi = {10.1007/978-3-662-45563-0_5/COVER},
   isbn = {9783662455623},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Business Rules,Linear Temporal Logic,Process Discovery,Temporal Logic Query Checking},
   pages = {75-92},
   publisher = {Springer Verlag},
   title = {Log-based understanding of business processes through temporal logic query checking},
   volume = {8841},
   url = {https://link.springer.com/chapter/10.1007/978-3-662-45563-0_5},
   year = {2014},
}
@inproceedings{Sun2019,
   abstract = {In the Internet of Things and industrial controlnetwork servers, a large number of logs will be formed everymoment. This log information, as an important basis for eventrecording and security auditing, provides important informa-tion for identifying threat sources, identifying threat degreeand judging threat impact. However, the current security loganalysis system usu...},
   author = {Yizhen Sun and Shaoming Guo and Zhongwei Chen},
   journal = {2019 15th International Conference on Mobile Ad-Hoc and Sensor Networks (MSN)},
   publisher = {IEEE},
   title = {Intelligent Log Analysis System for Massive and Multi-Source Security Logs: MMSLAS Design and Implementation Plan},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/9066044/},
   year = {2019},
}
@article{Amir-Mohammadian2020,
   abstract = {Audit logging provides post-facto analysis of runtime behavior for different purposes, including error detection, amelioration of system operations, and the establishment of security in depth. This necessitates some level of assurance on the quality of the generated audit logs, i.e., how well the audit log represents the events transpired during the execution. Information-algebraic techniques have been proposed to formally specify this relation and provide a framework to study correct audit log generation in a provable fashion. However, previous work fall short on how to guarantee this property of audit logging in concurrent environments. In this paper, we study an implementation model in a concurrent environment. We propose an algorithm that instruments a concurrent system according to a formal specification of audit logging requirements, so that any instrumented concurrent system guarantees correct audit log generation. As an application, we consider systems with microservices architecture, where logging an event by a microservice is conditioned on the occurrence of a collection of events that take place in other microservices of the system.},
   author = {Sepehr Amir-Mohammadian and Chadi Kari},
   doi = {10.1016/j.entcs.2020.08.007},
   issn = {15710661},
   journal = {Electronic Notes in Theoretical Computer Science},
   keywords = {Audit logging,Concurrent systems,Programming languages,Security},
   pages = {114-141},
   publisher = {Elsevier B.V.},
   title = {Correct audit logging in concurrent systems},
   volume = {351},
   year = {2020},
}
@article{Muthurajkumar2015,
   abstract = {Log Management has been an important service in Cloud Computing. In any business, maintaining the log records securely over a particular period of time is absolutely necessary for various reasons such as auditing, forensic analysis, evidence etc. In this work, Integrity and confidentiality of the log records are maintained at every stage of Log Management namely the Log Generation phase, Transmission phase and Storage phase. In addition to this, Log records may often contain sensitive information about the organization which should not be leaked to the outside world. In this paper, Temporal Secured Cloud Log Management Algorithm techniques are implemented to provide security to maintain transaction history in cloud within time period. In this work, security to temporal log management is provided by encrypting the log data before they are stored in the cloud storage. They are also stored in batches for easy retrieval. This work was implemented in Java programming language in the Google drive environment.},
   author = {S. Muthurajkumar and S. Ganapathy and M. Vijayalakshmi and A. Kannan},
   doi = {10.1016/j.procs.2015.02.098},
   issn = {18770509},
   journal = {Procedia Computer Science},
   keywords = {Cloud computing,Cloud security,Cloud storage,Log management},
   pages = {589-595},
   publisher = {Elsevier B.V.},
   title = {Secured temporal log management techniques for cloud},
   volume = {46},
   year = {2015},
}
@article{Accorsi2013,
   abstract = {This paper presents BBox, a digital black box to provide for authentic archiving (and, consequently, forensic evidence) for remote auditing in distributed systems. Based upon public key cryptography and trusted computing platforms, the BBox employs standard primitives to ensure the authenticity of records during the transmission from devices to the collector, as well as during their storage on the collector and keyword retrieval by authorized auditors. © 2012 Elsevier Ltd.},
   author = {Rafael Accorsi},
   doi = {10.1016/j.mcm.2012.06.035},
   issn = {08957177},
   issue = {7-8},
   journal = {Mathematical and Computer Modelling},
   keywords = {Public key cryptography,Remote auditing,Secure digital archiving,Secure log architecture},
   month = {4},
   pages = {1578-1591},
   title = {A secure log architecture to support remote auditing},
   volume = {57},
   year = {2013},
}
@article{Jalil2021,
   abstract = {Cloud computing is a model that enables users to store their data remotely and enjoy the applications and services provided by this model on demand through a common set of configurable computing resources, without the burden of local data storage as well as maintenance. However, users no longer have physical possession of external data, and this makes the task of protecting data integrity in cloud computing extremely important so that users can use the cloud storage as if it were local, without worrying about the need to verify its integrity. Hence, public cloud storage auditing is critical, so users can use a Third-party auditor (TPA) to verify data integrity. In order to provide effective TPA security, the audit should not achieves any security vulnerabilities to the privacy of user data and does not provide any additional online burden to the user. In this paper, we propose an effective public auditing system for cloud data based on the signature of Boneh-Lynn-Shacham (BLS), in order to ensure public auditing and maintain data privacy. The proposed system also realizes batch auditing and data dynamic process. In addition, the proposed system enhances the level of security authentication through an Automatic Blocker Protocol (ABP) to protect the system from unauthorized TPA. An extensive security and performance analysis shows that the proposed system is very secure and effective},
   author = {Baidaa Abdulrahman Jalil and Taha Mohammed Hasan and Ghassan Sabeeh Mahmood and Hazim Noman Abed},
   doi = {10.1016/j.jksuci.2021.04.001},
   issn = {22131248},
   journal = {Journal of King Saud University - Computer and Information Sciences},
   keywords = {Cloud computing,Privacy preserving,Public auditing,Third-party auditor},
   month = {7},
   publisher = {King Saud bin Abdulaziz University},
   title = {A secure and efficient public auditing system of cloud storage based on BLS signature and automatic blocker protocol},
   year = {2021},
}
@article{Putz2019,
   abstract = {Information systems in organizations are regularly subject to cyber attacks targeting confidential data or threatening the availability of the infrastructure. In case of a successful attack it is crucial to maintain integrity of the evidence for later use in court. Existing solutions to preserve integrity of log records remain cost-intensive or hard to implement in practice. In this work we present a new infrastructure for log integrity preservation which does not depend upon trusted third parties or specialized hardware. The system uses a blockchain to store non-repudiable proofs of existence for all generated log records. An open-source prototype of the resulting log auditing service is developed and deployed, followed by a security and performance evaluation. The infrastructure represents a novel software-based solution to the secure logging problem, which unlike existing approaches does not rely on specialized hardware, trusted third parties or modifications to the logging source.},
   author = {Benedikt Putz and Florian Menges and Günther Pernul},
   doi = {10.1016/j.cose.2019.101602},
   issn = {01674048},
   journal = {Computers and Security},
   keywords = {Digital forensics,Log auditing,Log management,Permissioned blockchain,Secure logging},
   month = {11},
   publisher = {Elsevier Ltd},
   title = {A secure and auditable logging infrastructure based on a permissioned blockchain},
   volume = {87},
   year = {2019},
}
@article{Soderstrom2013,
   abstract = {Log management and analysis is a vital part of organization's network management and system administration. Logs indicate current status of the system and contain information that refers to different security events, which occur within the system. Logs are used for different purposes, such as recording user activities, track authentication attempts, and other security events. Due to increasing number of threats against networks and systems, the number of security logs increases. However, many organizations that work in a distributed environment face following problems: log generation and storage, log protection, and log analysis. Moreover, ensuring that security, system and network administrators analyze log data in an effective way is another issue. In this research, we propose an approach for receiving, storing and administrating audit log events. Furthermore, we present a solution design that in a secure way allows organizations in distributed environments to send audit log transactions from different local networks to one centralized server. © 2013 The Authors.},
   author = {Olof Söderström and Esmiralda Moradian},
   doi = {10.1016/j.procs.2013.09.212},
   issn = {18770509},
   journal = {Procedia Computer Science},
   keywords = {Audit Log Event,Log Analysis,Log Server,Secure Log Management},
   pages = {1249-1258},
   publisher = {Elsevier B.V.},
   title = {Secure audit log management},
   volume = {22},
   year = {2013},
}
@article{Ali2021,
   abstract = {Audit logs are key resources that show the current state of the systems and user activities and are used for cyber forensics and maintenance. These logs are the only source that can help in finding traces of some malicious activities or troubleshooting a system failure. Insight view for trouble-free availability of computing resources and performance monitoring and meaningful forensic audit depends on the management and archival system of audit logs. These logs are prone to multidimensional threats and superusers or system administrators have unprecedented access to these logs and can alter these logs as and when required. Similarly, repudiation is another serious issue in computer forensics and non-repudiation can be provided by a secure recording of event logs. Periodic backups, encrypted data transfer, off-site storage and certificate based storage of these logs are commonly being used. In this survey, we searched for the requirements of securing audit logs and available approaches to secure these logs. Based on the available literature, a taxonomy of audit log management is developed. We have drawn a comparison between these approaches and also highlighted the current challenges to these logs security and their available options.},
   author = {Ahmad Ali and Mansoor Ahmed and Abid Khan},
   doi = {10.48129/KJS.V48I3.10624},
   issn = {23074116},
   issue = {3},
   journal = {Kuwait Journal of Science},
   keywords = {Audit Logs,Audit Logs Security,Log Management,Logs Immutability,Logs Storage},
   month = {7},
   publisher = {University of Kuwait},
   title = {Audit Logs Management and Security - A Survey},
   volume = {48},
   year = {2021},
}
@article{Milicevic2010,
   abstract = {Information security risks threaten the ability of organizations of reaching their operational and strategic goals. Increasing diversification of the information security landscapes makes addressing all risks a challenging task. Information security standards have positioned themselves as generic solutions to tackle a broad range of risks and try to guide security managers in their endeavors. However, it is not evident if such standards have the required holistic approach to be a solid foundation. In this paper a metamodel of the ISO 27001 security standard explicating its core concepts is presented. We then compare the constructed metamodel with various information security ontologies and analyze for comprehensiveness. We conclude with a discussion of core concepts in the information security domain. © 2010 IFIP International Federation for Information Processing.},
   author = {Danijel Milicevic and Matthias Goeken},
   doi = {10.1007/978-3-642-16283-1_13},
   isbn = {3642162827},
   issn = {18684238},
   journal = {IFIP Advances in Information and Communication Technology},
   keywords = {Grounded Theory,ISO 27001,Information Security Management,Metamodeling,Ontologies,Qualitative Data Analysis (QDA)},
   pages = {93-102},
   title = {Ontology-based evaluation of ISO 27001},
   volume = {341 AICT},
   year = {2010},
}
@article{Meriah2019,
   abstract = {Security management standards as ISO/IEC 27000 series provide guidelines, which enable to evaluate the security in the company on a continuous basis. Security ontology technology is the most recommended to make links between security concepts and related standards. This paper presents on a review of ontologies based ISO/IEC 27000 series security standards and provides recommendations for professionals and researchers who need to understand or incorporate one of ISO/IEC 27000 standards features to cover their business security needs. We select and examine in details six main ontologies focusing on the usage of ISO/IEC 27000 series security standards. For each security ontology, we review and then describe it in terms of aim, security concepts and ISO 27000 features. Based on this analysis, we propose a comparison between these ontologies considering several factors to pick out their benefits and limits in order to give a set of recommendations to security decision makers helping them to select an ontology regarding their security requirements.},
   author = {Ines Meriah and Latifa Ben Arfa Rabai},
   doi = {10.1016/j.procs.2019.09.447},
   issn = {18770509},
   journal = {Procedia Computer Science},
   keywords = {Iso\iec 27000 series,Ontology-based security standards,Security decision makers,Security ontology,Security risk management},
   pages = {85-92},
   publisher = {Elsevier B.V.},
   title = {Comparative study of ontologies based iso 27000 series security standards},
   volume = {160},
   year = {2019},
}
@inproceedings{Lopes2019,
   abstract = {Personal Data Protection has been among the most discussed topics lately and a reason for great concern among organizations. The EU General Data Protection Regulation (GDPR)is the most important change in data privacy regulation in 20 years. The regulation will fundamentally reshape the way in which data is handled across every sector. The organizations had two years to implement it. As refe...},
   author = {Isabel Maria Lopes and Teresa Guarda and Pedro Oliveira},
   journal = {2019 14th Iberian Conference on Information Systems and Technologies (CISTI)},
   publisher = {IEEE},
   title = {How ISO 27001 Can Help Achieve GDPR Compliance},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/8760937/},
   year = {2019},
}
@article{Alles2006,
   abstract = {In this paper we report on the approach we have developed and the lessons we have learned in an implementation of the monitoring and control layer for continuous monitoring of business process controls (CMBPC) in the US internal IT audit department of Siemens Corporation. The architecture developed by us implements a completely independent CMBPC system running on top of Siemens' own enterprise information system which has read-only interaction with the application tier of the enterprise system. Among our key conclusions is that "formalizability" of audit procedures and audit judgment is grossly underestimated. Additionally, while cost savings and expedience force the implementation to closely follow the existing and approved internal audit program, a certain level of reengineering of audit processes is inevitable due to the necessity to separate formalizable and non-formalizable parts of the program. Our study identifies the management of audit alarms and the prevention of the alarm floods as critical tasks in the CMBPC implementation process. We develop an approach to solving these problems utilizing the hierarchical structure of alarms and the role-based approach to assigning alarm destinations. We also discuss the content of the audit trail of CMBPC. © 2006 Elsevier Inc. All rights reserved.},
   author = {Michael Alles and Gerard Brennan and Alexander Kogan and Miklos A. Vasarhelyi},
   doi = {10.1016/J.ACCINF.2005.10.004},
   issn = {1467-0895},
   issue = {2},
   journal = {International Journal of Accounting Information Systems},
   keywords = {Automation,Continuous auditing,Continuous monitoring of business processes,Control settings,Controls,Formalization,Monitoring,Reengineering},
   month = {6},
   pages = {137-161},
   publisher = {Pergamon},
   title = {Continuous monitoring of business process controls: A pilot implementation of a continuous auditing system at Siemens},
   volume = {7},
   year = {2006},
}
@article{Caron2012,
   abstract = {The abundance of available event data, originating from process-aware information systems, creates opportunities for enterprise risk management applications at the intersection of the business & management, artificial intelligence and knowledge representation research fields. This paper proposes a rule-based process mining approach for dealing with uncertainty and risk. The applicability of the approach is demonstrated using the updating and debugging process of a social security service provider. © 2012 Springer-Verlag.},
   author = {Filip Caron and Jan Vanthienen and Bart Baesens},
   doi = {10.1007/978-3-642-30864-2_26/COVER},
   isbn = {9783642308635},
   issn = {21945357},
   journal = {Advances in Intelligent Systems and Computing},
   pages = {273-282},
   publisher = {Springer Verlag},
   title = {Rule-based business process mining: Applications for management},
   volume = {171 AISC},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-642-30864-2_26},
   year = {2012},
}
@article{July2009,
   author = {B Stineman - Retrieved July and undefined 2009},
   journal = {mercurymagazines.com},
   title = {Why business rules?: a case for business users of information technology},
   url = {https://www.mercurymagazines.com/pdf/WSW14061USEN.pdf},
   year = {2009},
}
@article{Macdonald2010,
   author = {A Macdonald - . com/software/integration/business-rule … and undefined 2010},
   journal = {websphereusergroup.co.uk},
   title = {The value of IBM WebSphere ILOG BRMS},
   url = {https://www.websphereusergroup.co.uk/wug/presentations/28/28_IntSIG_Value_of_IBM_ILOG_JRules-BRMS.pdf},
   year = {2010},
}
@article{Tang2006,
   author = {P Tang - AMT and undefined 2006},
   journal = {books.google.com},
   title = {Bip: Towards a Highly Configurable Business Intelligence Framework.},
   url = {https://books.google.com.br/books?hl=pt-BR&lr=&id=YAPvAgAAQBAJ&oi=fnd&pg=PA443&dq=+%22business+rule%22+%22audit+trail%22&ots=TJbrqtECRo&sig=tnF_VtX-Rl2sB5upzqa2KslsRes},
}
@article{Group2009,
   author = {B Stineman - IBM Software Group and undefined 2009},
   journal = {mercurymagazines.com},
   title = {IBM WebSphere ILOG Business Rule Management Systems: The Case for Architects and Developer},
   url = {https://www.mercurymagazines.com/pdf/WSW14091USEN.pdf},
   year = {2009},
}
@article{Mernissi2017,
   abstract = {Decision automation is expanding as many corporations capture and operate their business policies through business rules. Because laws and corporate regulations require transparency, decision automation must also provide some explanation capabilities. Most rule engines provide information about the rules that are executed, but rarely give an explanation about why those rules executed without degrading their performance. A need exists for a human readable decision trace that explains why decisions are made. This paper proposes a first approach to introduce causality to describe the existing (and sometimes hidden) relations in a decision trace of a Business Rule-Based System (BRBS). This involves a static analysis of the business rules and the construction of causal models.},
   author = {Karim El Mernissi and Pierre Feillet and Nicolas Maudet and Wassila Ouerdane},
   doi = {10.1007/978-3-319-60042-0_47/COVER},
   isbn = {9783319600413},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Automated decision,Business,Causal model,Intelligent systems,Symbolic decision-making system,rules},
   pages = {433-439},
   publisher = {Springer Verlag},
   title = {Introducing causality in business rule-based decisions},
   volume = {10350 LNCS},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-319-60042-0_47},
   year = {2017},
}
@inproceedings{Muehlen2007,
   abstract = {Process modeling and rule modeling languages are both used to document organizational policies and procedures. However, little work has been done to understand their synergies and overlap. Understanding the relationship between the two modeling types would allow organizations to maximize synergies and reduce their modeling effort. In this paper we use the well-e...},
   author = {Michael zur Muehlen and Marta Indulska and Gerrit Kamp},
   journal = {2007 Eleventh International IEEE EDOC Conference Workshop},
   publisher = {IEEE},
   title = {Business Process and Business Rule Modeling: A Representational Analysis},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/4566972/},
   year = {2007},
}
@article{Adams2020,
   abstract = {Blockchain technology enables various business transactions to be performed in an immutable and transparent manner. Within the business process management community, blockchain technology has been positioned as a way to better support the execution of inter-organisational business processes, where the entities involved may not completely trust each other. However, the architectures proposed thus far in the literature for blockchain-enabled business process management can be described as “heavy-weight”, since they promote the blockchain platform as the monolithic focal point of all business logic and process operations. We propose an alternative: a federated and flexible architecture that leverages the capabilities of blockchain, but without overloading the functionalities of the blockchain platform with those already extant in Business Process Management Systems (BPMSs). We illustrate its benefits, and demonstrate its feasibility, through the implementation of a prototype.},
   author = {Michael Adams and Suriadi Suriadi and Akhil Kumar and Arthur H.M. ter Hofstede},
   doi = {10.1007/978-3-030-58135-0_1/COVER},
   isbn = {9783030581343},
   issn = {18651356},
   journal = {Lecture Notes in Business Information Processing},
   keywords = {Blockchain,Business process automation,Business process management systems,Process flexibility},
   pages = {1-13},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Flexible Integration of Blockchain with Business Process Automation: A Federated Architecture},
   volume = {386 LNBIP},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-58135-0_1},
   year = {2020},
}
@article{Gomez-Lopez2011,
   abstract = {Business processes involve data that can be modified or updated by various activities. These data must satisfy the business rules associated to the process. These data are normally stored in a relational database, and hence the database has to be analyzed to determine whether the business rules can be satisfied. This paper presents a framework including a run-time auditing layer where the correctness of a database can be analyzed at different checkpoints of a business process according to the data flow. It provides an early detection of incorrect action on stored data. Furthermore, in order to manage the current business rules, the use of the constraint programming paradigm is proposed and the enlargement of the Constraint Database Management Systems to support business rules. © 2011 Springer-Verlag.},
   author = {María Teresa Gómez-López and Rafael M. Gasca},
   doi = {10.1007/978-3-642-20511-8_13/COVER},
   isbn = {9783642205101},
   issn = {18651348},
   journal = {Lecture Notes in Business Information Processing},
   keywords = {Constraint Programming,Reasoning related to business processes,business rules},
   pages = {146-157},
   publisher = {Springer Verlag},
   title = {Run-time auditing for business processes data using constraints},
   volume = {66 LNBIP},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-20511-8_13},
   year = {2011},
}
@article{Amna2022,
   author = {Anis R. Amna and Geert Poels},
   doi = {10.1109/ACCESS.2022.3173745},
   journal = {IEEE Access},
   month = {5},
   pages = {51723-51746},
   publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
   title = {Systematic Literature Mapping of User Story Research},
   volume = {10},
   year = {2022},
}
@inproceedings{Gunes2020,
   abstract = {User stories are commonly used to capture user needs in agile methods due to their ease of learning and understanding. Yet, the simple structure of user stories prevents us from capturing relations among them. Such relations help the developers to better understand and structure the backlog items derived from the user stories. One solution to this problem is to build goal models that provide explicit relations among goals but require time and effort to build. This paper presents a pipeline to automatically generate a goal model from a set of user stories by applying natural language processing (NLP) techniques and our initial heuristics to build realistic goal models. We first parse and identify the dependencies in the user stories, and store the results in a graph database to maintain the relations among the roles, actions, and objects mentioned in the set of user stories. By applying NLP techniques and several heuristics, we generate goal models that resemble human-built models. Automatically generating models significantly decreases the time spent on this tedious task. Our research agenda includes calculating the similarity between the automatically generated models and the expert-built models. Our overarching research goals are to provide i. an NLP-powered framework that generates goal models from a set of user stories, ii. several heuristics to generate goal models that resemble human-built models, and iii. a repository that includes sets of user stories, with corresponding human-built and automatically generated goal models.},
   author = {Tugce Gunes and Fatma Başak Basak F.B. B Aydemir and Tuğçe Güneş and Fatma Başak Basak F.B. B Aydemir and Tu˘ Gçe and Güne¸s Güne¸s and Fatma Ba¸sak and Ba¸sak Aydemir and University of Applied Sciences and Arts Northwestern Switzerland; University of Zurich},
   doi = {10.1109/RE48521.2020.00052},
   editor = {T Breaux and A Zisman and S Fricker and M Glinz},
   isbn = {9781728174389},
   issn = {23326441},
   journal = {Proceedings of the IEEE International Conference on Requirements Engineering},
   keywords = {Automatically generated,Generating models,Graph Databases,Learning and understanding,NAtural language processing,Natural language processing systems,Nlp techniques,Requirements engineering,Research agenda,Research goals,Simple structures,agile develop-ment,agile development,goal models,model driven development,natural language processing,requirements engi-neering,requirements engineering,user stories},
   month = {8},
   note = {Conference code: 163887<br/>Cited By :8<br/>Export Date: 21 August 2022<br/>Funding details: RE4DigiTR<br/>Funding details: Türkiye Bilimsel ve Teknolojik Araştirma Kurumu, TÜBITAK, 118C255<br/>Funding text 1: Acknowledgments This work has been partially supported by the Scientific and Technological Research Council of Turkey through BIDEB 2232 grant 118C255, Requirements Engineering for Digital Transformation(RE4DigiTR).<br/>References: Kassab, M., Neill, C., Laplante, P., State of Practice in Requirements Engineering: Contemporary Data (2014) Innovations in Systems and Software Engineering, 10 (4), pp. 235-241; <br/>Kassab, M., The Changing Landscape of Requirements Engineering Practices over the Past Decade (2015) EmpiRE, pp. 1-8; <br/>Lucassen, G., Dalpiaz, F., Werf, J.M.E.M.V.D., Brinkkemper, S., The use and effectiveness of user stories in practice (2016) Springer Int?l Pub, pp. 205-222; <br/>Cohn, M., (2004) User Stories Applied: For Agile Software Development, , Redwood City, CA, USA Addison-Wesley Professional; <br/>Horkoff, J., Aydemir, F.B., Cardoso, E., Li, T., Mate, A., Paja, E., Salnitri, M., Giorgini, P., Goal-oriented requirements engineering: An extended systematic mapping study (2019) Requir Eng, 24 (2), pp. 133-160; <br/>Dalpiaz, F., Franch, X., Horkoff, J., Istar 2.0 language guide (2016) ArXiv Preprint arXiv:1605 07767; <br/>Aydemir, F.B., Dalpiaz, F., Brinkkemper, S., Giorgini, P., Mylopoulos, J., The next release problem revisited: A new avenue for goal models (2018) Re Ieee, pp. 5-16; <br/>Nguyen, C.M., Sebastiani, R., Giorgini, P., Mylopoulos, J., Multiobjective reasoning with constrained goal models (2018) Requir Eng, 23 (2), pp. 189-225; <br/>Deeptimahanti, D.K., Sanyal, R., Static UML Model Generator from Analysis of Requirements (SUGAR) (2008) Advanced Software Engineering and Its Applications (ASEA; <br/>Deeptimahanti, D.K., Babar, M.A., An Automated Tool for Generating UML Models from Natural Language Requirements (2009) Automated Software Engineering, pp. 680-682; <br/>More, P.R., Phalnikar, R., Generating UML Diagrams from Natural Language Specifications (2012) International Journal of Applied Information Systems; <br/>Herchi, H., Abdessalem, W., From User Requirements to Uml Class Diagram (2012) CoRR abs/1211 0713; <br/>Zhou, N., Zhou, X., Automatic Acquisition of Linguistic Patterns for Conceptual Modeling (2004) Course Info 629: Concepts in Artificial Intelligence; <br/>Ibrahim, M., Ahmad, R., Class Diagram Extraction from Textual Requirements Using NLP Techniques (2015) IOSR-JCE, pp. 27-29; <br/>Letsholo, K.J., Zhao, L., Chioasca, E.-V., TRAM: A tool for transforming textual requirements into analysis models (2013) Proceedings of Ase; <br/>Abdessalem, W., Azzouz, Z.B., Singh, A., Dey, N., Ashour, A.S., Ghezala, H.B., Automatic Builder of Class Diagram(ABCD): An Application of UML Generation from Functional Requirements (2016) Journal of Software Practice and Experience; <br/>Robeer, M., Lucassen, G., Van Der Werf, M.E.J.M., Dalpiaz, F., Brinkkemper, S., Automated Extraction of Conceptual Models from User Stories via NLP (2016) RE?16, pp. 196-205; <br/>Lucassen, G., Robeer, M., Dalpiaz, F., Van Der Werf, M.E.J.M., Brinkkemper, S., Extracting conceptual models from user stories with Visual Narrator (2017) Requir Eng, 22 (3), pp. 339-358; <br/>Elallaoui, M., Nafil, K., Touahni, R., Automatic Transformation of User Stories into UML Use Case Diagrams using NLP Techniques (2018) Procedia Computer Science, 130, pp. 42-49; <br/>Elallaoui, M., Nafil, K., Touahni, R., Automatic generation of UML sequence diagrams from user stories in Scrum process (2015) International Conference on Intelligent Systems: Theories and Applications; <br/>Lucassen, G., Dalpiaz, F., Werf, J.M.E.M.V.D., Brinkkemper, S., Visualizing User Story Requirements at Multiple Granularity Levels via Semantic Relatedness (2016) Er; <br/>Arora, C., Sabetzadeh, M., Briand, L., Zimmer, F., Extracting Domain Models from Natural-Language Requirements: Approach and Industrial Evaluation (2016) Proc. ACM/ Ieee 19th MoDELS; <br/>Mesquita, R., Jaqueira, A., Agra, C., Lucena, M., Alencar, F., US2StarTool: Generating i Models from User Stories (2015) International I Workshop (IStar; <br/>Lin, J., Yu, H., Shen, Z., Miao, C., Using goal net to model user stories in agile software development (2014) 15th IEEE/ACIS International Conference on Snpd, , Jun; <br/>Wautelet, Y., Heng, S., Kolp, M., Mirbel, I., Unifying and Extending User Story Models (2014) CAiSE, pp. 211-225. , Jun; <br/>Wautelet, Y., Heng, S., Kolp, M., Mirbel, I., Poelmans, S., Building a rationale diagram for evaluating user story sets (2016) Rcis; <br/>Wautelet, Y., Heng, S., Hintea, D., Kolp, M., Poelmans, S., Bridging user story sets with the use case model (2016) Link S, Trujillo Jc (Eds) Proceedings of Er Workshops; <br/>Wautelet, Y., Heng, S., Kiv, S., Kolp, M., User-story driven development of multi-Agent systems: A process fragment for agile methods (2017) Computer Languages Systems &amp; Structures; <br/>Raymond, J.W., Gardiner, E.J., Willett, P., RASCAL: Calculation of Graph Similarity using Maximum Common Edge Subgraphs (2002) The Computer Journal, 45 (6), pp. 631-644; <br/>Koutra, D., Vogelstein, J.T., Faloutsos, C., DELTACON: A Principled Massive-Graph Similarity Function (2013) International Conference in Data Mining (SDM; <br/>Koutra, D., Vogelstein, J.T., Faloutsos, C., Algorithms for Graph Similarity and Subgraph Matching (2011) Ecological Inference Conference; <br/>Zheng, W., Zou, L., Lian, X., Wang, D., Zhao, D., Efficient Graph Similarity Search over Large Graph Databases (2014) Tkde; <br/>Dalpiaz, F., (2018) Requirements Data Sets (User Stories), , http://dx.doi.org/10.17632/7zbk8zsd8y.1},
   pages = {382-387},
   publisher = {IEEE Computer Society},
   title = {Automated Goal Model Extraction from User Stories Using NLP},
   volume = {2020-Augus},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093929965&doi=10.1109%2FRE48521.2020.00052&partnerID=40&md5=a6789004611d513beb0d519c4d276e48 https://ieeexplore.ieee.org/abstract/document/9218185/},
   year = {2020},
}
@article{Singh2022,
   author = {Harshita Singh and Hourieh Khalajzadeh and Sahba Paktinat and Ulrike M. Graetsch and John Grundy},
   issn = {2590-1184},
   keywords = {CONTEXT,Contextual modelling,FRAMEWORK,Human-centric aspects,Istar extension,Modelling,PEOPLE,PRISE guidelines,Persona,REQUIREMENTS,SOFTWARE,SYSTEM},
   month = {2},
   note = {Times Cited in Web of Science Core Collection: 0<br/>Total Times Cited: 0<br/>Cited Reference Count: 61},
   pages = {101091},
   publisher = {Elsevier},
   title = {Modelling human-centric aspects of end-users with iStar},
   volume = {68},
   year = {2022},
}
@article{,
   abstract = {iStar is a general-purpose goal-based modelling language used to model requirements at early and late phases of software development. It has been used in industrial and academic projects. Often the language is extended to incorporate new constructs related to an application area. The language is currently undergoing standardisation, so several studies have focused on the analysis of iStar variations to identify the similarities and defining a core iStar. However, we believe it will continue to be extended and it is important to understand how iStar is extended. This paper contributes to this purpose through the identification and analysis of the existing extensions and its constructs. A Systematic Literature Review was conducted to guide identification and analysis. The results point to 96 papers and 307 constructs proposed. The extensions and constructs were analysed according to well-defined questions in three dimensions: a general analysis; model-based analysis (to characterise the extensions from semantic and syntactic definitions); and a third dimension related to semiotic clarity. The application area targeted by the iStar extensions and their evolutions are presented as results of our analysis. The results point to the need for more complete, consistent and careful development of iStar extensions. The paper concludes with some discussions and future directions for this research field.},
   author = {Enyo Gonçalves and Jaelson Castro and João Araújo and Tiago Heineck and E Goncalves and Jaelson Castro and J Araujo and Tiago Heineck and Enyo Gonçalves and Jaelson Castro and João Araújo and Tiago Heineck},
   doi = {10.1016/j.jss.2017.11.023},
   issn = {0164-1212},
   journal = {Journal of Systems and Software},
   keywords = {Academic projects,Goal modelling,Language extensions,Model requirements,Model-based analysis,Modeling languages,Modelling language extensions,Semantics,Software design,Systematic Literature Review,Systematic literature review,Three dimensions,iStar},
   month = {3},
   note = {Cited By :30<br/>Export Date: 21 August 2022<br/>CODEN: JSSOD<br/>Correspondence Address: Gonçalves, E.; Universidade Federal do Ceará, Av. José de Freitas Queiroz, 5003, Cedro CEP 63900-000, Brazil; email: enyo@ufc.br<br/>Funding details: UID/CEC/04516/2013<br/>Funding details: Conselho Nacional de Desenvolvimento Científico e Tecnológico, CNPq<br/>Funding details: Erasmus+<br/>Funding text 1: The authors thank to CNPQ / Brazil (Conselho Nacional de Desenvolvimento Científico e Tecnológico) and Erasmus Mundus by the financial support to the execution of this work, Universidade Federal do Ceará, LER-UFPE and NOVA LINCS Research Laboratory (Ref. UID/CEC/04516/2013).<br/>References: Ameller, D., Considering Non-Functional Requirements in Model-Driven Engineering (2009), p. 12. , Master Thesis. Master in Computing Department of Computer Languages and Systems, Universitat Politècnica de Catalunya; Amyot, D., Horkoff, J., Gross, D., Mussbacher, G.A., Lightweight GRL profile for i* modeling (2009) Conceptual Modelling-ER, 5833; <br/>Asadi, M., Bagheri, E., Gasevic, D., Hatala, M., Goal-oriented requirements and feature modeling for software product line engineering (2010) Generative Programming and Component Engineering; <br/>Brambilla, M., Cabot, J., Wimmer, M., Model-Driven Software Engineering in Practice (2012), Morgan &amp; Claypool Publishers series Synthesis Lectures on Software Engineering; Bresciani, P., Fabrizio, S., Applying tropos requirements analysis for defining a tropos tool. Agent-oriented information system (2002) Proceedings of AOIS-2002: Fourth International Bi-Conference Workshop; <br/>Cares, C., Franch, X., Perini, A., (2008) iStarML: an XML-based model interchange format for i*. International i* Workshop; <br/>Cares, C., Franch, X., López, L., Marco, J., (2010) Definition and Uses of the i* Metamodel. International i* Workshop; <br/>Cares, C., Franch, X., A metamodelling approach for i* model translations (2011) Advanced Information Systems Engineering, , Springer Berlin Heidelberg; <br/>Carvallo, J.P., Franch, X., (2014) Lessons Learned in the Use of i* by Non-Technical Users. International i* Workshop; <br/>Dalpiaz, F., Borgida, A., Horkoff, J., Mylopoulos, J., Runtime goal models: keynote (2013) IEEE Seventh International Conference on Research Challenges in Information Science (RCIS), May, pp. 1-11; <br/>Dalpiaz, F., Franch, X., Horkoff, J., iStar 2.0 Language Guide. arXiv:1605.07767, May (2016), http://arxiv.org/pdf/1605.07767v1.pdf, Available in; Danesh, M., Loucopoulos, P., Yu, E., (2015) Dynamic Capabilities for Sustainable Enterprise IT – A Modeling Framework, , Conceptual Modelling-ER; <br/>Dardenne, A., van Lamsweerde, A., Fickas, S., Goal-directed requirements acquisition (1993) Sci. Comput. Program., 20, pp. 3-50; <br/>Dermeval, D., Vilela, J., Bittencourt, I., Castro, J., Isotani, S., Brito, P., Silva, A., Applications of ontologies in requirements engineering: a systematic review of the literature (2015) Requirements Eng. J.; <br/>Ding, W., Liang, P., Tang, A., van Vliet, H., Knowledge-based approaches in software documentation: a systematic literature review (2014) Inf. Softw. Technol., 56 (6), pp. 545-567; <br/>Dyb, T., Dingsyr, T., Empirical studies of agile software development: a systematic review (2008) Inf. Softw. Technol., 50 (9-10), pp. 833-859; <br/>Estrada, H., Alicia, M., Oscar, P., Goal-based business modelling oriented towards late requirements generation (2003) Conceptual Modelling-ER 2003, pp. 277-290. , Springer Berlin Heidelberg; <br/>France, R., Rumpe, B., Model-driven development of complex software: a research roadmap (2007) Future of Software Engineering, , IEEE Computer Society; <br/>Gans, G., Jarke, M., Kethers, S., Lakemeyer, G., Continuous requirements management for organisation networks: a (dis)trust-based approach (2003) Requirements Eng. J.; <br/>García, S., Romero, O., Raventós, R., DSS from an RE perspective: a systematic mapping (2016) J. Syst. Softw.; <br/>Gasparic, M., Janes, A., What recommendation systems for software engineering recommend: a systematic literature review (2016) J. Syst. Softw.; <br/>Giorgini, P., Rizzi, S., Garzetti, M., Goal-oriented Requirement Analysis For Data Warehouse Design (2005), DOLAP; Gomes, R., Guizzardi, R., Franch, X., Guizzardi, G., Wieringa, R., An empirical study to validate the use of ontological guidelines in the creation of i* models (2015) Proceedings of the Brazilian Seminar on Ontologies; <br/>Goodman, N., Languages of Art: An Approach to a Theory of Symbols (1968), Bobbs-Merrill Co. Indianapolis; Hernandes, E.M., Zamboni, A., Fabbri, S., Thommazo AD Using GQM and tam to evaluate start—a tool that supports systematic review (2012) CLEI Electron J., 15 (1), pp. 1-13; <br/>Horkoff, J., Elahi, G., Abdulhadi, S., Yu, E., Reflective analysis of the syntax and semantics of the i* framework (2008) Advances in Conceptual Modelling–Challenges and Opportunities, pp. 249-260. , Springer Berlin Heidelberg; <br/>Horkoff, J., Li, T., Li, F., Salnitri, M., Cardoso, E., Giorgini, P., Mylopoulos, J., Pimentel, J., Taking goal models downstream: a systematic roadmap (2014) IEEE Eighth International Conference on Research Challenges in Information Science (RCIS); <br/>Horkoff, J., Li, T., Li, F., Salnitri, M., Cardoso, P., J., E., Giorgini, P., Mylopoulos, J., Using goal models downstream: a systematic roadmap and literature review (2015) Int. J. Inf. Syst. Modell. Des. (IJISMD), 6 (2), pp. 1-42; <br/>Horkoff, J., Aydemir, F., Cardoso, E., Li, T., Maté, A., Paja, E., Salnitri, M., Giorgini, P., Goal-oriented requirements engineering: a systematic literature map (2016) Requirements Engineering Conference; <br/>Hui, B., Liaskos, S., Mylopoulos J Requirements analysis for customizable software: a goals-skills-preferences framework (2003) Proceedings of the 11th IEEE international requirements engineering conference (RE'03). Monterey Bay, pp. 117-126; <br/>Kelly, S., Tolvanen, J., Domain-Specific Modelling: Enabling Full Code Generation (2008), John Wiley &amp; Sons; Kitchenham, B., Charters, S., Guidelines for performing Systematic Literature Reviews in Software Engineering (2007) Engineering; <br/>Kitchenham, B., Brereton, P., A systematic review of systematic review process research in software engineering (2013) Inf. Softw. Technol., 55 (12), pp. 2049-2075; <br/>Start-state of the art through systematic review tool (2015), http://lapes.dc.ufscar.br/tools/start_tool, Accessed December Accessed December; López, L., Franch, X., Marco, J., Making explicit some implicit i* language decisions (2011) Conceptual Modelling–ER 2011, , Springer Berlin Heidelberg; <br/>Massacci, F., Mylopoulos, J., Zannone, N., Computer-aided support for secure tropos (2007) Autom. Softw. Eng., 14 (3), pp. 341-364; <br/>Mendonça, D., Ali, R., Rodrigues, G., Modelling and analysing contextual failures for dependability requirements (2014) Proceedings of the 9th International Symposium on Software Engineering for Adaptive and Self-Managing Systems, , ACM; <br/>Miles, R., Hamilton, K., Learning UML 2.0 (2006), O'Reilly; Moody, D., The “Physics” of notations: towards a scientific basis for constructing visual notations in software engineering (2009) IEEE Trans. Softw. Eng., 35 (5); <br/>Mussbacher, G., Amyot, D., Breu, R., Bruel, J., Cheng, B., Collet, P., Combemale, B., Whittle, J., The relevance of model-driven engineering thirty years from now (2014) Model-Driven Engineering Languages and Systems, pp. 183-200. , Springer International Publishing; <br/>Mylopoulos, J., Chung, L., Yu, E., From Object-Oriented to Goal-Oriented Requirements Analysis Commun (1999), ACM ACM; Mylopoulos, J., Chung, L., Liao, S., Wang, H., Yu, E., Exploring Alternatives During Requirements Analysis (2001), IEEE Software; OMG, http://www.omg.org/spec/UML/2.5, OMG Unified Modelling Language. Available in; Silva, C., Borba, C., Castro, J., G2SPL: Um Processo de Engenharia de Requisitos Orientada a Objetivos para Linhas de Produtos de Software (2010) Requirements Engineering Workshop (WER); <br/>Teruel, M., Navarro, E., López-Jaquero, V., Montero, F., González, P., CSRML tool: a visual studio extension for modeling CSCW requirements (2013) 6th International i* Workshop; <br/>Van Lamsweerde, A., Systematic Requirements Engineering – From System Goals to UML Models to Software Specifications (2008), Wiley; Wohlin, C., Runeson, P., Höst, M., Ohlsson, M.C., Regnell, B., Wesslén, A., Experimentation in Software Engineering: An Introduction (2000), Kluwer Academic Publishers Norwell; Wohlin, C., Guidelines for snowballing in systematic literature studies and a replication in software engineering (2014) Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering, p. 38. , ACM; <br/>Yu, E., Modelling Strategic Relationships for Process Reengineering (1995), University of Toronto Toronto; Yu, E., Towards modelling and reasoning support for early phase requirements engineering (1997) Proceedings of the 3rd IEEE international Conference on Requirements Engineering; <br/>Yu, Y., Lapouchnian, A., Liaskos, S., Mylopoulos, J., Leite, J., From goals to high-variability software design (2008) Foundations of Intelligent Systems, pp. 1-16. , Springer Berlin Heidelberg; <br/>Yu, E., Social modelling and i* (2009) Conceptual Modelling: Foundations and Applications. LNCS, 5600. , http://www.cs.toronto.edu/pub/eric/JMfest09-EY.pdf, A. Borgida V. Chaudhri P. Giorgini E. Yu Springer Available in; <br/>Yu, E., Giorgini, P., Maiden, N., Mylopoulos, J., (2011) Social Modelling for Requirements Engineering, , The MIT Press; <br/>Aguilar, J.A., Zaldíva, A., Tripp, C., Misra, S., Sánchez, S., Martínez, M., Garcia, O.A., Solution proposal for complex web application modeling with the I-Star framework (2014) 14th International Conference on Computacional Science and Its Applications; <br/>Alencar, F., Moreira, A.M.D., Araújo, J., Castro, J., Silva, C., Mylopoulos, J., Towards an approach to integrate i* with aspects (2006) Eight International Bi-Conference Workshop on Agent Oriented Information System (AOISA06); <br/>Alencar, F., Castro, J., Lucena, M., Santos, E., Silva, C., Araújo, J., Moreira, A., Towards modular i* models (2010) Proceedings of the ACM Symposium on Applied Computing; <br/>Ali, R., Dalpiaz, F., Giorgini, P., Location-based software modelling and analysis: tropos-based approach (2008) International Conference on Conceptual Modelling; <br/>Ali, R., Dalpiaz, F., Giorgini, P., Modelling and analyzing variability for mobile information systems (2008) International Conference on Computational Science and Its Applications; <br/>Ali, R., Dalpiaz, F., Giorgini, P.A., Goal modelling framework for self-contextualizable software (2009) Enterprise, Business-Process and Information Systems Modelling workshop on International Conference on Advanced Information Systems Engineering; <br/>Ali, R., Dalpiaz, F., Giorgini, P., A goal-based framework for contextual requirements modelling and analysis (2010) Requirements Eng. J.; <br/>Ali, R., Dalpiaz, F., Giorgini, P., Requirements-driven deployment (2014) J. Softw. Syst. Modell.; <br/>Amyot, D., Ghanavati, S., Horkoff, J., Mussbacher, G., Peyton, L., Yu, E., Evaluating goal models within the goal-oriented requirement language (2010) Int. J. Intell. Syst.; <br/>Asadi, M., Bagheri, E., Gasevic, D., Hatala, M., Mohabbati, B., Goal-driven software product line engineering (2011) ACM Symposium on Applied Computing; <br/>Asnar, Y., Giorgini, P., Mylopoulos, J., Goal-driven risk assessment in requirements engineering (2011) Requirements Eng. J.; <br/>Basak, F., Giorgini, P., Mylopoulos, J., Multi-objective risk analysis with goal models (2016) International Conference on Research Challenges in Information Science; <br/>Borba, C., Silva, C., A comparison of goal-oriented approaches to model software product lines variability (2009) International Conference on Conceptual Modeling, Lecture Notes in Computer Science; <br/>Bresciani, P., Donzelli, P., REF: a practical agent-based requirement engineering framework (2003) 22nd International Conference on Conceptual Modeling; <br/>Bresciani, P., Perini, A., Giorgini, P., Giunchiglia, F., Mylopoulos, J., Tropos: an agent-oriented software development methodology (2004) Auton. Agents Multi-Agent Syst. J.; <br/>Cai, Z., Yu, E., Addressing performance requirements using a goal and scenario-oriented approach (2002) International Conference on Advanced Information Systems Engineering; <br/>Cares, C., Franch, X., Mayol, E., Using antimodels to define agents' strategy (2007) 7th International Workshop Computational Logic in Multi-Agent Systems, Lecture Notes in Computer Science; <br/>Cares, C., Franch, X., Mayol, E., Extending tropos for a prolog implementation: a case study using the food collecting agent problem (2005) 6th International Workshop on Computacional Logic in Multi-Agent Systems; <br/>Chopra, A., Dalpiaz, F., Giorgini, P., Mylopoulos, J., Modelling and reasoning about service-oriented applications via goals and commitments (2010) International Conference on Advanced Information Systems Engineering; <br/>Chung, V., Considering role-based conflicts of interest in analyzing and designing e-Health systems with goal-oriented methodologies (2006) International Conference on Privacy, Security and Trust: Bridge the Gap Between PST Technologies and Business Services; <br/>Crook, R., Ince, D., Nuseibeh, B., Using i* to model access policies: relating actors to their organisational context (2011) Social Modelling for Requirements Engineering Book; <br/>Dalpiaz, F., Paja, E., Giorgini, P., Security requirements engineering via commitments (2011) 1st Workshop on Socio-Technical Aspects in Security and Trust (STAST); <br/>Danesh, M., Yu, E., Modelling enterprise capabilities with i*: reasoning on alternatives (2014) Workshops of International Conference on Advanced Information Systems Engineering; <br/>De Kinderen, S., Ma, Q., Requirements engineering for the design of conceptual modelling languages (2015) Appl. Ontol.; <br/>Dubois, E., Mayer, N., Rifaut, A., Improving risk-based security analysis with i* (2011) Social Modelling for Requirements Engineering Book; <br/>Duran, M., Pina, A., Mussbacher, G., Evaluation of reusable concern-oriented goal models (2015) Model-Driven Requirements Engineering Workshop (MoDRE) in IEEE International Requirements Engineering Conference; <br/>Elahi, G., Yu, E., Zannone, N., A vulnerability-centric requirements engineering framework: analyzing security attacks, countermeasures, and requirements based on vulnerabilities (2010) Requirements Eng. J.; <br/>Estrada, H., Martinez, A., Santillán, L.C., Pérez, J., A new service-based approach for enterprise modelling (2013) Computacion y Sistemas; <br/>Franch, X., Mate, A., Trujillo, J.C., Cares, C., On the joint use of i* with other modelling frameworks: a vision paper (2011) IEEE International Requirements Engineering Conference; <br/>Gailly, F., España, S., Poels, G., Pastor, O., Integrating business domain ontologies with early requirements modelling (2008) International Conference on Conceptual Modeling, , Springer; <br/>Gans, G., Jarke, M., Kethers, S., Lakemeyer, G., Jarke, M., Ellrich, L., Funken, C., Meister, M., Requirements modelling for organization networks: a (Dis)trust-based approach (2001) IEEE International Conference on Requirements Engineering; <br/>Gans, G., Lakemeyer, G., Jarke, M., Vits, T., SNet: a modelling and simulation environment for agent networks based on i* and ConGolog (2006) International Conference on Advanced Information Systems Engineering; <br/>Ghanavati, S., Amyot, D., Rifaut, A., Legal goal-oriented requirement language (Legal GRL) for modelling regulations (2014) 6th International Workshop on Modelling in Software Engineering, , MiSE; <br/>Gharib, M., Giorgini, P., Modelling and analyzing information integrity in safety critical systems (2013) Conference on Advanced Information Systems Engineering Workshops; <br/>Gharib, M., Giorgini, P., Analyzing trust requirements in socio-technical systems: a belief-based approach (2015) 8th IFIP Working Conference on the practice of Enterprise Modeling; <br/>Gharib, M., Giorgini, P., Modelling and reasoning about information quality requirements (2015) Requirements Engineering: Foundation for Software Quality; <br/>Giachetti, G., Franch, X., Marín, B., Pastor, O., Cares, C., López, L., Using measures to improve i* models for automatic interoperability in model-driven development processes (2011) Actas de las XVI Jornadas de Ingeniería del Software y Bases de Datos: A Coruña, 5-7 de septiembre de 2011; <br/>Giorgini, P., Massacci, F., Mylopoulos, J., Zannone, N., Modeling security requirements through ownership, permission and delegation (2005) IEEE International Requirements Engineering Conference; <br/>Giorgini, P., Massacci, F., Zannone, N., Security and trust requirements engineering (2005) International School on Foundations of Security Analysis and Design III; <br/>Giorgini, P., Rizzi, S., Garzetti, M., GRAnD: A goal-oriented approach to requirement analysis in data warehouses (2008) Decis. Support Syst. J.; <br/>Guimarães, F., Rodrigues, G., Ali, R., Batista, D., Pragmatic requirements for adaptive systems: a goal-driven modeling and analysis approach (2015) International Conference on Conceptual Modeling, , Springer International Publishing; <br/>Guizzardi, R., Perini, A., Dignum, V., socially grounded analysis of knowledge management systems and processes (2011) Social Modelling for Requirements Engineering Book; <br/>Guzman, A., Martinez, A., Agudelo, F., Estrada, H., Perez, J., Ortiz, J., A methodology for modeling Ambient Intelligence applications using i* framework (2016) International iStar Workshop in IEEE International Requirements Engineering Conference; <br/>Hayashi, S., Inoue, W., Kaiya, H., Saeki, M., Annotating goals with concerns in goal-oriented requirements engineering (2015) 11th International Conference on Software Technologies; <br/>Horkoff, J., Yu, E., Finding solutions in goal models: an interactive backward reasoning approach (2010) International Conference on Conceptual Modelling; <br/>Ingolfo, S., Siena, A., Mylopoulos, J., Susi, A., Perini, A., Arguing regulatory compliance of software requirements (2013) Data Knowl. Eng.; <br/>Ingolfo, S., Jureta, I., Siena, A., Perini, A., Susi, A., Nomos 3: Legal compliance of roles and requirements (2014) International Conference on Conceptual Modelling; <br/>Ingolfo, S., Siena, A., Mylopoulos, J., (2014) Goals and Compliance in Nòmos 3. International i* Workshop; <br/>Islam, S., Mouratidis, H., Kalloniatis, C., Hudic, A., Zechner, L., Model based process to support security and privacy requirements engineering (2012) Int. J. Secure Softw. Engineering; <br/>Lapouchnian, A., Yu, Y., Liaskos, S., Mylopoulos, J., Requirements-driven design of autonomic application software (2006) Proceedings of the conference of the Center for Advanced Studies on Collaborative research; <br/>Lapouchnian, A., Lespérance, Y., Modelling mental states in agent-oriented requirements engineering (2006) International Conference on Advanced Information Systems Engineering, Lecture Notes in Computer Science; <br/>Lapouchnian, A., Mylopoulos, J., Modeling domain variability in requirements engineering with contexts (2009) International Conference on Conceptual Modelling; <br/>Lei, Y., Ben, K., He, Z.A., Framework for self-adaptive software based on extended tropos goal model (2015) 7th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC); <br/>Li, F., Horkoff, J., Liu, L., Borgida, A., Guizzardi, G., Mylopoulos, J., Engineering requirements with desiree: an empirical evaluation (2016) International Conference on Advanced Information Systems Engineering, , Springer International Publishing; <br/>Li, T., Horkoff, J., Mylopoulos, J., Integrating security patterns with security requirements analysis using contextual goal models (2014) The Practice of Enterprise Modeling Working Conference; <br/>Liaskos, S., Lapouchnian, A., Yu, Y., Yu, E., Mylopoulos, J., On goal-based variability acquisition and analysis (2006) IEEE International Requirements Engineering Conference; <br/>Liaskos, S., McIlraith, S., Mylopoulos, J., Towards augmenting requirements models with preferences (2009) 24th IEEE/ACM International Conference on Automated Software Engineering (ASE'09); <br/>Liaskos, S., Mylopoulos, J., (2010) On Temporally Annotating Goal Models, International i* Workshop; <br/>Liaskos, S., McIlraith, S.A., Sohrabi, S., Mylopoulos, J., Representing and reasoning about preferences in requirements engineering (2011) Requirements Eng. J.; <br/>Liu, L., Yu, E., Mylopoulos, J., Security and privacy requirements analysis within a social setting (2003) IEEE International Conference on Requirements Engineering; <br/>Lockerbie, J., Maiden, N., Engmann, J., Randall, D., Jones, S., Bush, D., Exploring the impact of software requirements on system-wide goals: a method using satisfaction arguments and i* goal modelling (2012) Requirements Eng. J.; <br/>Louaqad, W., El Mohajir, M., A holonic extension of the i* framework (2014) Third IEEE International Colloquium in Information Science and Technology (CIST); <br/>Marosin, D., Ghanavati, S., Van Der Linden, D., A principle-based goal-oriented requirements language (GRL) for Enterprise Architecture (2014) International iStar Workshop; <br/>Marosin, D., van Zee, M., Ghanavati, S., Formalizing and modeling enterprise architecture (EA) principles with goal-oriented requirements language (GRL) (2016) International Conference on Advanced Information Systems Engineering, , Springer International Publishing; <br/>Mate, A., Trujillo, J., Franch, X., Adding semantic mod},
   pages = {1-33},
   publisher = {Elsevier},
   title = {A Systematic Literature Review of iStar extensions},
   volume = {137},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034790900&doi=10.1016%2Fj.jss.2017.11.023&partnerID=40&md5=c546819964b8d477fa6273faa8c1f04a},
   year = {2018},
}
@inproceedings{Kiv2021,
   abstract = {Agile has become one of the most popular software development approaches thanks to its flexible and evolutive features. To find further suitable practices, teams start to follow the tailoring approach by choosing only the fragments of different methods that fit their needs and context. Many tailoring approaches have been proposed by orienting different aspects such as process, resource and goal. While the interaction between team members is very important in agile methods, none of these approaches focuses on the socio-intentional aspect. In the literature, we can find many case studies that link socio-intentional aspects to the tailoring of agile practices. Even though it is helpful to know it, locating relevant information can be effort and time-consuming. This research proposes a socio-intentional framework that can analyze agile practices and indicate how to tailor them with the help of an evidence-based tool and a modeling language. This framework will allow practitioners to identify the right practices to achieve their goals and analyze their suitability and vulnerability. It will also indicate how to successfully implement them in the software development process.},
   author = {S. Kiv and S. Heng and Y. Wautelet and M. Kolp},
   doi = {10.1109/CBI52690.2021.10065},
   editor = {Guizzardi G Guizzardi G Montali M Proper H A Sales T P Almeida J.P.A. Bork D.},
   isbn = {9781665420693},
   journal = {Proceedings - 2021 IEEE 23rd Conference on Business Informatics, CBI 2021 - Main Papers},
   keywords = {Agile manufacturing systems,Agile methods,Agile practices,Evidence-based System,Evidence-based system,Intentional modeling,Modeling languages,Ontology,Ontology's,Process goals,Process resources,Socio-intentional Modeling,Socio-intentional modeling,Software design,Software development approach,Tailoring Agile,Tailoring agile},
   note = {<b>From Duplicate 1 (<i>Towards a Systematic Socio-Intentional Framework for Agile Methods Tailoring</i> - Kiv, S.; Heng, S.; Wautelet, Y.; Kolp, M.)<br/></b><br/><b>From Duplicate 1 (<i>Towards a Systematic Socio-Intentional Framework for Agile Methods Tailoring</i> - Kiv, S; Heng, S; Wautelet, Y; Kolp, M)<br/></b><br/>cited By 0; Conference of 23rd IEEE Conference on Business Informatics, CBI 2021 ; Conference Date: 1 September 2021 Through 3 September 2021; Conference Code:174534<br/><br/><br/>Conference code: 174534<br/>Export Date: 21 August 2022<br/>References: Ambler, S.W., Lines, M., The disciplined agile process decision framework (2016) International Conference on Software Quality, pp. 3-14. , Springer; <br/>Booch, G., Rumbaugh, J., Jacobson, I., (2005) Unified Modeling Language User Guide, , The 2nd Edition; <br/>Campanelli, A.S., Parreiras, F.S., Agile methods tailoring - A systematic literature review (2015) Journal of Systems and Software, 110, pp. 85-100; <br/>Chen, Y.-J., Development of a method for ontology-based empirical knowledge representation and reasoning (2010) Decision Support Systems, 50 (1), pp. 1-20; <br/>Chung, L., The NFR framework in action (2000) Non-Functional Requirements in Software Engineering, pp. 15-45. , Springer; <br/>Dalpiaz, F., Franch, X., Horkoff, J., (2016) Istar 2.0 Language Guide, , arXiv preprint; <br/>Damiani, E., A metamodel for modeling and measuring scrum development process (2007) Agile Processes in Software Engineering and Extreme Programming, XP 2007, Proceedings, pp. 74-83; <br/>Eckstein, J., (2013) Agile Software Development in The Large: Diving into The Deep, , Addison-Wesley; <br/>Esfahani, H.C., Yu, E., A repository of agile method fragments (2010) International Conference on Software Process, pp. 163-174. , Springer; <br/>Esfahani, H.C., Yu, E., Cabot, J., Situational evaluation of method fragments: An evidence-based goal-oriented approach (2010) International Conference on Advanced Information Systems Engineering, pp. 424-438. , Springer; <br/>Henderson-Sellers, B., Gonzalez-Perez, C., A comparison of four process metamodels and the creation of a new generic standard (2005) Information and Software Technology, 47 (1), pp. 49-65; <br/>Kiv, S., Agile manifesto and practices selection for tailoring software development: A systematic literature review (2018) International Conference on Product-Focused Software Process Improvement, pp. 12-30. , Springer; <br/>Kiv, S., Agile methods knowledge representation for systematic practices adoption (2019) International Conference on Agile Software Development, pp. 19-34. , Springer; <br/>Kiv, S., Towards a goal-oriented framework for partial agile adoption (2017) International Conference on Software Technologies, pp. 69-90. , Springer; <br/>Mikulenas, G., Butleris, R., Nemuraite, L., An approach for the metamodel of the framework for a partial agile method adaptation (2011) Information Technology And Control, 40 (1), pp. 71-82; <br/>Noy, N.F., McGuinness, D.L., (2001) Ontology Development 101: A Guide to Creating Your First Ontology, , https://protege.stanford.edu/publications/ontology\_development/ontology101.pdf; <br/>Noy, N.F., Hafner, C.D., The state of the art in ontology design: A survey and comparative review (1997) AI Magazine, 18 (3), p. 53; <br/>Potoniec, J., Dataset of ontology competency questions to SPARQL-oWL queries translations (2020) Data in Brief, p. 105098; <br/>Rao, L., Mansingh, G., Osei-Bryson, K.-M., Building ontology based knowledge maps to assist business process re-engineering (2012) Decision Support Systems, 52 (3), pp. 577-589; <br/>Schuppenies, R., Steinhauer, S., Software process engineering metamodel (2002) OMG Group, , November; <br/>Uschold, M., Gruninger, M., Ontologies: Principles, methods and applications (1996) The Knowledge Engineering Review, 11 (2), pp. 93-136; <br/>van Lamsweerde, A., (2009) Requirements Engineering: From System Goals to UML Models to Software, 10. , Chichester, UK: John Wiley &amp; Sons; <br/>Wautelet, Y., Unifying and extending user story models (2014) Int. Conf. on Advanced Information Systems Engineering., pp. 211-225. , Springer; <br/>Wautelet, Y., (2008) A Goal-Driven Project Management Framework for Multi-Agent Software Development: The Case of I-Tropos, , https://dial.uclouvain.be/pr/boreal/object/boreal:19678, PhD thesis. Université catholique de Louvain, Belgium; <br/>Yu, E., (2011) Social Modeling for Requirements Engineering, , MIT Press},
   pages = {143-152},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Towards a Systematic Socio-Intentional Framework for Agile Methods Tailoring},
   volume = {2},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123638875&doi=10.1109%2FCBI52690.2021.10065&partnerID=40&md5=461bd71943abd28ec47285271e6ad04b},
   year = {2021},
}
@book{Bennaceur2019,
   abstract = {Requirements engineering (RE) aims to ensure that systems meet the needs of their stakeholders including users, sponsors, and customers. Often considered as one of the earliest activities in software engineering, it has developed into a set of activities that touch almost every step of the software development process. In this chapter, we reflect on how the need for RE was first recognised and how its foundational concepts were developed. We present the seminal papers on four main activities of the RE process, namely, (1) elicitation, (2) modelling and analysis, (3) assurance, and (4) management and evolution. We also discuss some current research challenges in the area, including security requirements engineering as well as RE for mobile and ubiquitous computing. Finally, we identify some open challenges and research gaps that require further exploration.},
   author = {A. Bennaceur and T.T. T Tun and Y. Yu and B. Nuseibeh},
   doi = {10.1007/978-3-030-00262-6_2},
   isbn = {9783030002626},
   journal = {Handbook of Software Engineering},
   note = {Cited By :6<br/>Export Date: 21 August 2022<br/>Correspondence Address: Bennaceur, A.; The Open UniversityUnited Kingdom; email: Amel.Bennaceur@open.ac.uk<br/>References: Abowd, G.D., Beyond weiser: From ubiquitous to collective computing (2016) IEEE Comput, 49 (1), pp. 17-23. , http://dx.doi.org/10.1109/MC.2016.22; <br/>Alexander, I., Gore, sore, or what? (2011) IEEE Softw, 28 (1), pp. 8-10. , http://dx.doi.org/10.1109/MS.2011.7; <br/>Alexander, I.F., Maiden, N., (2005) Scenarios, Stories, Use Cases: Through the Systems Development Life-Cycle, , Wiley, New York; <br/>Alexander, I.F., Stevens, R., (2002) Writing Better Requirements, , Pearson Education, Harlow; <br/>Altran: Reveal Tm, , http://intelligent-systems.altran.com/fr/technologies/systems-engineering/revealtm.html; <br/>Alves, C.F., Finkelstein, A., Challenges in COTS decision-making: A goal-driven requirements engineering perspective (2002) Proceedings of the 14th International Conference On Software Engineering and Knowledge Engineering, SEKE 2002, Ischia, 15-19 July 2002, pp. 789-794. , http://doi.acm.org/10.1145/568760.568894; <br/>Avizienis, A., Laprie, J.C., Randell, B., Landwehr, C., Basic concepts and taxonomy of dependable and secure computing (2004) IEEE Trans. Dependable Secur. Comput, 1 (1), pp. 11-33. , https://doi.org/10.1109/TDSC.2004.2; <br/>Becker, C., Betz, S., Chitchyan, R., Duboc, L., Easterbrook, S.M., Penzenstadler, B., Seyff, N., Venters, C.C., Requirements: The key to sustainability (2016) IEEE Softw, 33 (1), pp. 56-65. , http://dx.doi.org/10.1109/MS.2015.158; <br/>Bencomo, N., Cleland-Huang, J., Guo, J., Harrison, R., (2014) IEEE 1st International Workshop On Artificial Intelligence For Requirements Engineering, AIRE 2014, , http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=6887463, 26 Aug 2014, Karlskrona. IEEE Computer Society, Washington; <br/>Bennaceur, A., Nuseibeh, B., The many facets of mediation: A requirements-driven approach for trading-off mediation solutions (2016) Managing Trade-offs In Adaptable Software Architectures, , http://oro.open.ac.uk/45253/, In: Mistrík, I., Ali, N., Grundy, J., Kazman, R., Schmerl, B. (eds.), Elsevier, New York; <br/>Berander, P., Andrews, A., Requirements prioritization (2005) Engineering and Managing Software Requirements, pp. 69-94. , In: Aurum, A., Wohlin, C. (eds.), Springer, Berlin; <br/>Besnard, P., Hunter, A., (2008) Elements of Argumentation, , The MIT Press, Cambridge; <br/>Boehm, B.W., Verifying and validating software requirements and design specifications (1984) IEEE Softw, 1 (1), pp. 75-88. , http://dx.doi.org/10.1109/MS.1984.233702; <br/>Boehm, B.W., A spiral model of software development and enhancement (1988) IEEE Comput, 21 (5), pp. 61-72. , http://dx.doi.org/10.1109/2.59; <br/>Boehm, B.W., Grünbacher, P., Briggs, R.O., Developing groupware for requirements negotiation: Lessons learned (2001) IEEE Softw, 18 (3), pp. 46-55. , http://dx.doi.org/10.1109/52.922725; <br/>Bosch, J., Speed, data, and ecosystems: The future of software engineering (2016) IEEE Softw, 33 (1), pp. 82-88. , http://dx.doi.org/10.1109/MS.2016.14; <br/>Bradner, S., (1997) Key Words For Use In RFCs to Indicate Requirement Levels, , http://www.ietf.org/rfc/rfc2119.txt; <br/>Breaux, T., Antón, A., Analyzing regulatory rules for privacy and security requirements (2008) IEEE Trans. Softw. Eng, 34 (1), pp. 5-20. , https://doi.org/10.1109/TSE.2007.70746; <br/>Brooks, F.P., Jr., No silver bullet essence and accidents of software engineering (1987) Computer, 20 (4), pp. 10-19. , http://dx.doi.org/10.1109/MC.1987.1663532; <br/>Cheng, B.H.C., Atlee, J.M., Research directions in requirements engineering (2007) Proceedings of the Workshop On the Future of Software Engineering, FOSE, pp. 285-303; <br/>Cheng, B.H.C., de Lemos, R., Giese, H., Inverardi, P., Magee, J., Andersson, J., Becker, B., Whittle, J., Software engineering for self-adaptive systems: A research roadmap (2009) Software Engineering For Self-Adaptive Systems [Outcome of a Dagstuhl Seminar], pp. 1-26; <br/>Clarke, E.M., Wing, J.M., Formal methods: State of the art and future directions (1996) ACM Comput. Surv, 28 (4), pp. 626-643; <br/>Cleland-Huang, J., Gotel, O., Zisman, A., (2012) Software and Systems Traceability, , http://dx.doi.org/10.1007/978-1-4471-2239-5, Springer, London; <br/>Cleland-Huang, J., Gotel, O., Hayes, J.H., Mäder, P., Zisman, A., Software traceability: Trends and future directions (2014) Proceedings of the Future of Software Engineering, FOSE@ICSE, pp. 55-69. , http://doi.acm.org/10.1145/2593882.2593891; <br/>Crook, R., Ince, D.C., Lin, L., Nuseibeh, B., Security requirements engineering: When anti-requirements hit the fan (2002) 10th Anniversary IEEE Joint International Conference On Requirements Engineering (RE 2002), Essen, 9-13 Sept 2002, pp. 203-205. , IEEE Computer Society, Washington; <br/>Dalpiaz, F., Franch, X., Horkoff, J., (2016) Istar 2.0 Language Guide, , http://arxiv.org/abs/1605.07767, CoRR abs/1605.07767; <br/>Davis, A.M., Tubío, O.D., Hickey, A.M., Juzgado, N.J., Moreno, A.M., Effectiveness of requirements elicitation techniques: Empirical results derived from a systematic review (2006) Proceedings of the 14th IEEE International Conference On Requirements Engineering, RE, pp. 176-185. , http://dx.doi.org/10.1109/RE.2006.17; <br/>Denning, P.J., Software quality (2016) Commun. ACM, 59 (9), pp. 23-25. , http://doi.acm.org/10.1145/2971327; <br/>Dey, A.K., Abowd, G.D., Salber, D., A conceptual framework and a toolkit for supporting the rapid prototyping of context-aware applications (2001) Hum.-Comput. Interact, 16 (2), pp. 97-166; <br/>Dieste, O., Juzgado, N.J., Systematic review and aggregation of empirical studies on elicitation techniques (2011) IEEE Trans. Softw. Eng, 37 (2), pp. 283-304. , http://dx.doi.org/10.1109/TSE.2010.33; <br/>Dung, P.M., On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games (1995) Artif. Intell, 77 (2), pp. 321-357. , http://dx.doi.org/10.1016/0004-3702(94)00041-X;http://www.sciencedirect.com/science/article/pii/000437029400041X; <br/>Easterbrook, S., (2004) What is Requirements Engineering?, , http://www.cs.toronto.edu/~sme/papers/2004/FoRE-chapter01-v7.pdf; <br/>Ebert, C., Duarte, C.H.C., Requirements engineering for the digital transformation: Industry panel (2016) 2016 IEEE 24th International Requirements Engineering Conference (RE), pp. 4-5. , https://doi.org/10.1109/RE.2016.21; <br/>Epifani, I., Ghezzi, C., Mirandola, R., Tamburrelli, G., Model evolution by run-time parameter adaptation (2009) Proceedings of the 31st International Conference On Software Engineering, ICSE '09, pp. 111-121. , http://dx.doi.org/10.1109/ICSE.2009.5070513, IEEE Computer Society, Washington; <br/>Fickas, S., Feather, M.S., Requirements monitoring in dynamic environments (1995) Proceedings of the Second IEEE International Symposium On Requirements Engineering, 1995, pp. 140-147. , https://doi.org/10.1109/ISRE.1995.512555; <br/>Filieri, A., Maggio, M., Angelopoulos, K., D'ippolito, N., Gerostathopoulos, I., Hempel, A.B., Hoffmann, H., Vogel, T., Software engineering meets control theory (2015) 2015 IEEE/ACM 10th International Symposium On Software Engineering For Adaptive and Self-managing Systems, pp. 71-82. , https://doi.org/10.1109/SEAMS.2015.12; <br/>Furtado, F., Zisman, A., Trace++: A traceability approach for agile software engineering (2016) Proceedings of the 24th International Requirements Engineering Conference, RE; <br/>Glinz, M., On non-functional requirements (2007) Proceedings of the 15th IEEE International Requirements Engineering Conference, RE, pp. 21-26. , https://doi.org/10.1109/RE.2007.45; <br/>Guenther Ruhe, M.N., Ebert, C., The vision: Requirements engineering in society (2017) Proceedings of the 25th International Requirements Engineering Conference-Silver Jubilee Track, RE, , https://www.ucalgary.ca/mnayebi/files/mnayebi/the-vision-requirementsengineering-in-society.pdf; <br/>Haley, C.B., Laney, R.C., Moffett, J.D., Nuseibeh, B., Security requirements engineering: A framework for representation and analysis (2008) IEEE Trans. Softw. Eng, 34 (1), pp. 133-153. , http://doi.ieeecomputersociety.org/10.1109/TSE.2007.70754; <br/>Heitmeyer, C.L., Labaw, B., Jeffords, R., A benchmark for comparing different approaches for specifying and verifying real-time systems (1993) Proceedings of the 10th International Workshop On Real-Time Operating Systems and Software; <br/>Herbsleb, J.D., Building a socio-technical theory of coordination: Why and how (outstanding research award) (2016) Proceedings of the 24th ACM SIGSOFT International Symposium On Foundations of Software Engineering, FSE 2016, Seattle, 13-18 Nov 2016, pp. 2-10. , http://doi.acm.org/10.1145/2950290.2994160; <br/>Hosseini, M., Shahri, A., Phalp, K., Ali, R., Four reference models for transparency requirements in information systems (2017) Requir. Eng, 23, pp. 1-25; <br/>(1998) Software Engineering Standards Committee and IEEE-SA Standards Board: IEEE Recommended Practice For Software Requirements Specifications, , IEEE Computer Society., Technical report, IEEE; <br/>(2001) Software Engineering-product Quality-part 1: Quality Model. Technical Report, ISO, , ISO/IEC 9126; <br/>(2016) Systems and Software Engineering-systems and Software Quality Requirements and Evaluation (square)-measurement of Quality In Use, , ISO/IEC 25022, Technical report, ISO; <br/>Jackson, M., (1995) Software Requirements &amp; Specifications: A Lexicon of Practice, Principles and Prejudices, , ACM Press/Addison-Wesley, New York; <br/>Jackson, M., (2001) Problem Frames: Analyzing and Structuring Software Development Problems, , Addison-Wesley Longman, Boston; <br/>Jackson, M., The name and nature of software engineering (2007) Advances In Software Engineering: Lipari Summer School 2007. Revised Tutorial Lectures In Advances In Software Engineering, pp. 1-38. , http://dx.doi.org/10.1007/978-3-540-89762-0_1, Springer, Berlin; <br/>Jackson, M., Zave, P., Deriving specifications from requirements: An example (1995) Proceedings of the 17th International Conference On Software Engineering, ICSE '95, pp. 15-24. , http://doi.acm.org/10.1145/225014.225016, ACM, New York; <br/>Jureta, I., Borgida, A., Ernst, N.A., Mylopoulos, J., The requirements problem for adaptive systems (2014) ACM Trans. Manag. Inf. Syst, 5 (3), p. 17. , http://doi.acm.org/10.1145/2629376; <br/>Kang, E., Jackson, D., Dependability arguments with trusted bases (2010) Proceedings of the 18th IEEE International Requirements Engineering Conference, RE '10, pp. 262-271. , http://dx.doi.org/10.1109/RE.2010.38, IEEE Computer Society, Washington; <br/>Karlsson, J., Ryan, K., A cost-value approach for prioritizing requirements (1997) IEEE Softw, 14 (5), pp. 67-74. , http://dx.doi.org/10.1109/52.605933; <br/>Kelly, T., Weaver, R., The goal structuring notation-a safety argument notation (2004) Proceedings of the Dependable Systems and Networks 2004 Workshop On Assurance Cases; <br/>Kramer, J., Magee, J., Self-managed systems: An architectural challenge (2007) Proceedings of the Future of Software Engineering Track, FOSE@ICSE, pp. 259-268. , http://dx.doi.org/10.1109/FOSE.2007.19; <br/>Laplante, P.A., (2013) Requirements Engineering For Software and Systems, , CRC Press, Boca Raton; <br/>Larson, E., (2003) Interoperability of Us and Nato Allied Air Forces: Supporting Data and Case Studies, , Technical report 1603, RAND Corporation; <br/>Letier, E., Heaven, W., Requirements modelling by synthesis of deontic input-output automata (2013) 35th International Conference On Software Engineering, ICSE '13, San Francisco, 18-26 May 2013, pp. 592-601. , http://dl.acm.org/citation.cfm?id=2486866; <br/>Leveson, N.G., Heimdahl, M.P.E., Hildreth, H., Reese, J.D., Requirements specification for process-control systems (1994) IEEE Trans. Softw. Eng, 20 (9), pp. 684-707. , https://doi.org/10.1109/32.317428; <br/>Lewis, G.A., Morris, E., Place, P., Simanta, S., Smith, D.B., Requirements engineering for systems of systems (2009) 2009 3rd Annual IEEE Systems Conference, pp. 247-252. , https://doi.org/10.1109/SYSTEMS.2009.4815806; <br/>Limoncelli, T.A., Automation should be like iron man, not ultron (2015) ACM Queue, 13 (8), p. 50. , http://doi.acm.org/10.1145/2838344.2841313; <br/>Lin, L., Nuseibeh, B., Ince, D.C., Jackson, M., Using abuse frames to bound the scope of security problems (2004) 12th IEEE International Conference On Requirements Engineering (RE 2004), Kyoto, 6-10 Sept 2004, pp. 354-355; <br/>Liu, L., Yu, E.S.K., Mylopoulos, J., Security and privacy requirements analysis within a social setting (2003) 11th IEEE International Conference On Requirements Engineering (RE 2003), 8-12 Sept 2003, Monterey Bay, pp. 151-161; <br/>Lutz, R.R., Analyzing software requirements errors in safety-critical, embedded systems (1993) Proceedings of IEEE International Symposium On Requirements Engineering, RE, pp. 126-133. , https://doi.org/10.1109/ISRE.1993.324825; <br/>Lutz, R.R., Software engineering for space exploration (2011) IEEE Comput, 44 (10), pp. 41-46. , https://doi.org/10.1109/MC.2011.264; <br/>Maiden, N.A.M., So, what is requirements work? (2013) IEEE Softw, 30 (2), pp. 14-15. , http://dx.doi.org/10.1109/MS.2013.35; <br/>Maiden, N.A.M., Rugg, G., ACRE: Selecting methods for requirements acquisition (1996) Softw. Eng. J, 11 (3), pp. 183-192. , http://dx.doi.org/10.1049/sej.1996.0024; <br/>Maiden, N.A.M., Gizikis, A., Robertson, S., Provoking creativity: Imagine what your requirements could be like (2004) IEEE Softw, 21 (5), pp. 68-75. , http://dx.doi.org/10.1109/MS.2004.1331305; <br/>Maiden, N.A.M., Robertson, S., Robertson, J., Creative requirements: Invention and its role in requirements engineering (2006) Proceedings of the 28th International Conference On Software Engineering, ICSE, pp. 1073-1074. , http://doi.acm.org/10.1145/1134512; <br/>Mancini, C., Rogers, Y., Bandara, A.K., Coe, T., Jedrzejczyk, L., Joinson, A.N., Price, B.A., Nuseibeh, B., Contravision: Exploring users' reactions to futuristic technology (2010) Proceedings of the 28th International Conference On Human Factors In Computing Systems, CHI 2010, Atlanta, 10-15 April 2010, pp. 153-162; <br/>Manna, Z., Pnueli, A., (1992) The Temporal Logic of Reactive and Concurrent Systems-Specification, , Springer, Berlin; <br/>Marca, D., Harandi: Problem set for the fourth international workshop on software specification and design (1987) Proceedings of the 4th International Workshop On Software Specification and Design; <br/>Martins, L.E.G., Gorschek, T., Requirements engineering for safety-critical systems: Overview and challenges (2017) IEEE Softw, 34 (4), pp. 49-57. , https://doi.org/10.1109/MS.2017.94; <br/>Maruyama, H., Machine learning as a programming paradigm and its implications to requirements engineering (2016) Asia-Pacific Requirements Engineering Symposium, APRES; <br/>Mavin, A., Wilkinson, P., Harwood, A., Novak, M., Easy approach to requirements syntax (EARS) (2009) Proceedings of the 17th IEEE International Requirements Engineering Conference, RE, pp. 317-322. , http://dx.doi.org/10.1109/RE.2009.9; <br/>Mirakhorli, M., Cleland-Huang, J., Tracing non-functional requirements (2012) Software and Systems Traceability, pp. 299-320. , http://dx.doi.org/10.1007/978-1-4471-2239-5_14, Springer, Berlin; <br/>Moon, M., Yeom, K., Chae, H.S., An approach to developing domain requirements as a core asset based on commonality and variability analysis in a product line (2005) IEEE Trans. Softw. Eng, 31 (7), pp. 551-569. , http://dx.doi.org/10.1109/TSE.2005.76; <br/>Nhlabatsi, A., Nuseibeh, B., Yu, Y., Security requirements engineering for evolving software systems: A survey (2010) Int. J. Secur. Softw. Eng, 1 (1), pp. 54-73; <br/>Nitto, E.D., Ghezzi, C., Metzger, A., Papazoglou, M.P., Pohl, K., A journey to highly dynamic, self-adaptive service-based applications (2008) Autom. Softw. Eng, 15 (3-4), pp. 313-341. , http://dx.doi.org/10.1007/s10515-008-0032-x; <br/>Niu, N., Easterbrook, S.M., So, you think you know others' goals? (2007) A Repertory Grid Study. IEEE Softw, 24 (2), pp. 53-61. , http://dx.doi.org/10.1109/MS.2007.52; <br/>Nuseibeh, B., Weaving together requirements and architectures (2001) IEEE Comput, 34 (3), pp. 115-117. , http://dx.doi.org/10.1109/2.910904; <br/>Nuseibeh, B., Easterbrook, S.M., Requirements engineering: A roadmap (2000) Proceedings of the Future of Software Engineering Track At the 22nd International Conference On Software Engineering, Future of Software Engineering Track, FOSE, pp. 35-46. , http://doi.acm.org/10.1145/336512.336523; <br/>Omoronyia, I., Cavallaro, L., Salehie, M., Pasquale, L., Nuseibeh, B., Engineering adaptive privacy: On the role of privacy awareness requirements (2013) 35th International Conference On Software Engineering, ICSE '13, San Francisco, 18-26 May 2013, pp. 632-641. , http://dx.doi.org/10.1109/ICSE.2013.6606609; <br/>Paolucci, M., Kawamura, T., Payne, T.R., Sycara, K.P., Semantic matching of web services capabilities (2002) Proceedings of the International Semantic Web Conference, ISWC, pp. 333-347; <br/>Parnas, D.L., Madey, J., Functional documents for computer systems (1995) Sci. Comput. Program, 25 (1), pp. 41-61. , http://dx.doi.org/10.1016/0167-6423(95)96871-J; <br/>Pohl, K., Assenova, P., Dömges, R., Johannesson, P., Maiden, N., Plihon, V., Schmitt, J.R., Spanoudakis, G., Applying ai techniques to requirements engineering: The nature prototype (1994) Proceedings of the ICSE-Workshop On Research Issues In the Intersection Between Software Engineering and Artificial Intelligence; <br/>Ramesh, B., Cao, L., Baskerville, R., Agile requirements engineering practices and challenges: An empirical study (2010) Inf. Syst. J, 20 (5), pp. 449-480. , http://dx.doi.org/10.1111/j.1365-2575.2007.00259.x; <br/>Riegel, N., Dörr, J., A systematic literature review of requirements prioritization criteria (2015) Proceedings of the 21st International Working Conference On Requirements Engineering: Foundation For Software Quality, REFSQ, pp. 300-317. , http://dx.doi.org/10.1007/978-3-319-16101-3_22; <br/>Robertson, S., Robertson, J., (1999) Mastering the Requirements Process, , ACM Press/Addison-Wesley, New York; <br/>Robertson, S., Robertson, J., (2012) Mastering the Requirements Process: Getting Requirements Right, , Addison-Wesley, Boston; <br/>Rumbaugh, J., Jacobson, I., Booch, G., (2004) The Unified Modeling Language Reference Manual, , Pearson Higher Education, London; <br/>Rushby, J., Security requirements specifications: How and what? (2001) Requirements Engineering For Information Security (SREIS), Indianapolis; <br/>Russo, D., Ciancarini, P., A proposal for an antifragile software manifesto (2016) Proc. Comput. Sci, 83, pp. 982-987; <br/>Salifu, M., Yu, Y., Nuseibeh, B., Specifying monitoring and switching problems in context (2007) 15th IEEE International Requirements Engineering Conference (RE 2007), , http://oro.open.ac.uk/10264/; <br/>Sawyer, P., Bencomo, N., Whittle, J., Letier, E., Finkelstein, A., Requirements-aware systems: A research agenda for re for self-adaptive systems (2010) 2010 18th IEEE International Requirements Engineering Conference, pp. 95-103. , https://doi.org/10.1109/RE.2010.21; <br/>(2012) Deliverable D22.1: Interoperability Concept, , http://www.secur-ed.eu/?page_id=33, SECUR-ED, C., fp7 SECUR-ED EU project; <br/>Silva Souza, V.E., Lapouchnian, A., Robinson, W.N., Mylopoulos, J., Awareness requirements for adaptive systems (2011) Proceedings of the 6th International Symposium On Software Engineering For Adaptive and Self-Managing Systems, SEAMS '11, pp. 60-69. , http://doi.acm.org/10.1145/1988008.1988018, ACM, New York; <br/>Sommerville, I., Sawyer, P., (1997) Requirements Engineering: A Good Practice Guide, 1st Edn, , Wiley, New York; <br/>Tun, T.T., Boucher, Q., Classen, A., Hubaux, A., Heymans, P., Relating requirements and feature configurations: A systematic approach (2009) Proceedings of the 13th International Conference On Software Product Lines, SPLC, pp. 201-210. , http://doi.acm.org/10.1145/1753235.1753263; <br/>van Lamsweerde, A., Elaborating security requirements by construction of intentional anti-models (2004) 26th International Conference On Software Engineering (ICSE 2004), Edinburgh, 23-28 May 2004, pp. 148-157. , In: Finkelstein, A., Estublier, J., Rosenblum, D.S. (eds.), IEEE Computer Society, Washington; <br/>van Lamsweerde, A., Requirements engineering: From craft to discipline (2008) Proceedings of the 16th ACM SIGSOFT International Symposium On Foundations of Software Engineering, 2008, Atlanta, 9-14 Nov 2008, pp. 238-249. , http://doi.acm.org/10.1145/1453101.1453133; <br/>van Lamsweerde, A., (2009) Requirements Engineering: From System Goals to UML Models to Software Specifications, , Wiley, Hoboken; <br/>van Lamsweerde, A., Darimont, R., Massonet, P., Goal-directed elaboration of requirements for a meeting scheduler: Problems and lessons learnt (1995) Proceedings of the 2nd IEEE International Symposium On Requirements Engineering, RE, pp. 194-203. , http://dx.doi.org/10.1109/ISRE.1995.512561; <br/>Viana, T., Bandara, A., Zisman, A., Towards a framework for managing inconsistencies in systems of systems (2016) Proceedings of the Colloquium On Software-intensive Systems-of-Systems At 10th European Conference On Software Architecture, , http://oro.open.ac.uk/48014/; <br/>Wakefield, J., (2017) Microsoft Chatbot is Taught to Swear On Twitter, , Visited on 30 Mar; <br/>Whittle, J., Sawyer, P., Bencomo, N., Cheng, B.H.C., Bruel, J.M., Relax: A language to address uncertainty in self-adaptive systems requirement (2010) Requir. Eng, 15 (2), pp. 177-196. , http://dx.doi.org/10.1007/s00766-010-0101-0; <br/>W},
   pages = {51-92},
   publisher = {Springer International Publishing},
   title = {Requirements engineering},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079686268&doi=10.1007%2F978-3-030-00262-6_2&partnerID=40&md5=4f87edcadf74e30c94838581dd3a06f6},
   year = {2019},
}
@inproceedings{Yang2021,
   abstract = {Goal-oriented modeling is an effective way for modeling and analyzing the requirements of users. It takes stakeholder s intentions as the main clue and analyzes their goals and tasks to construct hierarchical requirements model. Unified Modeling Language (UML) is a de facto standard for system requirements modeling and design. In practice, it is very desirable to have an approach to automatically transform user requirements into system requirements, then automatically generate system prototypes for requirements validation. In ICSE 19 and RE 19, we propose an approach and CASE tool RM2PT, which can automatically generate prototypes from system requirements in UML. In this paper, we focus on filling the gap between user and system requirements. Specifically, we propose an approach Goal2UCM to automatically generate the use case diagram, the system operations and interfaces of use cases from the goal-oriented model iStar based on the model-driven approach. We evaluate the proposed approach with the case study of CoCoME system. Overall, the result is satisfactory. The 93.6% of the iStar model elements can be transformed successfully, and the remaining parts of sub-goals and sub-Tasks can be refined and mapped into UML models manually. The proposed approach with the developed CASE tool can be applied to the software industry for requirements engineering.},
   author = {Y. Yang and Y. Bok and Z. Yang and E. Sheriff and T. Li},
   editor = {Pant V Ruiz M. Li T.},
   isbn = {16130073 (ISSN)},
   issn = {16130073},
   journal = {CEUR Workshop Proceedings},
   keywords = {Automatic Generation,De facto standard,Goal models,Goal-oriented modelling,Requirements engineering,Requirements modeling,Requirements validation,Software engineering,System prototype,System requirements,UML,Unified Modeling Language,Use cases diagrams,User requirements,goal model,system requirements,user requirements},
   note = {<b>From Duplicate 1 (<i>Goal2UCM: Automatic Generation of Use Case Model from iStar Model</i> - Yang, Y.; Bok, Y.; Yang, Z.; Sheriff, E.; Li, T.)<br/></b><br/><b>From Duplicate 2 (<i>Goal2UCM: Automatic Generation of Use Case Model from iStar Model</i> - Yang, Y; Bok, Y; Yang, Z; Sheriff, E; Li, T)<br/></b><br/>cited By 0; Conference of 14th International iStar Workshop, iStar 2021 ; Conference Date: 18 October 2021; Conference Code:172524<br/><br/><br/>Conference code: 172524<br/>Export Date: 21 August 2022<br/>References: Yang, Y., Li, X., Ke, W., Liu, Z., Automated prototype generation from formal requirements model (2020) IEEE Trans. Reliab, 69, pp. 632-656; <br/>Yang, Y., Li, X., Liu, Z., Ke, W., RM2PT: A tool for automated prototype generation from requirements model (2019) Proceedings of the 41st International Conference on Software Engineering: Companion Proceedings, ICSE 2019, pp. 59-62. , May 25-31; <br/>Yang, Y., Ke, W., Li, X., RM2PT: requirements validation through automatic prototyping (2019) 27th IEEE International Requirements Engineering Conference, pp. 484-485. , IEEE; <br/>Santander, V. F. A., Castro, J., Deriving use cases from organizational modeling (2002) 10th Anniversary IEEE Joint International Conference on Requirements Engineering (RE 2002), pp. 32-42. , 9-13 September IEEE Computer Society, 2002; <br/>Geraldino, G. C. L., Santander, V. F. A., The JGOOSE tool (2019) Proceedings of the 12th International i Workshop co-located with 38th International Conference on Conceptual Modeling (ER 2019), , J. Pimentel, J. P. Carvallo, L. López (Eds), CEUR-WS.org; <br/>Castro, J., Mylopoulos, J., Alencar, F. M. R., Filho, G. A. C., Integrating organizational requirements and object oriented modeling (2001) 5th IEEE International Symposium on Requirements Engineering (RE 2001), pp. 146-153. , IEEE Computer Society; <br/>Castro, J. F., Alencar, F. M. R., Santander, V. F. A., Silva, C. T. L., Integration of i and Object-Oriented Models (2010) Social Modeling for Requirements Engineering, pp. 457-484. , The MIT Press; <br/>Arora, C., Sabetzadeh, M., Briand, L. C., Zimmer, F., Extracting domain models from naturallanguage requirements: Approach and industrial evaluation (2016) Proceedings of the ACM/IEEE 19th, pp. 250-260. , ACM; <br/>Lucassen, G., Robeer, M., Dalpiaz, F., van der Werf, J. M. E. M., Brinkkemper, S., Extracting conceptual models from user stories with visual narrator (2017) Requir. Eng, 22, pp. 339-358},
   pages = {21-27},
   publisher = {CEUR-WS},
   title = {Goal2UCM: Automatic Generation of Use Case Model from iStar Model},
   volume = {2983},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118253659&partnerID=40&md5=d0cb363a4a4673efcafd1ed14a1af0ff},
   year = {2021},
}
@book{Faily2018,
   abstract = {Everyone expects the products and services they use to be secure, but 'building security in' at the earliest stages of a system's design also means designing for use as well. Software that is unusable to end-users and unwieldy to developers and administrators may be insecure as errors and violations may expose exploitable vulnerabilities. This book shows how practitioners and researchers can build both security and usability into the design of systems. It introduces the IRIS framework and the open source CAIRIS platform that can guide the specification of secure and usable software. It also illustrates how IRIS and CAIRIS can complement techniques from User Experience, Security Engineering and Innovation & Entrepreneurship in ways that allow security to be addressed at different stages of the software lifecycle without disruption. Real-world examples are provided of the techniques and processes illustrated in this book, making this text a resource for practitioners, researchers, educators, and students.},
   author = {S. Faily},
   doi = {10.1007/978-3-319-75493-2},
   isbn = {9783319763156},
   journal = {Designing Usable and Secure Software with IRIS and CAIRIS},
   note = {<b>From Duplicate 1 (<i>Designing usable and secure software with IRIS and CAIRIS</i> - Faily, S.)<br/></b><br/><b>From Duplicate 2 (<i>Designing usable and secure software with IRIS and CAIRIS</i> - Faily, S)<br/></b><br/>cited By 12<br/><br/><br/>Cited By :12<br/>Export Date: 21 August 2022<br/>Correspondence Address: Faily, S.; Department of Computing and Informatics, United Kingdom<br/>References: Sellen, A., Rogers, Y., Harper, R., Rodden, T., Reflecting human values in the digital age (2009) Commun ACM., 52 (3), pp. 58-66; <br/>(2011) The Cost of Cyber Crime: A Detics Report in Partnership with the Office of Cyber Security and Information Assurance in the Cabinet Office, , UK Cabinet Office; <br/>Nielsen, J., Guerrilla HCI: using discount usability engineering to penetrate the intimidation barrier (1994) Bias RG, Mayhew DJ, editors. Cost-justifying usability. Morgan Kaufmann, pp. 242-272; <br/>Schneier, B., (2000) Secrets and lies: digital security in a networked world, , JohnWiley &amp; Sons; <br/>Swiderski, F., Snyder, W., (2004) Threat modeling, , Microsoft Press; <br/>Ghezzi, C., Jazayeri, M., Mandrioli, D., (2003) Fundamentals of software engineering, , 2nd ed. Prentice Hall; <br/>(1999) ISO/IEC 13407: Human-Centered Design Processes for Interactive Systems, , ISO/IEC; <br/>Fléchais, I., Sasse, M.A., Hailes, S.M.V., Bringing security home: a process for developing secure and usable systems Proceedings of the 2003 new security paradigms workshop. ACM, pp. 49-57; <br/>den Braber, F., Hogganvik, I., Lund, M.S., Stølen, K., Vraalsen, F., Model-based security analysis in seven steps -a guided tour to the CORAS method (2007) BT Technol J., 25 (1), pp. 101-17; <br/>Maughan, D., The need for a national cybersecurity research and development agenda (2010) Commun ACM., 53 (2), pp. 29-31; <br/>(2011) Report: 1st Software and Usable Security Aligned for Good Engineering (SAUSAGE) Workshop, , http://www.thei3p.org/events/sausage2011.html; <br/>(2016) Developers need help too, , https://www.ncsc.gov.uk/blog-post/developers-need-help-too; <br/>(2011) Fifth International Workshop on Secure Software Engineering, , http://www.sintef.no/secse; <br/>(2011) Symposium On Usable Privacy and Security, , http://cups.cs.cmu.edu/soups; <br/>(2010) Design, 6. a. OED Online, , http://dictionary.oed.com/cgi/entry/50061846, Oxford University Press; <br/>Garfinkel, S., Lipford, H.R., Usable security: history, themes, and challenges (2014) Synth Lect Inf Secur Priv Trust., 5 (2), pp. 1-124; <br/>(2007) ISO/IEC 27002: Information Technology -Security Techniques -Code of Practice for Information Security Management, , ISO/IEC; <br/>Gollmann, D., (2006) Computer security, , 2nd ed. John Wiley &amp; Sons; <br/>(2005) ISO/IEC 27001: Information Technology -Security Techniques -Requirements, , ISO/IEC; <br/>Beynon-Davies, P., (2002) Information systems: an introduction to informatics in organisations, , Palgrave; <br/>Liebenau, J., Backhouse, J., (1990) Understanding information: an introduction, , Macmillan; <br/>(1983) Risk Assessment: A Study Group Report, , Royal Society; <br/>Schneier, B., (2003) Beyond fear: thinking sensibly about security in an uncertain world, , New York: Springer; <br/>Swiderski, F., Snyder, W., (2004) Threat modeling, , Microsoft Press; <br/>Shostack, A., (2014) Threat modeling: designing for security, , John Wiley &amp; Sons; <br/>Rescorla, E., Korver, B., (2003) Guidelines forWriting RFC Text on Security Considerations, p. 3552. , https://tools.ietf.org/html/rfc3552, Internet Architecture Board; <br/>Backhouse, J., Dhillon, G., Structures of responsibility and security of information systems (1996) Eur J Inf Syst., 5 (1), pp. 2-9; <br/>Blyth, A., Using stakeholders, domain knowledge, and responsibilities to specify information systems' requirements (1999) J Organ Comput Electron Commer., 9 (4), pp. 287-96; <br/>Strens, R., Dobson, J., How responsibility modelling leads to security requirements (1993) Proceedings of the 1992-1993 New Security Paradigms Workshop. ACM, pp. 143-9; <br/>Barman, S., (2002) Writing information security policies, , New Riders; <br/>Helokunnas, T., Kuusisto, R., Information security culture in a value net (2003) Proceedings of the 2003 engineering management conference. IEEE Computer Society, pp. 190-4; <br/>da Veiga, A., Eloff, J.H.P., An information security governance framework (2007) Inf Syst Manag., 24 (4), pp. 361-72; <br/>Thomson, K.L., von Solms, R., Information security obedience: a definition (2005) Comput Secur., 24 (1), pp. 69-75; <br/>Thomson, K.L., von Solms, R., Louw, L., Cultivating an organizational information security culture (2006) Comput Fraud Secur., 2006 (10), pp. 7-11; <br/>Faily, S., Fléchais, I., Designing and aligning e-Science security culture with design (2010) InfManag Comput Secur., 18 (5), pp. 339-49; <br/>James, H.L., Managing information systems security: a soft approach (1996) Proceedings of the information systems conference of new zealand. IEEE Computer Society, pp. 10-20; <br/>Checkland, P., Scholes, J., (1990) Soft systems methodology in action, , John Wiley &amp; Sons; <br/>Schneier, B., (2000) Secrets and lies: digital security in a networked world, , JohnWiley &amp; Sons; <br/>Brostoff, S., Sasse, M.A., Safe and sound: a safety-critical approach to security (2001) Proceedings of the 2001 New Security Paradigms Workshop. ACM, pp. 41-50; <br/>Saltzer, J.H., Schroeder, M.D., The protection of information in computer systems (1975) Proc IEEE., 63 (9), pp. 1278-308; <br/>Adams, A., Sasse, M.A., Users are not the enemy (1999) Commun ACM., 42, pp. 41-6; <br/>Whitten, A., Tygar, D., Why Johnny can't encrypt: a usability evaluation of PGP 5.0 (1999) Proceedings of the 8th USENIX security symposium. USENIX Association, pp. 169-84; <br/>Yee, K.P., Guidelines and strategies for secure interaction design Security and usability: designing secure systems that people can use, pp. 247-73. , In: Cranor LF, Garfinkel S, editors. O'Reilly Media; <br/>Garfinkel, S.L., (2005) Design principles and patterns for computer systems that are simultaneously secure and usable, , Cambridge; <br/>Gamma, E., Helm, R., Johnson, R., Vlissides, J., (1995) Design patterns: elements of reusable objectoriented software, , Addison-Wesley; <br/>Sasse, M.A., Brostoff, S., Weirich, D., Transforming the weakest link -a human/computer interaction approach to usable and effective security (2001) BT Technol J., 19 (3), pp. 122-31; <br/>Zurko, M.E., Simon, R.T., User-centered security (1996) Proceedings of the 1996 new security paradigms workshop. ACM, pp. 27-33; <br/>Birge, C., Enhancing research into usable privacy and security (2009) Proceedings of the 27th ACM international conference on design of communication. ACM, pp. 221-6; <br/>tom Markotten, D.G., User-centered security engineering Proceedings of the 4th Europen/USENIX conference, , 2002 Unpublished workshop proceedings; <br/>Nielsen, J., Mack, R.L., (1994) Usability inspection methods, , John Wiley &amp; Sons; <br/>Jendricke, U., tom Markotten, D.G., Usability meets security -the identity-manager as your personal security assistant for the internet (2000) Proceedings of the 16th annual computer security applications conference. IEEE Computer Society, pp. 344-53; <br/>Fléchais, I., Mascolo, C., Sasse, M.A., Integrating security and usability into the requirements and design process (2007) Int J Electron Secur Digit Forensics., 1 (1), pp. 12-26; <br/>Zurko, M.E., User-centered security: stepping up to the grand challenge (2005) Proceedings of the 21st annual computer security applications conference. IEEE Computer Society, pp. 14-27; <br/>Rumbaugh, J., Jacobson, I., Booch, G., (2005) The unified modeling language reference manual, , 2nd ed. Addison-Wesley; <br/>Star, S.L., Griesemer, J.R., Institutional ecology, translations and boundary objects: amateurs and professionals in berkeley's museum of vertebrate zoology, 1907-39 (1989) Social Stud Sci., 19 (3), pp. 387-420; <br/>Irestig, M., Eriksson, H., Timpka, T., The impact of participation in information system design: a comparison of contextual placements (2004) Proceedings of the 8th conference on participatory design. ACM, pp. 102-11; <br/>Holtzblatt, K., Jones, S., Contextual Inquiry: a participatory technique for systems design (1993) Schuler D, Namioka A, editors. Participatory design: principles and practice. Lawrence Erlbaum Associates, pp. 177-210; <br/>Hewett, T.T., Baecker, R., Card, S., Carey, T., Gasen, J., Mantei, M., (1996) 2. In: ACM SIGCHI curricula for human-computer interaction. ACM; <br/>Thimbleby, H., (2007) Press on: principles of interaction programming, , MIT Press; <br/>Coutaz, J., Calvary, G., HCI and software engineering: designing for user interface plasticity (2008) The human-computer interaction handbook: fundamentals, evolving technologies, and emerging applications, pp. 1107-25. , Sears A, Jacko JA, editors. Lawrence Erlbaum Associates; <br/>Ghezzi, C., Jazayeri, M., Mandrioli, D., (2003) Fundamentals of software engineering, , 2nd ed. Prentice Hall; <br/>Lauesen, S., (2005) User interface design: a software engineering perspective, , Pearson AddisonWesley; <br/>(1998) ISO 9241-11, , Ergonomic requirements for office work with visual display terminals (VDT)s -Part 11 Guidance on usability; <br/>Norman, D.A., (1988) The design of everyday things, , 1st ed., Basic books; <br/>Moggridge, B., (2007) Designing interactions, , MIT Press; <br/>Gould, J.D., Lewis, C., Designing for usability: key principles andwhat designers think (1985) Commun ACM, 28 (3), pp. 300-11; <br/>(1999) ISO/IEC 13407: Human-Centered Design Processes for Interactive Systems, , ISO/IEC; <br/>Noessel, C., Cooper, A., Reimann, R., Cronin, D., (2014) About face: the essentials of interaction design, , 4th ed. John Wiley &amp; Sons; <br/>Beyer, H., Holtzblatt, K., (1998) Contextual design: defining customer-centered systems, , MorganKaufmann Publishers Inc; <br/>Constantine, L.L., Lockwood, L.A.D., (1999) Software for use: a practical guide to the models and methods of usage-centered design, , Addison-Wesley; <br/>Holtzblatt, K., Wendell, J.B., Wood, S., (2005) Rapid contextual design: a how-to guide to key techniques for user-centered design, , Elsevier; <br/>Diaper, D., Understanding task analysis for human-computer interaction (2004) Diaper D, Stanton NA, editors. The handbook of task analysis for human-computer interaction. Lawrence Erlbaum Associates, pp. 5-47; <br/>Annett, J., Hierarchical task analysis (2004) Diaper D, Stanton NA, editors. The handbook of task analysis for human-computer interaction. Lawrence Erlbaum Associates, pp. 67-82; <br/>Kieras, D., GOMS models for task analysis (2004) Diaper D, Stanton NA, editors. The handbook of task analysis for human-computer interaction. Lawrence Erlbaum Associates, pp. 83-116; <br/>Go, K., Carroll, J.M., Scenario-based task analysis (2004) The handbook of task analysis for human-computer interaction, , In: Diaper D, Carroll JM, editors. Lawrence Erlbaum Associates; <br/>Rosson, M.B., Carroll, J.M., (2002) Usability engineering: scenario-based development of humancomputer, , Academic Press; <br/>Nathan, L.P., Klasnja, P.V., Friedman, B., Value scenarios: a technique for envisioning systemic effects of new technologies (2007) CHI '07: extended abstracts on Human factors in computing systems. ACM, pp. 2585-90; <br/>Cooper, A., (1999) The inmates are running the asylum: why high tech products drive us crazy and how to restore the sanity, , 2nd ed. Pearson Higher Education; <br/>Pruitt, J., Adlin, T., (2006) The persona lifecycle: keeping people in mind throughout product design, , Elsevier; <br/>Norman, D.A., Ad-Hoc personas and empathetic focus (2006) Pruitt J,Adlin T, editors. The persona lifecycle: keeping people in mind throughout product design. Morgan Kaufmann, pp. 154-7.; <br/>Cockton, G., Revisiting usability's three key principles (2008) CHI '08 extended abstracts on human factors in computing systems. ACM, pp. 2473-84; <br/>Moody, F., (1996) I sing the body electronic: a year with microsoft on the multimedia frontier, , USA: Penguin; <br/>Thimbleby, H., User-centered methods are insufficient for safety critical systems (2007) HCI and usability for medicine and health care, third symposium of the workgroup human-computer interaction and usability engineering of the austrian computer society. Springer: LNCS, pp. 1-20; <br/>Chapman, C.N., Milham, R.P., The persona's new clothes: methodological and practical arguments against a popular method (2006) Proceedings of the human factors and ergonomics society 50th annual meetin, pp. 634-6. , http://cnchapman.files.wordpress.com/2007/03/chapman-milham-personas-hfes2006-0139-0330.pdf; <br/>Norman, D.A., Human-centered design considered harmful (2005) Interactions., 12 (4), pp. 14-9; <br/>Bannon, L.J., From human factors to human actors: the role of psychology and human-computer interaction studies in system design (1991) Greenbaum JM, Kyng M, editors. Design at work: cooperative design of computer systems. L. Erlbaum Associates, pp. 25-44; <br/>Ackerman, M.S., The intellectual challenge of CSCW: the gap between social requirements and technical feasibility (2000) Human-Comput Interact., 15 (2), pp. 179-203; <br/>Randall, D., Harper, R., Rouncefield, M., (2007) Fieldwork for design: theory and practice, , Berlin: Springer; <br/>Matthews, T., Whittaker, S., Moran, T., Yuen, S., Collaboration personas: a new approach for designing workplace collaboration tools (2011) Proceedings of the 29th international conference on human factors in computing systems. ACM, pp. 2247-56; <br/>Thimbleby, H., Thimbleby, W., Internalist and externalist HCI (2007) Proceedings of the 21st British HCI group annual conference. British Computer Society, pp. 111-4; <br/>Dowell, J., Long, J., Towards a conception for an engineering discipline of human factors (1989) Ergonomics., 32 (11), pp. 1513-35; <br/>Seffah, A., Gulliksen, J., Desmarais, M.C., An introduction to human-centered software engineering: integrating usability in the development process (2005) SeffahA,Gulliksen J, DesmaraisMC, editors. Human-centered software engineering: integrating usability in the software development lifecycle. Berlin: Springer, pp. 3-14; <br/>Seffah, A., Gulliksen, J., Desmarais, M.C., (2005) Human-centered software engineering: integrating usability in the software development lifecycle, , Berlin: Springer; <br/>Seffah, A., Vanderdonckt, J., Desmarais, M.C., (2009) Human-centered software engineering: software engineering models., patterns and architectures for HCI, , Berlin: Springer; <br/>Seffah, A., Metzker, E., The obstacles andmyths of usability and software engineering (2004) Commun ACM., 47 (12), pp. 71-6; <br/>Constantine, L.L., (2006) Activity modeling: towards a pragmatic integration of activity theory with usage-centered design, , Laboratory for usage-centered software engineering; <br/>Eriksson, H.E., Penker, M., (2000) Business modeling with UML: business patterns at work, , JohnWiley &amp; Sons; <br/>Gulliksen, J., Goransson, B., Boivie, I., Persson, J., Blomkvist, S., Cajander, A., Key principles for user-centered systems design (2005) Seffah A, Gulliksen J, Desmarais MC, editors. Humancentered software engineering: integrating usability in the software development lifecycle. Berlin: Springer, pp. 17-35; <br/>Sutcliffe, A., Convergence or competition between software engineering and human computer interaction Human-centered software engineering: integrating usability in the software development lifecycle, , In: Seffah A, Gulliksen J, Desmarais MC, editors. Berlin: Springer; 2005; <br/>Zave, P., Classification of research efforts in requirements engineering (1997) ACM Comput Surv., 29 (4), pp. 315-21; <br/>Nuseibeh, B., Easterbrook, S., Requirements engineering: a roadmap (2000) ICSE '00: Proceedings of the conference on the future of software engineering. ACM, pp. 35-46; <br/>(1990) IEEE Standard Glossary of Software Engineering Terminology. IEEE Std 61012-1990; <br/>Sommerville, I., Sawyer, P., (1999) Requirements engineering: a good practice guide, , John Wiley &amp; Sons; <br/>Alexander, I., Beus-Dukic, L., (2009) Discovering requirements: how to specify products and services, , John Wiley &amp; Sons; <br/>Robertson, J., Robertson, S., (2009) Volere requirements specification template, , http://www.volere.co.uk/template.htm, 14th ed; <br/>(2010) IBM Rational DOORS, , http://www-01.ibm.com/software/awdtools/doors; <br/>Hoffmann, M., Kuhn, N., Weber, M., Bittner, M., Requirements for requirements management tools (2004) Proceedings of the 12th IEEE international requirements engineering conference. IEEE Computer Society, pp. 301-8; <br/>Fléchais, I., Sasse, M.A., Stakeholder involvement, motivation, responsibility, communication: how to design usable security in e-Science (2009) Int J Human-Comput Stud., 67 (4), pp. 281-96; <br/>Haukelid, K., Theories of (safety) culture revisited-an anthropological approach (2008) Saf Sci., 46 (3), pp. 413-26; <br/>Ruighaver, T., Maynard, S., Chang, S., Organisational security culture: extending the end-user perspective (2007) Comput Secur., 226 (1), pp. 56-62; <br/>Tøndel, I.A., Jaatun, M.G., Meland, P.H., Security requirements for the rest of us: a survey (2008) IEEE Softw., 25 (1), pp. 20-7; <br/>Firesmith, D.G., Engineering security requirements (2003) J Object Technol., 2 (1), pp. 53-68; <br/>Haley, C.B., Laney, R., Moffett, J.D., Nuseibeh, B., Security requirements engineering: a framework for representation and analysis (2008) IEEE Trans Softw Eng., 34 (1), pp. 133-53; <br/>Jackson, M., (2001) Problem frames: analysing and structuring software development problems, , Addison-Wesley; <br/>Hatebur, D., Heisel, M., Schmidt, H., A security engineering process based on patterns (2007) Proceedings of the 18th international conference on database and expert systems applications. IEEE Computer Society, pp. 734-8; <br/>Schmidt, H., Threat-and risk-analysis during early security requirements engineering (2010) Proceedings of the 5th international conference on availability, reliability and security. IEEE Computer Society, pp. 188-195; <br/>Vincent, M., Communicating requirements for business: UML or problem frames? (2008) Proceedings of the 3rd international workshop on applications and advances of problem frames. ACM, pp. 16-22; <br/>van Lamsweerde, A., (2009) Requirements engineering: from system goals to uml models to software specifications, , John Wiley &amp; Sons; <br/>Dardenne, A., van Lamsweerde, A., Fickas, S., Goal-directed requirements acquisition (1993) Sci Comput Programm., 20 (1-2), pp. 3-50; <br/>Aziz, B., Arenas, A., Bicarregui, J., Ponsard, C., Massonet, P., From goal-oriented requirements to event-B specifications (2009) Proceedings of the first NASA Formal methods symposium. NASA, pp. 96-105; <br/>van Lamsweerde, A., Letier, E., Handling obstacles in goal-oriented requirements engineering (2000) IEEE Trans Softw Eng., 26 (10), pp. 978-1005; <br/>Leveson, N., (1995) Safeware: system safety and computers, , Addison-Wesley; <br/>van Lamsweerde, A., Elaborating security requirements by construction of intentional antimodels (2004) Proceedings of the 26th international conference on software engineering. IEEE Computer Society, pp. 148-157; <br/>Gabriel, R.P., (2012) Worst is better, , http://dreamsongs.com/WorseIsBetter.html; <br/>Chung, L., Nixon, B.A., Yu, E., Mylopoulos, J., (2000) Non-functional requirements in software engineering, , Kluwer Academic; <br/>(2010) i web-site, , http://www.cs.toronto.edu/km/istar; <br/>Yu, E., Towards modeling and reasoning support for early-phase requirements engineering (1997) Proceedings of the 3rd IEEE international symposium on requirements engineering. IEEE Computer Society, pp. 226-235; <br/>Bresciani, P., Perini, A., Giorgini, P., Giunchiglia, F., Mylopoulos, J., Tropos: an agent-oriented software development methodology (2004) Autons Agents Multi-Agent Syst., 8 (3), pp. 203-36; <br/>Elahi, G., Yu, E., Zannone, N., A vulnerability-centric requirements engineering framework: analyzing security attacks, countermeasures, and requirements based on vulnerabilities (2010) Requir Eng., 15 (1), pp. 41-62; <br/>Elahi, G., Yu, E., Trust trade-off analysis for security requirements engineering. In: Proceedings of the 17th IEEE international requirements engineering conference IEEE Computer Society, pp. 243-248; <br/>Mouratidis, H., Giorgini, P., Secure tropos: a security-oriented extension of the tropos methodology (2007) Int J Softw Eng Knowl Eng., 17 (2), pp. 285-309; <br/>Moody, D.L., Heymans, P., Matulevicius, R., Improving the effectiveness of visual representations in requirements engineering: an evaluation of i visual syntax (2009) Proceedings of the 17th IEEE international requirements engineering conference. IEEE Computer Society, pp. 171-180; <br/>Miller, G.A., The magical number seven, plus or minus two: some limits on our capacity for processing information (1956) Psychol Rev., 63 (2), pp. 81-97; <br/>Nordbotten, J.C., Crosby, M.E., The effect of graphic style on data model interpretation (1999) Inf Syst J., 9 (2), pp. 139-56; <br/>Easterbrook, S., Yu, E., Aranda, J., Fan, Y., Horkoff, J., Leica, M., Do viewpoints lead to better conceptual models? An exploratory case study (2005) Proceedings of the 13th IEEE international requirements engineering conference. IEEE Computer Society, pp. 199-208; <br/>Maiden, N., Jones, S., Ncube, C., Lockerbie, J., Using in requirements projects: some experiences and lessons (2011) Social modeling for requirements engineering, , In: Yu E, editor. MIT Press; <br/>Dalpiaz, F., Paja, E., Giorgini, P., (2016) Security requirements engineering: designing secure sociotechnical systems, , MIT Press; <br/>Jacob},
   pages = {1-258},
   publisher = {Springer International Publishing},
   title = {Designing usable and secure software with IRIS and CAIRIS},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069824990&doi=10.1007%2F978-3-319-75493-2&partnerID=40&md5=134b6212da2ec0ec644f0a22f860859a},
   year = {2018},
}
@inproceedings{,
   abstract = {Software Engineering (SE), Human-Computer Interaction (HCI) and User Experience (UX) are correlated areas in computer science whose techniques are commonly applied, in a complementary way, to industrial projects' development and evolution. Several modelling approaches have been proposed by these areas, which can be used in a model-based approach to develop or evolve systems. When used in evolution, modelling is helpful to understand existing systems and represent new functionalities to be developed. iStar is a goal-based modelling language that can be useful in this scenario. Thus, this paper focuses on reporting the experience of using iStar in an industrial project to evolve an e-commerce application. iStar was used for two purposes: To describe the current context involving the company's ecommerce systems ("AS-IS") and to model the ideal scenario, envisioned from the activities of UX research and design ("TO-BE"). Modelling was done collaboratively, involving the design and development teams of the project. A survey was applied to participants to identify their opinion about the usage of iStar. The survey's analysis points to the increase and equality of the knowledge level about the application to be evolved (context of the project).},
   author = {E. Gonçalves and I. Monteiro},
   editor = {Pant V Ruiz M. Li T.},
   isbn = {16130073 (ISSN)},
   issn = {16130073},
   journal = {CEUR Workshop Proceedings},
   keywords = {Computer users,E-Commerce applications,Electronic commerce,Human computer interaction,Industrial Project.,Industrial programs,Industrial project.,Interaction experiences,Istar,Model-Based Engineering,Model-based OPC,Model-based engineering,Modeling languages,Report,Software engineering,Surveys,Users' experiences,iStar},
   note = {<b>From Duplicate 1 (<i>Reporting the Usage of iStar in a Model-Based Industrial Project to Evolve an e-Commerce Application</i> - Gonçalves, E.; Monteiro, I.)<br/></b><br/><b>From Duplicate 2 (<i>Reporting the Usage of iStar in a Model-Based Industrial Project to Evolve an e-Commerce Application</i> - Gonçalves, E; Monteiro, I)<br/></b><br/>cited By 0; Conference of 14th International iStar Workshop, iStar 2021 ; Conference Date: 18 October 2021; Conference Code:172524<br/><br/><br/>Conference code: 172524<br/>Export Date: 21 August 2022<br/>References: VanGundy, A. B., Brainwriting for new product ideas: An alternative to brainstorming (1983) Journal of Consumer Marketing, 1, pp. 67-74; <br/>Werle, D. R., Parisi, M. F., (2011) UX Canvas. Specialization thesis, , https://docplayer.com.br/1580861-Unc-universidade-do-contestado-fisam-faculdades-internacionais-san-martin-instituto-faber-ludens-especializacao-em-design-deinteracao.html, Universidade do Contestado; <br/>Bache, E., (2013) The Coding DOJO Handbook, , Emily Bache, Canada Leanpub; <br/>Gonçalves, E., Castro, J., Araujo, J., Heineck, T., A Systematic Literature Review of iStar Extensions (2018) Journal of Systems and Software, 137, pp. 1-33; <br/>Goncalves, E., Araujo, J., Castro, J., PRISE: A process to support iStar extensions (2020) Journal of Systems and Software, 168, p. 110649; <br/>Gonçalves, E., Araujo, J., Castro, J., A process to support the creation of iStar extensions (2020) Proceedings of the 35th Annual ACM Symposium on Applied Computing; <br/>Yu, E., Amyot, D., Mussbacher, G., Franch, X., Castro, J., Practical applications of i in industry: The state of the art (2013) 21st IEEE International Requirements Engineering Conference; <br/>Yu, E., (1995) Modelling Strategic Relationships for Process Reengineering, , PhD. Thesis in Computer Science, University of Toronto, Toronto; <br/>Dalpiaz, F., Franch, X., Horkoff, J., (2016) iStar 2.0 Language Guide, , arXiv:1605.07767; <br/>Kotonya, G., Sommerville, I., (1998) Requirements engineering: processes and techniques, , John Wiley &amp; Sons, Inc; <br/>Esfahani, H. C., Cabot, J., Yu, E., Adopting agile methods: Can goal-oriented social modeling help? (2010) 4th International Conference on Research Challenges in Information Science (RCIS), , IEEE; <br/>Gothelf, J., Using Proto-Personas for Executive Alignment, , https://uxmag.com/articles/using-proto-personas-for-executive-Alignment; <br/>Brambilla, M., Cabot, J., Wimmer, M., Model-Driven Software Engineering in Practice (2012) Morgan &amp; Claypool Publishers series Synthesis Lectures on Software Engineering; <br/>Scheidegger, M. E. S., (2011) Integrando Scrum e a Modelagem de Requisitos Orientada a Objetivos por meio do SCRUM i, , Diss. Dissertação de Mestrado. UFPE-CIN; <br/>Lima, P., Vilela, J., Gonçalves, E., Pimentel, J., Holanda, A., Castro, J., Alencar, F., Lencastre, M., An extended systematic mapping study about the scalability of i models (2016) CLEI Electronic Journal, 19 (3), pp. 7-7; <br/>Mesquita, R., Nascimento, R., Souza, L., Lucena, M., (2019) Using iStar Framework for Planning and Monitoring Sprints in Scrum Projects: An Experience Report, , iStar ER; <br/>Tawhid, R., Braun, E., Cartwright, N., Alhaj, M., Mussbacher, G., Shamsaei, A., Amyot, D., Richards, G., Towards Outcome-Based Regulatory Compliance in Aviation Security (2012) IEEE International. Requirements Engineering Conference, pp. 267-272; <br/>Franch, X., Managing Risk in Open Source Software Adoption (2013) Proc. 8th Int. Conf. on Software Engineering and Applications (ICSOFT-EA 2013), , SciTePress},
   pages = {8-14},
   publisher = {CEUR-WS},
   title = {Reporting the Usage of iStar in a Model-Based Industrial Project to Evolve an e-Commerce Application},
   volume = {2983},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118245654&partnerID=40&md5=a842a66805a11b2c00004520211973e1},
   year = {2021},
}
@inproceedings{Portugal2019,
   abstract = {Modeling Non-Functional Requirements (NFRs) is a challenge, mainly from the subjective character of their elicitation. Modeling NFRs as a collaborative process may create a consensus among stakeholders, combining modeling and elicitation in a learning cycle. However, the collaboration presents challenges, such as the different viewpoints, the influence of creativity, the modeling tool, the domain/scope of the target software system, and the fundamental aspects of configuration management. This paper reports our observations on collaboratively modeling an intentional model for the handling of fruits and vegetables in a futuristic kitchen.},
   author = {R.L.Q. L Q Portugal and J.C.S. C S do Prado Leite},
   editor = {Lopez L Pimentel J. Carvallo J.P.},
   isbn = {16130073 (ISSN)},
   issn = {16130073},
   journal = {CEUR Workshop Proceedings},
   keywords = {Automation,Collaboration,Configuration Management,Configuration management,Intelligent buildings,Intentional Modeling,Intentional modeling,Non-Functional Requirements,Non-functional requirements,Qualitative Requirements,Smart Homes,Smart homes},
   note = {<b>From Duplicate 1 (<i>Challenges in modeling non-functional requirements collaboratively</i> - Portugal, R.L.Q. L Q; do Prado Leite, J.C.S. C S)<br/></b><br/><b>From Duplicate 2 (<i>Challenges in modeling non-functional requirements collaboratively</i> - Portugal, R L Q; do Prado Leite, J C S)<br/></b><br/>cited By 1; Conference of 12th International i* Workshop, iStar 2019 ; Conference Date: 4 November 2019; Conference Code:153782<br/><br/><br/>Conference code: 153782<br/>Cited By :1<br/>Export Date: 21 August 2022<br/>References: Yu, E.S., Towards modelling and reasoning support for early-phase requirements engineering (1997) Requirements Engineering Proceedings of the Third IEEE International Symposium, pp. 226-235; <br/>Whitehead, J., Collaboration in software engineering: A roadmap (2007) Future of Software Engineering, pp. 214-225. , IEEE Computer Society; <br/>Paetsch, F., Eberlein, A., Maurer, F., Requirements engineering and agile software development (2003) Twelfth IEEE International Workshops on Enabling Technologies: Infrastructure for Collaborative Enterprises. (WET ICE), pp. 308-313; <br/>Gray, J., Rumpe, B., The evolution of model editors: Browser-and cloud-based solutions (2016) Software &amp; Systems Modeling, 15 (2), pp. 303-305; <br/>Dabbish, L., Stuart, C., Tsay, J., Herbsleb, J., Social coding in GitHub: Transparency and collaboration in an open software repository (2012) Proceedings of the ACM Conference on Computer Supported Cooperative Work, pp. 1277-1286. , February; <br/>Lanubile, F., Ebert, C., Prikladnicki, R., Vizcaíno, A., Collaboration tools for global software engineering (2010) IEEE Software, 27 (2), pp. 52-55; <br/>Leite, J.C.S.P., The prevalence of code over models: Turning it around with transparency (2018) Keynote in IEEE 8th International Model-Driven Requirements Engineering Workshop (MoDRE), pp. 56-57; <br/>Hibshi, H., Breaux, T.D., Broomell, S.B., Assessment of risk perception in security requirements composition (2015) 2015 IEEE 23rd International Requirements Engineering Conference (RE), pp. 146-155; <br/>Cavalcanti, R., Portugal, R.L.Q., Teixeira, B., Leite, J.C.S.P., (2018) Em Busca Dos Requisitos Para Susana: Requisitos Para Uma Humanóide Construtora De Requisistos, , http://bib-di.inf.puc-rio.br/techreports/2018.htm, Technical-Report, PUC-Rio; <br/>Leite, J.C.S.P., Elicitation awareness in conceptual modeling: The role of transparency (2015) Keynote Talk at Istar´15 - 8th International I* Workshop, , http://wwwdi.inf.puc-rio.br/~julio/transp-istar-15-ottawa.pdf, Ottawa; <br/>Cunha, H., Leite, J.C.S.P., Reusing non-functional patterns in i∗ modeling (2014) IEEE 4th International Workshop on Requirements Patterns (RePa), pp. 25-32; <br/>Chung, L., Leite, J.C.S.P., On non-functional requirements in software engineering (2009) Conceptual Modeling: Foundations and Applications, pp. 363-379. , Springer, Berlin, Heidelberg; <br/>Ameller, D., Ayala, C., Cabot, J., Franch, X., Non-functional requirements in architectural decision making (2013) IEEE Software, 30 (2), pp. 61-67; <br/>Mylopoulos, J., Chung, L., Yu, E., From object-oriented to goal-oriented requirements analysis (1999) Communications of the ACM, 42 (1), pp. 31-37; <br/>Leite, J.C.S.P., (1988) Viewpoint Resolution in Requirements Elicitation, , http://www-di.inf.puc-rio.br/~julio//bd.htm, Doctoral Thesis, University of California, Irvine); <br/>Franch, X., Fostering the adoption of i* by practitioners: Some challenges and research directions (2010) Intentional Perspectives on Information Systems Engineering, pp. 177-193. , Springer, Berlin, Heidelberg; <br/>Doorn, J.H., Hadad, G.D., Elizalde, M.C., García, A.R., Carnero, L.O., Críticas Cognitivas a Heurísticas Orientadas a Modelos (2019) 22nd Workshop on Requirements Engineering, (WER19); <br/>Portugal, R.L.Q., Cavalcanti, R., Leite, J.C.S.P., Modelo intencional para a gestão de frutas e vegetais em uma cozinha futurista. (Version iStar19) (2019) Zenodo, , http://doi.org/10.5281/zenodo.3387709, GitHub: https://git.io/JeOGK; <br/>Lutz, R.R., RE at 50, with a focus on the last 25 years (2017) Proceedings of 25th IEEE International Requirements Engineering Conference (RE), pp. 482-483; <br/>Pimentel, J., Castro, J., PISTAR tool–a pluggable online tool for goal modeling (2018) Proceedings of 26th IEEE International Requirements Engineering Conference (RE), pp. 498-499; <br/>Portugal, R.L.Q., PiStar tool adapted during a model-driven elicitation (version istar19) (2019) Zenodo, , http://doi.org/10.5281/zenodo.3418479; <br/>Potts, C., Invented requirements and imagined customers: Requirements engineering for off-the-shelf software (1995) Proceedings of IEEE International Symposium on Requirements Engineering (RE'95), pp. 128-130; <br/>Carmeli, A., Schaubroeck, J., The influence of leaders' and other referents' nor-mative expectations on individual involvement in creative work (2007) The Leadership Quarterly, 18 (1), pp. 35-48; <br/>Amabile, T.M., Conti, R., Coon, H., Lazenby, J., Herron, M., Assessing the work environment for creativity (1996) Academy of Management Journal, 39 (5), pp. 1154-1184},
   publisher = {CEUR-WS},
   title = {Challenges in modeling non-functional requirements collaboratively},
   volume = {2490},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074811218&partnerID=40&md5=13d7e8b724907542cf580ccd5744cffe},
   year = {2019},
}
@inproceedings{Nguyena2019,
   abstract = {Recognition of the critical role of software in promoting and maintaining sustainability has been emerging recently. Yet, over the last 60 years a vast amount of software has been developed and deployed without any consideration of sustainability. This paper presents a case study on how relevant sustainability requirements have been identified (using goal-oriented techniques) and integrated into an existing software design and implementation. It also discusses metrics used for evaluation of the sustainability effects of the new design. The evaluation of this study demonstrates that the re-designed software is both more extendable and more sustainable. Thus, this paper presents an example process and case study for undertaking sustainability-centred software re-design, as well as reports on a number of trade-offs and interdependencies between the studied sustainability requirements.},
   author = {N. Nguyena and R. Chitchyan},
   editor = {B Penzenstadler and C C Venters and R Chitchyan},
   isbn = {16130073 (ISSN)},
   issn = {16130073},
   journal = {CEUR Workshop Proceedings},
   keywords = {Ecodesign,Economic and social effects,Goal-oriented,Number of trades,Requirements engineering,Role of software,Software design,Software design and implementation,Sustainable development},
   note = {Conference code: 156891<br/>Export Date: 21 August 2022<br/>References: Canon, C., Mahaux, M., Integrating the complexity of sustainability in requirements engineering (2012) First International Workshop on Requirements for Sustainable Systems; <br/>Dick, E.K.M., Drangmeister, J., Naumann, S., Green software engineering with agile methods (2013) GREENS, pp. 78-85. , IEEE; <br/>Cabot, J., Easterbrook, M.S., Horkoff, J., Lessard, L., Liaskos, S., Mazón, N.J., Integrating sustainability in decision-making processes: A modelling strategy (2009) Software Engineering - Companion Volume, 2009. ICSE-Companion 2009. 31st International Conference on, pp. 207-210; <br/>Mahaux, M., Heymans, P., Saval, G., Discovering sustainability requirements: An experience report. (2011) Requirements Engineering: Foundation for Software Quality. 17th International Working Conference, REFSQ 2011, pp. 19-33; <br/>Berners-Lee, M., (2011) How Bad Are Bananas?, , Canada: Greystone Books; <br/>Naumann, S., Dick, M., Kern, E., Johann, T., The greensoft model: A reference model for green and sustainable software and its engineering (2011) Sustainable Computing: Informatics and Systems, 1 (4), pp. 294-304; <br/>Steimer, F.L., Steimer, T., Green e-business (2010) 2010 4th IEEE International Conference on Digital Ecosystems and Technologies (DEST 2010), pp. 252-253; <br/>Korte, M., Lee, K., Fung, C.C., Sustainability in information systems: Requirements and emerging technologies (2012) 2012 International Conference on Innovation Management and Technology Research, pp. 481-485; <br/>Minas, L., Ellison, B., (2009) The Problem of Power Consumption in Servers, , http://lass.cs.umass.edu/∼shenoy/courses/fall09/691gc/slides/Lec12-1-Slides-Power\%20Consumption\%20in\%20Servers_CS697GC_Fall2009-V02.ppt, Fall Accessed: 20/03/2013; <br/>Oracle and/or Its Affiliates the Source for Java Technology Collaboration, , https://java.net/projects/petstore/sources/svn/show/tags/javapetstore-2_0-ea-v5_June_07?rev=699, Accessed: 20/12/2012; <br/>Istarquickguide, , http://istar.rwth-aachen.de/tiki-index.php?page=iStarQuickGuide, Accessed: 20/03/2013; <br/>Blackburn, M., (2008) Five Ways to Reduce Data Center Server Power Consumption, , White paper, The Green Grid; <br/>Spring Source Hyperic Sigar - System Information Gatherer and Reporter, , https://support.hyperic.com/display/SIGAR/Home, Accessed: 20/12/2012; <br/>Mozilla Foundation(US) add-ons. https://addons.mozilla.org/en-US/firefox/addon/firebug/. Accessed: 12/05/2013; Energy saving power meterUR - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078359497&amp;partnerID=40&amp;md5=865cfcb5e438dd3a801d17df47920abe<br/><br/><b>From Duplicate 2 (<i>Systems re-design for sustainability: PetShop Case study</i> - Nguyena, N.; Chitchyan, R.)<br/></b><br/><b>From Duplicate 1 (<i>Systems re-design for sustainability: PetShop Case study</i> - Nguyena, N; Chitchyan, R)<br/></b><br/>cited By 0; Conference of 8th International Workshop on Requirements Engineering for Sustainable Systems, RE4SuSy 2019 ; Conference Date: 24 September 2019; Conference Code:156891},
   publisher = {CEUR-WS},
   title = {Systems re-design for sustainability: PetShop Case study},
   volume = {2541},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078359497&partnerID=40&md5=865cfcb5e438dd3a801d17df47920abe},
   year = {2019},
}
@inproceedings{Goldin2015,
   abstract = {Requirements elicitation for content rich systems is a huge challenge. Even using known techniques, does not avoid diving into content details and get lost in hyperspace. We propose the GQR (Goal-Question- Result) model to support the Requirements Elicitation process in elevating the elicitation discussions from specific data elements to contextual structures of goal-question-result related content requirements, while scoping the discussion to specific stakeholders. This paper characterizes the GQR model, shows its advantages relative to the previous methods and illustrate its application by means of two case studies, one biomedical and another related to CMMI requirements processes.},
   author = {L. Goldin},
   doi = {10.5220/0005645600420048},
   editor = {Fraga A Exman I. Llorens J.},
   isbn = {9789897581625},
   journal = {SKY 2015 - Proceedings of the 6th International Workshop on Software Knowledge, in conjuction with IC3K 2015},
   keywords = {Biomedical,CMMI,Content rich systems,GQM (goal- question-metric),GQR model,Goal-oriented analysis,Goal-question-metric,Requirements elicitation,Requirements engineering,Requirements management},
   note = {<b>From Duplicate 1 (<i>GQR model to support requirements elicitation for content rich systems</i> - Goldin, L.)<br/></b><br/><b>From Duplicate 1 (<i>GQR model to support requirements elicitation for content rich systems</i> - Goldin, L)<br/></b><br/>cited By 0; Conference of 6th International Workshop on Software Knowledge, SKY 2015 ; Conference Date: 12 November 2015 Through 14 November 2015; Conference Code:117184<br/><br/><br/>Conference code: 117184<br/>Export Date: 21 August 2022<br/>Correspondence Address: Goldin, L.; Afeka Tel-Aviv Academic College of EngineeringIsrael; email: L_goldin@computer.org<br/>References: Anton, A., Goal-Based requirements analysis (1996) Proceedings of JeRE '96, College of Computing Georgia Institute of Technology Atlanta, , Georgia, USA; <br/>Bertrand, P., GRAIL/KAOS: An environment for goal driven requirements engineering (1998) ICSE'98 - 20th International Conference on Software Engineering, , IEEE-ACM; <br/>Biedermann, J., Grierson, D., (1995) A Generic Model for Building Design, , Springer Publisher; <br/>Begent, R., Challenges of ultra large scale integration of biomedical computing systems (2005) 18th IEEE International Symposium on Computer-Based Medical Systems, , IEEE; <br/>Chen, P.P., The entity-relationship model: Toward a unified view of data (1966) ACM Transactions on Database Systems, , ACM; <br/>Christel, M., Kang, K., (1992) Issues in Requirements Elicitation, (CMU/SEI-92-TR-012), , Software Engineering Institute, Carnegie Mellon University; <br/>Cockburn, A., (2001) Writing Effective Use Cases, , Addison Wesley; <br/>Cohn, M., (2004) User Stories Applied: for Agile Software Development, , Addison-Wesley; <br/>CMMI, (2015) Capability Maturity Model Integration (CMMI) Overview, , http://cmmiinstitute.com/, Software Engineering Institute; <br/>Dardenne, A., Goal-Directed requirements acquisition (1993) Science of Computer Programming; <br/>(2015) IEEE Standards for Software Development Process, , https://en.wikipedia.org/wiki/MIL-STD-498, DOD-Mil-std 498, IEEE; <br/>EBI, (2015) European Bioinformatics Institute, , http://www.ebi.ac.uk; <br/>Finkelstein, Developing an integrative platform for cancer research: A requirements engineering perspective (2006) Fifth E-science All Hands Meeting (AHM2006), E-science; <br/>Kavakli, E., Loucopoulos, P., Goal driven requirements engineering: Evaluation of current methods (2003) Department of Cultural Technology and Communication, , University of the Aegean. Copyright EMMSAD'03; <br/>Lapouchnian, A., Goal-Oriented requirements engineering: An overview of the current research (2005) Department of Computer, Science, , University Of Toronto; <br/>NCBI, (2015) The National Center for Biotechnology Information, , http://www.ncbi.nlm.nih.gov; <br/>NCI, (2015) National Cancer Institute - Center for Biomedical Informatics &amp; Information Technology, , http://cbiit.nci.nih.gov/; <br/>NCRI, (2015) Cancer Informatics Initiative, , www.cancerinformatics.org.uk; <br/>Neetu, K., Pillai, A., A survey on global requirements elicitation issues and proposed research framework (2013) ICSESS 2013, Proceedings of 2013 IEEE 4th International Conference on Software Engineering and Service Science, , IEEE; <br/>Pharma, K.G.B., (2015) The Pharmacogenomics Knowledgebase, , http://www.pharmgkb.org/; <br/>(2015) The Public Library of Science (PLOS), , http://www.plos.org/, PLOS; <br/>Robertson, S., Robertson, J., (1999) Mastering the Requirements Process, , Addison-Wesley, Cambridge, MA, USA; <br/>Solingen, A., Berghout, E., (1999) The Goal/Question/Metric Method, , McGraw-Hill Publishing Company, New York, NY, USA; <br/>Zdravkovic, Using i∗ to Capture consumer preferences as requirements for software product lines, iStar 2013 (2013) Proceedings of the 6th International I∗ Workshop},
   pages = {46-52},
   publisher = {SciTePress},
   title = {GQR model to support requirements elicitation for content rich systems},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960851570&doi=10.5220%2F0005645600420048&partnerID=40&md5=b8a766b32f892907440b71d26f53ff92},
   year = {2015},
}
@inproceedings{,
   abstract = {The proceedings contain 10 papers. The topics discussed include: strategy assessment using goal models: software industry as a case study example; reporting the usage of iStar in a model-based industrial project to evolve an e-commerce application; modeling non-functional requirements of a reactive system; Goal2UCM: automatic generation of use case model from iStar model; extending iStar2.0 metamodel to define data structures; a preliminary framework for constructing iStar models from user stories; a learner-friendly approach for using the iStar modeling framework: an ongoing study; and beyond conventional model-driven development: from strategy to code.},
   editor = {M Ruiz and T Li and V Pant},
   isbn = {16130073 (ISSN)},
   issn = {16130073},
   journal = {CEUR Workshop Proceedings},
   note = {Conference code: 172524<br/>Export Date: 21 August 2022<br/><br/><b>From Duplicate 2 (<i>iStar 2021 - Proceedings of the 14th International iStar Workshop, co-located with 40th International Conference on Conceptual Modeling, ER 2021</i> - Ruiz M. Li T., Pant V)<br/></b><br/><b>From Duplicate 1 (<i>iStar 2021 - Proceedings of the 14th International iStar Workshop, co-located with 40th International Conference on Conceptual Modeling, ER 2021</i> - Ruiz M. Li T., Pant V)<br/></b><br/>cited By 0; Conference of 14th International iStar Workshop, iStar 2021 ; Conference Date: 18 October 2021; Conference Code:172524},
   publisher = {CEUR-WS},
   title = {iStar 2021 - Proceedings of the 14th International iStar Workshop, co-located with 40th International Conference on Conceptual Modeling, ER 2021},
   volume = {2983},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118279333&partnerID=40&md5=eaaebc261e00cb820bb04592945c6f09},
   year = {2021},
}
@article{Pimentel2012,
   abstract = {Some quality attributes are known to have an impact on the overall architecture of a system, so that they are required to be properly handled from the early beginning of the software development. For example, adaptability is a key concern for autonomic and adaptive systems, which brings to them the capability to alter their behavior in response to changes on their surrounding environments. In this paper, we propose a Strategy for Transition between Requirements and Architectural Models for Adaptive systems (STREAM-A). In particular, we use goal models based on the i* (i-Star) framework to support the design and evolution of systems that require adaptability. To obtain software architectures for such systems, the STREAM-A approach uses model transformations from i* models to architectural models expressed in Acme. Both the requirements and the architectural model are refined to accomplish the adaptability requirement. © 2011 Springer-Verlag London Limited.},
   author = {João Pimentel and Márcia Lucena and Jaelson Castro and Carla Silva and Emanuel Santos and Fernanda Alencar},
   doi = {10.1007/S00766-011-0126-Z/FIGURES/22},
   issn = {09473602},
   issue = {4},
   journal = {Requirements Engineering},
   keywords = {Adaptive systems,Architectural design,FRAMEWORK,GOAL,Mapping between requirements model and architectur,Model-driven engineering,ORIENTED REQUIREMENTS,Requirements engineering},
   month = {11},
   note = {Times Cited in Web of Science Core Collection: 12<br/>Total Times Cited: 13<br/>Cited Reference Count: 50},
   pages = {259-281},
   publisher = {Springer},
   title = {Deriving software architectural models from requirements models for adaptive systems: The STREAM-A approach},
   volume = {17},
   url = {https://link.springer.com/article/10.1007/s00766-011-0126-z},
   year = {2012},
}
@article{Casarotto2022,
   abstract = {The process of deriving use cases from organizational models built using the i ∗ framework has been presented in previous works. However, the guidelines used to derive use cases are based on the elements of the original version of i*. More recently, the new version i ∗ 2.0 has been proposed incorporating important changes which must be evaluated in relation to their impact on the use case derivation process. In this work, this evaluation is carried out, the guidelines modified and applied to an example showing the improvements obtained.},
   author = {Bruno Luiz B.L. Casarotto and G.C. C Gustavo Cesar Lopes Geraldino and Victor Francisco V.F. F Araya Santander and Ivonei Freitas Da Silva and Marco Antonio M.A. A Toranzo Cespedes},
   doi = {10.1109/TLA.2022.9661458},
   issn = {15480992},
   issue = {2},
   journal = {IEEE Latin America Transactions},
   keywords = {I* frameworks,Organizational Modeling,Organizational models,Requirement engineering,Requirements Engineering,Software engineering,Use Cases,Use case},
   month = {2},
   note = {<b>From Duplicate 1 (<i>Using of i∗(iStar) 2.0 for Improving the Use Cases Derivation</i> - Casarotto, Bruno Luiz B.L.; Lopes Geraldino, G.C. C Gustavo Cesar; Araya Santander, Victor Francisco V.F. F; Freitas Da Silva, Ivonei; Toranzo Cespedes, Marco Antonio M.A. A)<br/></b><br/><b>From Duplicate 1 (<i>Using of i∗(iStar) 2.0 for Improving the Use Cases Derivation</i> - Casarotto, B L; Lopes Geraldino, G C; Araya Santander, V F; Freitas Da Silva, I; Toranzo Cespedes, M A)<br/></b><br/>cited By 0<br/><br/><br/>Export Date: 21 August 2022<br/>References: Kotonya, G., Sommerville, I., (1998) Requirements Engineering: Processes and Techniques, , USA: Wiley; <br/>Lapouchnian, A., (2005) Goal-Oriented Requirements Engineering: An Overview of the Current Research, , Department of Computer Science, University of Toronto; <br/>Lamsweerde, A., Requirements engineering in the year 00: A research perspective (2000) 22nd International Conference on Software Engineering (ICSE 2000), , Limerick, Ireland; <br/>Pressmann, R., Mixin, B., (2015) Software Engineering, , 8th ed., USA: McGraw-Hill; <br/>Boehm, B., (1981) Software Engineering Economics, , NJ, USA: Prentice Hall PTR Upper Saddle River; <br/>Sommerville, I., (2015) Software Engineering, , 10th ed., USA: Pearson Education, Inc; <br/>Lamsweerde, A., Goal-oriented requirements engineering: A guided tour (2001) 5th IEEE International Symposium on Requirements Engineering, , Toronto, Canada; <br/>Santander, V.F.A., Castro, J., Deriving use cases from organizational modeling (2002) IEEE Joint International Requirements Engineering Conference-RE 02, , Essen, Germany; <br/>Blokdyk, G., (2020) UML A Complete Guide, , USA: Kindle; <br/>Yu, E., (1995) Modelling Strategic Relationships for Process Reengineering, , Ph.D. Thesis. Department of Computer Science, University of Toronto; <br/>Dalpiaz, F., Franch, X., Horkoff, J., (2016) IStar 2.0 Language Guide, , https://sites.google.com/site/istarlanguage/home.Acessadoem17/12/2020; <br/>Vicente, A.A., Santander, V.F.A., Castro, J.F.B., Silva, I.F., Matus, F.G.R., Jgoose: A requirements engineering tool to integrate i organizational modeling with use cases in uml (2009) Ingeniare, Revista Chilena de Ingenieriá (En Línea), 17, pp. 6-20; <br/>Geraldino, G.C.L., Santander, V.F.A., The jgoose tool (2019) 12th International i Workshop (IStar 2019)-38th International Conference on Conceptual Modeling (ER 2019), , Salvador, Brasil; <br/>Cockburn, A., (2000) Writing Effective Use Cases, , Boston, MA, USA: Addison Wesley Longman Publishing Co; <br/>Santander, V.F.A., Silva, I.F., Avaliando a utilização da Ferramenta JGOOSE no Processo de Ensino e Aprendizagem na Engenharia de Requisitos Um Relato de Experiência (2014) XIX Conferência Internacional Sobre Informática Na Educação (TISE), , Fortaleza, Brasil; <br/>http://www.inf.unioeste.br/les/index.php/listadownload, JGOOSE.. Acessado em 17/12/2020; Merlin, L.P., Silva, A.L.B., Santander, V.F.A., Silva, I.F., Castro, J., Integrating the E4J editor to the JGOOSE tool (2015) Requirements Engineering Workshop, , Lima, Peru; <br/>Peliser, D., Santander, V.F.A., Freitas, I., Andrade, S.C., Schemberger, E., E4J Use Cases: um editor de diagrama de casos de uso integrado à ferramenta JGOOSE (2016) 35th In-ternational Conference of the Chilean Computer Science Society (SCCC 2016), , Valparaí-so, Chile. NY 12571 USA: IEEE Catalog Number CFP16139-ART; <br/>Brischke, M., Santander, V.F.A., Silva, I., Melhorando a ferramenta JGOOSE (2012) 2012. In: 15th Workshop on Requirements Engineering, , Buenos Aires, 24 a 27 de Abril; <br/>Girotto, A.N., Santander, V.F.A., Silva, I.F., Toranzo, M.A., Uma proposta para derivar Casos de Uso a partir de modelos BPMN com suporte computacional (2017) 36th International Conference of the Chilean Computer Science Society (SCCC 2017), , Arica, Chile; <br/>Yu, E., Giorgini, P., Maiden, N., Mylopoulos, J., (2011) Social Modeling for Requirements Engineering, , USA: The MIT Press; <br/>Horko, J., Li, T., Li, F., Salnitri, M., Cardoso, E., Giorgini, P., Mylopoulos, J., Pimentel, J., Taking goal models downstream: A systematic roadmap (2014) International Conference on Research Challenges in Information Science, pp. 1-12. , IEEE; <br/>Horko, J., Yu, E., Comparison and evaluation of goal-oriented satisfaction analysis techniques (2013) Requirements Engineering, 18 (3), pp. 199-222; <br/>Gonçalves, E., Castro, J., Araújo, J., Heineck, T., A systematic literature review of istar extensions (2018) Journal of Systems and Software, 137, pp. 1-33; <br/>Gonçalves, E., Oliveira, M.A., Monteiro, I., Castro, J., Araújo, J., Understanding what is important in iStar extension proposals: The viewpoint of researchers (2019) Requirements Engineering, 24, pp. 55-84; <br/>Gonçalves, E., Araújo, J., Castro, J., PRISE: A process to support iStar extensions (2020) Journal of Systems and Software, 168, p. 110649; <br/>Tiwari, S., Gupta, A., A systematic literature review of use case specifications research (2015) Information Software Technology, pp. 128-158; <br/>Guedes, G., Silva, C., Castro, J., Goals and scenarios for requirements engineering of software product lines (2011) Proceedings of the 5th International i Workshop (IStar 2011); <br/>Jaqueira, A., Lucena, M., Aranha, E., Alencar, F., Castro, J., Using i models to enrich user stories (2013) Proceedings of the 6th International i Workshop (IStar 2013); <br/>Mesquita, R., Jaqueira, A., Agra, C., Lucena, M., Alencar, F., Us2startool: Generating i models from user stories (2015) Proceedings of the Eighth International i Workshop (IStar 2015); <br/>Agra, C., Souza, A., Melo, J., Lucena, M., Alencar, F., Specifying guidelines to transform i Model into User Stories: An overview (2015) Proceedings of the Eighth International i Workshop (IStar 2015); <br/>Wautelet, Y., Kolp, M., On the integration of i into RUP (2013) IStar, pp. 61-66; <br/>López, L., Aydemir, F.B., Dalpiaz, F., Horkoff, J., An empirical evaluation roadmap for iStar 2.0 (2016) Proceedings of the Ninth International i Workshop (istar'16), 1674, pp. 55-60; <br/>Yasin, A., Liu, L., Recent studies on i: A survey (2017) IStar, pp. 79-84},
   pages = {198-207},
   publisher = {IEEE Computer Society},
   title = {Using of i∗(iStar) 2.0 for Improving the Use Cases Derivation},
   volume = {20},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122352555&doi=10.1109%2FTLA.2022.9661458&partnerID=40&md5=c385a438a5ad0f568def1bd26a0144ee},
   year = {2022},
}
@article{Gralha2015,
   abstract = {Goal-oriented Requirements Engineering approaches have become popular in the Requirements Engineering community as they provide expressive modelling languages for requirements elicitation and analysis. However, as a common challenge, such approaches are still struggling when it comes to managing the accidental complexity of their models. Furthermore, those models might be incomplete, resulting in insufficient information for proper understanding and implementation. In this paper, we provide a set of metrics, which are formally specified and have tool support, to measure and analyse complexity and completeness of goal models, in particular social goal models (e.g. i<sup>∗</sup>). Concerning complexity, the aim is to identify refactoring opportunities to improve the modularity of those models, and consequently reduce their accidental complexity. With respect to completeness, the goal is to automatically detect model incompleteness. We evaluate these metrics by applying them to a set of well-known system models from industry and academia. Our results suggest refactoring opportunities in the evaluated models, and provide a timely feedback mechanism for requirements engineers on how close they are to completing their models.},
   author = {Catarina Gralha and João Araújo and Miguel Goulão},
   doi = {10.1016/j.is.2015.03.006},
   issn = {03064379 (ISSN)},
   journal = {Information Systems},
   keywords = {Goal-oriented requirements engineering,Goal-oriented requirements models,Model assessment,Modeling languages,Modelling language,Requirements Models,Requirements elicitation,Requirements engineering,Software metrics,System models,Timely feedback,i},
   month = {10},
   note = {<b>From Duplicate 1 (<i>Metrics for measuring complexity and completeness for social goal models</i> - Gralha, Catarina; Araújo, João; Goulão, Miguel)<br/></b><br/><b>From Duplicate 2 (<i>Metrics for measuring complexity and completeness for social goal models</i> - Gralha, C; Araújo, J; Goulão, M)<br/></b><br/>cited By 9<br/><br/><br/>Cited By :9<br/>Export Date: 21 August 2022<br/>CODEN: INSYD<br/>Correspondence Address: Gralha, C.; NOVA-LINCS, Departamento de Informática, Universidade Nova de LisboaPortugal<br/>References: Van Lamsweerde, A., Goal-oriented requirements engineering: A guided tour (2001) 5th IEEE International Symposium on Requirements Engineering, IEEE Computer Society, pp. 249-262. , Washington, DC, USA; <br/>Van Lamsweerde, A., (2009) Requirements Engineering, , 1st ed. John Wiley &amp; Sons, Inc. New Jersey, USA; <br/>Yu, E., (1995), Modelling strategic relationships for process reengineering (Ph.D. thesis), Canada; ITU-T: Recommendation Z.151 (10/12), User requirements notation (URN) - language definition, 2012; Espada, P., Goulão, M., Araújo, J., A framework to evaluate complexity and completeness of kaos goal model (2013) 25th International Conference on Advanced Information Systems Engineering, pp. 562-577. , CAiSE '13, Springer-Verlag, Valencia, Spain; <br/>Brooks, F.P., (1995) The Mythical Man-Month: Essays on Software Engineering, , Addison-Wesley Publishing Company Reading, MA, USA; <br/>ISO/IEC JTC1, OMG, Information technology - object management group object constraint language (OCL), 2012; Gralha, C., Goulão, M., Araújo, J., Identifying modularity improvement opportunities in goal-oriented requirements models (2014) Advanced Information Systems Engineering, Springer International Publishing, pp. 91-104. , Thessaloniki, Greece; <br/>Almeida, C., Goulão, M., Araújo, J., A systematic comparison of i∗modelling tools based on syntactic and well-formedness rules (2013) 6th International I∗Workshop (IStar 2013), CEUR Workshop Proceedings, 978, pp. 43-48. , J. Castro, J. Horkoff, N. Maiden, E. Yu (Eds.); <br/>http://www.istar.rwth-aachen.de/tiki-index.php?page=Comparing+the+i%2A+Tools, i∗wiki, Comparing the i∗tools (last access: February 2015). URL; http://eclipse.org/epsilon/doc/book/, D. Kolovos, L. Rose, A. Garcáa-DomInguez, R. Paige, The Epsilon Book, Eclipse Foundation, 2013 URL; Steinberg, D., Budinsky, F., Paternostro, M., Merks, E., (2009) EMF: Eclipse Modeling Framework, , Addison-Wesley Professional Boston, USA; <br/>http://www.eclipse.org/gmf-tooling/, Eclipse, Eclipse graphical modeling framework (gmf) tooling (last access: February 2015). URL; http://www.wiki.eclipse.org/index.php/Ecore_Tools, Eclipse, Ecore tools (last access: February 2015). URL; Basili, V.R., Caldiera, G., Rombach, H.D., (1994) The Goal Question Metric Paradigm, Encyclopedia of Software Engineering, , 1st ed., vol. 2, John Wiley &amp; Sons, Inc., New Jersey, USA; <br/>El Emam, K., Benlarbi, S., Goel, N., Rai, S.N., The confounding effect of class size on the validity of object-oriented metrics (2001) IEEE Trans. Softw. Eng., 27 (7), pp. 630-650; <br/>Subramanyam, R., Krishnan, M.S., Empirical analysis of ck metrics for object-oriented design complexity implications for software defects (2003) IEEE Trans. Softw. Eng., 29 (4), pp. 297-310; <br/>http://www.istar.rwth-aachen.de/tiki-index.php?page=Guideline+%28Intermediate%2CLayout%29+Use+the+specialized+actors+notation+to+the+degree+that+you+can+gain+advantage+in+instantiating+the+actual+stakeholders.&amp;structure=i%2A+Guide, i∗wiki, Guideline (intermediate,layout) use the specialized actors notation to the degree that you can gain advantage in instantiating the actual stakeholders (last access: February 2015). URL; Castro, J., Kolp, M., Mylopoulos, J., A requirements-driven development methodology (2001) 13th International Conference on Advanced Information Systems Engineering, pp. 108-123. , CAiSE '01, Springer-Verlag, Interlaken, Switzerland; <br/>Silva, C., Castro, J., Tedesco, P., Silva, I., Describing agent-oriented design patterns in tropos (2005) Proceedings of the 19th Brazilian Symposium in Software Engineering, pp. 27-78. , Uberlandia, Minas Gerais, Brazil; <br/>Engmann, J., (2009), Evaluating the impact of evolving requirements on wider system goals: using i∗methodology integrated with satisfaction arguments to evaluate the impact of changing requirements in hiv/aids monitoring systems in the UK (Master's thesis), England; Lockerbie, J., Maiden, N.A.M., Engmann, J., Randall, D., Jones, S., Bush, D., Exploring the impact of software requirements on system-wide goals: A method using satisfaction arguments and i∗goal modelling (2012) Requir. Eng., 17, pp. 227-254; <br/>Lima, C., Paes, J., Rodovalho, A., Dermeval, D., Buarque, A., (2011) MyCourses - A Course Scheduling System, , Centro de Informática, Universidade Federal de Pernambuco, Brazil; <br/>Borba, C.C., (2009) Uma abordagem orientada a objetivos para as fases de requisitos de linhas de produtos de software, , (Master's thesis), Brazil; <br/>Borba, C.C., Henrique, J., Xavier, L., (2009) BTW - If You Go, My Advice to You, Centro de Informática, , Universidade Federal de Pernambuco, Brazil; <br/>An, Y., Dalrymple, P.W., Rogers, M., Gerrity, P., Horkoff, J., Yu, E., Collaborative social modeling for designing a patient wellness tracking system in a nurse-managed health care center (2009) Proceedings of the 4th International Conference on Design Science Research in Information Systems and Technology, DESRIST '09, pp. 21-214. , Association for Computing Machinery; <br/>http://www.score-contest.org/, SCORE Contest, Score - student contest on software engineering (last access: February 2015). URL; Hevner, A.R., March, S.T., Park, J., Ram, S., (2004) Design Science in Information Systems Research, 28 (1), pp. 75-105. , MIS Q; <br/>Wieringa, R., Design science methodology: Principles and practice (2010) Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering, 2, pp. 493-494. , ACM, Cape Town, South Africa; <br/>Brown, W.J., Malveau, R.C., McCormick, H.W., Mowbray, T.J., (1998) AntiPatterns: Refactoring Software, Architectures, and Projects in Crisis, , John Wiley &amp; Sons New Jersey, USA; <br/>Horkoff, J., Yu, E., Comparison and evaluation of goal-oriented satisfaction analysis techniques (2012) Requir. Eng., 18 (3), pp. 199-222; <br/>Hilts, A., Yu, E., Design and evaluation of the goal-oriented design knowledge library framework (2012) Proceedings of the 2012 IConference, IConference '12, pp. 384-391. , Association for Computing Machinery, Toronto, Canada; <br/>Ramos, R., Castro, J., Araújo, J., Moreira, A., Alencar, F., Airdoc - An approach to improve requirements documents (2008) 22th Brazilian Symposium on Software Engineering, , SBES; <br/>De Vasconcelos, A.M.L., De La Vara, J.L., Sanchez, J., Pastor, O., Towards cmmi-compliant business process-driven requirements engineering (2012) 8th International Conference on the Quality of Information and Communications Technology, QUATIC '12, IEEE Computer Society, pp. 193-198. , Lisbon, Portugal; <br/>Franch, X., Grau, G., Towards a catalogue of patterns for defining metrics over i∗models (2008) 20th International Conference on Advanced Information Systems Engineering, pp. 197-212. , CAiSE '08, Springer-Verlag; <br/>Franch, X., A method for the definition of metrics over i∗models (2009) 21st International Conference on Advanced Information Systems Engineering, pp. 201-215. , CAiSE '09, Springer-Verlag, Amsterdam, The Netherlands; <br/>Colomer, D., Franch, X., Stargro: Building i∗metrics for agile methodologies (2014) 7th International I∗Workshop (IStar 2014), CEUR Workshop Proceedings, 1157. , F. Dalpiaz, J. Horkoff (Eds.); <br/>Zowghi, D., Gervasi, V., On the interplay between consistency, completeness, and correctness in requirements evolution (2003) Inf. Softw. Technol., 45 (14), pp. 993-1009},
   pages = {346-362},
   publisher = {Elsevier Ltd},
   title = {Metrics for measuring complexity and completeness for social goal models},
   volume = {53},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933674144&doi=10.1016%2Fj.is.2015.03.006&partnerID=40&md5=f47ce3f8da319f639bbc7dc25d3ee185},
   year = {2015},
}
@article{Pacheco2018,
   abstract = {Requirements elicitation is a critical activity that forms part of the requirements engineering process because it has to discover what the software must do through a solid understanding of the wishes and needs of the various stakeholders and to transform them into software requirements. However, in spite of its relevance, there are only a few systematic literature reviews that provide scientific evidence about the effectiveness of the techniques used to elicit software requirements. This study presents a systematic review of relevant literature on requirements elicitation techniques, from 1993 to 2015, by addressing two research questions: Which mature techniques are currently used for eliciting software requirements? and Which mature techniques improve the elicitation effectiveness? Prior literature assumes that such 'maturity' leads to a better-quality understanding of stakeholders' desires and needs, and thus an increased likelihood that a resulting software will satisfy those requirements. This research paper found 140 studies to answer these questions. The findings describe which elicitation techniques are effective and in which situations they work best, taking into account the product which must be developed, the stakeholders' characteristics, the type of information obtained, among other factors.},
   author = {Carla Pacheco and Ivan Garcia and Miryam Reyes},
   doi = {10.1049/iet-sen.2017.0144},
   issn = {1751-8814},
   issue = {4},
   journal = {IET Software},
   keywords = {Computer software,Critical activities,Elicitation techniques,Requirements elicitation,Requirements elicitation techniques,Requirements engineering,Requirements engineering process,Scientific evidence,Software requirements,Systematic literature review,formal specification,formal verification,requirements elicitation,requirements engineering process,software requirements,systems analysis},
   month = {8},
   note = {Cited By :42<br/>Export Date: 21 August 2022<br/>Correspondence Address: Garcia, I.; Division de Estudios de Posgrado, Carretera a Acatlima Km. 2.5, Mexico; email: ivan@mixteco.utm.mx<br/>References: Brooks, F.P., Jr., No silver bullet: Essence and accidents of software engineering (1987) IEEE Comput., 20 (4), pp. 10-19; <br/>Sommerville, I., (2010) Software Engineering, , Addison-Wesley, New York, NY, USA 9th edn; <br/>Hull, E., Jackson, K., Dick, J., (2011) Requirements Engineering, , Springer-Verlag, Berlin Heidelberg 3rd edn; <br/>Christel, M.G., Kang, K.C., (1992) Issues in Requirements Elicitation, , Technical Report CMU/SEI-92-TR-012 or ESC-TR-92-012, Software Engineering Institute, Carnegie Mellon University; <br/>Kotonya, G., Sommerville, I., (1998) Requirements Engineering: Processes and Techniques, , John Wiley and Sons, Oxford; <br/>Berenbach, B., Paulish, D.J., Kazmeier, J., (2009) Software &amp; Systems Requirements Engineering: In Practice, , McGraw-Hill, Inc., New York, NY, USA; <br/>Walia, G.S., Carver, J.C., A systematic literature review to identify and classify software requirement errors (2009) Inf. Softw. Technol., 51 (7), pp. 1087-1109; <br/>The CHAOS Report, , http://www.standishgroup.com, accessed 10 December 2015; <br/>Hickey, A.M., Davis, A.M., Requirements elicitation and elicitation technique selection: A model for two knowledge-intensive software development processes (2003) Proc. 36th Hawaii Int. Conf. System Sciences, Big Island, HI, USA, pp. 96-105. , February; <br/>Condori-Fernandez, N., Daneva, M., Sikkel, K., A systematic mapping study on empirical evaluation of software requirements specifications techniques (2009) Proc. Third Int. Symp. Empirical Software Engineering and Measurement, Washington, DC, USA, pp. 502-505. , October; <br/>Nicolás, J., Toval, A., On the generation of requirements specifications from software engineering models: A systematic literature review (2009) Inf. Softw. Technol., 51 (9), pp. 1291-1307; <br/>Pacheco, C., Garcia, I., A systematic literature review of stakeholder identification methods in requirements elicitation (2012) J. Syst. Softw., 85 (9), pp. 2171-2181; <br/>Riegel, N., Doerr, J., A systematic literature review of requirements prioritization criteria (2015) Requirements Engineering: Foundation for Software Quality, pp. 300-317. , Fricker, S.A., Schneider, K. (Eds.) Springer International Publishing, Berlin Heidelberg; <br/>Davis, A., Dieste, O., Hickey, A., Effectiveness of requirements elicitation techniques: Empirical results derived from a systematic review (2006) Proc. 14th IEEE Int. Requirements Engineering Conf., pp. 179-188. , Minneapolis/St. Paul, MN, USA, October; <br/>Dieste, O., Lopez, M., Ramos, F., Updating a systematic review about selection of software requirements elicitation techniques (2008) Proc. 11th Workshop on Requirements Engineering, Barcelona, Catalonia, Spain, pp. 96-103. , November; <br/>Svensson, R.B., Höst, M., Regnell, B., Managing quality requirements: A systematic review (2010) Proc. 36th EUROMICRO Conf. Software Engineering and Advanced Applications, Lille, TBD, France, pp. 261-268. , October; <br/>Dieste, O., Juristo, N., Systematic review and aggregation of empirical studies on elicitation techniques (2011) IEEE Trans. Softw. Eng., 37 (2), pp. 283-304; <br/>Kitchenham, B.A., Procedures for performing systematic reviews (2004) Joint Technical Report Software Engineering Group. Department of Computer Science, Keele University (UK) and Empirical Software Engineering, National ICT Australia; <br/>Beecham, S., Baddoo, N., Hall, T., Motivation in software engineering: A systematic review (2007) Inf. Softw. Technol. J., 50 (9-10), pp. 860-878; <br/>Kitchenham, B.A., Dyba, T., Jorgensen, M., Evidence-based software engineering (2004) Proc. 26th Int. Conf. Software Engineering, Edinburgh, UK, pp. 273-281. , May; <br/>Kuhrmann, M., Fernández, D.M., Daneva, M., On the pragmatic design of literature studies in software engineering: An experience-based guideline (2016) Empir. Softw. Eng., 22, pp. 1-40; <br/>Davis, A., Overmyer, S., Jordan, K., Identifying and measuring quality in a software requirements specification (1993) Proc. First Int. Software Metrics Symp., Baltimore, MD, USA, pp. 141-152. , May; <br/>Loucopoulos, P., Karakostas, V., (1995) Systems Requirements Engineering, , McGraw-Hill, Inc., New York, NY, USA; <br/>Sommerville, I., Sawyer, P., (1997) Requirements Engineering: A Good Practice Guide, , John Wiley &amp; Sons, Oxford 4th edn; <br/>Hofmann, H.F., Lehner, F., Requirements engineering as a success factor in software projects (2001) IEEE Softw., 18 (4), pp. 58-66; <br/>Hickey, A.M., Davis, A.M., Elicitation technique selection: How do experts do it (2003) Proc. 11th IEEE Int. Requirements Engineering Conf., Monterey, CA, USA, pp. 169-178. , September; <br/>Zowghi, D., Coulin, C., Requirements elicitation: A survey of techniques, approaches, and tools (2005) Engineering and Managing Software Requirements, pp. 19-46. , Aurum, A., Wohlin, C. (Eds.) Springer-Verlag, Berlin Heidelberg; <br/>Zhang, Z., Effective requirements development-a comparison of requirements elicitation techniques (2007) Proc. 15th Software Quality Management Conf., Tampere, Finland, pp. 225-240; <br/>Harter, D.E., Krishnan, M.S., Slaughter, S.A., Effects of process maturity on quality, cycle time, and effort in software product development (2000) Manage. Sci., 46 (4), pp. 451-466; <br/>Jiang, L., Eberlein, A., Far, B.H., A methodology for the selection of requirements engineering techniques (2008) Softw. Syst. Model., 7 (3), pp. 303-328; <br/>About WordNet Word Net 3.1, , http://wordnetprinceton.edu, Princeton University Princeton University accessed 12 February 2016; <br/>Kitchenham, B.A., Charters, S., Guidelines for Performing Systematic Literature Reviews in Software Engineering, , EBSE Technical Report, EBSE-2007-01, Software Engineering Group, School of Computers Science and Mathematics, Keele University (UK) and Department of Computer Science, University of Durham, UK; <br/>Fleiss, J., Measuring nominal scale agreement among many raters (1971) Psychol. Bull., 76 (5), pp. 378-382; <br/>International Software Engineering Research Network, , http://isern.iese.de/Portal, accessed 15 March 2015; <br/>Nuseibeh, B., Easterbrook, S., Requirements engineering: A roadmap (2000) Proc. ACM Conf. Future of Software Engineering, Limerick, Ireland, pp. 35-46. , June; <br/>Coulin, C.R., (2007) A Situational Approach and Intelligent Tool for Collaborative Requirements Elicitation, , Doctoral dissertation, University of Technology, Sydney; <br/>Tiwari, S., Rathore, S.S., Gupta, A., Selecting requirement elicitation techniques for software projects (2012) Proc. Sixth Int. Conf. Software Engineering, Indore, Madhya Pradesh, pp. 1-10. , September; <br/>Sharma, S., Pandey, S.K., Revisiting requirements elicitation techniques (2013) Int. J. Comput. Appl., 75 (12), pp. 35-39; <br/>Arif, M., Sarwar, S., Identification of requirements using goal oriented requirements elicitation process (2015) Int. J. Comput. Appl., 120 (15), pp. 17-21; <br/>Serna, M.E., Analysis and selection to requirements elicitation techniques (2012) Proc. Seventh Colombian Computing Congress, Medellin, Colombia, pp. 1-7. , October; <br/>Al Mrayat, O.I., Norwawi, N.M., Basir, N., Requirements elicitation techniques: Comparative study (2013) Int. J. Recent Dev. Eng. Technol., 1 (3), pp. 1-10; <br/>Gunda, S.G., (2008) Requirements Engineering: Elicitation Techniques, , Master's dissertation, University West, Trollhättan, Sweden; <br/>Kausar, S., Tariq, S., Riaz, S., Guidelines for the selection of elicitation techniques (2010) Proc. Sixth Int. Conf. Emerging Technologies, Islamabad, Pakistan, pp. 265-269. , October; <br/>Al Balushi, T.H., Sampaio, P.R.F., Dabhi, D., Elicito: A quality ontology-guided NFR elicitation tool (2007) Requirements Engineering: Foundation for Software Quality, pp. 306-319. , Sawyer, P., Paech, B., Heymans, P. (Eds.) Springer-Verlag, Germany;<br/><br/><b>From Duplicate 2 (<i>Requirements elicitation Techniques: A systematic literature review based on the maturity of the techniques</i> - Pacheco, Carla; Garcia, Ivan; Reyes, Miryam)<br/></b><br/><b>From Duplicate 3 (<i>Requirements elicitation Techniques: A systematic literature review based on the maturity of the techniques</i> - Pacheco, C; Garcia, I; Reyes, M)<br/></b><br/>cited By 42},
   pages = {365-378},
   publisher = {The Institution of Engineering and Technology},
   title = {Requirements elicitation Techniques: A systematic literature review based on the maturity of the techniques},
   volume = {12},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051708229&doi=10.1049%2Fiet-sen.2017.0144&partnerID=40&md5=1dc94bcf27e1f66d42c2fff649db581a https://onlinelibrary.wiley.com/doi/full/10.1049/iet-sen.2017.0144 https://onlinelibrary.wiley.com/doi/abs/10},
   year = {2018},
}
@article{Luna2010,
   abstract = {Web designers usually ignore how to model real user expectations and goals, mainly due to the large and heterogeneous audience of the Web. This fact leads to websites which are difficult to comprehend by visitors and complex to maintain by designers; these problems could be ameliorated if users are able to evaluate the application under development providing their feedback. To this aim, in this paper we present an approach for using the i* framework for modeling users' goals with mockups and WebSpec diagrams for detailing the specification of Web requirements, in such a way that the process of evaluating i* models for Web applications can be automated thus improving users' feedback during the development process. Also, as part of our development approach, we derive the domain and navigational models by defining a set of automatic transformations to a specific Web modeling method. Finally, we illustrate our approach with a case study to show its applicability and describe a prototype tool that supports the process. © Rinton Press.},
   author = {E.R. R Luna and I. Garrigos and J.-N. Mazon and J. Trujillo and G. Rossi},
   issn = {15409589},
   issue = {4},
   journal = {Journal of Web Engineering},
   keywords = {And o. diaz,Goal evaluation communicated by: m. gaedke,I*,M. grossniklaus,Mockups,Requirement engineering,Web requirements},
   note = {Cited By :5<br/>Export Date: 21 August 2022<br/>Correspondence Address: Luna, E. R.; LIFIA, Argentina; email: erobles@lifia.info.unlp.edu.ar<br/>References: Casteleyn, S., Woensel, W.V., Houben, G.-J., A semantics-based aspect-oriented approach to adaptation in Web engineering (2007) Hypertext, pp. 189-198; <br/>Cachero, C., Gómez, J., Advanced conceptual modeling of Web applications: Embedding operation interfaces in navigation design (2002) JISBD, pp. 235-248; <br/>Casteleyn, S., Garrigós, I., Troyer, O.D., Automatic runtime validation and correction of the navigational design of Web sites (2005) APWeb, pp. 453-463; <br/>Koch, N., Software engineering for adaptive hypermedia systems: Reference model, modeling techniques and development process (2001) Softwaretechnik- Trends, 21 (1); <br/>Ceri, S., Manolescu, I., Constructing and integrating data-centric web applications: Methods, tools, and techniques (2003) VLDB, p. 1151; <br/>Rossi, G., Schwabe, D., Designing personalized Web applications (2001) WWW, pp. 275-284; <br/>Koch, N., Reference model, modeling techniques and development process software engineering for adaptive hypermedia systems (2002) KI, 16 (3), pp. 40-41; <br/>Garrigós, I., (2008) A-OOH: Extending Web Application Design With Dynamic Personalization. PhD Thesis, , University of Alicante, Spain; <br/>Daniel, F., Matera, M., Morandi, A., Mortari, M., Pozzi, G., Active rules for runtime adaptivity management (2007) AEWSE; <br/>Martin, C., (2003) Agile Software Development: Principles, Patterns, and Practices, , Prentice Hall PTR, Upper Saddle River, NJ, USA; <br/>Yu, E., (1995) Modelling Strategic Relationships For Process Reenginering, , PhD thesis, University of Toronto, Canada; <br/>Yu, E., Towards modeling and reasoning support for early-phase requirements engineering (1997) RE, pp. 226-235; <br/>Escalona, M.J., Koch, N., Requirements engineering for Web applications - a comparative study (2004) J. Web Eng, 2 (3), pp. 193-212; <br/>Nguyen, D.C., Perini, A., Tonella, P., A goal-oriented software testing methodology (2007) AOSE, pp. 58-72; <br/>Robles, E., Garrigós, I., Grigera, J., Winckler, M., Capture and evolution of Web requirements using WebSpec B. Benatallah, F. Casati, G. Kappel, and G. Rossi, Editors, ICWE, Volume 6189 of Lecture Notes In Computer Science, pp. 173-188. , Springer, 2010; <br/>http://www.omg.org/cgi-bin/doc?ptc/2005-11-01, QVT Language; Garrigós, I., Mazón, J.-N., Trujillo, J., A requirement analysis approach for using i* in Web engineering (2009) ICWE, pp. 151-165; <br/>Estrada, H., Rebollar, A.M., Pastor, O., Mylopoulos, J., An empirical evaluation of the i* framework in a model-based software generation environment (2006) CAiSE, pp. 513-527; <br/>Strohmaier, M., Horkoff, J., Yu, E.S.K., Aranda, J., Easterbrook, S.M., Can patterns improve i* modeling? two exploratory studies (2008) REFSQ, pp. 153-167; <br/>Kleppe, A., Warmer, J., Bast. MDA Explained. The Practice and Promise of The Model Driven Architecture (2003) Addison Wesley; <br/>Czarnecki, K., Helsen, S., Classification of model transformation approaches In Proceedings of the 2nd OOPSLA Workshop on Generative Technique in the Context of the Model Driven Architecture Anaheim, , October 2003; <br/>Gerber, A., Lawley, M., Raymond, K., Steel, J., Wood, A., Transformation: The missing link of MDA In A. Corradini, H. Ehrig, H.-J. Kreowski, and G. Rozenberg, editors, ICGT, volume 2505 of Lecture Notes in Computer Science, pages 90-105. Springer, 2002; Sendall, S., Kozaczynski, W., Model transformation: The heart and soul of model-driven software development (2003) IEEE Software, 20 (5), pp. 42-45; <br/>http://www.omg.org/cgi-bin/doc?ptc/03-10-14, OCL; Horkoff, J., Yu, E., Evaluating goal achievement in enterprise modeling an interactive procedure and experiences In W. Aalst, J. Mylopoulos, N. M. Sadeh, M. J. Shaw, C. Szyperski, A. Persson, and J. Stirna, Editors, the Practice of Enterprise Modeling, pp. 145-160. , volume 39 of Lecture Notes in Business Information Processing, Springer Berlin Heidelberg, 2009. 10.1007/978-3-642-05352-812; <br/>Schwabe, D., Rossi, G., An object oriented approach to Web-based applications design (1998) TAPOS, 4 (4), pp. 207-225; <br/>Valderas, P., Pelechano, V., Pastor, O., A transformational approach to produce Web application prototypes from a web requirements model (2007) Int. J. Web Eng. Technol, 3 (1), pp. 4-42; <br/>Koch, N., Zhang, G., Escalona, M.J., Model transformations from requirements to Web system design (2006) ICWE, pp. 281-288; <br/>Bolchini, D., Paolini, P., Goal-driven requirements analysis for hypermedia-intensive Web applications (2004) Requir. Eng, 9 (2), pp. 85-103; <br/>Molina, F.M., Pardillo, J., Toval, J.A., Modelling Web-based systems requirements using WRM (2008) WISE Workshops, pp. 122-131; <br/>Chung, L., Nixon, B.A., Yu, E., Mylopoulos, J., Non-Functional Requirements In Software Engineering (THE KLUWER INTERNATIONAL SERIES IN SOFTWARE ENGINEERING Volume 5), , Springer, 1st edition, October 1999; <br/>Liu, L., Yu, E., Designing information systems in social context: A goal and scenario modelling approach (2004) Inf. Syst, 29 (2), pp. 187-203; <br/>Amyot, D., Ghanavati, S., Horkoff, J., Mussbacher, G., Peyton, L., Yu, E., Evaluating goal models within the goal-oriented requirement language Int. J. Intell. Syst, 25 (8); <br/>http://istar.rwth-aachen.de, i* wikiUR - https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650036430&amp;partnerID=40&amp;md5=9a29e625c24f0fc075e70686abb2ea33<br/><br/><b>From Duplicate 2 (<i>An i*-based approach for modeling and testing web requirements</i> - Luna, E.R. R; Garrigos, I.; Mazon, J.-N.; Trujillo, J.; Rossi, G.)<br/></b><br/><b>From Duplicate 1 (<i>An i*-based approach for modeling and testing web requirements</i> - Luna, E R; Garrigos, I; Mazon, J.-N.; Trujillo, J; Rossi, G)<br/></b><br/>cited By 5},
   pages = {302-326},
   title = {An i*-based approach for modeling and testing web requirements},
   volume = {9},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650036430&partnerID=40&md5=9a29e625c24f0fc075e70686abb2ea33},
   year = {2010},
}
@article{Wautelet2019,
   abstract = {Model-driven engineering is used for managing software systems development. In most cases, requirements representations are transformed into design diagrams themselves transformed into code, ensuring traceability. Such an approach can nevertheless take roots on a more abstract level and be used for IT governance. Goal-oriented requirements engineering can be adapted to model strategic objectives aimed to lead the organization to an enhanced competitive position in the long-term. Most IT governance and management frameworks are driven by the concept of service. The latter allows to package the work offer of an IT provider. Within their realization, such services align or misalign with strategic objectives. This paper proposes a model-driven IT governance process allowing to evaluate the alignment of business IT services to strategic objectives; it follows the 3 stages of IT governance: evaluate, direct and monitor. The approach allows to integrate the governance level as a (graphical) strategic layer made of long-term objectives that business IT services potentially contribute or hamper to attain. The strategic layer is custom developed for each organization and linked with organizational representations in a model-driven fashion to study business and IT alignment. The framework is called MoDrIGo; it is applied onto a case study in a hospital.},
   author = {Y. Wautelet},
   doi = {10.1016/j.jss.2018.12.024},
   issn = {01641212},
   journal = {Journal of Systems and Software},
   keywords = {Business strategy,I*,IT Governance,IT Strategy,IT governance,IT strategies,Iodine,Model-Driven engineering,Model-driven Engineering,Requirements engineering,Service-based model,Service-based modeling,Software design},
   note = {Cited By :23<br/>Export Date: 21 August 2022<br/>CODEN: JSSOD<br/>References: Avison, D.E., Jones, J., Powell, P., Wilson, D., Using and validating the strategic alignment model (2004) J. Strateg. Inf. Syst., 13 (3), pp. 223-246; <br/>Bannerman, P., Software development governance: a meta-management perspective (2009) Proceedings of the 2009 ICSE Workshop on Software Development Governance, pp. 3-8. , IEEE Computer Society; <br/>Bleistein, S.J., Aurum, A., Cox, K., Ray, P.K., Strategy-oriented alignment in requirements engineering: linking business strategy to requirements of e-business systems using the SOARE approach (2004) J. Res. Pract. Inf. Technol., 36 (4); <br/>Bleistein, S.J., Cox, K., Verner, J.M., Validating strategic alignment of organizational IT requirements using goal modeling and problem diagrams (2006) J. Syst. Softw., 79 (3), pp. 362-378; <br/>Bleistein, S.J., Cox, K., Verner, J.M., Phalp, K., B-SCP: a requirements analysis framework for validating strategic alignment of organizational IT based on strategy, context, and process (2006) Inf. Softw. Technol., 48 (9), pp. 846-868; <br/>Bradley, R.V., Pratt, R.M.E., Byrd, T.A., Outlay, C.N., Jr., D.E.W., Enterprise architecture, IT effectiveness and the mediating role of IT alignment in US hospitals (2012) Inf. Syst. J., 22 (2), pp. 97-127; <br/>Bresciani, P., Perini, A., Giorgini, P., Giunchiglia, F., Mylopoulos, J., Tropos: an agent-oriented software development methodology (2004) Auton. Agent. Multi. Agent Syst., 8 (3), pp. 203-236; <br/>Casadesus-Masanell, R., Ricart, J.E., From strategy to business models and onto tactics (2010) Long Range Plann., 43 (2-3), pp. 195-215; <br/>Cashore, B., Legitimacy and the privatization of environmental governance: how non–state market–driven (nsmd) governance systems gain rule–making authority (2002) Governance, 15 (4), pp. 503-529; <br/>Chung, L., Nixon, B., Yu, E., Mylopoulos, J., Non-Functional Requirements in Software Engineering (2000), Kluwer Academic Publishing Dordrecht; Engelsman, W., Quartel, D.A.C., Jonkers, H., van Sinderen, M., Extending enterprise architecture modelling with business goals and requirements (2011) Enterprise IS, 5 (1), pp. 9-36; <br/>Ferrario, R., Guarino, N., Towards an ontological foundation for services science (2008) FIS, pp. 152-169. , Domingue J. Fensel D. Traverso P. Springer; <br/>Freeman, R.E., Strategic management: A stakeholder approach (2010), Cambridge university press London; Group, T.O., (2017), http://pubs.opengroup.org/architecture/archimate3-doc/toc.html, Archimate® 3.0.1 specification; Haesen, R., Snoeck, M., Lemahieu, W., Poelmans, S., On the definition of service granularity and its architectural impact (2008) CAiSE, pp. 375-389. , Bellahsene Z. Léonard M. Springer; <br/>Henderson, J.C., Venkatraman, H., Strategic alignment: leveraging information technology for transforming organizations (1993) IBM Syst. J., 32 (1), pp. 472-484; <br/>International Organization for Standardization, ISO/IEC 38500: Corporate Governance of Information Technology (2008), The International Organization for Standardization (ISO) Geneva; ISACA, COBIT 5: A Business Framework for the Governance and Management of Enterprise IT (2012), ISACA Rolling Meadows; itSMF, ITIL Foundation Handbook (2012), The Stationery Office Norwich; Jackson, M., System behaviours and problem frames: concepts, concerns and the role of formalisms in the development of cyber-physical systems (2015) Dependable Software Systems Engineering, NATO Science for Peace and Security Series, D: Information and Communication Security, 40, pp. 79-104. , Irlbeck M. Peled D.A. Pretschner A. IOS Press; <br/>Jackson, M.A., Problem frames - Analysing and structuring software development problems (2000), http://www.pearsoned.co.uk/Bookshop/detail.asp?item=100000000004768, Pearson Education Amsterdam; Josey, A., Archimate® 3.0.1 - A pocket guide (2017), Van Haren; Kolp, M., Wautelet, Y., Descartes Architect: Design Case Tool for Agent-Oriented Repositories, Techniques, Environments and Systems (2018) Louvain School of Management, Université catholique de Louvain, Louvain-la-Neuve, Belgium; <br/>Kolp, M., Wautelet, Y., Faulkner, S., Sociocentric design of multi-agent architectures (2011) Social Modeling for Requirements Engineering, , Yu E. Giorgini P. Maiden N. Mylopoulos J. MIT Press; <br/>Krafzig, D., Banke, K., Slama, D., Enterprise SOA: Service-Oriented Architecture Best Practices (2005), Prentice Hall Professional Upper Saddle River; Maté, A., Trujillo, J., Franch, X., Adding semantic modules to improve goal-oriented analysis of data warehouses using i-star (2014) J. Syst. Softw., 88, pp. 102-111; <br/>Nolan, R., McFarlan, F.W., Information technology and the board of directors (2005) Harv. Bus. Rev., 83 (10), p. 96; <br/>Pastor, O., Estrada, H., Martínez, A., The Strengths and Weaknesses of the i* Framework: An Experimental Evaluation (2011), pp. 607-645. , MIT Press Cambridge, USA; Phillips, A.B., Merrill, J.A., Innovative use of the integrative review to evaluate evidence of technology transformation in healthcare (2015) J. Biomed. Inf., 58, pp. 114-121; <br/>Schroth, C., The service-oriented enterprise (2007) J. Enterp. Archit., 3 (4), pp. 73-80; <br/>Tallon, P.P., Kraemer, K.L., Gurbaxani, V., Executives’ Perceptions of the business value of information technology: a process-oriented approach (2000) J. Manag. Inf. Syst., 16 (4), pp. 145-174. , http://www.jmis-web.org/articles/926; <br/>Wang, M., Bandara, K.Y., Pahl, C., Process as a service (2010) 2010 IEEE International Conference on Services Computing, SCC 2010, Miami, Florida, USA, July 5–10, 2010, pp. 578-585. , IEEE Computer Society; <br/>Wautelet, Y., Achbany, Y., Kolp, M., A service-oriented framework for mas modeling (2008) ICEIS (3-1), pp. 120-128. , Cordeiro J. Filipe J; <br/>Wautelet, Y., Kolp, M., Business and model-driven development of BDI multi-agent systems (2016) Neurocomputing, 182, pp. 304-321; <br/>Wautelet, Y., Kolp, M., Heng, S., Poelmans, S., Developing a multi-agent platform supporting patient hospital stays following a socio-technical approach: management and governance benefits (2018) Telemat. Inf., 35 (4), pp. 854-882; <br/>Wautelet, Y., Kolp, M., Penserini, L., Service-driven iterative software project management with i-tropos (2018) J. Univ. Comput. Sci., 24 (7), pp. 975-1011; <br/>Wautelet, Y., Velghe, M., Heng, S., Poelmans, S., Kolp, M., On modelers ability to build a visual diagram from a user story set: a goal-oriented approach (2018) Requirements Engineering: Foundation for Software Quality - 24th International Working Conference, REFSQ 2018, Utrecht, The Netherlands, March 19–22, 2018, Proceedings, pp. 209-226. , Kamsties E. Horkoff J. Dalpiaz F. Springer; <br/>Wilson, J.M., Business processes: modelling and analysis for re-engineering and improvement (1996) J. Oper. Res. Soc., 47 (4), pp. 595-596; <br/>Yu, E., Modeling Strategic Relationships for Process Reengineering (2011), pp. 1-153. , MIT Press Cambridge, USAUR - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059127055&amp;doi=10.1016%2fj.jss.2018.12.024&amp;partnerID=40&amp;md5=24841bdb851f1fac327cb9d548c48e1a<br/><br/><b>From Duplicate 2 (<i>A model-driven IT governance process based on the strategic impact evaluation of services</i> - Wautelet, Y.)<br/></b><br/><b>From Duplicate 1 (<i>A model-driven IT governance process based on the strategic impact evaluation of services</i> - Wautelet, Y)<br/></b><br/>cited By 23},
   pages = {462-475},
   publisher = {Elsevier Inc.},
   title = {A model-driven IT governance process based on the strategic impact evaluation of services},
   volume = {149},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059127055&doi=10.1016%2Fj.jss.2018.12.024&partnerID=40&md5=24841bdb851f1fac327cb9d548c48e1a},
   year = {2019},
}
@article{Pape2016,
   abstract = {The popularity of business intelligence (BI) systems to support business analytics has tremendously increased in the last decade. The determination of data items that should be stored in the BI system is vital to ensure the success of an organisation's business analytic strategy. Expanding conventional BI systems often leads to high costs of internally generating, cleansing and maintaining new data items whilst the additional data storage costs are in many cases of minor concern - what is a conceptual difference to big data systems. Thus, potential additional insights resulting from a new data item in the BI system need to be balanced with the often high costs of data creation. While the literature acknowledges this decision problem, no model-based approach to inform this decision has hitherto been proposed. The present research describes a prescriptive framework to prioritise data items for business analytics and applies it to human resources. To achieve this goal, the proposed framework captures core business activities in a comprehensive process map and assesses their relative importance and possible data support with multi-criteria decision analysis.},
   author = {T. Pape},
   doi = {10.1016/j.ejor.2016.01.052},
   issn = {03772217},
   issue = {2},
   journal = {European Journal of Operational Research},
   keywords = {Additional datum,Big data,Business analytics,Business intelligence,Business intelligence systems,Competitive intelligence,Core business,Costs,Data requirements,Decision making,Decision problems,Decision theory,Digital storage,Human resources,Information analysis,Model based approach,Multi-criteria decision analysis,Personnel},
   note = {<b>From Duplicate 1 (<i>Prioritising data items for business analytics: Framework and application to human resources</i> - Pape, T.)<br/></b><br/><b>From Duplicate 1 (<i>Prioritising data items for business analytics: Framework and application to human resources</i> - Pape, T)<br/></b><br/>cited By 45<br/><br/><br/>Cited By :45<br/>Export Date: 21 August 2022<br/>CODEN: EJORD<br/>Correspondence Address: Pape, T.; Department of Mathematics, 4 Taviton Street, United Kingdom; email: t.pape@surrey.ac.uk<br/>References: Anderson, C., The end of theory: The data deluge makes the scientific method obsolete (2008) Wired Magazine, 16 (7), pp. 3-4; <br/>Aral, S., Brynjolfsson, E., Wu, L., Three-way complementarities: Performance pay, human resource analytics, and information technology (2012) Management Science, 58 (5), pp. 913-931; <br/>Ariely, D., Au, W.T., Bender, R.H., Budescu, D.V., Dietz, C.B., Gu, H., The effects of averaging subjective probability estimates between and within judges (2000) Journal of Experimental Psychology: Applied, 6 (2), pp. 130-147; <br/>Armstrong, M., Taylor, S., (2014) Armstrong's Handbook of Human Resource Management Practice, , 12th ed. Kogan Page Publishers; <br/>For Information Systems, A., (2012) IS Journals Impact Factors (ISI Citations) 2011, , http://aisnet.org/?ISICitations2011, Association for Information Systems Retrieved 01.08.15; <br/>Ballou, D.P., Tayi, G.C., Enhancing data quality in data warehouse environments (1999) Communications of the ACM, 42 (1), pp. 73-78; <br/>Bana, E., Costa, C.A., Les problématiques de l'aide à la décision: Vers l'enrichissement de la trilogie choix-tri-rangement (1996) RAIRO - Recherche Opérationnelle, 30 (2), pp. 191-216; <br/>Bana Costa, E.C.A., Corrêa, É.C., De Corte, J.-M., Vansnick, J.-C., Facilitating bid evaluation in public call for tenders: A socio-technical approach (2002) Omega, 30 (3), pp. 227-242; <br/>Bana Costa, E.C.A., Vansnick, J.C., MACBETH - An interactive path towards the construction of cardinal value functions (1994) International Transactions in Operational Research, 1 (4), pp. 489-500; <br/>Bassi, L., Carpenter, R., McMurrer, D., (2010) HR Analytics Handbook, , Reed Business; <br/>Becker, B.E., Ulrich, D., Huselid, M.A., Huselid, M., (2001) The HR Scorecard, , Harvard Business Press; <br/>Belton, V., Stewart, T., (2002) Multiple Criteria Decision Analysis: An Integrated Approach, , Springer; <br/>Beynon-Davies, P., Strategic data planning (2004) Database Systems, pp. 323-333. , P. Beynon-Davies 3rd ed. Palgrave MacMillan; <br/>Bordoloi, S.K., Matsuo, H., Human resource planning in knowledge-intensive operations: A model for learning with stochastic turnover (2001) European Journal of Operational Research, 130 (1), pp. 169-189; <br/>Bottomley, P.A., Doyle, J.R., A comparison of three weight elicitation methods: Good, better, and best (2001) Omega, 29 (6), pp. 553-560; <br/>Boudreau, J.W., (2010) Retooling HR: Using Proven Business Tools to Make Better Decisions about Talent, , Harvard Business Press; <br/>Bragg, S.M., (2014) Financial Analysis: A Business Decision Guide, , 2nd ed. Accounting Tools; <br/>Bratton, J., Gold, J., (2012) Human Resource Management: Theory and Practice, , 5th ed. Palgrave Macmillan; <br/>Canós, L., Liern, V., Soft computing-based aggregation methods for human resource management (2008) European Journal of Operational Research, 189 (3), pp. 669-681; <br/>Carifio, J., Rocco, J.P., Ten common misunderstandings, misconceptions, persistent myths and urban legends about Likert scales and Likert response formats and their antidotes (2007) Journal of Social Sciences, 3 (3), pp. 106-116; <br/>Cascio, W., Boudreau, J., (2011) Investing in People: Financial Impact of Human Resourse Initiatives, , 2nd ed. FT Press; <br/>Checkland, P.B., Information systems and systems thinking: Time to unite? (1988) International Journal of Information Management, 8 (4), pp. 239-248; <br/>Checkland, P.B., Holwell, S., (1997) Information, Systems and Information Systems: Making Sense of the Field, , Wiley; <br/>Chen, H., Chiang, R.H., Storey, V.C., Business intelligence and analytics: From big data to big impact (2012) MIS Quarterly, 36 (4), pp. 1165-1188; <br/>Chiang, R.H.L., Goes, P., Stohr, E.A., Business intelligence and analytics education, and program development: A unique opportunity for the information systems discipline (2012) ACM Transactions on Management Information Systems, 3 (3). , Article 12; <br/>Choudhury, V., Sampler, J.L., Information specificity and environmental scanning: An economic perspective (1997) MIS Quarterly, 21 (1), pp. 25-53; <br/>Corne, D., Dhaenens, C., Jourdan, L., Synergies between operations research and data mining: The emerging use of multi-objective approaches (2012) European Journal of Operational Research, 221 (3), pp. 469-479; <br/>Davenport, T.H., Harris, J.G., De Long, D.W., Jacobson, A.L., Data to knowledge to results (2001) California Management Review, 43 (2), pp. 117-138; <br/>Davenport, T.H., Harris, J.G., Shapiro, J., Competing on talent analytics (2010) Havard Business Review, 10 (88), pp. 52-58; <br/>De Mauro, A., Greco, M., Grimaldi, M., What is big data? A consensual definition and a review of key research topics (2015) AIP Conference Proceedings, 1644, pp. 97-104; <br/>Delen, D., Demirkan, H., Data, information and analytics as services (2013) Decision Support Systems, 55 (1), pp. 359-363; <br/>Deloitte, (2014) Global Human Capital Trends 2014: Engaging the 21st-century Workforce, , Deloitte University Press; <br/>Dori, D., Feldman, R., Sturm, A., From conceptual models to schemata: An object-process-based data warehouse construction method (2008) Information Systems, 33 (6), pp. 567-593; <br/>Drucker, P.F., The information executive's truly need (1995) Harvard Business Review, 73 (1), pp. 54-62; <br/>Eckerson, W.W., (2005) The Keys to Enterprise Business Intelligence: Critical Success Factors, , The Data Warehousing Institute; <br/>Eckerson, W.W., (2010) Performance Dashboards: Measuring, Monitoring, and Managing Your Business, , John Wiley &amp; Sons; <br/>Edwards, W., How to use multiattribute utility measurement for social decision making (1977) IEEE Transactions on Systems Man and Cybernetics, 7 (5), pp. 326-340; <br/>Edwards, W., Barron, F.H., SMARTS and SMARTER: Improved simple methods for multiattribute utility measurement (1994) Organizational Behavior and Human Decision Processes, 60 (3), pp. 306-325; <br/>Edwards, W., Von Winterfeldt, D., Moody, D.L., Simplicity in decision analysis: An example and a discussion (1988) Decision Making: Descriptive, Normative and Prescriptive Interactions, pp. 443-464. , D. Bell H. Raiffa A. Tversky Wiley; <br/>Eppler, M., Helfert, M., A classification and analysis of data quality costs (2004) Proceedings of International Conference on Information Quality, pp. 311-325; <br/>Even, A., Shankaranarayanan, G., Berger, P.D., Economics-driven data management: An application to the design of tabular data sets (2007) IEEE Transactions on Knowledge and Data Engineering, 19 (6), pp. 818-831; <br/>Feigin, G., (2011) Supply Chain Planning and Analytics: The Right Product in the Right Place at the Right Time, , Business Expert Press; <br/>Fitz-Enz, J., (2010) The New HR Analytics: Predicting the Economic Value of Your Company's Human Capital Investments, , Amacom; <br/>Fitz-Enz, J., Davison, B., (2001) How to Measure Human Resource Management, , 3rd ed. McGraw-Hill; <br/>Fortuin, L., Performance indicators - Why, where and how? (1988) European Journal of Operational Research, 34 (1), pp. 1-9; <br/>French, S., Maule, J., Papamichail, N., (2009) Decision Behaviour, Analysis and Support, , Cambridge University Press; <br/>Gilad, B., Gilad, T., A systems approach to business intelligence (1985) Business Horizons, 28 (5), pp. 65-70; <br/>Giorgini, P., Rizzi, S., Garzetti, M., GRAnD: A goal-oriented approach to requirement analysis in data warehouses (2008) Decision Support Systems, 45 (1), pp. 4-21; <br/>Golfarelli, M., Rizzi, S., Cella, I., Beyond data warehousing: What's next in business intelligence? (2004) Proceedings of the 7th ACM International Workshop on Data Warehousing and OLAP, pp. 1-6; <br/>Green, D., (2014) 6 Takeaways from the HR Analytics Innovation Summit, , https://www.linkedin.com/pulse/20140616150124-12091643-six-takeways-from-the-hr-analytics-innovation-summit, Retrieved 09.08.15; <br/>Grigsby, M., (2015) Marketing Analytics: A Practical Guide to Real Marketing Science, , Kogan Page; <br/>Infohrm, (2010) 100 Critical Human Captial Questions - How Well Do You Really Know Your Organisation?, , http://www.akt.co.il/Multimedia/upl_doc/doc_150212_33287.pdf, Akt Available at:. Retrieved 29.12.13; <br/>Jackson, J., Are US utility standby rates inhibiting diffusion of customer-owned generating systems? (2007) Energy Policy, 35 (3), pp. 1896-1908; <br/>Jacobs, K., (2014) Can HR Add Value to Its Data and Analytics?, , HR Magazine 22nd May; <br/>Jenkins, A.M., Naumann, J.D., Wetherbe, J.C., Empirical investigation of systems development practices and results (1984) Information &amp; Management, 7 (2), pp. 73-82; <br/>Judd, P., Paddock, C., Wetherbe, J.C., Decision impelling differences: An investigation of management by exception reporting (1981) Information &amp; Management, 4 (5), pp. 259-267; <br/>Kart, L., Heudecker, N., Buytendijk, F., (2013) Survey Analysis: Big Data Adoption in 2013 Shows Substance behind the Hype, , Gartner; <br/>Katal, A., Wazid, M., Goudar, R.H., Big data: Issues, challenges, tools and good practices (2013) Sixth International IEEE Conference on Contemporary Computing, pp. 404-409; <br/>Kavanagh, M.J., Thite, M., (2009) Human Resource Information Systems: Basics, Applications, and Future Directions, , Sage Publications; <br/>Keeney, R.L., (1992) Value-focused Thinking: A Path to Creative Decisionmaking, , Harvard University Press; <br/>Keeney, R.L., Common mistakes in making value trade-offs (2002) Operations Research, 50 (6), pp. 935-945; <br/>Keeney, R.L., Raiffa, H., (1976) Decisions with Multiple Objectives: Preferences and Value Tradeoffs, , Wiley; <br/>Keeney, R.L., Von Winterfeldt, D., Practical value models (2007) Advances in Decision Analysis - From Foundations to Applications, pp. 232-252. , W. Edwards R. Miles D. von Winterfeldt Cambridge University Press; <br/>Kim, S., Gilbertson, J., Information requirements of cancer center researchers focusing on human biological samples and associated data (2007) Information Processing &amp; Management, 43 (5), pp. 1383-1401; <br/>Kirsch, L.J., Haney, M.H., Requirements determination for common systems: Turning a global vision into a local reality (2006) The Journal of Strategic Information Systems, 15 (2), pp. 79-104; <br/>Lawyer, J., Chowdhury, S., Best practices in data warehousing to support business initiatives and needs (2004) Proceedings of the 37th Annual Hawaii International Conference on System Sciences; <br/>Lederer, A.L., Prasad, J., Information systems software cost estimating: A current assessment (1993) Journal of Information Technology, 8 (1), pp. 22-33; <br/>Lee, A.C., Lee, J.C., Lee, C.F., (2009) Financial Analysis, Planning &amp; Forecasting: Theory and Application, , 2nd ed. World Scientific; <br/>Levenson, A., Using targeted analytics to improve talent decisions (2011) People and Strategy, 34 (2), pp. 34-43; <br/>Liberatore, M., Luo, W., The analytics movement: Implications for operations research (2010) Interfaces, 40 (4), pp. 313-324; <br/>Liberatore, M., Luo, W., INFORMS and the analytics movement: The view of the membership (2011) Interfaces, 41 (6), pp. 578-589; <br/>Likert, R., A technique for the measurement of attitudes (1932) Archives of Psychology, 140, pp. 1-55; <br/>Lim, E.P., Chen, H., Chen, G., Business intelligence and analytics: Research directions (2012) ACM Transactions on Management Information Systems, 3 (4), p. 17; <br/>Lin, Y.H., Tsai, K.M., Shiang, W.J., Kuo, T.C., Tsai, C.H., Research on using ANP to establish a performance assessment model for business intelligence systems (2009) Expert Systems with Applications, 36 (2), pp. 4135-4146; <br/>Loeb, K.A., Rai, A., Ramaprasad, A., Sharma, S., Design, development and implementation of a global information warehouse: A case study at IBM (1998) Information Systems Journal, 8 (4), pp. 291-311; <br/>Loshin, D., Data Requirements Analysis (2012) Business Intelligence: The Savvy Manager's Guide., pp. 91-104. , D. Loshin 2nd ed. Newnes; <br/>Lustig, I., Dietrich, B., Johnson, C., Dziekan, C., The analytics journey (2010) Analytics Magazine, 3 (6), pp. 11-13; <br/>March, S.T., Hevner, A.R., Integrated decision support systems: A data warehousing perspective (2007) Decision Support Systems, 43 (3), pp. 1031-1043; <br/>Marshall, M., Pagel, C., French, C., Utley, M., Allwood, D., Fulop, N., Pope, C., Goldmann, A., Moving improvement research closer to practice: The researcher-in-residence model (2014) BMJ Quality and Safety, 23 (10), pp. 801-805; <br/>Martínez, A.B., Lista, E.A.G., Florez, L.C.G., Toward an agile and soft method to develop business intelligence solutions (2013) The International Journal of Soft Computing and Software Engineering, 3 (3), pp. 75-82; <br/>Marx, F., Mayer, J.H., Winter, R., Six principles for redesigning executive information systems - Findings of a survey and evaluation of a prototype (2011) ACM Transactions on Management Information Systems, 2 (4). , Article 26; <br/>Maté, A., Trujillo, J., A trace metamodel proposal based on the model driven architecture framework for the traceability of user requirements in data warehouses (2012) Information Systems, 37 (8), pp. 753-766; <br/>Maté, A., Trujillo, J., Franch, X., Adding semantic modules to improve goal-oriented analysis of data warehouses using I-star (2014) Journal of Systems and Software, 88, pp. 102-111; <br/>Mau-Crimmins, T., De Steiguer, J.E., Dennis, D., AHP as a means for improving public participation: A pre-post experiment with university students (2005) Forest Policy and Economics, 7 (4), pp. 501-514; <br/>Mayer-Schönberger, V., Cukier, K., (2013) Big Data: A Revolution That Will Transform How We Live, Work and Think, , John Murray; <br/>Mazón, J.N., Trujillo, J., A hybrid model driven development framework for the multidimensional modeling of data warehouses (2009) ACM SIGMOD Record, 38 (2), pp. 12-17; <br/>Mazón, J.N., Trujillo, J., Lechtenbörger, J., Reconciling requirement-driven data warehouses with data sources via multidimensional normal forms (2007) Data &amp; Knowledge Engineering, 63 (3), pp. 725-751; <br/>Mishra, P., Padhy, N., Panigrahi, R., The survey of data mining applications and feature scope (2013) Asian Journal of Computer Science &amp; Information Technology, 2 (4), pp. 68-77; <br/>Mitchell, B., Gamlem, C., (2012) The Big Book of HR, , Career Press; <br/>Morrison, R., (2015) Data-driven Organization Design: Sustaining the Competitive Edge Through Organizational Analytics, , Kogan Page; <br/>Mortenson, M.J., Doherty, N.F., Robinson, S., Operational research from Taylorism to terabytes: A research agenda for the analytics age (2015) European Journal of Operational Research, 241 (3), pp. 583-595; <br/>Negash, S., Gray, P., Business intelligence (2008) Handbook on Decision Support Systems, pp. 176-193. , F. Burstein C. Holsapple 2nd ed. Springer; <br/>Olafsson, S., Li, X., Wu, S., Operations research and data mining (2008) European Journal of Operational Research, 187 (3), pp. 1429-1448; <br/>Olson, D.L., (2003) Managerial Issues of Enterprise Resource Planning Systems, , McGraw-Hill; <br/>Olszak, C.M., Ziemba, E., Critical success factors for implementing business intelligence systems in small and medium enterprises on the example of upper Silesia, Poland (2012) Interdisciplinary Journal of Information, Knowledge, and Management, 7 (12), pp. 129-150; <br/>Ormerod, R.J., Putting soft or methods to work: Information systems strategy development at Sainsbury's (1995) Journal of the Operational Research Society, 46 (3), pp. 277-293; <br/>Ormerod, R., Putting soft or methods to work: Information systems strategy development at Richards Bay (1996) Journal of the Operational Research Society, 47 (9), pp. 1083-1097; <br/>Ormerod, R.J., Putting soft or methods to work: Information systems strategy development at Palabora (1998) Omega, 26 (1), pp. 75-98; <br/>Ormerod, R.J., Putting soft or methods to work: The case of IS strategy for the UK Parliament (2005) Journal of the Operational Research Society, 56 (12), pp. 1379-1398; <br/>Paim, F.R.S., De Castro, J.F.B., DWARF: An approach for requirements definition and management of data warehouse systems (2003) Proceedings of the 11th IEEE International Requirements Engineering Conference, pp. 75-84; <br/>Pandit, K., Marmanis, H., (2008) Spend Analysis: The Window into Strategic Sourcing, , J. Ross Publishing; <br/>Phillips, L.D., Bana Costa, E.C.A., Transparent prioritisation, budgeting and resource allocation with multi-criteria decision analysis and decision conferencing (2007) Annals of Operations Research, 154 (1), pp. 51-68; <br/>Phillips, L.D., Phillips, M.C., Facilitated work groups: Theory and practice (1993) Journal of the Operational Research Society, 44 (6), pp. 533-549; <br/>Pöyhönen, M., Hämäläinen, R.P., On the convergence of multiattribute weighting methods (2001) European Journal of Operational Research, 129 (3), pp. 569-585; <br/>Prakash, N., Gosain, A., An approach to engineering the requirements of data warehouses (2008) Requirements Engineering, 13 (1), pp. 49-72; <br/>Ramakrishnan, T., Jones, M.C., Sidorova, A., Factors influencing business intelligence (BI) data collection strategies: An empirical investigation (2012) Decision Support Systems, 52 (2), pp. 486-496; <br/>Romero, O., Abelló, A., Automatic validation of requirements to support multidimensional design (2010) Data &amp; Knowledge Engineering, 69 (9), pp. 917-942; <br/>Romero, O., Abelló, A., A framework for multidimensional design of data warehouses from ontologies (2010) Data &amp; Knowledge Engineering, 69 (11), pp. 1138-1157; <br/>Rosenhead, J., Mingers, J., (2001) Rational Analysis for a Problematic World Revisited: Problem Structuring Methods for Complexity, Uncertainty and Conflict, , Wiley; <br/>Rouhani, S., Ghazanfari, M., Jafari, M., Evaluation model of business intelligence for enterprise systems using fuzzy TOPSIS (2012) Expert Systems with Applications, 39 (3), pp. 3764-3771; <br/>Saaty, T.L., How to make a deision: The Analytical Hiearchy Process (1990) European Journal of Operational Research, 48 (1), pp. 9-26; <br/>Sahay, B.S., Ranjan, J., Real time business intelligence in supply chain analytics (2008) Information Management &amp; Computer Security, 16 (1), pp. 28-48; <br/>Salo, A.A., Keisler, J., Morton, A., An invitation to portfolio decision analysis (2011) Portfolio Decision Analysis: Improved Methods for Resource Allocation, pp. 3-27. , A. Salo J. Keisler A. Morton Springer; <br/>Scheer, A.W., Abolhassan, F., Jost, W., Kirchmer, M., (2002) Business Process Excellence - ARIS in Practice, , Springer; <br/>Schoemaker, P.J., Waid, C.C., An experimental comparison of different approaches to determining weights in additive value models (1982) Management Science, 28 (2), pp. 182-196; <br/>Sen, R., Sen, T.K., A meta-modeling approach to designing e-warehousing systems (2005) Journal of Organizational Computing and Electronic Commerce, 15 (4), pp. 285-316; <br/>Shanks, G., Darke, P., Understanding corporate data models (1999) Information &amp; Management, 35 (1), pp. 19-30; <br/>Shen, L.Z., Liu, S.H., Chen, S.K., Design and implementation of ETL in police intelligence data warehouse system (2012) Applied Mechanics and Materials, 121, pp. 3865-3869; <br/>(2012) The State of Talent Management - Developing the Agile Workforce, Driving Business Value, , http://www.silkroad.com/Resources/eBooks.html, SilkRoad Retrieved 29.12.13; <br/>Slinger, G., Morrision, R., Will organization design be affected by big data? (2014) Journal of Organization Design, 3 (3), pp. 17-26; <br/>Smith, T., (2013) HR Analytics: The What, Why and How, , Numerical Insights LLC; <br/>Smith, P.C., Goddard, M., Performance management and operational research: A marriage made in heaven? (2002) Journal of the Operational Research Society, 53 (3), pp. 247-255; <br/>Sørensen, G.C., Fountas, S., Nash, E., Pesonen, L., Bochtis, D., Pedersen, S.M., Conceptual model of a future farm management information system (2010) Computers and Electronics in Agriculture, 72 (1), pp. 37-47; <br/>Surowiecki, J., (2005) The Wisdom of Crowds, , Little Brown Book Group; <br/>Takecian, P.L., Methodological guidelines for reducing the complexity of data warehouse development for transactional blood bank systems (2013) Decision Support Systems, 55 (3), pp. 728-739; <br/>Telhada, J., Dias, A.C., Sampaio, P., Pereira, G., Carvalho, M.S., An integrated simulation and business intelligence framework for designing and planning demand responsive transport systems (2013) Lecture Notes in Computer Science, 8197, pp. 98-112; <br/>Tuomi, I., Data is more than knowledge: Implications of the reversed knowledge hierarchy for knowledge management and organization memory (2000) Journal of Management Information Systems, 16 (3), pp. 103-117; <br/>Vaidya, O.S., Kumar, S., Analytic hierarchy process: An overview of applications (2006) European Journal of Operational Research, 169 (1), pp. 1-29; <br/>Venkatesan, R., Farris, P., Wilcox, R.T., (2014) Cutting-edge Marketing Analytics: Real World Cases and Data Sets for Hands on Learning, , Pearson Education; <br/>Von Winterfeldt, D., Edwards, W., (1986) Decision Analysis and Behavioral Research, , Cambridge University Press; <br/>Wang, T., Hu, J., Zhou, H., Design and implementation of an ET},
   pages = {687-698},
   publisher = {Elsevier B.V.},
   title = {Prioritising data items for business analytics: Framework and application to human resources},
   volume = {252},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968324845&doi=10.1016%2Fj.ejor.2016.01.052&partnerID=40&md5=54920ae05e66277620f93a775baec65a},
   year = {2016},
}
@inproceedings{Dalpiaz2019,
   abstract = {[Team Overview] The Requirements Engineering Lab at Utrecht University conducts research on techniques and software tools that help people express better requirements in order to ultimately deliver better software products. [Past Research] We have focused on natural language processing-powered tools that analyze user stories to identify defects, to extract conceptual models that deliver an overview of the used concepts, and to pinpoint terminological ambiguity. Also, we studied how reviews for competing products can be analyzed via natural language processing (NLP) to identify new requirements. [Research Plan] The gained knowledge from our experience with NLP in requirements engineering (RE) triggers new research lines concerning the synergy between humans and NLP, including the use of intelligent chatbots to elicit requirements, the automated synthesis of creative requirements, and the maintenance of traceability via linguistic tooling. Copyright © 2019 by the paper’s authors.},
   author = {F Dalpiaz and S Brinkkemper},
   editor = {P Spoletini and P Mader and D M Berry and F Dalpiaz and M Daneva and A Ferrari and X Franch and S Gregory and E C Groen and A Herrmann and A Hess and F Houdek and O Karras and A Koziolek and K Lauenroth and C Palomares and M Sabetzadeh and N Seyff and N Seyff and M Trapp and A Vogelsang and T Weyer},
   isbn = {16130073 (ISSN)},
   issn = {16130073},
   keywords = {Automated synthesis,Chatbots,Computer software selection and evaluation,Conceptual model,NAtural language processing,Natural language processing systems,Requirements engineering,Research plans,Software products,User stories},
   note = {Conference code: 148547<br/>Export Date: 21 August 2022<br/>References: Aydemir, F.B., Dalpiaz, F., Brinkkemper, S., Giorgini, P., Mylopoulos, J., The next release problem revisited: A new avenue for goal models (2018) Proc. of RE; <br/>Brinkkemper, S., Brand, N., Moormann, J., Deterministic modelling procedures for automatic analysis and design tools (1988) Computerized Assistance During The Information Systems Life Cycle, pp. 117-160; <br/>Dalpiaz, F., Franch, X., Horkoff, J., (2016) iStar 2.0 Language Guide, , cs.SE; <br/>Dalpiaz, F., Parente, M., Re-SWot: From user feedback to requirements via competitor analysis (2019) Proc. of REFSQ; <br/>De Sousa Webber, F., (2015) Semantic Folding Theory and Its Application in Semantic Fingerprinting, , arXiv preprint; <br/>Dalpiaz, F., van der Schalk, I., Brinkkemper, S., Aydemir, F.B., Lucassen, G., Detecting terminological ambiguity in user stories: Tool and experimentation (2019) Information &amp; Software Technology; <br/>Dalpiaz, F., van der Schalk, I., Lucassen, G., Pinpointing ambiguity and incompleteness in requirements engineering via information visualization and NLP (2018) Proc. of REFSQ; <br/>Friesen, E., Bäumer, F.S., Geierhos, M., CordulA: Software requirements extraction utilizing chatbot as communication interface (2018) Proceedings of First Workshop on Natural Language Processing for Requirements Engineering; <br/>Groen, E.C., Seyff, N., Ali, R., Dalpiaz, F., Doerr, J., Guzman, E., Hosseini, M., Stade, M., The crowd in requirements engineering: The landscape and challenges (2017) IEEE Software, 34 (2), pp. 44-52; <br/>Klooster, M., Brinkkemper, S., Harmsen, F., Wijers, G., Intranet facilitated knowledge management: A theory and tool for defining situational methods (1997) Advanced Information Systems Engineering, 9th International Conference CAiSE’97, pp. 303-317. , Barcelona, Catalonia, Spain, June 16-20, 1997, Proceedings; <br/>Lombriser, P., Dalpiaz, F., Lucassen, G., Brinkkemper, S., Gamified requirements engineering: Model and experimentation (2016) Proc. of REFSQ, Volume 9619 of LNCS, pp. 171-187; <br/>Lucassen, G., Dalpiaz, F., van der Werf, J.M., Brinkkemper, S., The use and effectiveness of user stories in practice (2016) Proc. of REFSQ, Volume 9619 of LNCS, pp. 205-222; <br/>Lucassen, G., Dalpiaz, F., van der Werf, J.M., Brinkkemper, S., Visualizing user story requirements at multiple granularity levels via semantic relatedness (2016) Proc. of ER, Volume 9974 of LNCS, pp. 463-478. , Springer; <br/>Lucassen, G., Dalpiaz, F., Jan Martijn, E., van der Werf, M., Brinkkemper, S., Improving agile requirements: The quality user story framework and tool (2016) Requirements Engineering, 21 (3), pp. 383-403; <br/>Lucassen, G., Dalpiaz, F., van der Werf, J.M., Brinkkemper, S., Improving user story practice with the GRImm method: A multiple case study in the software industry (2017) Proc. of REFSQ, Volume 10153 of LNCS, pp. 235-252; <br/>Lucassen, G., Robeer, M., Dalpiaz, F., van der Werf, J.M., Brinkkemper, S., Extracting conceptual models from user stories with visual narrator (2017) Requirements Engineering, 22 (3), pp. 339-358; <br/>Müter, L., Deoskar, T., Mathijssen, M., Brinkkemper, S., Dalpiaz, F., Refinement of user stories into backlog items: Linguistic structure and action verbs (2019) Proc. of REFSQ; <br/>Natt och Dag, J., Gervasi, V., Brinkkemper, S., Regnell, B., A linguistic-engineering approach to large-scale requirements management (2005) IEEE Software, 22 (1), pp. 32-39; <br/>Regnell, B., Brinkkemper, S., Market-driven requirements engineering for software products (2005) Engineering and Managing Software Requirements, pp. 287-308; <br/>Slob, G.-J., Dalpiaz, F., Brinkkemper, S., Lucassen, G., The interactive narrator tool: Effective requirements exploration and discussion through visualization (2018) Proc. of the Poster Track of REFSQ; <br/>Zhang, Y., Harman, M., Ochoa, G., Ruhe, G., Brinkkemper, S., An empirical study of meta- and hyper-heuristic search for multi-objective release planning (2018) ACM Trans. Softw. Eng. Methodol., 27 (1), pp. 31-332<br/><br/><b>From Duplicate 2 (<i>Research on NLP for RE at Utrecht University: A report</i> - Dalpiaz, F; Brinkkemper, S)<br/></b><br/>cited By 0; Conference of 2019 Joint of International Conference on Requirements Engineering: Foundation for Software Quality Workshops, Doctoral Symposium, Live Studies Track, and Poster Track, REFSQ-JP 2019 ; Conference Date: 18 March 2019; Conference Code:148547},
   publisher = {CEUR-WS},
   title = {Research on NLP for RE at Utrecht University: A report},
   volume = {2376},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068047828&partnerID=40&md5=aa328c67adc927bee51fda871cb518e2},
   year = {2019},
}
@article{Bimonte2017,
   abstract = {Designing and building a Data Warehouse (DW), and associated OLAP cubes, are long processes, during which decision-maker requirements play an important role. But decision-makers are not OLAP experts and can find it difficult to deal with the concepts behind DW and OLAP. To support DW design in this context, we propose: (i) a new rapid prototyping methodology, integrating two different DM algorithms, to define dimension hierarchies according to decision-maker knowledge; (ii) a complete UML Profile, to define a DW schema that integrates both the DM algorithms; (iii) a mapping process to transform multidimensional schemata according to the results of the DM algorithms; (iv) a tool implementing the proposed methodology; (v) a full validation, based on a real case study concerning bird biodiversity. In conclusion, we confirm the rapidity and efficacy of our methodology and tool in providing a multidimensional schema to satisfy decision-maker analytical needs.},
   author = {S. Bimonte and L. Sautot and L. Journaux and B. Faivre},
   doi = {10.4018/IJDWM.2017010101},
   issn = {15483924},
   issue = {1},
   journal = {International Journal of Data Warehousing and Mining},
   keywords = {Analytical needs,Biodiversity,Data mining,Data warehouse,Data warehouses,Decision makers,Decision making,Dimension hierarchies,Mapping process,Methodologies and tools,Multi-dimensional model,Multidimensional schemata,OLAP,Rapid prototyping,Uml profiles},
   note = {<b>From Duplicate 1 (<i>Multidimensional model design using data mining: A rapid prototyping methodology</i> - Bimonte, S.; Sautot, L.; Journaux, L.; Faivre, B.)<br/></b><br/><b>From Duplicate 2 (<i>Multidimensional model design using data mining: A rapid prototyping methodology</i> - Bimonte, S; Sautot, L; Journaux, L; Faivre, B)<br/></b><br/>cited By 21<br/><br/><br/>Cited By :21<br/>Export Date: 21 August 2022<br/>References: Abdelhédi, F., Pujolle, G., Teste, O., Zurfluh, G., Computer-aided data-mart design (2011) Proceedings of ICEIS, 1, pp. 239-246; <br/>Abelló, A., Samos, J., Saltor, F., YAM2: A multidimensional conceptual model extending UML (2006) Information Systems, 31 (6), pp. 541-567; <br/>Adami, G., Avesani, P., Sona, D., Bootstrapping for hierarchical document classification (2003) Proceedings of the Twelfth International Conference on Information and Knowledge Management, pp. 295-302. , ACM; <br/>Bentayeb, F., K-Means based approach for olap dimension updates (2008) Proceedings of the10th International Conference on Enterprise Information Systems (ICEIS '08), pp. 531-534; <br/>Bentayeb, F., Khemiri, R., Adapting OLAP analysis to user's constraints through semantic hierarchies (2013) Proceedings of ICEIS, 1, pp. 193-200; <br/>Bimonte, S., Boulil, K., Pinet, F., Kang, M.-A., Design of complex spatio-multidimensional models with the icsolap uml profile-an implementation in magicdraw (2013) Proceedings of ICEIS, 1, pp. 310-315; <br/>Bimonte, S., Edoh-Alove, E., Nzih, H., Kang, M.-A., Rizzi, S., ProtOLAP: Rapid OLAP prototyping with on-demand data supply (2013) Proceedings of the Sixteenth International Workshop on Data Warehousing and OLAP, pp. 61-66. , ACM; <br/>Bishop, C.M., (1995) Neural Networks for Pattern Recognition, , Oxford university press; <br/>Boulil, K., Bimonte, S., Pinet, F., Conceptual model for spatial data cubes: A UML profile and its automatic implementation (2015) Computer Standards &amp; Interfaces, 38, pp. 113-132; <br/>Cuzzocrea, A., Malerba, D., Olap over continuous domains via density-based hierarchical clustering (2011) Knowledge-Based and Intelligent Information and Engineering Systems, pp. 559-570. , Springer; <br/>Cortes, C., Vapnik, V., Support-vector networks (1995) Machine Learning, 20 (3), pp. 273-297; <br/>Cover, T.M., Hart, P.E., Nearest neighbor pattern classification (1967) IEEE Transactions on Information Theory, 13 (1), pp. 21-27; <br/>Cravero, A., Sepúlveda, S., Multidimensional design paradigms for data warehouses: A systematic mapping study (2014) Journal of Software Engineering and Applications, 7 (1), pp. 53-61; <br/>Eder, J., Koncilia, C., Mitsche, D., Automatic detection of structural changes in data warehouses (2003) Data Warehousing and Knowledge Discovery, pp. 119-128. , Springer; <br/>Favre, C., Bentayeb, F., Boussaid, O., A knowledge-driven data warehouse model for analysis evolution (2006) Frontiers in Artificial Intelligence and Applications, 143, p. 271; <br/>Fayyad, U., Piatetsky-Shapiro, G., Smyth, P., From data mining to knowledge discovery in databases (1996) AI Magazine, 1996, pp. 37-54; <br/>Frochot, B., Nesting birds assemblages along the river Loire: Result from a 12 years-study (2003) Alauda, 71 (2), pp. 179-190; <br/>Golfarelli, M., Rizzi, S., Data warehouse testing: A prototype-based methodology (2011) Information and Software Technology, 53 (11), pp. 1183-1198; <br/>Gower, J.C., A general coefficient of similarity and some of its properties (1971) Biometrics, 27 (4), pp. 857-871; <br/>Han, J., OLAP mining: An integration of OLAP with data mining (1997) Proceedings of the 7th IFIP, , Citeseer; <br/>Hubert, G., Teste, O., (2010) Analyse Multigraduelle OLAP, , arXiv preprint arXiv: 1005.0217; <br/>Huynh, T.N., Schiefer, J., Prototyping data warehouse systems (2001) Data Warehousing and Knowledge Discovery, pp. 195-207. , Springer; <br/>Inmon, W.H., (2005) Building the Data Warehouse, , John wiley &amp; sons; <br/>Jain, A.K., Murty, M.N., Flynn, P.J., Data clustering: A review (1999) ACM Computing Surveys (CSUR), 31 (3), pp. 264-323; <br/>Jensen, M.R., Holmgren, T., Pedersen, T.B., Discovering multidimensional structure in relational data (2004) Data Warehousing and Knowledge Discovery, pp. 138-148. , Springer; <br/>Jovanovic, P., Romero, O., Simitsis, A., Abello, A., Ore: An iterative approach to the design and evolution of multi-dimensional schemas (2012) Proceedings of the Fifteenth International Workshop on Data Warehousing and OLAP, pp. 1-8. , ACM; <br/>Kimbal, R., (1996) The Data Warehouse Toolkit: Practical Techniques for Building Dimensional Data Warehouses, , NY: John Willey &amp; Sons; <br/>Kojadinovic, I., Agglomerative hierarchical clustering of continuous variables based on mutual information (2004) Computational Statistics &amp; Data Analysis, 46 (2), pp. 269-294; <br/>Lau, H.C.W., Chin, K.S., Pun, K.F., Ning, A., Decision supporting functionality in a virtual enterprise network (2000) Expert Systems with Applications, 19 (4), pp. 261-270; <br/>Leonhardi, B., Mitschang, B., Pulido, R., Sieb, C., Wurst, M., Augmenting olap exploration with dynamic advanced analytics (2010) Proceedings of the 13th International Conference on Extending Database Technology, pp. 687-692. , ACM; <br/>Liu, W., Luo, Y., Applications of clustering data mining in customer analysis in department store (2005) Proceedings of 2005 International Conference on Services Systems and Services Management ICSSSM '05, pp. 1042-1046. , IEEE; <br/>Luján-Mora, S., Trujillo, J., Song, I.-Y., A UML profile for multidimensional modeling in data warehouses (2006) Data &amp; Knowledge Engineering, 59 (3), pp. 725-769; <br/>Martin, R.C., (2003) Agile Software Development: Principles, Patterns, and Practices, , Prentice Hall PTR; <br/>Maté, A., Llorens, H., De Gregorio, E., Tardío, R., Gil, D., Muñoz-Terol, R., Trujillo, J., A novel multidimensional approach to integrate big data in business intelligence (2015) Journal of Database Management, 26 (2), pp. 14-31; <br/>Maté, A., Trujillo, J., A trace metamodel proposal based on the model driven architecture framework for the traceability of user requirements in data warehouses (2012) Information Systems, 37 (8), pp. 753-766; <br/>Maté, A., Trujillo, J., Tracing conceptual models evolution in data warehouses by using the model driven architecture (2014) Computer Standards &amp; Interfaces, 36 (5), pp. 831-843; <br/>Maté, A., Trujillo, J., Franch, X., Adding semantic modules to improve goal-oriented analysis of data warehouses using I- star (2014) Journal of Systems and Software, 88, pp. 102-111; <br/>Messaoud, R.B., Boussaid, O., Rabaséda, S., A new OLAP aggregation based on the AHC technique (2004) Proceedings of the 7th ACM International Workshop on Data Warehousing and OLAP, pp. 65-72. , ACM; <br/>Pedersen, T.B., Jensen, C.S., Dyreson, C.E., A foundation for capturing and querying complex multidimensional data (2001) Information Systems, 26 (5), pp. 383-423; <br/>Pérez-Martínez, J.M., Berlanga-Llavoria, R., Aramburu-Caboa, M.J., Pedersen, T.B., Contextualizing data warehouses with documents (2008) Decision Support Systems, 45 (1), pp. 77-94; <br/>Phipps, C., Davis, K.C., Automating data warehouse conceptual schema design and evaluation (2002) Proceedings of the 4th International Workshop on Design and Management of Data Warehouses, , Citeseer; <br/>Ralph, C.J., Scott, J.M., (1981) Estimating Numbers of Terrestrial Birds; <br/>Rehman, N.U., Mansmann, S., Weiler, A., Scholl, M.H., Discovering dynamic classification hierarchies in OLAP dimensions (2012) Proceedings of the 20th International Conference on Foundations of Intelligent Systems, LNCS, 7661, pp. 425-434. , Springer-Verlag; <br/>Rizzi, S., UML-based conceptual modeling of pattern-bases (2004) Proceedings of the International Workshop on Pattern Representation and Management; <br/>Romero, O., Abelló, A., A survey of multidimensional modeling methodologies (2009) International Journal of Data Warehousing and Mining, 5 (2), pp. 1-23; <br/>Romero, O., Abelló, A., Automatic validation of requirements to support multidimensional design (2010) Data &amp; Knowledge Engineering, 69 (9), pp. 917-942; <br/>Sautot, L., Bimonte, S., Journaux, L., Faivre, B., A methodology and tool for rapid prototyping of data warehouses using data mining: Application to birds biodiversity (2014) Model and Data Engineering, LNCS, 8748, pp. 250-257. , Springer; <br/>Sautot, L., Faivre, B., Journaux, L., Molin, P., The hierarchical agglomerative clustering with gower index: A methodology for automatic design of olap cube in ecological data processing context (2015) Ecological Informatics, 26, pp. 217-230; <br/>Shih, P., Liu, C., Face detection using discriminating feature analysis and support vector machine (2006) Pattern Recognition, 39 (2), pp. 260-276; <br/>Song, Q., Hu, W., Xie, W., Robust support vector machine with bullet hole image classification (2002) IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews, 32 (4), pp. 440-448; <br/>Tebourski, W., Karâa, W.B.A., Ghezala, H.B., Semi-automatic data warehouse design methodologies: A survey (2013) Int. J. Comput. Sci. Issues IJCSI, 10 (5), pp. 48-54; <br/>Thenmozhi, M., Vivekanandan, K., A tool for data warehouse multidimensional schema design using ontology (2013) Int. J. Comput. Sci. Issues, 10 (2), pp. 161-168; <br/>Torlone, R., Conceptual multidimensional models (2003) Multidimensional Databases, pp. 69-90. , IGI Global; <br/>Tufféry, S., (2011) Data Mining and Statistics for Decision Making, , John Wiley &amp; Sons; <br/>Usman, M., Asghar, S., Fong, S., Data mining and automatic OLAP schema generation (2010) Proceedings of the 2010 Fifth International Conference on Digital Information Management ICDIM), pp. 35-43. , IEEE; <br/>Usman, M., Pears, R., A methodology for integrating and exploiting data mining techniques in the design of data warehouses (2010) Proceedings of the 2010 6th International Conference OnAdvanced Information Management and Service (IMS), pp. 361-367. , IEEE; <br/>Van Solingen, R., Basili, V., Caldiera, G., Rombach, H.D., Goal question metric (gqm) approach (2002) Encyclopedia of Software Engineering; <br/>Vapnik, V.N., An overview of statistical learning theory. Neural Networks (1999) IEEE Transactions on, 10 (5), pp. 988-999; <br/>Ward, J.H., Jr., Hierarchical grouping to optimize an objective function (1963) Journal of the American Statistical Association, 58 (301), pp. 236-244; <br/>Zhang, C., Huang, Y., Cluster By: A new SQL extension for spatial data aggregation (2007) Proceedings of the 15th Annual ACM International Symposium on Advances in Geographic Information Systems, p. 53. , ACM; <br/>Zubcoff, J., Pardillo, J., Trujillo, J., Integrating clustering data mining into the multidimensional modeling of data warehouses with UML profiles (2007) Data Warehousing and Knowledge Discovery, pp. 199-208. , Springer; <br/>Zubcoff, J., Pardillo, J., Trujillo, J., A UML profile for the conceptual modelling of data-mining with time-series in data warehouses (2009) Information and Software Technology, 51 (6), pp. 977-992},
   pages = {1-35},
   publisher = {IGI Global},
   title = {Multidimensional model design using data mining: A rapid prototyping methodology},
   volume = {13},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008967532&doi=10.4018%2FIJDWM.2017010101&partnerID=40&md5=5e0c1fa56b9ace8d783d27b027736eb0},
   year = {2017},
}
@inproceedings{Paes2011,
   abstract = {i* (iStar) framework is one of the most accepted goal-oriented requirement modeling framework. Different research groups have developed their own variant/dialect of the i* modeling language, including their own supporting tools. We understand that these variants are part of a family of languages based on i*. In this paper, we compare various i* based languages to identify their common and variable characteristics. We then propose a core metamodel and a process to configure it to generate graphical editors for any i* based language through a CASE Tool. As a running example, we show how to derive the metamodel for the Aspectual i* modeling language and generate a graphic editor for this language.},
   author = {J. Paes and C. Lima and E. Santos and C. Silva and J. Castro},
   city = {Rio de Janeiro},
   journal = {14th Ibero-American Conference on Software Engineering and 14th Workshop on Requirements Engineering, CIbSE 2011},
   keywords = {Automatic Generation,Goal oriented requirements engineering,Goal-oriented requirements engineering,Graphical editors,Metamodeling,Requirement modeling,Requirements engineering,Research groups,Software Product Line,Software engineering,Software product lines,Variability},
   note = {Conference code: 100506<br/>Cited By :2<br/>Export Date: 21 August 2022<br/>Correspondence Address: Centro de Informática, , Recife, Brazil<br/>References: Yu, E., Castro, J., Perini, A., Strategic Actors Modeling with i*. Tutorial Notes (2008) 16th Intl. Conf. On Requirements Engineering, Spain: IEEE Computer Society, pp. 1-60; <br/>Franch, X., Incorporating Modules into the i*Framework (2010) Proceedings of 22th International Conference, pp. 454-469; <br/>Grau, G., (2010) IStar Guide, , http://istar.rwth-aachen.de/tiki-view_articles.php; <br/>Amyot, D., Horkoff, J., Gross, D., Mussbacher, G., A Lightweight GRL Profile for i* Modeling (2009) Proceedings of RIGIM 2009, pp. 254-264; <br/>Susi, A., Perini, A., Mylopoulos, J., The Tropos metamodel and its use (2005) Informatica, 29, pp. 401-408; <br/>Silva, C., Borba, C., Castro, J., G2SPL: Um Processo de Engenharia de Requisitos Orientada a Objetivos para Linhas de Produtos de Software (2010) Proceeding of the 13th Workshop On Requirements Engineering, pp. 1-11; <br/>Alencar, F., Castro, J., Lucena, M., Santos, E., Silva, C., Araújo, J., Moreira, A., Towards Modular i* Models (2010) Requirement Engineering Track, At 25th ACM Symposium On Applied Computing, pp. 292-297. , SAC 2010, Sierre, Switzerland: ACM Press; <br/>Pohl, K., Bockle, G., Linden, F., (2005) Software Product Line Engineering, , Springer-Verlag Berlin Heidelberg; <br/>(2010) Graphical Modelling Framework, , http://www.eclipse.org/modeling/gmp, GMF; <br/>(2010) AGILE Project, , http://portal.cin.ufpe.br/ler/Projects/AGILE.aspx; <br/>(2010) Object Management Group: Object Constraint Language (OCL), , http://www.omg.org/spec/OCL/2.2/, Version 2.2; <br/>Ayala, C., Cares, C., Carvallo, J., Grau, G., Haya, M., Salazar, G., Franch, X., Quer, C., A Comparative Analysis of i*-Based Agent-Oriented Modeling Languages (2005) Proceedings of SEKE 05, pp. 43-50<br/><br/><b>From Duplicate 2 (<i>Agile: Automatic generation of i* languages</i> - Paes, J.; Lima, C.; Santos, E.; Silva, C.; Castro, J.)<br/></b><br/><b>From Duplicate 2 (<i>Agile: Automatic generation of i* languages</i> - Paes, J; Lima, C; Santos, E; Silva, C; Castro, J)<br/></b><br/>cited By 2; Conference of 14th Ibero-American Conference on Software Engineering, CIbSE 2011 and 14th Workshop on Requirements Engineering, WER 2011 ; Conference Date: 27 April 2011 Through 29 April 2011; Conference Code:100506},
   pages = {239-244},
   title = {Agile: Automatic generation of i* languages},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886702743&partnerID=40&md5=13f312ad23a3c282f52f1f51f862f591},
   year = {2011},
}
@article{Kuhar2021,
   abstract = {Numerous visual notations are present in technical and business domains. Notations have to be cognitively effective to ease the planning, documentation, and communication of the domains’ concepts. Semantic transparency (ST) is one of the elementary principles that influence notations’ cognitive effectiveness. However, the principle is criticized for not being well defined and challenges arise in the evaluations and applications of ST. Accordingly, this research’s objectives were to answer how the ST principle is defined, operationalized, and evaluated in present notations as well as applied in the design of new notations in ICT and related areas. To meet these objectives, a systematic literature review was conducted with 94 studies passing the selection process criteria. The results reject one of the three aspects, which define semantic transparency, namely “ST is achieved with the use of icons.” Besides, taxonomies of related concepts and research methods, evaluation metrics, and other findings from this study can help to conduct verifiable ST-related experiments and applications, consequently improving the visual vocabularies of notations and effectiveness of the resulting diagrams.},
   author = {S. Kuhar and G. Polančič},
   doi = {10.1007/s10270-021-00888-9},
   issn = {16191366},
   issue = {6},
   journal = {Software and Systems Modeling},
   keywords = {Business domain,Cognitive effectiveness,Evaluation metrics,Process criterion,Semantic transparencies,Semantic transparency,Semantics,Systematic literature review,Transparency,Visual notations,Visual vocabularies},
   note = {<b>From Duplicate 1 (<i>Conceptualization, measurement, and application of semantic transparency in visual notations: A systematic literature review</i> - Kuhar, S.; Polančič, G.)<br/></b><br/><b>From Duplicate 1 (<i>Conceptualization, measurement, and application of semantic transparency in visual notations: A systematic literature review</i> - Kuhar, S; Polančič, G)<br/></b><br/>cited By 0<br/><br/><br/>Export Date: 21 August 2022<br/>Correspondence Address: Kuhar, S.; Faculty of Electrical Engineering and Computer Science, Koroška cesta 46, Slovenia; email: sasa.kuhar@um.si<br/>References: Abraham, R., Guidelines for architecture models as boundary objects (2017) Architectural Coordination of Enterprise Transformation, pp. 193-210. , https://doi.org/10.1007/978-3-319-69584-6_19, Proper, H.A., Winter, R., Aier, S., de Kinderen, S, Springer, Cham; <br/>Akiki, P.A., CHAIN: Developing model-driven contextual help for adaptive user interfaces (2018) J. Syst. Softw., 135, pp. 165-190; <br/>Algablan, A.S., Some, S.S., A visual syntax for Larman’s operation contracts (2016) 2016 International Conference on Engineering &amp; MIS (ICEMIS), pp. 1-9. , https://doi.org/10.1109/ICEMIS.2016.7745358; <br/>Almorsy, M., Grundy, J., SecDSVL: A domain-specific visual language to support enterprise security modelling (2014) 2014 23Rd Australian Software Engineering Conference, pp. 152-161. , https://doi.org/10.1109/ASWEC.2014.18; <br/>Amrit, C., Tax, N., Towards understanding the understandability of uml models (2014) Proceedings of the 6Th International Workshop on Modeling in Software Engineering, Mise 2014, pp. 49-54. , https://doi.org/10.1145/2593770.2593779, Association for Computing Machinery, New York, NY, USA; <br/>Andersson, A., Krogstie, J., Implementation and first evaluation of a molecular modeling Language (2015) Enterprise, Business-Process and Information Systems Modeling, 214, pp. 293-308. , https://doi.org/10.1007/978-3-319-19237-6_19, Gaaloul, K., Schmidt, R., Nurcan, S., Guerreiro, S., Ma, Q, Springer, Cham; <br/>A framework for empirical evaluation of model comprehensibility (2007) International Workshop on Modeling in Software Engineering (MISE’07: ICSE Workshop 2007), P., p. 7; <br/>Bajaj, A., Rockwell, S., (2004) COGEVAL: A Propositional Framework Based on Cognitive Theories to Evaluate Conceptual Models, , undefined; <br/>Basak Aydemir, F., Giorgini, P., Mylopoulos, J., Multi-objective risk analysis with goal models (2016) 2016 IEEE Tenth International Conference on Research Challenges in Information Science (RCIS), Vol. 2016-Augus, pp. 1-10. , https://doi.org/10.1109/RCIS.2016.7549302, IEEE; <br/>Batini, C., Ceri, S., Navathe, S.B., (1992) Conceptual Database Design: An Entity-Relationship Approach, , Benjamin-Cummings Publishing Co. Inc, Redwood City; <br/>Bayrak, G., Ocker, F., Vogel-Heuser, B., Evaluation of selected control programming languages for process engineers by means of cognitive effectiveness and dimensions (2017) J. Softw. Eng. Appl., 10 (5), pp. 457-481; <br/>van Den Bergh, J., Luyten, K., Coninx, K., Cap3: Context-sensitive abstract user interface specification (2011) Proceedings of the 3Rd ACM SIGCHI Symposium on Engineering Interactive Computing Systems, EICS ’11, pp. 31-40. , https://doi.org/10.1145/1996461.1996491, Association for Computing Machinery, New York, NY, USA; <br/>Bhattacherjee, A., Social Science Research: Principles, Methods, and Practices, 2nd edn (2012) Global Text Project; <br/>Blackwell, A., Green, T., Notational systems-the cognitive dimensions of notations framework (2003) HCI Models, Theories, and Frameworks: Toward a Multidisciplinary Science, pp. 103-133. , https://doi.org/10.1016/B978-155860808-5/50005-8; <br/>Bocanegra, J., Pavlich-Mariscal, J., Carrillo-Ramos, A., DMLAS: A Domain-Specific Language for designing adaptive systems (2015) 2015 10Th Computing Colombian Conference (10CCC), pp. 47-54. , https://doi.org/10.1109/ColumbianCC.2015.7333411, IEEE; <br/>Boone, S., Bernaert, M., Roelens, B., Mertens, S., Poels, G., Evaluating and Improving the Visualisation of CHOOSE, an Enterprise Architecture Approach for SMEs (2014) Lecture Notes in Business Information Processing, 197, pp. 87-102. , https://doi.org/10.1007/978-3-662-45501-2_7, Springer; <br/>Bork, D., Schrüffer, C., Karagiannis, D., Intuitive understanding of domain-specific modeling languages: proposition and application of an evaluation technique (2019) Conceptual Modeling, pp. 311-319. , Laender AHF, Pernici B, Lim EP, Oliveira JPM, (eds), Springer, Cham; <br/>Breitenbücher, U., Binz, T., Kopp, O., Leymann, F., Schumm, D., Vino4TOSCA: A Visual Notation for Application Topologies Based on TOSCA (2012) On the Move to Meaningful Internet Systems: OTM 2012, 7565, pp. 416-424. , https://doi.org/10.1007/978-3-642-33606-5_25, Hutchison, D., Kanade, T., Kittler, J., Kleinberg, J.M., Mattern, F., Mitchell, J.C., Naor, M., Nierstrasz, O., Rangan, C Pandu, Steffen, B., Sudan, M., Terzopoulos, D., Tygar, D., Vardi, M.Y., Weikum, G., Meersman, R., Panetto, H., Dillon, T., Rinderle-Ma, S., Dadam, P., Zhou, X., Pearson, S., Ferscha, A., Bergamaschi, S., Cruz, I.F. (eds.), Springer Berlin Heidelberg, Berlin, Heidelberg; <br/>Breitenbücher, U., Hirmer, P., Képes, K., Kopp, O., Leymann, F., Wieland, M., A situation-aware workflow modelling extension (2015) Proceedings of the 17Th International Conference on Information Integration and Web-Based Applications &amp; Services, Iiwas ’15, pp. 1-7. , https://doi.org/10.1145/2837185.2837248, Association for Computing Machinery, New York, NY, USA; <br/>Britton, C., Jones, S., Untrained eye: how languages for software specification support understanding in untrained users (1999) Human-Comput. Interact., 14 (1), pp. 191-244; <br/>Britton, C., Jones, S., Kutar, M., Loomes, M., Robinson, B., Evaluating the intelligibility of diagrammatic languages used in the specification of software (2000) Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 1889, pp. 376-391. , https://doi.org/10.1007/3-540-44590-0_32, Springer Verlag; <br/>Brochado De Miranda, T.R., (2017) Software Language Engineering: Interaction and Usability Modeling of Language Editors, , Ph.D. thesis; <br/>Burkhard, R.A., Knowledge visualization The use of complementary visual representations for the transfer of knowledge (2005) A Model, a Framework, and Four New Approaches. Ph.D. Thesis, SWISS FEDERAL INSTITUTE OF TECHNOLOGY ZÜRICH, Zurich, , https://doi.org/10.3929/ethz-a-005004486; <br/>Caire, P., Genon, N., Heymans, P., Moody, D.L., Visual notation design 2.0: Towards user comprehensible requirements engineering notations (2013) 2013 21St IEEE International Requirements Engineering Conference (RE), pp. 115-124. , https://doi.org/10.1109/RE.2013.6636711, IEEE; <br/>https://dictionary.cambridge.org/, Cambridge University Press, Cambridge Dictionary, English Dictionary; Canché, M., Ochoa, S.F., Perovich, D., Gutierrez, F.J., Analysis of notations for modeling user interaction scenarios in ubiquitous collaborative systems (2019) J. Ambient Intell. Human. Comput.; <br/>Chandler, D., (2007) Semiotics the Basics, , http://analepsis.files.wordpress.com/2011/08/69249454-chandler-semiotics.pdf, Routledge, London; <br/>Chen, L., Babar, M.A., Zhang, H., Towards an evidence-based understanding of electronic data sources (2010) Proceedings of the 14Th International Conference on Evaluation and Assessment in Software Engineering, EASE’10, pp. 135-138. , BCS Learning &amp; Development Ltd., Swindon, GBR; <br/>Claes, J., Vanderfeesten, I., Pinggera, J., Reijers, H.A., Weber, B., Poels, G., A visual analysis of the process of process modeling (2015) IseB, 13 (1), pp. 147-190; <br/>Constantine, L., Henderson-Sellers, B., Notation Matters: Part 1 - Framing the Issues (1995) Tech. Rep.; <br/>Cortes-Cornax, M., Dupuy-Chessa, S., Rieu, D., Mandran, N., Evaluating the appropriateness of the BPMN 2.0 standard for modeling service choreographies: using an extended quality framework (2016) Softw. Syst. Model., 15 (1), pp. 219-255; <br/>da Silva Teixeira, M.D.G., de Almeida Falbo, R., Guizzardi, G., Can Ontologies Systematically Help in the Design of Domain-Specific Visual Languages? (2013) Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), Vol. 8185 LNCS, pp. 737-754. , https://doi.org/10.1007/978-3-642-41030-7_54, Springer; <br/>Da, S.T., Quirino, G.K., Gailly, F., Falbo, R.D.A., Guizzardi, G., Barcellos, M.P., PoN-S: A systematic approach for applying the physics of notation (PoN) (2016) Lecture Notes in Business Information Processing, 248, pp. 432-447. , https://doi.org/10.1007/978-3-319-39429-9_27; <br/>Dangarska, Z., Figl, K., Mendling, J., An Explorative Analysis of the Notational Characteristics of the Decision Model and Notation (DMN) (2016) 2016 IEEE 20Th International Enterprise Distributed Object Computing Workshop (EDOCW), pp. 1-9. , https://doi.org/10.1109/EDOCW.2016.7584345; <br/>Del-Río-Ortega, A., Resinas, M., Durán, A., Bernárdez, B., Ruiz-Cortés, A., Toro, M., Visual ppinot: a graphical notation for process performance indicators (2019) Bus. Inform. Syst. Eng., 61 (2), pp. 137-161; <br/>Diamantopoulou, V., Mouratidis, H., Applying the physics of notation to the evaluation of a security and privacy requirements engineering methodology (2018) Inform. Comput. Secur., 26 (4), pp. 382-400; <br/>Diprose, J.P., Macdonald, B.A., Hosking, J.G., Ruru: A spatial and interactive visual programming language for novice robot programming (2011) 2011 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC), pp. 25-32. , https://doi.org/10.1109/VLHCC.2011.6070374; <br/>Djurica, D., Mendling, J., Figl, K., The impact of associative coloring and representational formats on decision-making: An eye-tracking study (2020) Information Systems and Neuroscience, pp. 305-313. , Davis FD, Riedl R, Brocke J, Léger PM, Randolph A, Fischer T, (eds), Springer, Cham; <br/>Dobesova, Z., Using the physics of notation to analyse model builder diagrams (2013) Int. Multidiscip. Sci. GeoConf. Surv. Geolo. Min. Ecol. Manag. SGEM, 1, pp. 595-602; <br/>Dobesova, Z., Visual Language for geodatabase design (2013) Int. Multidiscip. Sci. GeoConf. Surv. Geolo. Min. Ecol. Manag. SGEM, 1, pp. 603-610;},
   pages = {2155-2197},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Conceptualization, measurement, and application of semantic transparency in visual notations: A systematic literature review},
   volume = {20},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106307212&doi=10.1007%2Fs10270-021-00888-9&partnerID=40&md5=13dc5daa4dd1a060ad6f75d8ef883143},
   year = {2021},
}
@article{Goncalves2020,
   author = {E Goncalves and João Araujo and Jaelson Castro and Enyo Gonçalves and João Araujo and Jaelson Castro},
   issn = {0164-1212},
   keywords = {Extension,FRAMEWORK,Goal modelling,Modelling language,Process,SECURE TROPOS,iStar},
   month = {10},
   note = {Times Cited in Web of Science Core Collection: 4<br/>Total Times Cited: 4<br/>Cited Reference Count: 70},
   pages = {110649},
   publisher = {Elsevier},
   title = {PRISE: A process to support iStar extensions},
   volume = {168},
   year = {2020},
}
@article{Shahin2021,
   abstract = {Software architecture is critical in succeeding with Development and Operations (DevOps). However, designing software architectures that enable and support DevOps (DevOps-driven software architectures) is a challenge for organizations. We assert that one of the essential steps towards characterizing DevOps-driven architectures is to understand architectural design issues raised in DevOps. At the same time, some of the architectural issues that emerge in the DevOps context (and their corresponding architectural practices or tactics) may stem from the context (i.e., domain) and characteristics of software organizations. To this end, we conducted a mixed-methods study that consists of a qualitative case study of two teams in a company during their DevOps transformation and a content analysis of Stack Overflow and DevOps Stack Exchange posts to understand architectural design issues in DevOps. Our study found eight specific and contextual architectural design issues faced by the two teams and classified architectural design issues discussed in Stack Overflow and DevOps Stack Exchange into 11 groups. Our aggregated results reveal that the main characteristics of DevOps-driven architectures are being loosely coupled and prioritizing deployability, testability, supportability, and modifiability over other quality attributes. Finally, we discuss some concrete implications for research and practice.},
   author = {Mojtaba Shahin and Ali Rezaei Nasab and Muhammad Ali Babar},
   doi = {10.1002/SMR.2379},
   journal = {Journal of Software: Evolution and Process},
   keywords = {DevOps,Stack Overflow,continuous delivery,qualitative study,software architecture},
   pages = {e2379},
   title = {A qualitative study of architectural design issues in DevOps},
   url = {https://onlinelibrary-wiley.ez67.periodicos.capes.gov.br/doi/full/10.1002/smr.2379 https://onlinelibrary-wiley.ez67.periodicos.capes.gov.br/doi/abs/10.1002/smr.2379 https://onlinelibrary-wiley.ez67.periodicos.capes.gov.br/doi/10.1002/smr.2379},
   year = {2021},
}
@article{Daun2021,
   author = {Marian Daun and Jennifer Brings and Lisa Krajinski and Viktoria Stenkova and Torsten Bandyszak},
   city = {ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES},
   doi = {10.1007/s00766-021-00347-3},
   issn = {0947-3602},
   issue = {3},
   journal = {REQUIREMENTS ENGINEERING},
   keywords = {Collaborative cyber-physical systems,EARLY REQUIREMENTS,FRAMEWORK,GOAL,GRL,Goal modeling,TROPOS,iStar},
   month = {9},
   note = {Times Cited in Web of Science Core Collection: 1<br/>Total Times Cited: 1<br/>Cited Reference Count: 112},
   pages = {325-370},
   publisher = {SPRINGER},
   title = {A GRL-compliant iStar extension for collaborative cyber-physical systems},
   volume = {26},
   year = {2021},
}
@inproceedings{Wang2021,
   abstract = {User stories have been increasingly adopted in practice due to their intuitive structure, which specify users needs for the software system from their perspectives. As iStar modeling framework also emphasizes user perspectives, some of its concepts and relationships can potentially be aligned with user stories. In this paper, we propose to (semi-) automatically derive iStar models based on user stories, with the aim of facilitating iStar modeling and promoting the practical adoption of iStar models. Specifically, this paper focuses on investigating an appropriate way of balancing the manual and automatic analysis during the modeling process, based on which we present a visionary framework.},
   author = {Chunhui Wang and Chao Wu and Tong Li and Zhiguo Liu},
   editor = {M Ruiz and T Li and V Pant},
   isbn = {16130073 (ISSN)},
   journal = {CEUR Workshop Proceedings},
   keywords = {Automatic analysis,Istar,Manual analysis,Model construction,Model-based OPC,Modelling framework,Software-systems,User need,User stories,User story,Users perspective,iStar},
   note = {Conference code: 172524<br/>Export Date: 21 August 2022<br/>Funding details: 2021MS06024<br/>Funding details: Beijing Municipal Commission of Education<br/>Funding text 1: This work is supported by the Project of Beijing Municipal Education Commission (No.KM202110005025) and the Natural Science of Foundation of Inner Mongolia Province (No.2021MS06024).<br/>References: Cohn, M., (2004) user story applied for agile software development, , Addison-Wesley Professional; <br/>Horkoff, J., Aydemir, F. B., Cardoso, E., Li, T., Maté, A., Paja, E., Salnitri, M., Giorgini, P., Goal-oriented requirements engineering: An extended systematic mapping study (2019) Requirements Engineering, 24, pp. 133-160; <br/>Wautelet, Y., Heng, S., Kolp, M., Mirbel, I., Poelmans, S., Building a rationale diagram for evaluating user story sets (2016) 2016 IEEE Tenth International Conference on Research Challenges in Information Science (RCIS), pp. 1-12; <br/>Lin, J., Yu, H., Shen, Z., Miao, C., Using goal net to model user stories in agile software development (2014) 15th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing, SNPD 2014, pp. 1-6. , Las Vegas, NV, USA, June 30-July 2, IEEE Computer Society, 2014; <br/>Göne, T., Aydemir, F. B., Automated goal model extraction from user stories using nlp (2020) IEEE Requirements Engineering Conference; <br/>Raharjana, I. K., Siahaan, D., Fatichah, C., User stories and natural language processing: A systematic literature review (2021) IEEE Access, 9, pp. 53811-53826; <br/>Patton, J., Economy, W. P., Fowler, F., Cooper, A., Cagan, A. M., User story mapping : discover the whole story, build the right product, O? (2014), Reilly Media; Robeer, M., Lucassen, G., Dalpiaz, F., Brinkkemper, S., Automated extraction of conceptual models from user stories via nlp (2016) Requirements Engineering Conference; <br/>Meryem Elallaoui, A., Khalid Nafil, B., Raja Touahni, A., Automatic transformation of user stories into uml use case diagrams using nlp techniques (2018) International Conference on Ambient Systems, Networks and Technologies, pp. 42-49; <br/>Patton, J., Telling better user stories (2009) The software testing and quality engineering magazine, 11, pp. 24-29; <br/>Dalpiaz, F., Franch, X., Horkoff, J., istar 2.0 language guide (2016) Computing Research Repository, pp. 1-15; <br/>Lucassen, G., Dalpiaz, F., van der Werf, J. M. E. M., Brinkkemper, S., Improving agile requirements: The quality user story framework and tool (2016) Requirements Engineering, 21, pp. 383-403; <br/>Wang, C., Jin, Z., Zhao, H., Chui, M., An approach for improving the quality of user stories (2021) Journal of Computer Research and Development, 58, pp. 731-748; <br/>Grau, G., Franch, X., Mayol, E., Ayala, C. P., Cares, C., Haya, M., Navarrete, F., Quer, C., Risd: A methodology for building i-strategic dependency models (2005) International Conference on Software Engineering and Knowledge Engineering, pp. 259-266},
   pages = {35-41},
   publisher = {CEUR-WS},
   title = {A Preliminary Framework for Constructing iStar Models from User Stories},
   volume = {2983},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118235178&partnerID=40&md5=732e87c57eec06b3844fb6dc973864da http://ceur-ws.org/Vol-2983/iStar21_paper_6.pdf},
   year = {2021},
}
@inproceedings{Paes2011,
   abstract = {i* is goal-oriented requirement modeling framework with an increasing use in industry and academy. One of the main challenges in adopting this framework, is the diversity of variants/dialects of the i* modeling language. These variants were created by different research groups to address their particular purposes and are supported by specific CASE tools. Considering them, it is possible to identify a set of common modeling elements, as well as a set of different modeling elements. We understand that these variants are part of the same family of i* based languages. Hence, a specific i* based language can be obtained from an i* language family, similarly to a product obtained from a software product line. To define the core assets of such i* language family, we identify their common and variable characteristics by comparing various i* based languages. From this comparison, we propose a core metamodel for the i* language family and a process to configure it to generate graphical editors for any i* based language. As a running example, we show how to derive the metamodel for the Aspectual i* modeling language and generate a graphic editor for this language. © 2011 IEEE.},
   author = {J. Paes and J. Castro and C. Silva and E. Santos and C. Lima and FAPESP; CNPq; Google; Mack Pesquisa; CAPES; PRCEU-USP},
   city = {Sao Paulo},
   doi = {10.1109/SBES.2011.14},
   isbn = {9780769546032},
   journal = {Proceedings - 25th Brazilian Symposium on Software Engineering, SBES 2011},
   keywords = {Core asset,Goal Oriented Requirements Engineering,Goal-oriented,Goal-oriented requirements engineering,Graphic editors,Graphical editors,Meta model,Metamodeling,Modeling elements,Modeling languages,Network architecture,Requirement modeling,Requirements engineering,Research groups,Software Product Line,Software Product Lines,Software design,Variability},
   note = {<b>From Duplicate 1 (<i>An approach to generate tools for i* languages</i> - Paes, J.; Castro, J.; Silva, C.; Santos, E.; Lima, C.)<br/></b><br/><b>From Duplicate 1 (<i>An approach to generate tools for i* languages</i> - Paes, J; Castro, J; Silva, C; Santos, E; Lima, C)<br/></b><br/>cited By 1; Conference of 25th Brazilian Symposium on Software Engineering, SBES 2011 ; Conference Date: 28 September 2011 Through 30 September 2011; Conference Code:87461<br/><br/><br/>Conference code: 87461<br/>Cited By :1<br/>Export Date: 21 August 2022<br/>Correspondence Address: Paes, J.; Centro de Informática, , Recife, Brazil; email: jpsj2@cin.ufpe.br<br/>References: Yu, E., (1995) Modeling Strategic Relationships for Process Reengineering, , Ph.D. thesis, Department of Computer Science, University of Toronto, Canada; <br/>Franch, X., Incorporating Modules into the i*Framework (2010) LNCS, 6051, pp. 439-454. , Proceedings of 22th Conference on Advanced Information Systems Engineering (CAiSE'11). Springer Berlin / Heidelberg, 2010; <br/>Annosi, M.C., De Pascale, A., Gross, D., Yu, E., Analyzing Software Process Alignment with Organizational Business Strategies using an Agent- And Goal-oriented Analysis Technique - an Experience Report (2008) Proceedings of the 3rd International I*Workshop. CEUR Workshop Proceedings, 322, pp. 9-12. , Recife, Brazil; <br/>Carvallo, J.P., Franch, X., On the use of i*for Architecting Hybrid Systems: A Method and an Evaluation Report (2009) LNBIP, 39, pp. 38-53. , Proceedings of the 2nd Working Conference on The Practice of Enterprise Modeling (PoEM'09). Springer Berlin / Heidelberg; <br/>Grau, G., iStar Guide, , http://istar.rwthaachen.de/tiki-view_articles.php, Available at: Last access: July 2010; <br/>Amyot, D., Horkoff, J., Gross, D., Mussbacher, G., A Lightweight GRL Profile for i*Modeling (2009) LNCS, 5833, pp. 254-264. , Proceedings of the Workshop on Requirements, Intentions and Goals in Conceptual Modeling (RiGIM'09) at 29th International Conference on Conceptual Modeling (ER'09), Springer Berlin / Heidelberg; <br/>Susi, A., Perini, A., Mylopoulos, J., Giorgini, P., The Tropos metamodel and its use (2005) Informatica (Ljubljana), 29 (4), pp. 401-408; <br/>Silva, C., Borba, C., Castro, J., A Goal Oriented Approach to Identify and Configure Feature Models for Software Product Lines Proceeding of the 13th Workshop on Requirements Engineering, Rio de Janeiro, Brazil, 2011, pp. 395-406; <br/>Alencar, F., Castro, J., Lucena, M., Santos, E., Silva, C., Araújo, J., Moreira, A., Towards Modular i*Models (2010) Requirement Engineering Track, at 25th ACM Symposium on Applied Computing, SAC 2010, pp. 292-297. , Sierre, Switzerland: ACM Press; <br/>Lucena, M., Santos, E., Silva, C., Alencar, F., Silva, M., Castro, J., Towards a Unified Metamodel for i* (2008) Proceedings of the 2nd Research Challenges in Information Science, , Marrakech, Marrocos.: IEEE Computer Society Press; <br/>Pohl, K., Bockle, G., Linden, F., (2005) Software Product Line Engineering, , Springer-Verlag Berlin Heidelberg; <br/>Linden, F., Schmid, K., Rommes, E., (2007) Software Product Lines in Action - The Best Industrial Practice in Product Line Engineering, , Paris: Springer Berlin / Heidelberg; <br/>Graphical Modelling Framework (GMF), , http://www.eclipse.org/modeling/gmp/, Available at: Last access: July 2010; <br/>AGILE Project, , http://portal.cin.ufpe.br/ler/Projects/AGILE.aspx, Available at: Last access: July 2010; <br/>Czarnecki, K., Eisenecker, U.W., (2000) Generative Programming: Methods Tools and Applications, , New York, NY, USA: ACM Press/Addison-Wesley Publishing Co; <br/>EMF, Eclipse Modeling Framework, , http://www.eclipse.org/emf, Available at: Last access: July 2010; <br/>Model Driven Architecture, , http://www.omg.org/mda/, Available at: Last access: July 2010; <br/>Paes, J., Lima, C., Santos, E., Silva, C., Castro, J., AGILE: Automatic Generation of i*Languages Proceedings of the 14th Ibero-american Conference on Software Engineering (CIbSE'11), Rio de Janeiro, Brazil, 2011; <br/>Object Constraint Language (OCL), , http://www.omg.org/spec/OCL/2.2/, Version 2.2, Available at: Last access: July 2010; <br/>Ayala, C., Cares, C., Carvallo, J., Grau, G., Haya, M., Salazar, G., Franch, X., Quer, C., A Comparative Analysis of i*-Based Agent-Oriented Modeling Languages Proceedings of the 17th International Conference on Software Engineering and Knowledge Engineering (SEKE'05), Taipei, Taiwan, 2005, pp. 43-50; <br/>Yu, E., Castro, J., Perini, A., Strategic Actors Modeling with i* (2008) 16th Intl. Conf. on Requirements Engineering, pp. 1-60. , Tutorial Notes, Spain: IEEE Computer Society; <br/>Bresciani, P., Perini, A., Giorgini, P., Giunchiglia, F., Mylopoulos, J., Tropos: An agent-oriented software development methodology (2004) Autonomous Agents and Multi-Agent Systems, 8 (3), pp. 203-236. , DOI 10.1023/B:AGNT.0000018806.20944.ef; <br/>Goal-oriented Requirement Language (GRL), , http://www.cs.toronto.edu/km/GRL/, Available at: Last access: July 2010},
   pages = {243-252},
   title = {An approach to generate tools for i* languages},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-82255180636&doi=10.1109%2FSBES.2011.14&partnerID=40&md5=d7dd0b14557ba2be243a8c3c1806a6b7},
   year = {2011},
}
@inproceedings{Franch2011,
   abstract = {The use of the i* (iStar) framework by the requirements engineering community is many-fold. Among the several possible engineering cases, we are particularly interested here in the joint use of i* with other modelling frameworks to obtain what we call i*-based frameworks. In this context, several challenges need to be overcome: theoretical, technical, methodological and community-related. In this paper, we review current i*-based frameworks under several possible scenarios and observe that not all of these challenges are always addressed, and even more, there is lack of guidelines or well-accepted methodological design steps on how to overcome these issues. Then, we detail the several engineering artifacts and techniques whose consideration in i*-based frameworks may help to overcome them. To illustrate the vision, we present the case of combining i* with data warehouse models, from the initial definition of the ontology to the final implementation using profiles. Finally, we provide a research agenda to apply the proposed vision including a final reflection on defining a maturity model as a convenient way to support forthcoming research in the topic. © 2011 IEEE.},
   author = {Xavier Franch and Alejandro Maté and Juan C. J.C. Trujillo and Carlos Cares and IEEE; IEEE Computer Society; Fondazione Bruno Kessler; Universita degli Studi di Trento; SIEMENS},
   city = {Trento},
   doi = {10.1109/RE.2011.6051642},
   isbn = {9781457709234},
   journal = {Proceedings of the 2011 IEEE 19th International Requirements Engineering Conference, RE 2011},
   keywords = {DW,Data warehouses,Design steps,Engineering,GORE,Goal-oriented requirements engineering,Iodine,Maturity model,Modelling framework,Ontology,Requirements engineering,Research agenda,data warehouse,goal-oriented requirements engineering,i,iStar},
   note = {Conference code: 87498<br/>Cited By :20<br/>Export Date: 21 August 2022<br/>Correspondence Address: Franch, X.; Univ. Politècnica de Catalunya (UPC), Barcelona, Spain; email: franch@essi.upc.edu<br/>References: Van Lamsweerde, A., Goal-Oriented Requirements Engineering ISRE 2001; <br/>Yu, E., (2011) Modelling Strategic Relationships for Process Reengineering, , 5; <br/>The I*Wiki, , http://istar.rwth-aachen.de, Last accessed: June 2011; <br/>Estrada, H., Martínez, A., Pastor, O., Mylopoulos, J., An Empirical Evaluation of the i*Framework in a Model-Based Software Generation Environment CAiSE 2006; <br/>Yu, E., Giorgini, P., Maiden, N., Mylopoulos, J., (2011) Social Modeling for Requirements Engineering, , The MIT Press; <br/>Cares, C., Franch, X., A Metamodelling Approach for i*Model Translations CAiSE 2011; <br/>Alencar, F.M.R., Marín, B., Giachetti, G., Pastor, O., Castro, J., Pimentel, J.H., From i*Requirements Models to Conceptual Models of a Model Driven Development Process PoEM 2009; <br/>Krishna, A., Vilkomir, S.A., Ghose, A.K., Consistency preserving Co-evolution of Formal Specifications and Agent-oriented Conceptual Models (2009) IST, 51 (2); <br/>Santander, V., Castro, J., Deriving Use Cases from Organizational Modeling RE 2002; <br/>Jiang, L., Topaloglou, T., Borgida, A., Mylopoulos, J., Incorporating Goal Analysis in Database Design: A Case Study from Biological Data Management RE 2006; <br/>Van Deursen, A., Klint, P., Visser, J., Domain-Specific Languages: An Annotated Bibliography (2000) ACM SIGPLAN Notices, 35 (6); <br/>Grau, G., Franch, X., A Goal-Oriented Approach for the Generation and Evaluation of Alternative Architectures RIGiM 2007; <br/>Esfahani, H.C., Cabot, J., Yu, E., Adopting Agile Methods: Can Goal-Oriented Social Modeling help? RCIS 2010; <br/>Gailly, F., España, S., Poels, G., Pastor, O., Integrating Business Domain Ontologies with Early Requirements Modelling RIGiM 2008; <br/>Siena, A., Perini, A., Susi, A., Mylopoulos, J., A Meta-Model for Modelling Law-Compliant Requirements RELAW 2009; <br/>Mouratidis, H., Giorgini, P., Secure Tropos: A Security-Oriented Extension of the Tropos Methodology (2007) IJSEKE, 17 (2); <br/>Giorgini, P., Rizzi, S., Garzetti, M., Goal-oriented Requirement Analysis for Data Warehouse Design DOLAP 2005; <br/>Estrada, H., Martínez, A., Pastor, O., Mylopoulos, J., Giorgini, P., Extending Organizational Modeling with Business Services Concepts: An Overview of the Proposed Architecture ER 2010; <br/>Liu, L., Chi, C., Yu, E., Strategic Capability Modeling of Services SOCCER 2006; <br/>Alencar, F., Castro, J., Moreira, A., Araújo, J., Silva, C., Ramos, R., Mylopoulos, J., Integration of Aspects with i*Models AOIS 2008; <br/>Giorgini, P., Mylopoulos, J., Perini, A., Susi, A., (2011) The Tropos Methodology and Software Development Environment, , 5; <br/>(2003) MDA Guide Version 1.0.1, , http://www.enterprisearchitecture.info/Images/MDA/MDA%20Guide%20v1-0-1. pdf, The OMG; <br/>Kimball, R., (2009) The Data Warehouse Toolkit, , Wiley-India; <br/>Mazón, J.-N., Pardillo, J., Trujillo, J., A Model-driven Goal-oriented Requirement Engineering Approach for Data Warehouses ER 2007; <br/>Prieto-Díaz, R., Domain Analysis: An Introduction (1990) ACM SIGSOFT SEN, 15 (2); <br/>Choi, N., Song, I.-Y., Han, H., A Survey on Ontology Mapping (2006) ACM SIGMOD Record, 35 (3); <br/>Franch, X., Fostering the Adoption of i*by Practitioners: Some Challenges and Research Directions (2010) Intentional Perspectives on Information Systems Engineering, , Springer; <br/>Moody, D.L., Heymans, P., Matulevièius, R., Visual Syntax does Matter: Improving the Cognitive Effectiveness of the i*Visual Notation (2010) REJ, 15 (2); <br/>Cabot, J., Yu, E., Improving Requirements Specifications in Model-Driven Development Processes ChaMDE 2008; <br/>Cares, C., Franch, X., Mayol, E., Quer, C., (2011) A Reference Model for I*, , 5; <br/>Alencar, F., Castro, J., Lucena, M., Santos, E., Silva, C., Araújo, J., Moreira, A., Towards Modular i*Models SAC 2010; <br/>Franch, X., Incorporating Modules into the i*Framework CAiSE 2010; <br/>Franch, X., Grau, G., Mayol, E., Quer, C., Ayala, C., Cares, C., Navarrete, F., Botella, P., Systematic Construction of i*Strategic Dependency Models for Socio-Technical Systems (2007) IJSEKE, 17 (1); <br/>Oliveira, A., Leite, J., Cysneiros, L.M., Lucena, C., I*Diagnoses: A Quality Process for Building i*Models CAiSE Forum 2008; <br/>Balasubramanian, S., Gupta, M., Structural Metrics for Goal Based Business Process Design and Evaluation (2005) BPMN, 11 (6); <br/>Franch, X., On the Quantitative Analysis of Agent-Oriented Models CAISE 2006; <br/>Franch, X., Grau, G., Quer, C., A Framework for the Definition of Metrics for Actor-Dependency Models RE 2004; <br/>Franch, X., A Method for the Definition of Metrics over i*Models CAiSE 2009; <br/>Serrano, M.A., Trujillo, J., Calero, C., Piattini, M., Metrics for Data Warehouse Conceptual Models Understandability (2007) IST, 49 (8); <br/>Vassiliadis, P., (2000) Data Warehouse Modeling and Quality Issues, , PhD thesis, Department of Electrical and Computer Engineering; <br/>Franch, X., Grau, G., Towards a Catalogue of Patterns for Defining Metrics over i*Models CAiSE 2008; <br/>Giorgini, P., Mylopoulos, J., Nicciarelli, E., Sebastiani, R., Formal Reasoning Techniques for Goal Models ER 2002; <br/>Horkoff, J., Yu, E., Qualitative, Interactive, Backward Analysis of i*Models iStar 2008; <br/>Wachsmuth, G., Metamodel Adaptation and Model Co-adaptation ECOOP 2007; <br/>Cares, C., Franch, X., Perini, A., Susi, A., Towards Interoperability of i*models using iStarML (2011) CSI, 33 (1); <br/>Colomer, D., López, L., Cares, C., Franch, X., Model Interchange and Tool Interoperability in the i*Framework: A Proof of Concept WER 2011; <br/>Grau, G., Franch, X., Ávila, S., J-PRiM: A Java Tool for a Process Reengineering i*Methodology RE 2006; <br/>Susi, A., Perini, A., Mylopoulos, J., Giorgini, P., The Tropos Metamodel and its Use (2005) Informatica, 29 (4); <br/>Amyot, D., Horkoff, J., Gross, D., Mussbacher, G., A Lightweight GRL Profile for i*Modeling RIGiM 2009, , G; <br/>Lucena, M., Santos, E., Silva, C., Alencar, F., Silva, M., Castro, J., Towards a Unified Metamodel for i* RCIS 2008; <br/>Brinkkemper, S., Method Engineering: Engineering of Information Systems Development Methods and Tools (1996) IST, 38 (4); <br/>Basili, V., Caldiera, G., Rombach, D., The Goal Question Metric approach (1994) Encyclopedia of Software Engineering, , J. Marciniak (ed.)},
   pages = {133-142},
   title = {On the joint use of i* with other modelling frameworks: A vision paper},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-82455212699&doi=10.1109%2FRE.2011.6051642&partnerID=40&md5=be6dc1e746a1094ec7b55052f51ef5fe},
   year = {2011},
}
@inproceedings{Debnath2021,
   abstract = {Requirements are elicited from the customer and other stakeholders through an iterative process of interviews, prototyping, and other interactive sessions. Many communication phenomena may emerge in these early iterations, that lead initial ideas to be transformed, renegotiated, or reframed. Understanding how this process takes place can help in solving possible communication issues as well as their consequences. In this work, we perform an exploratory study of descriptive nature to understand in which way requirements get transformed from initial ideas into documented needs. To this end, we select 30 subjects that act as requirements analysts, and we perform a set of elicitation sessions with a fictional customer. The customer is required to study a sample requirements document for a system beforehand and to answer the questions of the analysts about the system. After the elicitation sessions, the analysts produce user stories for the system. These are compared with the original ones by two researchers to assess to which extent and in which way the initial requirements evolved throughout the interactive sessions. Our results show that between 30% and 38% of the produced user stories include content that can be fully traced to the initial ones, while the rest of the content is dedicated to new requirements. We also show what types of requirements are introduced through the elicitation process, and how they vary depending on the analyst. Our work contributes to theory in requirements engineering, with empirically grounded, quantitative data, concerning the impact of elicitation activities with respect to initial ideas.},
   author = {Sourav Debnath and Paola Spoletini and Alessio Ferrari},
   doi = {10.1109/RE51729.2021.00028},
   editor = {A Moreira and K Schneider and M Vierhauser and J Cleland-Huang},
   isbn = {9781665428569},
   journal = {Proceedings of the IEEE International Conference on Requirements Engineering},
   keywords = {Empirical studies,Exploratory studies,Information,Interactive session,Iterative process,Requirements analyst,Requirements document,Requirements elicitation,Requirements engineering,Requirements evolution,Sales,User stories},
   note = {Conference code: 174516<br/>Export Date: 21 August 2022<br/>References: Zowghi, D., Coulin, C., Requirements elicitation: A survey of techniques, approaches, and tools (2005) Engineering and Managing Software Requirements, pp. 19-46. , Springer; <br/>Dieste, O., Juristo, N., Systematic review and aggregation of empirical studies on elicitation techniques (2010) IEEE Transactions on Software Engineering, 37 (2), pp. 283-304; <br/>Fernández, D.M., Wagner, S., Kalinowski, M., Felderer, M., Mafra, P., Vetro, A., Conte, T., Lassenius, C., Naming the pain in requirements engineering (2017) Empirical Software Engineering, 22 (5), pp. 2298-2338; <br/>Kamsties, E., Berry, D.M., Paech, B., Kamsties, E., Berry, D., Paech, B., Detecting ambiguities in requirements documents using inspections (2001) Proceedings of the First Workshop on Inspection in Software Engineering (WISE'01), pp. 68-80; <br/>Bano, M., Zowghi, D., Ferrari, A., Spoletini, P., Inspectors academy: Pedagogical design for requirements inspection training (2020) 2020 IEEE 28th International Requirements Engineering Conference (RE). IEEE, pp. 215-226; <br/>Walia, G.S., Carver, J.C., Using error abstraction and classification to improve requirement quality: Conclusions from a family of four empirical studies (2013) Empirical Software Engineering, 18 (4), pp. 625-658; <br/>Tjong, S.F., Berry, D.M., The design of sree-a prototype potential ambiguity finder for requirements specifications and lessons learned (2013) International Working Conference on Requirements Engineering: Foundation for Software Quality, pp. 80-95. , Springer; <br/>Gervasi, V., Ferrari, A., Zowghi, D., Spoletini, P., Ambiguity in requirements engineering: Towards a unifying framework (2019) From Software Engineering to Formal Methods and Tools, and Back, pp. 191-210. , Springer; <br/>Arora, C., Sabetzadeh, M., Briand, L.C., An empirical study on the potential usefulness of domain models for completeness checking of requirements (2019) Empirical Software Engineering, 24 (4), pp. 2509-2539; <br/>Ferrari, A., Dell'Orletta, F., Spagnolo, G.O., Gnesi, S., Measuring and improving the completeness of natural language requirements (2014) International Working Conference on Requirements Engineering: Foundation for Software Quality, pp. 23-38. , Springer; <br/>Ferrari, A., Spoletini, P., Donati, B., Zowghi, D., Gnesi, S., Interview review: Detecting latent ambiguities to improve the requirements elicitation process (2017) 2017 IEEE 25th International Requirements Engineering Conference (RE). IEEE, pp. 400-405; <br/>Salger, F., Requirements reviews revisited: Residual challenges and open research questions (2013) 2013 21st IEEE International Requirements Engineering Conference (RE). IEEE, pp. 250-255; <br/>Harker, S.D.P., Eason, K.D., Dobson, J.E., The change and evolution of requirements as a challenge to the practice of software engineering (1993) [1993] Proceedings of the IEEE International Symposium on Requirements Engineering, pp. 266-272; <br/>Crowne, M., Why software product startups fail and what to do about it. evolution of software product development in startup companies (2002) IEEE International Engineering Management Conference, 1, pp. 338-343; <br/>Grubb, A.M., Chechik, M., Looking into the crystal ball: Requirements evolution over time (2016) 2016 IEEE 24th International Requirements Engineering Conference (RE), pp. 86-95; <br/>Zowghi, D., Gervasi, V., On the interplay between consistency, completeness, and correctness in requirements evolution (2003) Information and Software Technology, 45 (14), pp. 993-1009; <br/>Alicia, G., Marsha, C., Formal reasoning for analyzing goal models that evolve over time (2021) Requirements Engineering; <br/>Hayes, J.H., Antoniol, G., Guéhéneuc, Y.-G., Prereqir: Recovering pre-requirements via cluster analysis (2008) 2008 15th Working Conference on Reverse Engineering. IEEE, pp. 165-174; <br/>Carreño, L.V.G., Winbladh, K., Analysis of user comments: An approach for software requirements evolution (2013) 2013 35th International Conference on Software Engineering (ICSE), pp. 582-591; <br/>Maalej, W., Kurtanovíc, Z., Nabil, H., Stanik, C., On the automatic classification of app reviews (2016) Requirements Engineering, 21 (3), pp. 311-331; <br/>Pagano, D., Maalej, W., User feedback in the appstore: An empirical study (2013) 2013 21st IEEE International Requirements Engineering Conference (RE). IEEE, pp. 125-134; <br/>Guzman, E., Ibrahim, M., Glinz, M., A little bird told me: Mining tweets for requirements and software evolution (2017) 2017 IEEE 25th International Requirements Engineering Conference (RE), pp. 11-20; <br/>Khan, J., Liu, L., Wen, L., Ali, R., (2019) Crowd Intelligence in Requirements Engineering: Current Status and Future Directions, pp. 245-261. , 03; <br/>Morales-Ramirez, I., Kifetew, F.M., Perini, A., Speech-acts based analysis for requirements discovery from online discussions (2019) Inf. Syst., 86, pp. 94-112. , https://doi.org/10.1016/j.is.2018.08.003; <br/>Ali, R., Dalpiaz, F., Giorgini, P., Silva Souza, V., (2011) Requirements Evolution: From Assumptions to Reality, 81, pp. 372-382. , 01; <br/>Robertson, J., Requirements analysts must also be inventors (2005) IEEE Software, 22 (1), p. 48; <br/>Lemos, J., Alves, C., Duboc, L., Rodrigues, G., A systematic mapping study on creativity in requirements engineering (2012) Proceedings of the ACM Symposium on Applied Computing, , 03; <br/>Maiden, N., Jones, S., Pitts, I.K., Neill, R., Zachos, K., Milne, A., (2010) Requirements Engineering As Creative Problem Solving: A Research Agenda for Idea Finding, pp. 57-66. , 09; <br/>Sternberg, R., Press, C.U., (1999) Handbook of Creativity, , https://books.google.com/books?id=d1KTEQpQ6vsC, Cambridge University Press; <br/>Nguyen, L., Shanks, G., A framework for understanding creativity in requirements engineering (2009) Information and Software Technology, 51 (3), pp. 655-662. , https://www.sciencedirect.com/science/article/pii/S0950584908001250; <br/>Maiden, N., Robertson, S., Integrating creativity into requirements processes: Experiences with an air traffic management system (2005) 13th IEEE International Conference on Requirements Engineering (RE'05), pp. 105-114; <br/>Herrmann, A., Mich, L., Berry, D.M., Creativity techniques for requirements elicitation: Comparing four-step epmcreate-based processes (2018) 2018 IEEE 7th International Workshop on Empirical Requirements Engineering (EmpiRE), pp. 1-7; <br/>Mahaux, M., Nguyen, L., Gotel, O., Mich, L., Mavin, A., Schmid, K., Collaborative creativity in requirements engineering: Analysis and practical advice (2013) IEEE 7th International Conference on Research Challenges in Information Science (RCIS); <br/>Grube, P.P., Schmid, K., Selecting creativity techniques for innovative requirements engineering (2008) 2008 Third International Workshop on Multimedia and Enjoyable Requirements Engineering-Beyond Mere Descriptions and with More Fun and Games, pp. 32-36; <br/>Svensson, R.B., Taghavianfar, M., Selecting creativity techniques for creative requirements: An evaluation of four techniques using creativity workshops (2015) 2015 IEEE 23rd International Requirements Engineering Conference (RE), pp. 66-75; <br/>Horkoff, J., Maiden, N., Lockerbie, J., Creativity and goal modeling for software requirements engineering (2015) Ser. C&amp;C '15. ACM, pp. 165-168; <br/>Horkoff, J., Maiden, N.A.M., Creative leaf: A creative istar modeling tool (2016) Proceedings of the Ninth International i Workshop, Ser. CEUR Workshop Proceedings, 1674, pp. 25-30. , CEUR-WS. org; <br/>Davis, A., Dieste, O., Hickey, A., Juristo, N., Moreno, A.M., Effectiveness of requirements elicitation techniques: Empirical results derived from a systematic review (2006) 14th IEEE International Requirements Engineering Conference (RE'06), pp. 179-188; <br/>Dalpiaz, F., Requirements data sets (user stories) Mendeley Data, V1, , http://dx.doi.org/10.17632/7zbk8zsd8y.1; <br/>Bano, M., Zowghi, D., Ferrari, A., Spoletini, P., Donati, B., Teaching requirements elicitation interviews: An empirical study of learning from mistakes (2019) Requirements Engineering, 24 (3), pp. 259-289; <br/>Pitts, M., Browne, G., Stopping behavior of systems analysts during information requirements elicitation (2004) J. of Management Information Systems, 21 (6), pp. 203-226; <br/>Whittenberger, A., (2014) The Top 8 Mistakes in Requirements Elicitation, , https://www.batimes.com/articles/the-top-8-mistakes-in-requirements-elicitation.html; <br/>Sutcliffe, A., Sawyer, P., Requirements elicitation: Towards the unknown unknowns (2013) 2013 21st IEEE International Requirements Engineering Conference (RE), pp. 92-104; <br/>Pacheco, C., Garcia, I., A systematic literature review of stakeholder identification methods in requirements elicitation (2012) Journal of Systems and Software, 85 (9), pp. 2171-2181; <br/>Ferrari, A., Spoletini, P., Gnesi, S., Ambiguity as a resource to disclose tacit knowledge (2015) 2015 IEEE 23rd International Requirements Engineering Conference (RE), pp. 26-35; <br/>Hadar, I., Soffer, P., Kenzi, K., The role of domain knowledge in requirements elicitation via interviews: An exploratory study (2014) Requir. Eng., 19 (2), pp. 143-159; <br/>Leite, J., Freeman, P., Requirements validation through viewpoint resolution (1991) IEEE Transactions on Software Engineering, 17 (12), pp. 1253-1269; <br/>Lucassen, G., Dalpiaz, F., Van Der Werf, M.E.J.M., Brinkkemper, S., Forging high-quality user stories: Towards a discipline for agile requirements (2015) 2015 IEEE 23rd International Requirements Engineering Conference (RE), pp. 126-135; <br/>Abbas, M., Ferrari, A., Shatnawi, A., Paul, E., Is requirements similarity a good proxy for software similarity? an empirical investigation in industry (2021) The 27th International Working Conference on Requirements Engineering: Foundation for Software Quality, , http://www.es.mdh.se/pdfpublications/6142.pdf; <br/>Stol, K.-J., Fitzgerald, B., The abc of software engineering research (2018) ACM Transactions on Software Engineering and Methodology (TOSEM), 27 (3), pp. 1-51; <br/>Svahnberg, M., Aurum, A., Wohlin, C., Using students as subjects-an empirical evaluation (2008) Proceedings of the Second ACM-IEEE International Symposium on Empirical Software Engineering and Measurement, pp. 288-290; <br/>Falessi, D., Juristo, N., Wohlin, C., Turhan, B., Münch, J., Jedlitschka, A., Oivo, M., Empirical software engineering experts on the use of students and professionals in experiments (2018) Empirical Software Engineering, 23 (1), pp. 452-489; <br/>Salman, I., Misirli, A.T., Juristo, N., Are students representatives of professionals in software engineering experiments? (2015) 2015 IEEE/ACM 37th IEEE International Conference on Software Engineering, 1, pp. 666-676. , IEEE; <br/>Ferrari, A., Spoletini, P., Bano, M., Zowghi, D., Sapeer and reversesapeer: Teaching requirements elicitation interviews with role-playing and role reversal (2020) Requirements Engineering, 25 (4), pp. 417-438<br/><br/><b>From Duplicate 2 (<i>From Ideas to Expressed Needs: An Empirical Study on the Evolution of Requirements during Elicitation</i> - Debnath, Sourav; Spoletini, Paola; Ferrari, Alessio)<br/></b><br/><b>From Duplicate 1 (<i>From Ideas to Expressed Needs: An Empirical Study on the Evolution of Requirements during Elicitation</i> - Debnath, S; Spoletini, P; Ferrari, A)<br/></b><br/>cited By 0; Conference of 29th IEEE International Requirements Engineering Conference, RE 2021 ; Conference Date: 20 September 2021 Through 24 September 2021; Conference Code:174516},
   pages = {233-244},
   publisher = {IEEE Computer Society},
   title = {From Ideas to Expressed Needs: An Empirical Study on the Evolution of Requirements during Elicitation},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123216141&doi=10.1109%2FRE51729.2021.00028&partnerID=40&md5=93fbe7ba0118edd00fbd3477ec89ddb2},
   year = {2021},
}
@inproceedings{Michael2021,
   abstract = {Systems providing their end-users behavior assistance must be customized to meet those users' needs and behavior goals. We investigate how it is possible to improve the engineering process of such systems and provide humans better support by using human behavior goals not only for analysis but also in the design and run-time of a system. Current research focuses either on the analysis phase of a system or uses goals at the run-time of an application. They do not consider the mapping of human behavior goals from one phase to the other one needed for generative approaches. Within this paper, we present our vision towards the use of goals and goal modeling for human behavior assistance in generated systems. We show its application by adding assistive functionalities to an existing, full-size real-world information system. For the engineering of this system, we follow a model-driven, generative software engineering approach. Our prototypical implementation towards the vision of using goal models for human behavior assistance shows the feasibility of the approach and provides first insights into how it could improve the assistive abilities of systems towards end-user goals.},
   author = {J. Michael and B. Rumpe and L.T. T Zimmermann and Association for Computing Machinery (ACM); IEEE Computer Society; IEEE Technical Council on Software Engineering (TCSE); Special Interest Group on Software Engineering (SIGSOFT)},
   doi = {10.1109/MODELS-C53483.2021.00060},
   isbn = {9781665424844},
   journal = {Companion Proceedings - 24th International Conference on Model-Driven Engineering Languages and Systems, MODELS-C 2021},
   keywords = {Assistive,Assistive System,Assistive system,Behavioral research,End-users,Goal Modeling,Goal models,Human Behavior Goals,Human behavior goal,Human behaviors,Human-Centered Engineering,Human-centered engineering,Model-Driven Software Engineering,Model-driven software engineerings,Runtimes,Software engineering,User behaviors},
   note = {<b>From Duplicate 1 (<i>Goal Modeling and MDSE for Behavior Assistance</i> - Michael, J.; Rumpe, B.; Zimmermann, L.T. T)<br/></b><br/><b>From Duplicate 1 (<i>Goal Modeling and MDSE for Behavior Assistance</i> - Michael, J; Rumpe, B; Zimmermann, L T)<br/></b><br/>cited By 1; Conference of 24th International Conference on Model-Driven Engineering Languages and Systems, MODELS-C 2021 ; Conference Date: 10 October 2021 Through 15 October 2021; Conference Code:175737<br/><br/><br/>Conference code: 175737<br/>Cited By :1<br/>Export Date: 21 August 2022<br/>References: Hölldobler, K., Michael, J., Ringert, J.O., Rumpe, B., Wortmann, A., Innovations in model-based software and systems engineering (2019) The Journal of Object Technology, 18 (1), pp. 1-60; <br/>Rafferty, J., Nugent, C.D., Liu, J., Chen, L., From activity recognition to intention recognition for assisted living within smart homes (2017) Ieee Trans. on Human-Machine Systems, 47 (3); <br/>Al MacHot, F., Mayr, H.C., Michael, J., Behavior modeling and reasoning for ambient support: Hcm-l modeler (2014) Int. Conf. on Industrial, Engineering &amp; Other Applications of Applied Intelligent Sys. (IEA-AIE 2014), , ser. Lecture Notes in Artificial Intelligence; <br/>Völter, M., Stahl, T., Bettin, J., Haase, A., Helsen, S., Czarnecki, K., (2013) Model-Driven Software Development: Technology, Engineering, Management, Ser. Wiley Software Patterns Series., , Wiley; <br/>Michael, J., Mayr, H.C., Conceptual modeling for ambient assistance (2013) Conceptual Modeling -ER'13, Ser. Lncs, 8217, pp. 403-413. , Springer; <br/>Giersich, M., Forbrig, P., Fuchs, G., Kirste, T., Reichart, D., Schumann, H., Towards an integrated approach for task modeling and human behavior recognition (2007) Human-Computer Interaction. Interaction Design and Usability, pp. 1109-1118. , J. A. Jacko, Ed. Springer; <br/>Parvin, P., Paternò, F., Chessa, S., Anomaly detection in the elderly daily behavior (2018) 14th Int. Conf. on Intelligent Environments (IE'18), pp. 103-106; <br/>Michael, J., Steinberger, C., Context modeling for active assistance (2017) Proc. of the Er Forum 2017 and the Er 2017 Demo Track at ER'17., pp. 221-234; <br/>Ali, R., Dalpiaz, F., Giorgini, P., A goal-based framework for contextual requirements modeling and analysis (2010) Requirements Engineering, 15 (4), pp. 439-458; <br/>Elkobaisi, M.R., Mayr, H.C., Shekhovtsov, V.A., Conceptual human emotion modeling (hem) (2020) Advances in Conceptual Modeling, pp. 71-81. , G. Grossmann and S. Ram, Eds. Springer; <br/>Grundy, J., Khalajzadeh, H., Mcintosh, J., Towards human-centric model-driven software engineering (2020) 15th Int. Conf. on Evaluation of Novel Approaches to Software Engineering (ENASE'20), INSTICC., pp. 229-238. , SciTePress; <br/>Curumsing, M.K., Fernando, N., Abdelrazek, M., Vasa, R., Mouzakis, K., Grundy, J., Emotion-oriented requirements engineering: A case study in developing a smart home system for the elderly (2019) Journal of Systems and Software, 147, pp. 215-229; <br/>Steinberger, C., Michael, J., Towards cognitive assisted living 3.0 (2018) Int. Conf. on Pervasive Computing and Communications Workshops (PerCom Workshops 2018)., pp. 687-692; <br/>Van Riemsdijk, M.B., Dastani, M., Winikoff, M., Goals in agent systems: A unifying framework (2008) 7th Int. Joint Conf. on Autonomous Agents and Multiagent Systems (AAMAS '08), pp. 713-720; <br/>Baskar, J., Janols, R., Guerrero, E., Nieves, J.C., Lindgren, H., A multipurpose goal model for personalised digital coaching (2017) Agents and Multi-Agent Systems for Health Care., pp. 94-116. , Springer; <br/>Bolender, T., Bürvenich, G., Dalibor, M., Rumpe, B., Wortmann, A., Self-Adaptive manufacturing with digital twins (2021) 2021 Int. Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS)., pp. 156-166; <br/>Fox, M., Long, D., Pddl2.1 : An extension to pddl for expressing temporal planning domains (2003) Journal of Artificial Intelligence Research, 20, pp. 61-124; <br/>Dalpiaz, F., Borgida, A., Horkoff, J., Mylopoulos, J., Runtime goal models: Keynote (2013) Ieee 7th Int. Conf. on Research Challenges in Information Science, pp. 1-11; <br/>Mernik, M., Heering, J., Sloane, A.M., When and how to develop domain-specific languages (2005) Acm Comput. Surv., 37 (4); <br/>Adam, K., Michael, J., Netz, L., Rumpe, B., Varga, S., Enterprise information systems in academia and practice: Lessons learned from a mbse project (2020) 40 Years Emisa (EMISA'19), Ser. Lni, P-304, pp. 59-66. , GI e.V; <br/>Leont'Ev, A.N., (1978) Activity, Consciousness, and Personality., , Englewood Cliffs, NJ: Prentice-Hall; <br/>(2016), https://www.britannica.com/topic/teleology, Teleology, Encyclopaedia Britannica, last access: 15.7.2021, [Online]; Eccles, J.S., Wigfield, A., Motivational beliefs, values, and goals (2002) Annual Review of Psychology, 53, pp. 109-132; <br/>Van Lamsweerde, A., Goal-oriented requirements engineering: A guided tour (2001) 5th Ieee Int. Symposium on Requ. Eng., pp. 249-262; <br/>Michael, J., Rumpe, B., Varga, S., Human behavior, goals and model-driven software engineering for assistive systems (2020) Enterprise Modeling and Information Systems Architectures (EMSIA 2020), 2628, pp. 11-18. , CEUR Workshop Proceedings; <br/>Rolland, C., Salinesi, C., Modeling goals and reasoning with them (2005) Engineering and Managing Software Requirements, pp. 189-217. , A. Aurum and C. Wohlin, Eds. Springer; <br/>Horkoff, J., Maiden, N., Lockerbie, J., Creativity and goal modeling for software requirements engineering (2015) Proc. Acm Sigchi Conf. on Creativity and Cognition, Ser. C&amp;c '15., pp. 165-168; <br/>Morandini, M., Penserini, L., Perini, A., Marchetto, A., Engineering requirements for adaptive systems (2017) Requirements Engineering, 22 (1), pp. 77-103; <br/>Heim, R., Mir Seyed Nazari, P., Ringert, J.O., Rumpe, B., Wortmann, A., Modeling robot and world interfaces for reusable tasks (2015) Intelligent Robots and Systems Conference (IROS'15).; <br/>Braubach, L., Pokahr, A., Moldt, D., Lamersdorf, W., Goal representation for bdi agent systems (2005) Programming Multi-Agent Systems., pp. 44-65. , Springer; <br/>Kaptelinin, V., Activity theory: Implications for human-computer interaction (2001) Context and Consciousness. Activity Theory and Humancomputer Interaction., pp. 103-116. , MIT Press; <br/>McCrickard, D.S., Chewar, C.M., Somervell, J.P., Ndiwalana, A., A model for notification systems evaluation-Assessing user goals for multitasking activity (2003) Acm Transactions on Computer-Human Interaction (TOCHI), 10 (4), pp. 312-338; <br/>Sutcliffe, A., Sawyer, P., Modeling personalized adaptive systems (2013) Advanced Information Systems Engineering., , Springer; <br/>Overbeek, S., Frank, U., Köhling, C., A language for multiperspective goal modelling: Challenges, requirements and solutions (2015) Computer Standards &amp; Interfaces, 38, pp. 1-16; <br/>Bock, A., Frank, U., Memo goalml: A context-enriched modeling language to support reflective organizational goal planning and decision processes (2016) Conceptual Modeling, Ser. LNCS., 9974, pp. 515-529. , Springer Int; <br/>Yu, E., Modeling strategic relationships for process reengineering (2011) Social Modeling for Requirements Engineering; <br/>Liu, L., From Ito Istar 2.0: An Evolving Social Modelling; <br/>Dalpiaz, F., Franch, X., Horkoff, J., (2016) IStar 2.0 Language Guide, 5; <br/>Dardenne, A., Van Lamsweerde, A., Fickas, S., Goal-directed requirements acquisition (1993) Science of Computer Programming, 20 (1), pp. 3-50; <br/>(2007) A Kaos Tutorial, , O. Respect-IT; <br/>Antón, A.I., Goal-based requirements analysis (1996) Int. Conf. on Requirements Engineering (ICRE '96), pp. 136-144; <br/>Schönmann, F., (2003) BDI-Architektur (Beliefs -Desires -Intentions); <br/>Hölldobler, K., Rumpe, B., (2017) MontiCore 5 Language Workbench Edition 2017, Ser. Aachener Informatik-Berichte, Software Engineering, Band 32, , Shaker Verlag, December; <br/>Gerasimov, A., Michael, J., Netz, L., Rumpe, B., Agile generator-based gui modeling for information systems (2021) Modelling to Program (M2P), pp. 113-126. , A. Dahanayake, O. Pastor, and B. Thalheim, Eds. Springer, March; <br/>Greifenberg, T., Look, M., Roidl, S., Rumpe, B., Engineering tagging languages for dsls (2015) Conf. on Model-Driven Engineering Languages and Systems (MODELS'15)., pp. 34-43; <br/>Dalibor, M., Michael, J., Rumpe, B., Varga, S., Wortmann, A., Towards a model-driven architecture for interactive digital twin cockpits (2020) Conceptual Modeling, pp. 377-387. , G. Dobbie, U. Frank, G. Kappel, S. W. Liddle, and H. C. Mayr, Eds. Springer; <br/>Gerasimov, A., Heuser, P., Ketteniß, H., Letmathe, P., Michael, J., Netz, L., Rumpe, B., Varga, S., Generated enterprise information systems: Mdse for maintainable co-development of frontend and backend (2020) Comp. Proc. of Modellierung 2020 Short, Workshop and Tools &amp; Demo Papers. Ceur Workshop Proceedings, pp. 22-30; <br/>France, R., Rumpe, B., Model-driven development of complex software: A research roadmap (2007) Future of Software Engineering (FOSE '07), pp. 37-54. , May; <br/>Rolland, C., Salinesi, C., (2009) Supporting Requirements Elicitation Through Goal/Scenario Coupling., pp. 398-416. , Springer; <br/>Kraiem, N., Kaffela, H., Dimassi, J., Al Khanjar, Z., Mapping from map models to bpmn processes (2014) Journal of Software Engineering, 8 (4), pp. 252-264; <br/>Guizzardi, R., Reis, A.N., A method to align goals and business processes (2015) Conceptual Modeling, Ser. Lncs, 9381, pp. 79-93. , P. Johannesson, M. L. Lee, S. W. Liddle, A. L. Opdahl, and O. Pastor López, Eds. Springer International Publishing; <br/>Ghasemi, M., Amyot, D., From event logs to goals: A systematic literature review of goal-oriented process mining (2020) Requirements Engineering, 25 (1), pp. 67-93; <br/>Castelfranchi, C., Falcone, R., Founding autonomy: The dialectics between (social) environment and agent's architecture and powers (2004) Agents and Computational Autonomy, Ser. LNCS., 2969, pp. 40-54. , Springer; <br/>Rafferty, J., Chen, L., Nugent, C., Ontological goal modelling for proactive assistive living in smart environments (2013) Ubiquitous Computing and Ambient Intelligence. Context-Awareness and Context-Driven Interaction, Ser. LNCS., 8276, pp. 262-269. , Springer; <br/>Morandini, M., Penserini, L., Perini, A., Automated mapping from goal models to self-Adaptive systems (2008) 23rd IEEE/ACM Int. Conf. on Automated Software Engineering, pp. 485-486; <br/>Penserini, L., Perini, A., Susi, A., Morandini, M., Mylopoulos, J., A design framework for generating bdi-Agents from goal models (2007) 6th Int. Joint Conf. on Autonomous Agents and Multiagent Systems (AAMAS '07).; <br/>Anda, A.A., Amyot, D., Arithmetic semantics of feature and goal models for adaptive cyber-physical systems (2019) Ieee 27th Int. Requirements Engineering Conference (RE'19), pp. 245-256; <br/>Luo, A., Su, C., Pan, S., Goal-oriented knowledge-driven neural dialog generation system (2019) Natural Language Processing and Chinese Computing., pp. 701-712. , Springer; <br/>Pokahr, A., Braubach, L., Lamersdorf, W., Jadex: A bdi reasoning engine (2005) Multi-Agent Programming, Ser. Multiagent Systems, Artificial Societies, and Simulated Organizations., 15, pp. 149-174. , Boston, MA: Springer US},
   pages = {370-379},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Goal Modeling and MDSE for Behavior Assistance},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123982473&doi=10.1109%2FMODELS-C53483.2021.00060&partnerID=40&md5=47fe84d6c48c96af21061191ad551872},
   year = {2021},
}
@inproceedings{Nguyen2017,
   abstract = {Requirements Engineering is closely intertwined with Information Technology (IT) Governance. Aligning IT Governance principles with Requirements-Driven Software Processes allows then to propose governance and management rules for software development to cope with stakeholders' requirements and expectations. Typically, the goal of IT Governance in software engineering is to ensure that the results of a software organization business processes meet the strategic requirements of the organization. Requirements-driven software processes, such as (I-)Tropos, are development processes using high-level social-oriented models to drive the software life cycle both in terms of project management and forward engineering techniques. To consolidate both perspectives, this paper proposes a process framework called GI-Tropos to extend I-Tropos allowing to align requirements-driven software processes with IT governance.},
   author = {V.H.A. H A Nguyen and M. Kolp and Y. Wautelet and S. Heng and Control and Communication (INSTICC) Institute for Systems and Technologies of Information},
   doi = {10.5220/0006431303380345},
   editor = {J Cardoso and J Cardoso and L Maciaszek and L Maciaszek and van Sinderen M and E Cabello},
   isbn = {9789897582622},
   journal = {ICSOFT 2017 - Proceedings of the 12th International Conference on Software Technologies},
   keywords = {Development process,Forward engineerings,IT governance,Life cycle,Process framework,Project management,Requirements engineering,Software design,Software life cycles,Software organization,Software process,Strategic requirements},
   note = {Conference code: 129817<br/>Cited By :5<br/>Export Date: 21 August 2022<br/>References: Ambler, S.W., Lines, M., (2012) Disciplined Agile Delivery: A Practitioner's Guide to Agile Software Delivery in the Enterprise, , IBM Press, 1st edition; <br/>Bannerman, P.L., Software development governance: A meta-management perspective (2009) 2009 ICSE Workshop on Software Development Governance, pp. 3-8; <br/>Castro, J., Kolp, M., Mylopoulos, J., Towards requirements-driven information systems engineering: The tropos project (2002) Inf. Syst., 27 (6), pp. 365-389; <br/>Chulani, S., Williams, C., Yaeli, A., Software development governance and its concerns (2008) Proceedings of the 1st International Workshop on Software Development Governance, pp. 3-6. , SDG'08, New York, NY, USA. ACM; <br/>Dalpiaz, F., Franch, X., Horkoff, J., Istar 2.0 language guide (2016) CoRR, , abs/1605.07767; <br/>Dardenne, A., Van Lamsweerde, A., Fickas, S., Goal-directed requirements acquisition (1993) Science of Computer Programming, 20 (1-2), pp. 3-50; <br/>Fuxman, A., Liu, L., Mylopoulos, J., Pistore, M., Roveri, M., Traverso, P., Specifying and analyzing early requirements in tropos (2004) Requirements Engineering, 9 (2), pp. 132-150; <br/>(2012) Cobit, p. 5. , Isaca, ISA; <br/>Kalumbilo, M., Finkelstein, A., Linking strategy, governance, and performance in software engineering (2014) Proceedings of the 7th International Workshop on Cooperative and Human Aspects of Software Engineering, pp. 107-110. , CHASE 2014, New York, NY, USA. ACM; <br/>Kolp, M., Giorgini, P., Mylopoulos, J., Organizational multi-agent architectures: A mobile robot example (2002) First I. Joint Conf. on Autonomous Agents &amp; Multiagent Systems, AAMAS 2002, July 15-19, 2002, Bologna, Italy, pp. 94-95; <br/>Kolp, M., Mylopoulos, J., Software architectures as organizational structures (2001) Proc. ASERC Workshop on the Role of Software Architectures in the Construction, Evolution, and Reuse of Software Systems, , Edmonton, Canada; <br/>Kruchten, P., (2003) The Rational Unified Process: An Introduction, , Addison-Wesley, 3rd edition; <br/>Kruchten, P., Contextualizing agile software development (2013) Journal of Software: Evolution and Process, 25 (4), pp. 351-361; <br/>Mylopoulos, J., Kolp, M., Giorgini, P., Agentoriented software development (2002) Hellenic Conference on Artificial Intelligence, pp. 3-17. , Springer Berlin Heidelberg; <br/>(2013) A Guide to the Project Management Body of Knowledge, , PMI, Project Management Institute; <br/>Sommerville, I., (2010) Software Engineering, , Addison-Wesley Publishing Company, USA, 9th edition; <br/>Van Lamsweerde, A., (2009) Requirements Engineering: From System Goals to UML Models to Software Specifications, , Wiley Publishing, 1st edition; <br/>Van Lamsweerde, A., Letier, E., From object orientation to goal orientation: A paradigm shift for requirements engineering (2002) Radical Innovations of Software and Systems Engineering in the Future, 9th International Workshop, RISSEF 2002, Venice, Italy, October 7-11, 2002, pp. 325-340. , Revised Papers; <br/>Wautelet, Y., (2008) A Goal-driven Project Management Framework for Multiagent Software Development: The Case of I-tropos, , PhD thesis, Universite catholique de Louvain; <br/>Wautelet, Y., Achbany, Y., Kolp, M., A serviceoriented framework for MAS modeling (2008) ICEIS 2008 - Proc. of the 10th Int. Conf. on Enterprise Information Systems, ISAS-1, pp. 120-128. , Barcelona, Spain, June 12-16, 2008; <br/>Wautelet, Y., Kolp, M., Goal driven iterative software project management (2011) ICSOFT 2011 - Proceedings of the 6th International Conference on Software and Data Technologies, 2, pp. 44-53. , Seville, Spain, 18-21 July, 2011; <br/>Wautelet, Y., Kolp, M., Business and modeldriven development of BDI multi-agent systems (2016) Neurocomputing, 182, pp. 304-321; <br/>Weill, P.R.J., (2004) IT Governance: How Top Performers Manage IT Decision Rights for Superior Results, Watertown, , MA: Harvard Business School Press; <br/>Yu, E., Giorgini, P., Maiden, N., Mylopoulos, J., (2011) Social Modeling for Requirements Engineering, , The MIT Press<br/><br/><b>From Duplicate 2 (<i>Aligning requirements-driven software processes with it governance</i> - Nguyen, V.H.A. H A; Kolp, M.; Wautelet, Y.; Heng, S.)<br/></b><br/><b>From Duplicate 1 (<i>Aligning requirements-driven software processes with it governance</i> - Nguyen, V H A; Kolp, M; Wautelet, Y; Heng, S)<br/></b><br/>cited By 5; Conference of 12th International Conference on Software Technologies, ICSOFT 2017 ; Conference Date: 24 July 2017 Through 26 July 2017; Conference Code:129817},
   pages = {338-345},
   publisher = {SciTePress},
   title = {Aligning requirements-driven software processes with it governance},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029323226&doi=10.5220%2F0006431303380345&partnerID=40&md5=b8263b4a1bd8d6c0507a8e8e2537eb29},
   year = {2017},
}
@inproceedings{Haidar2019,
   abstract = {Requirements engineering (RE) techniques can play a determinant role when making the strategic decision to adopt an Agile Product Line approach to the production of software-intensive systems. This paper proposes an integrated goal and feature-based metamodel for agile software product lines. The aim is to allow analysts and developers to produce specifications that precisely capture the stakeholder's needs and intentions as well as to manage product line variabilities. Adopting practices from requirements engineering, especially goal and feature models, helps designing the domain and application engineering tiers of an agile product line. Such an approach allows a holistic perspective integrating human, organizational and agile aspects to better understand product lines dynamic business environments. It helps bridging the gap between product lines structures and requirements models, and proposes an integrated framework to all actors involved in the product line architecture.},
   author = {H. Haidar and M. Kolp and Y. Wautelet and Control and Communication (INSTICC) Institute for Systems and Technologies of Information},
   doi = {10.5220/0006849000900101},
   editor = {L Maciaszek and L Maciaszek and van Sinderen M},
   isbn = {9789897583209},
   journal = {ICSOFT 2018 - Proceedings of the 13th International Conference on Software Technologies},
   keywords = {AgiFPL,Agile Product Line Engineering,Computer software,Feature,Feature Model,Feature modeling,Goal Model,Goal modeling,Product design,Product line engineering,Requirements Engineering,Requirements engineering,Software design},
   note = {Conference code: 150354<br/>Cited By :1<br/>Export Date: 21 August 2022<br/>References: Acher, M., Collet, P., Lahire, P., France, R.B., Separation of concerns in feature modeling (2012) Proceedings of the 11th International Conference on Aspect-Oriented Software Development, pp. 1-12. , New York, NY, USA; <br/>Apel, S., Batory, D., Kästner, C., Saake, G., (2013) Feature-Oriented Software Product Lines, , Springer-Verlag, Berlin Heidelberg; <br/>Asadi, M., Gröner, G., Mohabbati, B., Gasevic, D., Goal-oriented modeling and verification of feature-oriented product liens (2016) Softw Syst Model, 15, p. 257; <br/>Borba, C., Silva, C., A comparison of goal-oriented approaches to model software product lines variability (2009) LNCS, 5833, pp. 184-253. , Springer-Verlag; <br/>Cohn, M., (2004) User Stories Applied for Agile Software Development, , Boston: Pearson Education Inc; <br/>Da Silva, I.F., Neto, P., O'Leary, P., De Almeida, E., De Lemos, S.R., (2011) Agile Software Product Lines: A Systematic Mapping Study, 41 (8), pp. 899-920. , 2011; <br/>Dalpiaz, F., Franch, X., Horkoff, J., (2016) iStar 2.0 Language Guide, Cs, , https://arxiv.org/pdf/1605.07767v3.pdf, SE 2016, accessed on 17-09-2017; <br/>Ernst, N.A., Borgida, A., Mylopoulos, J., Jureta, I.J., Agile requirements evolution via paraconsistent reasoning (2012) Proceedings of CAiSE'12, pp. 382-397. , Springer, Berlin; <br/>Haidar, H., Kolp, M., Wautelet, Y., Agile product line engineering: The AGIFPL method (2017) Proceedings of the 12th International Conference on Software Technologies -, 1, pp. 275-285. , ICSOFT, Madrid, Spain; <br/>Haidar, H., Kolp, M., Wautelet, Y., Goal-oriented requirement engineering for agile software product lines: An overview (2017) LouRIM Working Paper Series, , http://hdl.handle.net/2078.1/185846, 2017/02; <br/>Jaqueira, A., Lucena, M., Alencar, F.M.R., Castro, J., Aranha, E., Using I* models to enrich user stories (2013) The Proceedings of the 6th International I* Workshop, pp. 55-60; <br/>Leffingwell, D., (2011) Agile Software Requirements, , Addison-Wesley Professional; <br/>O'Regan, G., Z formal specification language (2013) Mathematics in Computing, , Springer, London; <br/>Pohl, K., Böckle, G., Van Der Linden, F.J., (2010) Software Product Line Engineering: Foundations, Principles and Techniques, , Springer Publishing Company, Inc; <br/>Wautelet, Y., Heng, S., Kolp, M., Mirbel, I., Unifying and extending user story models (2014) CAiSE 2014, 8484. , Springer, Cham; <br/>Yu, E., Giorgini, P., Maiden, N., Mylopoulos, J., (2011) Social Modeling for Requirements Engineering, , eds. MIT, Cambridge, MA<br/><br/><b>From Duplicate 2 (<i>Formalizing agile software product lines with a RE metamodel</i> - Haidar, H.; Kolp, M.; Wautelet, Y.)<br/></b><br/><b>From Duplicate 1 (<i>Formalizing agile software product lines with a RE metamodel</i> - Haidar, H; Kolp, M; Wautelet, Y)<br/></b><br/>cited By 1; Conference of 13th International Conference on Software Technologies, ICSOFT 2018 ; Conference Date: 26 July 2018 Through 28 July 2018; Conference Code:150354},
   pages = {90-101},
   publisher = {SciTePress},
   title = {Formalizing agile software product lines with a RE metamodel},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071513394&doi=10.5220%2F0006849000900101&partnerID=40&md5=fe6777b90d1e74626eb2bff5747d189b},
   year = {2019},
}
@article{Jabbari2018,
   abstract = {DevOps as a new way of thinking for software development and operations has received much attention in the industry, while it has not been thoroughly investigated in academia yet. The objective of this study is to characterize DevOps by exploring its central components in terms of principles, practices and their relations to the principles, challenges of DevOps adoption, and benefits reported in the peer-reviewed literature. As a key objective, we also aim to realize the relations between DevOps practices and benefits in a systematic manner. A systematic literature review was conducted. Also, we used the concept of benefits dependency network to synthesize the findings, in particular, to specify dependencies between DevOps practices and link the practices to benefits. We found that in many cases, DevOps characteristics, ie, principles, practices, benefits, and challenges, were not sufficiently defined in detail in the peer-reviewed literature. In addition, only a few empirical studies are available, which can be attributed to the nascency of DevOps research. Also, an initial version of the DevOps benefits dependency network has been derived. The definition of DevOps principles and practices should be emphasized given the novelty of the concept. Further empirical studies are needed to improve the benefits dependency network presented in this study.},
   author = {Ramtin Jabbari and Nauman bin Ali and Kai Petersen and Binish Tanveer},
   doi = {10.1002/SMR.1957},
   issn = {20477481},
   issue = {11},
   journal = {Journal of Software: Evolution and Process},
   keywords = {DevOps,benefits and values,challenges,development and operations,principles and practices,systematic literature review},
   month = {11},
   publisher = {John Wiley and Sons Ltd},
   title = {Towards a benefits dependency network for DevOps based on a systematic literature review},
   volume = {30},
   year = {2018},
}
@article{Faustino2022,
   abstract = {Among current IT work cultures, DevOps stands out as one of the most adopted worldwide. The focus of this culture is on bridging the gap between development and operations teams, enabling collaborative effort toward quickly producing software, without sacrificing its quality and support. DevOps is used to tackle a variety of issues; as such, there are differing benefits reported by authors when performing their analysis. For this research, we aim to reach consensus on the DevOps benefits reported in existing literature. To accomplish this objective, two systematic literature reviews. The first intends to find all benefits reported in the literature, while the second review will be used to map the benefits found in the first one with DevOps implementation case studies, providing empirical evidences of each benefit. To strengthen the results, the concept-centric approach is used. During this research it was possible to observe that the most reported benefits are aligned with the DevOps premises of better collaboration between developers and operators, delivering software and products quicker. Based on DevOps implementation case studies, most reported benefits include a faster time to market as well as improvements in synergy and automation. Less reported benefits include a reduction in failed changes and security issues.},
   author = {João Faustino and Daniel Adriano and Ricardo Amaro and Rubén Pereira and Miguel Mira da Silva},
   doi = {10.1002/SPE.3096},
   issn = {1097024X},
   journal = {Software - Practice and Experience},
   keywords = {DevOps,agile,benefits,case study,systematic literature review},
   publisher = {John Wiley and Sons Ltd},
   title = {DevOps benefits: A systematic literature review},
   year = {2022},
}
@article{Erich2017,
   abstract = {Organizations are introducing agile and lean software development techniques in operations to increase the pace of their software development process and to improve the quality of their software. They use the term DevOps, a portmanteau of development and operations, as an umbrella term to describe their efforts. In this paper, we describe the ways in which organizations implement DevOps and the outcomes they experience. We first summarize the results of a systematic literature review that we performed to discover what researchers have written about DevOps. We then describe the results of an exploratory interview-based study involving 6 organizations of various sizes that are active in various industries. As part of our findings, we observed that all organizations were positive about their experiences and only minor problems were encountered while adopting DevOps.},
   author = {F. M.A. Erich and C. Amrit and M. Daneva},
   doi = {10.1002/SMR.1885},
   issn = {20477481},
   issue = {6},
   journal = {Journal of Software: Evolution and Process},
   keywords = {DevOps,agile software development,empirical study,qualitative interviews,software development life cycle,systematic literature review},
   month = {6},
   publisher = {John Wiley and Sons Ltd},
   title = {A qualitative study of DevOps usage in practice},
   volume = {29},
   year = {2017},
}
@article{Rafi2021,
   abstract = {DevOps is a new software engineering paradigm adopted by various software organizations to develop the quality software within time and budget. The implementation of DevOps practices is critical, and there are no guidelines to assess and improve the DevOps activities in software organizations. Hence, there is a need to develop a readiness model for DevOps (RMDevOps) with an aim to assist the practitioners for implementation of DevOps practices in software firms. To achieve the study objective, we conducted a systematic literature review (SLR) study to identify the critical challenges and associated best practices of DevOps. A total of 18 challenges and 73 best practices were identified from the 69 primary studies. The identified challenges and best practices were further evaluated by conducting a survey with industry practitioners. The RMDevOps was developed based on other well-established models in software engineering domain, for example, software process improvement readiness model (SPIRM) and software outsourcing vendor readiness model (SOVRM). Finally, case studies were conducted with three different organizations with an aim to validate the developed model. The results show that the RMDevOps is effective to assess and improve the DevOps practices in software organizations.},
   author = {Saima Rafi and Wu Yu and Muhammad Azeem Akbar and Sajjad Mahmood and Ahmed Alsanad and Abdu Gumaei},
   doi = {10.1002/SMR.2323},
   issn = {20477481},
   issue = {4},
   journal = {Journal of Software: Evolution and Process},
   keywords = {best practices,case study,guidelines,readiness model},
   month = {4},
   publisher = {John Wiley and Sons Ltd},
   title = {Readiness model for DevOps implementation in software organizations},
   volume = {33},
   year = {2021},
}
@article{,
   abstract = {In recent years, stakeholders expect the delivery of new features almost every day to give fast feedback about changes or issues in software projects. To achieve this, they have opted to try new practices such as those from DevOps to improve their processes. Organizations such as IBM, Facebook, Amazon, and so on are implementing a DevOps culture. However, for small organizations, which are a large majority in the software development market, it is complicated to adopt DevOps because there is no guide to follow. Organizations have experienced issues trying to implement a DevOps approach, mainly for reasons such as a lack of process, guidance, or uncoordinated activities among organizations. Moreover, some organizations have a wrong understanding of DevOps: as just a set of automation tools to achieve a continuous release. This paper presents a guide to implement or reinforce a DevOps approach. Besides, the results of a case study of implementing the guide in a small organization are included. These results have shown that the guide helped the team: to identify its gaps toward the implementation of a DevOps approach; to have a better understanding of DevOps; and to facilitate the DevOps implementation providing a set of tasks, roles, and metrics.},
   author = {Mirna Muñoz and Mario Negrete Rodríguez},
   doi = {10.1002/SMR.2342},
   issn = {20477481},
   journal = {Journal of Software: Evolution and Process},
   keywords = {DevOps,ISO/IEC 29110,case study,guidance,process,state of the art},
   publisher = {John Wiley and Sons Ltd},
   title = {A guidance to implement or reinforce a DevOps approach in organizations: A case study},
   year = {2021},
}
@book{Haidar2019,
   abstract = {Requirements engineering (RE) techniques play a determinant role within Agile Product Lines development methods; these notably allow to establish the relevance to adopt or not the product line approach for software-intensive systems production. This paper proposes an integrated goal and feature-based meta-model for agile software product lines development. The main objective is to permit the sepecification of the requirements that precisely capture stakeholder’s needs and intentions as well as the management of product line variabilities. Adopting practices from requirements engineering, especially goal and feature models, helps designing the domain and application engineering tiers of an agile product line. Such an approach allows a holistic perspective integrating human, organizational and agile aspects to better understand product lines dynamic business environments. It helps bridging the gap be-tween product lines structures and requirements models, and proposes an integrated framework to all actors involved in the product line architecture. In this paper we show how our proposed metamodel can be applied to the requirements engineering stage of an agile product line development mainly for feature-oriented agile product lines such as our own methodology called AgiFPL.},
   author = {Hassan Haidar and Manuel Kolp and Yves Wautelet},
   doi = {10.1007/978-3-030-29157-0_6},
   editor = {van Sinderen M Maciaszek L.A. Maciaszek L.A.},
   isbn = {9783030291563},
   journal = {Communications in Computer and Information Science},
   keywords = {AgiFPL,Agile product line engineering,Computer software,Engineering goal model,Feature,Feature model,Feature modeling,Goal modeling,Object oriented programming,Product design,Product line engineering,Requirements,Requirements engineering,Software design},
   note = {<b>From Duplicate 2 (<i>An Integrated Requirements Engineering Framework for Agile Software Product Lines</i> - Haidar, H; Kolp, M; Wautelet, Y)<br/></b><br/>cited By 2; Conference of 13th International Conference on Software Technologies, ICSOFT 2018 ; Conference Date: 26 July 2018 Through 28 July 2018; Conference Code:230049},
   pages = {124-149},
   publisher = {Springer Verlag},
   title = {An Integrated Requirements Engineering Framework for Agile Software Product Lines},
   volume = {1077},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071697174&doi=10.1007%2F978-3-030-29157-0_6&partnerID=40&md5=344b40c2a58212a7f0010cec0c4dabf2 https://link.springer.com/chapter/10.1007/978-3-030-29157-0_6},
   year = {2019},
}
@article{Abad2017,
   abstract = {Peer Reviewed},
   author = {Karina Abad and Wilson Pérez and Juan Pablo Carvallo and Xavier Franch},
   doi = {10.1007/978-3-319-69904-2_3},
   isbn = {978-3-319-69904-2},
   issn = {16113349},
   journal = {Conceptual Modeling: 36th International Conference, ER 2017: Valencia, Spain, November 6-9, 2017: proceedings},
   keywords = {Computer software,Context model,Enginyeria de sistemes,Goal,Goal-oriented model,Model reuse,Programari,Reusabilitat,Reusability,Systems engineering,iStar framework,oriented model,Àrees temàtiques de la UPC::Informàtica::Sistemes},
   pages = {36-49},
   publisher = {Springer},
   title = {A catalogue of reusable context model elements based on the i* framework},
   volume = {10650 LNCS},
   url = {https://upcommons.upc.edu/handle/2117/119063 https://link.springer.com/chapter/10.1007/978-3-319-69904-2_3},
   year = {2017},
}
@inproceedings{Wautelet2017,
   abstract = {This paper summarizes previous works done by the authors on User Story (US) template unification and visual requirements models generation out of a US set. Indeed, transformation of a US set tagged using templates from a unified model to a Goal-Oriented model called the Rationale Tree and to a UML Use-Case Diagram are previous contributions summarized here. It also introduces the genuine contribution of generating a UML class diagram from a US set. Future research - notably on the use of the transformations in real life-case studies - is also discussed. Finally, the CASE tool supporting the approaches is overviewed.},
   author = {Y. Wautelet and S. Heng and M. Kolp and M Kolp - REFSQ Workshops and undefined 2017 and M. Kolp},
   editor = {Knauss E Susi A Dalpiaz F Kifetew F M Tenbergen B Palomares C Seffah A Forbrig P Berry D M Daneva M Knauss A Siena A Daun M Herrmann A Kirikova M Groen E C Horkoff J Maeder P Massacci F Ralyte J Ameller D. Dieste O.},
   issn = {16130073},
   journal = {CEUR Workshop Proceedings},
   keywords = {Agile Development,Agile development,Computer software selection and evaluation,Forestry,Goal oriented modeling,Rationale Tree,Requirements Models,Requirements engineering,UML class diagrams,Uml use case diagrams,Use case diagram,Use-Case Diagram,User Story,User stories},
   note = {<b>From Duplicate 3 (<i>Perspectives on user story based visual transformations</i> - Wautelet, Y; Heng, S; Kolp, M)<br/></b><br/>cited By 2; Conference of 2017 Joint REFSQ Workshops, Doctoral Symposium, Research Method Track, and Poster Track, co-located with the 23rd International Conference on Requirements Engineering: Foundation for Software Quality, REFSQ 2017 ; Conference Date: 27 February 2017; Conference Code:126776},
   publisher = {CEUR-WS},
   title = {Perspectives on user story based visual transformations},
   volume = {1796},
   url = {https://dial.uclouvain.be/pr/boreal/object/boreal%3A195929/datastream/PDF_01/view https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016181818&partnerID=40&md5=d1728af051fb57b9af3aab065e3fe51d},
   year = {2017},
}
@article{Gambo2021,
   abstract = {Requirements engineering has critical importance in the significant and successful number of software development projects involving multiple stakeholders to deliver high-quality software-intensive systems. The stakeholders' statements concerning the desired systems are expressed as goals to be achieved by the system in goal-oriented requirements engineering (GORE). In socio-technical systems (STS), the goals are achieved by cooperating with man-made agents within the software-to-be and human agents. However, as stakeholders often chase after mismatching goals subjectively, identifying and resolving conflicts in requirements becomes an inevitable part of GORE. This paper outlines the urgent need and processes required to investigate conflicts in the agile agent-oriented modeling (AAOM) methodology for engineering STS. We present a pragmatic view of our proposed strategy in a framework from a deductive and qualitative research perspective. The proposed strategy can attach stakeholders' corresponding roles to the hierarchical goal model's goals, which naturally brings out the stakeholder's needs and intentions. Additionally, it can relate the goal models to the most popular artifacts of agile software engineering. Thus, our pragmatic view builds upon well-established STS, especially in utilizing AAOM methodology.},
   author = {IP Ishaya Gambo and K Taveter - ICSOFT and undefined 2021 and Kuldar Taveter},
   doi = {10.5220/0010605703330341},
   isbn = {9789897585234},
   keywords = {Agile Methodology,Conflict Resolution,Requirements Engineering,Socio-technical Systems,Stakeholders},
   pages = {333-341},
   publisher = {SciTePress},
   title = {No Title},
   url = {https://www.researchgate.net/profile/Ishaya-Gambo/publication/353396404_A_Pragmatic_View_on_Resolving_Conflicts_in_Goal-oriented_Requirements_Engineering_for_Socio-technical_Systems/links/60fd5d26169a1a0103b6445c/A-Pragmatic-View-on-Resolving-Conflicts-in},
   year = {2021},
}
@inproceedings{Penha2017,
   author = {F Penha and M Lucena and L Lucena and F Alencar and C Agra},
   editor = {de la Vara J L Brito Isabel B.I.},
   isbn = {9789873806988},
   note = {<b>From Duplicate 1 (<i>A scalable and modular textual notation for iStar requirements models [Uma Notação Textual Modular e Escalável para Modelos de Requisitos iStar]</i> - Penha, F; Lucena, M; Lucena, L; Alencar, F; Agra, C)<br/></b><br/>cited By 0; Conference of 20th Workshop em Engenharia de Requisitos, WER 2017 - 20th Requirements Engineering Workshop, WER 2017 ; Conference Date: 22 May 2017 Through 23 May 2017; Conference Code:160997<br/><br/><b>From Duplicate 2 (<i>A modular and scalable textual notation for iStar requirements models [Uma Notação Textual Modular e Escalável para Modelos de Requisitos iStar]</i> - Penha, F; Lucena, M; Lucena, L; Alencar, F; Agra, C)<br/></b><br/>cited By 0; Conference of 20th Ibero-American Conference on Software Engineering, CIbSE 2017 ; Conference Date: 22 May 2017 Through 23 May 2017; Conference Code:128317},
   pages = {43-56},
   title = {No Title},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089821722&partnerID=40&md5=5d8ff7a38e73eb421518920a9186fc75 https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026653049&partnerID=40&md5=746ee69827da32efe0cdf0eafad2350e},
   year = {2017},
}
@inproceedings{Mesquita2021,
   author = {R. Mesquita and F. Penha and M. Lucena},
   journal = {CIbSE 2021 - XXIV Ibero-American Conference on Software Engineering},
   note = {<b>From Duplicate 1 (<i>A goal-oriented approach to substantiate and guide product owner decisions in Scrum projects [Uma Abordagem Orientada a Meta para Fundamentar e Orientar as Decisões do Product Owner em Projetos Scrum]</i> - Mesquita, R; Penha, F; Lucena, M)<br/></b><br/>cited By 0; Conference of 24th Ibero-American Conference on Software Engineering, CIbSE 2021 ; Conference Date: 30 August 2021 Through 3 September 2021; Conference Code:176054},
   publisher = {Ibero-American Conference on Software Engineering},
   title = {A goal-oriented approach to substantiate and guide product owner decisions in Scrum projects | Uma Abordagem Orientada a Meta para Fundamentar e Orientar as Decisões do Product Owner em Projetos Scrum},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123564579&partnerID=40&md5=5a8dbf48f2dfe2136004317ea86f805a},
   year = {2021},
}
@book{Lucassen2018,
   abstract = {[Context and motivation] Goal orientation is an unrealized promise in the practice of requirements engineering (RE). Conversely, lightweight approaches such as user stories have gained substantial adoption. As critics highlight the limitations of user stories, Job Stories are emerging as an alternative that embeds goal-oriented principles by emphasizing situation, motivation and expected outcome. This new approach has not been studied in research yet. [Question/Problem] Scientific foundations are lacking for the job story artifact and there are no actionable methods for effectively applying job stories. Thus, practitioners may end up creating their own flavor of job stories that may fail to deliver the promised value of the Jobs-to-be-Done theory. [Principal ideas/results] We integrate multiple approaches based on job stories to create a conceptual model of job stories and to construct a generic method for Jobs-to-be-Done Oriented RE. Applying our job story method to an industry case study, we highlight benefits and limitations. [Contribution] Our method aims to bring job stories from craft to discipline, and to provide systematic means for applying Jobs-to-be-Done orientation in practice and for assessing its effectiveness.},
   author = {Garm Lucassen and Maxim van de Keuken and Fabiano Dalpiaz and Sjaak Brinkkemper and Gijs Willem G.W. W Sloof and Johan Schlingmann},
   doi = {10.1007/978-3-319-77243-1_14},
   editor = {Dalpiaz F Kamsties E. Horkoff J.},
   isbn = {9783319772424},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Agile development,Case study,Computer software selection and evaluation,Conceptual model,Goal orientations,Industry case studies,Job stories,Jobs-to-be-done,Motivation,Problem orientation,Requirements engineering,Scientific foundations},
   note = {<b>From Duplicate 3 (<i>Jobs-to-be-done oriented requirements engineering: A method for defining job stories</i> - Lucassen, G; van de Keuken, M; Dalpiaz, F; Brinkkemper, S; Sloof, G W; Schlingmann, J)<br/></b><br/>cited By 10; Conference of 24th International Working Conference on Requirements Engineering Foundation for Software Quality, REFSQ 2018 ; Conference Date: 19 March 2018 Through 22 March 2018; Conference Code:211359},
   pages = {227-243},
   publisher = {Springer Verlag},
   title = {Jobs-to-be-done oriented requirements engineering: A method for defining job stories},
   volume = {10753 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-319-77243-1_14 https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043387616&doi=10.1007%2F978-3-319-77243-1_14&partnerID=40&md5=e2066ded948c7b0581cbd395b576095c},
   year = {2018},
}
@inproceedings{,
   abstract = {The proceedings contain 19 papers. The topics discussed include: elicitation awareness in conceptual modeling: the role of transparency; goal modeling education with GRL: experience report; exploiting online discussions in collaborative distributed requirements engineering; an initial approach to reuse non-functional requirements knowledge; from unknown to known impacts of organizational changes on socio-technical systems; iStar in practice: on the identification of reusable SD context models elements; modeling the monitoring and adaptation of context-sensitive systems; a textual syntax with tool support for the goal-oriented requirement language; making means-end-maps workable for recommending teaching methods; formalization of i∗ mapping rules for class diagram; US2StarTool: generating i∗ models from user stories; specifying guidelines to transform i∗ model into user stories: an overview; and implementing GPI, a language for organizational alignment.},
   editor = {Liaskos S Filho G.C. Castro J.},
   issn = {16130073},
   journal = {CEUR Workshop Proceedings},
   note = {<b>From Duplicate 2 (<i>CEUR Workshop Proceedings</i> - Filho G.C. Castro J., Liaskos S)<br/></b><br/>cited By 0; Conference of 8th International i*Workshop, iStar 2015 - In conjunction with the 23rd International Requirements Engineering Conference, RE 2015 ; Conference Date: 24 August 2015 Through 25 August 2015; Conference Code:114496},
   publisher = {CEUR-WS},
   title = {CEUR Workshop Proceedings},
   volume = {1402},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943428350&partnerID=40&md5=8623b55e94b54b6e8cfa1dfe289065c4},
   year = {2015},
}
@book{Horkoff2014,
   abstract = {Modeling languages have been evaluated through empirical studies, comparisons of language grammars, and ontological analyses. In this paper we take the first approach, evaluating the expressiveness and effectiveness of Techne, a requirements modeling language, by applying it to three requirements problems from the literature. We use our experiences to propose a number of language improvements for Techne, addressing challenges discovered during the studies. This work presents an example evaluation of modeling language expressiveness and effectiveness through realistic case studies.},
   author = {J. Horkoff and F.B. B Aydemir and F.-L. Li and T. Li and J. Mylopoulos},
   doi = {10.1007/978-3-319-12206-9_21},
   editor = {Jarke M Purao S Yu E. Dobbie G.},
   isbn = {9783319122052},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Economic and social effects,Empirical evaluations,Goal modeling,Model reasonings,Modeling languages,RequirementsModeling,Trade-off analysis},
   note = {<b>From Duplicate 1 (<i>Evaluating modeling languages: An example from the requirements domain</i> - Horkoff, J; Aydemir, F B; Li, F.-L.; Li, T; Mylopoulos, J)<br/></b><br/>cited By 12; Conference of 33rd International Conference on Conceptual Modeling, ER 2014 ; Conference Date: 27 October 2014 Through 29 October 2014; Conference Code:109659},
   pages = {260-274},
   publisher = {Springer Verlag},
   title = {Evaluating modeling languages: An example from the requirements domain},
   volume = {8824},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910624813&doi=10.1007%2F978-3-319-12206-9_21&partnerID=40&md5=fc51494885a3d646ce75c6083cdbed38},
   year = {2014},
}
@book{Duffau2018,
   abstract = {The result of productive processes is commonly accompanied by a set of justifications which can be, depending on the product, process-related qualities, traceability documents, product-related experiments, tests or expert reports, etc. In critical contexts, it is mandatory to substantiate that a product’s development has been carried out appropriately which results in an inflation of the quantity of justification documents. This mass of document and information is difficult to manage and difficult to assess (in terms of soundness). In this paper, we report on the experience gained on two industrial case studies, in which we applied a justification elicitation approach based on justification diagrams and justification pattern diagrams in order to identify necessary and sufficient justification documentation.},
   author = {C. Duffau and T. Polacsek and M. Blay-Fornarino},
   doi = {10.1007/978-3-319-91563-0_5},
   editor = {Reijers H A Krogstie J.},
   isbn = {9783319915623},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Certification,Industrial case study,Information systems,Information use,Justification,Productive process,Quality requirements,Requirements elicitation,Requirements engineering,Systems engineering},
   note = {<b>From Duplicate 2 (<i>Support of justification elicitation: Two industrial reports</i> - Duffau, C; Polacsek, T; Blay-Fornarino, M)<br/></b><br/>cited By 5; Conference of 30th International Conference on Advanced Information Systems Engineering, CAiSE 2018 ; Conference Date: 11 June 2018 Through 15 June 2018; Conference Code:214019},
   pages = {71-86},
   publisher = {Springer Verlag},
   title = {Support of justification elicitation: Two industrial reports},
   volume = {10816 LNCS},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048497306&doi=10.1007%2F978-3-319-91563-0_5&partnerID=40&md5=2b1ff61b33f4a89fb0c2101840f8300b},
   year = {2018},
}
@book{Nagl2008,
   abstract = {This short section aims at evaluating the academic outcome of the CRC IMPROVE from 1997 up to now and also sketches its further outcome as TC 61. We do this by regarding different perspectives: (a) The number and value of publications, (b) the contribution to international conference activities, and (c) how we laid the basis for or accelerated the career of young scientists. Joint activities together with industry and their consequences are discussed in sections 5.1 and 8.3. © 2008 Springer-Verlag Berlin Heidelberg.},
   author = {M. Nagl},
   doi = {10.1007/978-3-540-70552-9_39},
   isbn = {9783540705512},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Artificial intelligence,Computer science,Computers,Joint activity},
   note = {<b>From Duplicate 2 (<i>Review from academic success perspective</i> - Nagl, M)<br/></b><br/>cited By 0},
   pages = {774-849},
   publisher = {Springer Verlag},
   title = {Review from academic success perspective},
   volume = {4970 LNCS},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-50849103402&doi=10.1007%2F978-3-540-70552-9_39&partnerID=40&md5=571388ce0c635c89c07ad796bd304605},
   year = {2008},
}
@book{Wautelet2016,
   abstract = {User Stories (US) are mostly used as basis for representing requirements in agile development. Written in a direct manner, US fail in producing a visual representation of the main system-to-be functions. A Use-Case Diagram (UCD), on the other hand, intends to provide such a view. Approaches that map US sets to a UCD have been proposed; they however consider every US as a Use Case (UC). Nevertheless, a valid UC should not be an atomic task or a sub-process but enclose an entire scenario of the system use instead. A unified model of US templates to tag US sets was previously build. Within functional elements, it notably distinguishes granularity levels. In this paper, we propose to transform specific elements of a US set into a UCD using the granularity information obtained through tagging. In practice, such a transformation involves continuous round-tripping between the US and UC views; a CASE-tool supports this.},
   author = {Yves Wautelet and Samedi Heng and Diana Hintea and Manuel Kolp and Stephan Poelmans},
   doi = {10.1007/978-3-319-47717-6_11},
   editor = {Trujillo J C Link S.},
   isbn = {9783319477169},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Agile development,Agile manufacturing systems,Artificial intelligence,Computer science,Computers,Functional elements,Granularity levels,SCRUM,UML,Unified Modeling,Use case diagram,User stories,User story,Visual representations,X ray photoelectron spectroscopy,XP,nocv1},
   note = {<b>From Duplicate 3 (<i>Bridging user story sets with the use case model</i> - Wautelet, Y; Heng, S; Hintea, D; Kolp, M; Poelmans, S)<br/></b><br/>cited By 28; Conference of 35th International Conference on Conceptual Modelling, ER 2016 held in conjunction with Workshops on AHA, MoBiD, MORE-BI, MReBA, QMMQ, SCME and WM2SP, 2016 ; Conference Date: 14 November 2016 Through 17 November 2016; Conference Code:185749},
   pages = {127-138},
   publisher = {Springer Verlag},
   title = {Bridging user story sets with the use case model},
   volume = {9975 LNCS},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-319-47717-6_11 https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995965482&doi=10.1007%2F978-3-319-47717-6_11&partnerID=40&md5=7dfe03bea1320796bc0d01c9a2c5c742},
   year = {2016},
}
@article{Carvallo2012,
   abstract = {Modern enterprise engineering (EE) requires deep understanding of organizations and their interaction with their context. Because of this, in early phases of the EE process, enterprise context models are often built and used to reason about organizational needs with respects to actors in their context and vice versa. However, far from simple, this task is usually cumbersome because of knowledge and communication gaps among technical personnel performing EE activities and their administrative counterparts. In this paper, we propose the use of strategic patterns expressed with the i* language aimed to help bridging this gap. Patterns emerged from several industrial applications of our DHARMA method, and synthesize knowledge about common enterprise strategies, e.g. CRM. Patterns have been constructed based on the well-known Porter's model of the 5 market forces and built upon i* strategic dependency models. In this way technical and administrative knowledge and skills are synthesized in a commonly agreeable framework. The use of patterns is illustrated with an industrial example in the telecom field. © 2012 Springer-Verlag.},
   author = {Juan Pablo Carvallo and Xavier Franch},
   doi = {10.1007/978-3-642-34163-2_3},
   isbn = {9783642341625},
   issn = {18651348},
   journal = {Lecture Notes in Business Information Processing},
   keywords = {enterprise context model,enterprise pattern,framework,i,iStar,market forces,strategic dependencies},
   pages = {40-59},
   publisher = {Springer Verlag},
   title = {Building strategic enterprise context models with i*: A pattern-based approach},
   volume = {131 LNBIP},
   year = {2012},
}
@article{Soares2020,
   abstract = {No Framework Scrum, o Product Owner (PO) assume o papel central no processo de
desenvolvimento, sendo o encarregado por fazer a comunicação entre o cliente e os desenvolvedores. Nessa intermediação, ele gerencia o Product Backlog, que mantém uma lista
de itens a serem desenvolvidos, correspondentes às necessidades do cliente. Diante disso,
a academia tem explorado os desafios do PO, principalmente relacionados às atividades
de planejamento onde, nesse contexto, a tomada de decisão é vista como a sua tarefa
mais importante. Porém, a falta de informações estruturadas e que fundamentem as suas
escolhas, faz com que o mesmo, por muitas vezes, tome decisões equivocadas ou se omita
dessa responsabilidade. Na Engenharia de Requisitos Orientada a Metas, os requisitos são
descritos a partir das metas organizacionais dos stakeholders e, segundo a literatura, a
sua especificação pode trazer diversos benefícios em termos de capacidade de organização
da informação. A maioria dos projetos Scrum utiliza histórias de usuário para especificar
os requisitos e, embora elas contenham a definição da meta, a mesma não é evidenciada
no processo de desenvolvimento. Isto posto, este trabalho tem como objetivo proporcionar uma apresentação das informações organizacionais, inerentes ao produto ou serviço
desejado, em uma disposição que fundamente e oriente as tomadas de decisões do PO.
Para tal fim, foi proposto um novo modelo, o Product Backlog Orientado a Metas, que
busca evidenciar as metas e seus relacionamentos com as histórias de usuário. O estudo
avaliativo realizado encontrou evidências de que o artefato proporciona informações mais
estruturadas ao PO e, consequentemente, contribui para as suas tomadas de decisões.},
   author = {Renato Mesquita Soares},
   keywords = {GORE,História do usuário,Planejamento,Product backlog,Product owner,Scrum,masterThesis},
   month = {7},
   publisher = {Universidade Federal do Rio Grande do Norte},
   title = {Product backlog orientado a metas em projetos scrum para fundamentar as tomadas de decisões do product owner},
   url = {https://repositorio.ufrn.br/handle/123456789/32354},
   year = {2020},
}
@article{Aguilar2014,
   abstract = {In Web Engineering (WE), several Goal-oriented Requirements Engineering (GORE) approaches have emerged using its advantages, such as the representation of actors, their intentions, goals and the tasks needed to achieve the goal, for requirements specification with promising results. Regrettably, the use of GORE approaches has one, among others, gap detected, the scalability. In these modeling frameworks, when the designer performs the requirements specification, the requirements diagram (model) trends to rapidly grow, becoming very difficult to use in projects with a considerable amount of requirements changing and growing constantly. In this paper, we propose an association form for the i* goal-oriented modeling framework in order to define the creation of two type of modules: Navigational and Service modules, since these are the two types of functional requirements more used for requirements specification in our proposal. Furthermore, we provide an example of application. Finally, with this approach, the benefits are: firstly, the scalability of the Web requirements model will be increased, therefore the model will be less complex and easier to understand and maintain, and secondly, the construction of modeling tools improving the user experience, the maintainability of the models and its reuse. © 2014 Springer International Publishing.},
   author = {José Alfonso Aguilar and Anibal Zaldívar and Carolina Tripp and Sanjay Misra and Salvador Sánchez and Miguel Martínez and Omar Vicente García},
   doi = {10.1007/978-3-319-09156-3_10/COVER},
   isbn = {9783319091556},
   issn = {16113349},
   issue = {PART 5},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Goal-oriented Requirements Engineering,Requirements Modeling,Scalability,Web Engineering,i-star},
   pages = {135-145},
   publisher = {Springer Verlag},
   title = {A solution proposal for complex web application modeling with the i-star framework},
   volume = {8583 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-319-09156-3_10},
   year = {2014},
}
@article{Tenso2017,
   abstract = {The use of agile methods in software engineering is a standard practice and user stories are established artefacts used for breaking complex system requirements into smaller subsets. However, user stories do not suffice for understanding the big picture of system requirements. While there are methods that try to solve this problem, they lack visual tool support and are too heavy for smaller projects. We have earlier proposed a novel agile agent-oriented modelling (AAOM) method for filling this gap. The AAOM method comprises a visual approach to requirements engineering in agile projects that is based on goal models originating in agent-oriented modelling and connects goals intuitively to user stories. The purpose of the study reported in this article was evaluating the AAOM method for requirements engineering in two real-life case studies. The qualitative evaluation explores the applicability of AAOM for requirements engineering in agile software development processes.},
   author = {Tanel Tenso and Alexander Horst Norta and Hannes Rootsi and Kuldar Taveter and Irina Vorontsova},
   doi = {10.1109/REW.2017.24},
   isbn = {9781538634882},
   journal = {Proceedings - 2017 IEEE 25th International Requirements Engineering Conference Workshops, REW 2017},
   keywords = {Agile software engineering,Case study,Goal model,User story},
   month = {9},
   pages = {268-275},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Enhancing requirements engineering in agile methodologies by agent-oriented goal models: Two empirical case studies},
   year = {2017},
}
@article{Sandfreni2016,
   abstract = {Requirements engineering approach through intentional perspective is one of the arguments that appear in the field of requirement engineering. That approach can explain the characteristics of the behavior of an actor. The usage Goal Based Workflow and KAOS method in iStar modeling might help the system analyst to gain knowledge about the internal process inside each of actor sequentially, such that the whole sequential activity to achieve the goal are exposed clearly in those actor’s internal process. The adoption of the concept of the role of RACI diagram on Role Based Goal Oriented Model system analyst gain complete knowledge about requirements of actor who involve in a system. System analyst might also distinguish the dependency between each actor in each process. Those dependencies are exhibited in strategic dependency model. In addition, the internal activities of the actor are also shown in strategic rationale model.},
   author = {S. Sandfreni and Ir Kridanto Surendro},
   doi = {10.1051/MATECCONF/20165201004},
   issn = {2261-236X},
   journal = {MATEC Web of Conferences},
   keywords = {chemistry,conference,engineering,materials,open access,proceedings},
   month = {4},
   pages = {01004},
   publisher = {EDP Sciences},
   title = {Requirements Engineering Model: Role Based Goal Oriented Model},
   volume = {52},
   url = {https://www.matec-conferences.org/articles/matecconf/abs/2016/15/matecconf_icdes2016_01004/matecconf_icdes2016_01004.html},
   year = {2016},
}
@article{Kaiya2016,
   abstract = {Several actors such as human, organization, software applications and hardware units perform our daily activities such as medical care, entertainment and so on. We call each daily activity a socio-technical system (STS), and we also call actors except human and organizations Machines. Human and organizations in an STS become better than ever when new Machines are introduced into the STS and they are beneficial to human and organizations. Although modelling goal dependencies in such a STS contributes to identifying beneficial Machines because such a dependency can represent an actor asks some Machine to achieve his own goal. It is however not easy for modelers to describe a correct dependency. We thus proposed and exemplified an extended modelling notation called Goal Dependency Model with Objucts (GDMO) based on strategic dupendency (SD) in i∗. In GTMO, objects related to a goalan SD are explicitly specified Modelers can determine an actor has the right to want the goal0to be achieved because relationships between the actor and the objects such as ownership clarify the right. They can also determine another actor has the ability to achieve the goal. In addition, relationships among objects, i.e. a domain model, can suggest missing SDs, and the boundary0of an STS can be determined witxout omission.},
   author = {Haruhiko Kaiya},
   doi = {10.1016/J.PROCS.2016.08.242},
   issn = {1877-0509},
   journal = {Procedia Computer Science},
   keywords = {Early requirements analysis,Goal-oriented requirements engineering,Strategic dependency,istar},
   month = {1},
   pages = {791-800},
   publisher = {Elsevier},
   title = {Modelling Goal Dependencies and Domain Model Together},
   volume = {96},
   year = {2016},
}
@article{Li2021,
   abstract = {Digital twin (DT) provides a solution for supporting the interconnection between the physical world and the virtual world. When implementing DT integration, it is challenging to implement interface definition, information and service integration across DTs. This paper proposes a semantic modeling approach with a High-Level Architecture (HLA) to support the DT integration. The semantic modeling approach based on Graph-Object-Property-Point-Role-Relationship (GOPPRR) meta-meta models is used to realize the integrated formalisms of heterogeneous DTs. HLA is used to support interface definition and service integration between virtual entities of DT. Finally, a case of an unmanned aerial vehicle (UAV) landing on ship is used to verify the flexibility of this approach. From the results, we find the GOPPRR ontology and HLA specification enables to provide a unified formalism of the DTs of UAV and the ship, and to implement data exchange during the distributed simulation execution.},
   author = {Han Li and Jinzhi Lu and Xiaochen Zheng and Guoxin Wang and Dimitris Kiritsis},
   doi = {10.1007/978-3-030-85910-7_24/COVER},
   isbn = {9783030859091},
   issn = {1868422X},
   journal = {IFIP Advances in Information and Communication Technology},
   keywords = {Digital Twin,Distributed Simulation,Ontology,Semantic Model},
   pages = {228-236},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Supporting Digital Twin Integration Using Semantic Modeling and High-Level Architecture},
   volume = {633 IFIP},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-85910-7_24},
   year = {2021},
}
@inproceedings{Duffau2020,
   abstract = {The creation of a product, whether it is an object or a service, is accompanied by the production of justifications which may be, depending on the case, elements of conformity in the context of quality, traceability documents, experimental reports, expert reports, etc. In critical contexts, such as medical, railway or aeronautics, it is mandatory to convince a certifying authority that the development of a product has been carried out correctly. This obligation leads to an inflation of justification documents, which makes it difficult to read and to understand this set of justifications. To structure these justifications, it may be useful to use Justification Diagrams. However, these diagrams, while useful, are only an informal graphical notation. In this article, we define a formal semantics of the Justification Diagram and we give the first hints of what could be a software to support the design of such diagrams. © 2018 INFORSID 2018 - Actes du 36th Congres INFORSID. All rights reserved.},
   author = {C Duffau and T Polacsek and M Blay-Fornarino},
   journal = {INFORSID 2018 - Actes du 36th Congres INFORSID},
   keywords = {Certifying authorities;  Formal Semantics;  Graphical notation,Formal methods;  Railroad transportation;  Semantics,Graphic methods},
   note = {cited By 0; Conference of 36th Annuel Congres INFormatique des ORganisations et Systemes d'Information et de Decision, INFORSID 2018 - 36th Annual Congress on Informatics for Organization, Decision and Information Systems, INFORSID 2018 ; Conference Date: 28 May 2018 Through 31 May 2018;  Conference Code:160526},
   pages = {109-124},
   publisher = {INFORSID},
   title = {A semantics for justification diagrams [Une sémantique pour les patrons de justification]},
   url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089499448&partnerID=40&md5=fb3807d8f62f5418b6b4945366873a26},
   year = {2020},
}
@article{,
   abstract = {Many applications by design depend on costly trusted third-party auditors. One such example is the industrial application case of federated multidisciplinary optimization (MDO), in which different organizations contribute to a complex engineering design effort. Although blockchain and distributed ledger technology (DLT) has strong potential in reducing the dependence on such intermediaries, the architectural complexity involved in designing a solution is daunting. In this paper, we analyze the architectural variants for decentralized private data sharing while guaranteeing auditability and non-repudiation of data access operations, as well availability of the shared data. The architectural variants analyzed focus on attaining: (i) confidential data exchange, (ii) governing access to the shared data, (iii) providing data access auditability, and (iv) data validation or conflict resolution. We systematically enumerate architectural decisions at the levels of: storage, policy-based file access control, data encryption methods, and auditability mechanisms for private data. The main contribution of this work is a comprehensive overview of architectural variants for decentralized control of private en-crypted data, and the involved trade-offs in terms of performance, storage overhead, auditable trust and security. These findings are validated in the context of the aforementioned industry case that involves federated multidisciplinary optimization (MDO). CCS CONCEPTS • Software and its engineering → Peer-to-peer architectures; • Applied computing → Enterprise information systems;},
   author = {Vincent Reniers and Dimitri Van Landuyt and Paolo Viviani and Bert Lagaisse and Riccardo Lombardi and Wouter Joosen},
   city = {New York, NY, USA},
   doi = {10.1145/3297280},
   isbn = {9781450359337},
   journal = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
   keywords = {Blockchain storage,Decentralized data access control,Decentralized private data auditing,Distributed shared ledger},
   publisher = {ACM},
   title = {Analysis of Architectural Variants for Auditable Blockchain-based Private Data Sharing KEYWORDS},
   url = {https://doi.org/10.1145/3297280.3297316},
}
@article{Michael2020,
   abstract = {Auditing is an increasingly essential tool for the defense of computing systems, but the unwieldy nature of log data imposes significant burdens on administrators and analysts. To address this issue, a variety of techniques have been proposed for approximating the contents of raw audit logs, facilitating efficient storage and analysis. However, the security value of these approximated logs is difficult to measure - relative to the original log, it is unclear if these techniques retain the forensic evidence needed to effectively investigate threats. Unfortunately, prior work has only investigated this issue anecdotally, demonstrating sufficient evidence is retained for specific attack scenarios. In this work, we address this gap in the literature through formalizing metrics for quantifying the forensic validity of an approximated audit log under differing threat models. In addition to providing quantifiable security arguments for prior work, we also identify a novel point in the approximation design space - that log events describing typical (benign) system activity can be aggressively approximated, while events that encode anomalous behavior should be preserved with lossless fidelity. We instantiate this notion of Attack-Preserving forensic validity in LogApprox, a new approximation technique that eliminates the redundancy of voluminous file I/O associated with benign process activities. We evaluate LogApprox alongside a corpus of exemplar approximation techniques from prior work and demonstrate that LogApprox achieves comparable log reduction rates while retaining 100% of attack-identifying log events. Additionally, we utilize this evaluation to illuminate the inherent trade-off between performance and utility within existing approximation techniques. This work thus establishes trustworthy foundations for the design of the next generation of efficient auditing frameworks.},
   author = {Noor Michael and Jaron Mink and Jason Liu and Sneha Gaur and Wajih Ul Hassan and Adam Bates},
   doi = {10.1145/3427228.3427272},
   isbn = {9781450388580},
   journal = {ACM International Conference Proceeding Series},
   keywords = {Auditing,Data Provenance,Digital Forensics},
   month = {12},
   pages = {189-202},
   publisher = {Association for Computing Machinery},
   title = {On the Forensic Validity of Approximated Audit Logs},
   url = {https://doi.org/10.1145/3427228.3427272},
   year = {2020},
}
@article{Adlam2020,
   abstract = {In recent years, electronic health records (EHRs) have been subject to data breaches; this indicates that the current EHR infrastructure is no longer suitable for safeguarding health records. Audit logs are one of the key processes in identifying culprits responsible for these data breaches. Audit logs are often tampered with to cover a criminal's tracks. Blockchain technology could provide a tamper-evident audit log process to improve the current EHR infrastructure. Hyperledger Fabric has been identified as a permissioned blockchain technology suited towards use-cases that require privacy. An experiment was conducted with Hyperledger Fabric to demonstrate how permissioned blockchain technology could be used to generate an audit trail for an EHR system. The proposed blockchain infrastructure is presented as a data-flow diagram and smart contracts were employed to explore the capabilities permissioned blockchain technology could provide in terms of improving the EHR audit log process. The result of the experiment revealed that permissioned blockchain technology could serve as an alternative to the audit log process employed by traditional EHRs.},
   author = {Ryno Adlam and Bertram Haskins},
   doi = {10.1145/3415088.3415118},
   isbn = {9781450375580},
   journal = {ACM International Conference Proceeding Series},
   keywords = {audit log,blockchain,electronic health records},
   month = {9},
   publisher = {Association for Computing Machinery},
   title = {A permissioned blockchain approach to electronic health record audit logs},
   year = {2020},
}
@inproceedings{Chen2017,
   abstract = {In cloud storage, the data owner and data users can easily perform cooperative work on shared data. In this paper, we focus on operation behavior auditing in shared cloud storage, which is significant for the avoidance of potential crimes in the cloud and equitable accountability determination for the forensic investigation in shared cloud. We first introduce a novel secure public auditin...},
   author = {Zhaoyi Chen and Hui Tian and Jing Lu and Wenqi Chen and Yiqiao Cai and Tian Wang and Yonghong Chen},
   journal = {2017 IEEE International Conference on Computational Science and Engineering (CSE) and IEEE International Conference on Embedded and Ubiquitous Computing (EUC)},
   publisher = {IEEE},
   title = {Secure Public Audit for Operation Behavior Logs in Shared Cloud Storage},
   volume = {2},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/8005972/},
   year = {2017},
}
@article{Chen2021,
   abstract = {Audit log contains the trace of different activities in computing systems, which makes it critical for security management, censorship, and forensics. However, experienced attackers may delete or modify the audit log after their attacks, which makes the audit log unavailable in attack investigation. In this paper, we focus on the log integrity audit ...},
   author = {Jing Chen and Xin Chen and Kun He and Ruiying Du and Weihang Chen and Yang Xiang},
   journal = {IEEE Transactions on Dependable and Secure Computing},
   publisher = {IEEE},
   title = {DELIA: Distributed Efficient Log Integrity Audit Based on Hierarchal Multi-Party State Channel},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/9465691/},
   year = {2021},
}
@inproceedings{Zhao2018,
   abstract = {Audit logs are widely used in information systems nowadays. In cloud computing and cloud storage environment, audit logs are required to be encrypted and outsourced on remote servers to protect the confidentiality of data and the privacy of users. The searchable encrypted audit logs support a search on the encrypted audit logs. In this paper, we prop...},
   author = {Weiwei Zhao and Li Qiang and Huanying Zou and Aixin Zhang and Jianhua Li},
   journal = {2018 5th IEEE International Conference on Cyber Security and Cloud Computing (CSCloud)/2018 4th IEEE International Conference on Edge Computing and Scalable Cloud (EdgeCom)},
   publisher = {IEEE},
   title = {Privacy-Preserving and Unforgeable Searchable Encrypted Audit Logs for Cloud Storage},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/8421848/},
   year = {2018},
}
@article{Ahmad2022,
   abstract = {Audit trails are critical components in enterprise business applications, typically used for storing, tracking, and auditing data. Entities in the audit trail applications have weak trust boundaries, which expose them to various security risks and attacks. To harden the security and develop secure by design applications, blockchain technology has been recently introduced in the [...},
   author = {Ashar Ahmad and Muhammad Saad and Mohammed Al Ghamdi and DaeHun Nyang and David Mohaisen},
   issue = {1},
   journal = {IEEE Systems Journal},
   publisher = {IEEE},
   title = {BlockTrail: A Service for Secure and Transparent Blockchain-Driven Audit Trails},
   volume = {16},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/9528166/},
   year = {2022},
}
@inproceedings{Kumar2018,
   abstract = {We are living in the era of information technology. Our day to day life is heavily dependent on it and hence it's safe and secure functioning become very crucial and important. Every minute, there are thousands of cyber-attacks taking place around the world. Attackers are continuously evolving sophisticated and stealthy techniques to target the victim. They takes all the precautionary measures to ...},
   author = {Manish Kumar and Ashish Kumar Singh and T V Suresh Kumar},
   journal = {2018 9th International Conference on Computing, Communication and Networking Technologies (ICCCNT)},
   publisher = {IEEE},
   title = {Secure Log Storage Using Blockchain and Cloud Infrastructure},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/8494085/},
   year = {2018},
}
@inproceedings{Ning2017,
   abstract = {Secure logging as an indispensable part of any secure system in practice is well-understood by both academia and industry. However, providing security for audit logs on an untrusted machine in a large distributed system is still a challenging task. The emergence and wide availability of log management tools prompted plenty of work in the security community that allows clien...},
   author = {Fangxiao Ning and Yu Wen and Gang Shi and Dan Meng},
   journal = {2017 IEEE 36th International Performance Computing and Communications Conference (IPCCC)},
   publisher = {IEEE},
   title = {Efficient tamper-evident logging of distributed systems via concurrent authenticated tree},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/8280476/},
   year = {2017},
}
@inproceedings{Tian2020,
   abstract = {With the continuous progress of the information age, e-commerce, the Internet of things and other emerging Internet areas are gradually emerging. Massive amount of structured data auditing becomes a major issue. Log files and other data can be uploaded to the cloud via the Internet to guard against potential threats. Difficulty now is how to realize the data in the field of data aud...},
   author = {Zhenxun Tian},
   journal = {2020 IEEE International Conference on Power, Intelligent Computing and Systems (ICPICS)},
   publisher = {IEEE},
   title = {Design and Implementation of Distributed Government Audit System Based on Multidimensional Online Analysis},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/9202396/},
   year = {2020},
}
@inproceedings{Ahmad2022,
   abstract = {Audit systems maintain detailed logs of security-related events on enterprise machines to forensically analyze potential incidents. In principle, these logs should be safely stored in a secure location (e.g., network storage) as soon as they are produced, but this incurs prohibitive slowdown to a monitored machine. Hence, existing audit systems protect batched logs as...},
   author = {Adil Ahmad and Sangho Lee and Marcus Peinado},
   journal = {2022 IEEE Symposium on Security and Privacy (SP)},
   publisher = {IEEE},
   title = {HARDLOG: Practical Tamper-Proof System Auditing Using a Novel Audit Device},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/9833745/},
   year = {2022},
}
@inproceedings{Rose2017,
   abstract = {Computing systems produce large amounts of system log information at a scale wildly disproportionate to the growth of computing and bandwidth resources. This growth outpaces the ability of human auditors and administrators to digest such quantities of data via manual analysis. This situation is only expected to worsen over time as more and more data become available due to expanded technolog...},
   author = {Isis Rose and Nicholas Felts and Alexander George and Emily Miller and Max Planck},
   journal = {2017 IEEE Cybersecurity Development (SecDev)},
   publisher = {IEEE},
   title = {Something Is Better Than Everything: A Distributed Approach to Audit Log Anomaly Detection},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/8077810/},
   year = {2017},
}
@inproceedings{Ben2018,
   abstract = {In the context of Advanced Persistent Threat-s (APTs), system audit log-based intrusion forensics has been proposed to carry out attack investigation. System audit log is highly suitable for intrusion forensics because it records the interactions among system entities in detail. However, system audit log has a fatal shortcoming due to its massive growth of lo...},
   author = {Yongming Ben and Yanni Han and Ning Cai and Wei An and Zhen Xu},
   journal = {2018 IEEE 24th International Conference on Parallel and Distributed Systems (ICPADS)},
   publisher = {IEEE},
   title = {T-Tracker: Compressing System Audit Log by Taint Tracking},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/8645035/},
   year = {2018},
}
@inproceedings{,
   abstract = {Personal data have been compiled and harnessed by a great number of establishments to execute their legal activities. Establishments are legally bound to maintain the confidentiality and security of personal data. Hence it is a requirement to provide access logs for the personal information. Depending on the needs and capacity, personal data can be opened to the users via platforms such as f...},
   author = {Abdulsamet Haşiloğlu and Abdulkadir Bali},
   journal = {2018 6th International Symposium on Digital Forensic and Security (ISDFS)},
   publisher = {IEEE},
   title = {Central audit logging mechanism in personal data web services},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/8355333/},
   year = {2018},
}
@inproceedings{Wen2017,
   abstract = {Log is an important document produced and retained by the computer system, recording a large number of criminals who use computers to commit crimes. It is a very important source of clues and evidence against computer crime. To fully use log to implement computer forensics, two problems need to be solved: one is to extract the log in a timely manner in accordance with the procedu...},
   author = {Linbin Wen},
   journal = {2017 International Conference on Computer Technology, Electronics and Communication (ICCTEC)},
   publisher = {IEEE},
   title = {Research on System Design and Implementation of Computer Forensics Based on Log},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/8789302/},
   year = {2017},
}
@inproceedings{Raja2017,
   abstract = {In recent year, Cloud Computing can act as a next generation architecture for computing resource over the internet. Traditionally the business organization and enterprise uses cloud data center with firewall to store their information and uses security methods to protect data from intruders. However in cloud computing managing the log records for their huge information and log audit...},
   author = {J. Raja and M. Ramakrishnan},
   journal = {2016 Eighth International Conference on Advanced Computing (ICoAC)},
   publisher = {IEEE},
   title = {Implementing continuous auditing and compression technique in log auditing},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/7951769/},
   year = {2017},
}
@article{,
   abstract = {After a seminal article introducing-evidence based software engineering in 2004, systematic reviews (SR) have been increasingly used as a method for conducting secondary studies in software engineering. Our goal is to critically appraise the use of SR in software engineering with respect to the research questions asked and the ways the questions were used in the reviews. We analyzed 53 literature reviews that had been collected in two published tertiary studies. We found that over 65% of the research questions asked in the reviews were exploratory and only 15% investigated causality questions. We concluded that there is a need for a consistent use of terminology to classify secondary studies and that reports of literature reviews should follow reporting guidelines to support assessment and comparison. © 2010 ACM.},
   author = {Fabio Q.B. Da Silva and André L.M. Santos and Sérgio C.B. Soares and A. César C. França and Cleviton V.F. Monteiro},
   doi = {10.1145/1852786.1852830},
   isbn = {9781450300391},
   journal = {ESEM 2010 - Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
   keywords = {mapping studies,software engineering,systematic reviews},
   publisher = {Association for Computing Machinery},
   title = {A critical appraisal of systematic reviews in software engineering from the perspective of the research questions asked in the reviews},
   year = {2010},
}
@article{Wohlin2014,
   abstract = {Background: Systematic literature studies have become common in software engineering, and hence it is important to understand how to conduct them efficiently and reliably. Objective: This paper presents guidelines for conducting literature reviews using a snowballing approach, and they are illustrated and evaluated by replicating a published systematic literature review. Method: The guidelines are based on the experience from conducting several systematic literature reviews and experimenting with different approaches. Results: The guidelines for using snowballing as a way to search for relevant literature was successfully applied to a systematic literature review. Conclusions: It is concluded that using snowballing, as a first search strategy, may very well be a good alternative to the use of database searches.},
   author = {Claes Wohlin},
   doi = {10.1145/2601248.2601268},
   isbn = {9781450324762},
   journal = {dl.acm.org},
   keywords = {D2 [Software Engineering]: Management,Measurement Keywords Systematic literature review,and G3 [Probability and Statistics]: Experimental design General Terms Experimentation,replication,snowball search,snowballing,systematic mapping studies},
   publisher = {Association for Computing Machinery},
   title = {Guidelines for Snowballing in Systematic Literature Studies and a Replication in Software Engineering},
   url = {https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.709.9164&rep=rep1&type=pdf},
   year = {2014},
}
@article{,
   abstract = {Context: Many modern software systems must deal with changes and uncertainty. Traditional dependability requirements engineering is not equipped for this since it assumes that the context in which a system operates be stable and deterministic, which often leads to failures and recurrent corrective maintenance. The Contextual Goal Model (CGM), a requirements model that proposes the idea of context-dependent goal fulfillment, mitigates the problem by relating alternative strategies for achieving goals to the space of context changes. Additionally, the Runtime Goal Model (RGM) adds behavioral constraints to the fulfillment of goals that may be checked against system execution traces. Objective: This paper proposes GODA (Goal-Oriented Dependability Analysis) and its supporting framework as concrete means for reasoning about the dependability requirements of systems that operate in dynamic contexts. Method: GODA blends the power of CGM, RGM and probabilistic model checking to provide a formal requirements specification and verification solution. At design time, it can help with design and implementation decisions; at runtime it helps the system self-adapt by analyzing the different alternatives and selecting the one with the highest probability for the system to be dependable. GODA is integrated into TAO4ME, a state-of-the-art tool for goal modeling and analysis. Results: GODA has been evaluated against feasibility and scalability on Mobee: a real-life software system that allows people to share live and updated information about public transportation via mobile devices, and on larger goal models. GODA can verify, at runtime, up to two thousand leaf-tasks in less than 35ms, and requires less than 240 KB of memory. Conclusion: Presented results show GODA's design-time and runtime verification capabilities, even under limited computational resources, and the scalability of the proposed solution.},
   author = {Danilo Filgueira Mendonça and Genaína Nunes Rodrigues and Raian Ali and Vander Alves and Luciano Baresi},
   keywords = {Dependability,Goal modeling,Probabilistic model checking,Runtime analysis},
   month = {12},
   pages = {245-264},
   title = {GODA: A goal-oriented requirements engineering framework for runtime dependability analysis},
   volume = {80},
   year = {2016},
}
@article{Yue2011,
   abstract = {Model transformation is one of the basic principles of Model Driven Architecture. To build a software system, a sequence of transformations is performed, starting from requirements and ending with implementation. However, requirements are mostly in the form of text, but not a model that can be easily understood by computers; therefore, automated transformations from requirements to analysis models are not easy to achieve. The overall objective of this systematic review is to examine existing literature works that transform textual requirements into analysis models, highlight open issues, and provide suggestions on potential directions of future research. The systematic review led to the analysis of 20 primary studies (16 approaches) obtained after a carefully designed procedure for selecting papers published in journals and conferences from 1996 to 2008 and Software Engineering textbooks. A conceptual framework is designed to provide common concepts and terminology and to define a unified transformation process. This facilitates the comparison and evaluation of the reviewed papers. © 2010 The Author(s).},
   author = {Tao Yue and Lionel C. Briand and Yvan Labiche},
   doi = {10.1007/S00766-010-0111-Y/TABLES/5},
   issn = {09473602},
   issue = {2},
   journal = {Requirements Engineering},
   keywords = {Analysis model,Natural language,Requirements,Systematic review,Traceability,Transformation},
   month = {6},
   pages = {75-99},
   publisher = {Springer},
   title = {A systematic review of transformation approaches between user requirements and analysis models},
   volume = {16},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/article/10.1007/s00766-010-0111-y},
   year = {2011},
}
@article{Haj2021,
   abstract = {To support decision making, organizations tend to operate according to Business Rules, which are usually represented in a natural language format easily understood by all intervenors. According to the business rules manifesto by the Business Rules Group (OMG), rules build on facts, and facts build on concepts as expressed by terms. To avoid ambiguity and misunderstanding, the standardization of the terminology used at the business level becomes a persistent need. However, doing so manually is error prone and time consuming, especially that the Business Rules are the subject of continuous updating. In this paper, we present an automated approach to generate the Business Vocabulary from textual statements of Business Rules. Our approach is distinguished from existing works in that it extracts the Terminological Dictionary as described by the Semantic of Business Vocabulary and Rules (SBVR) standard to provide a more comprehensive meaning for each concept. Accordingly, an in-depth Natural Language Processing (NLP) is used to extract not only flat list of terms and relations, but also extra specifications and implicit knowledge. With a satisfactory result, our approach has proved its capability to automatically generate the SBVR Terminological Dictionary from large number of natural language business rules statements.},
   author = {Abdellatif Haj and Youssef Balouki and Taoufiq Gadi},
   doi = {10.1002/SMR.2339},
   issn = {2047-7481},
   issue = {5},
   journal = {Journal of Software: Evolution and Process},
   keywords = {SBVR,business rules,business vocabulary,software automation,terminological dictionary},
   month = {5},
   pages = {e2339},
   publisher = {John Wiley & Sons, Ltd},
   title = {Automated generation of terminological dictionary from textual business rules},
   volume = {33},
   url = {https://onlinelibrary-wiley.ez67.periodicos.capes.gov.br/doi/full/10.1002/smr.2339 https://onlinelibrary-wiley.ez67.periodicos.capes.gov.br/doi/abs/10.1002/smr.2339 https://onlinelibrary-wiley.ez67.periodicos.capes.gov.br/doi/10.1002/smr.2339},
   year = {2021},
}
@article{Karpovic2012,
   abstract = {Structured language, based on Semantics of Business Vocabulary and Business Rules (SBVR), can be seen as domain expert friendly means for developing OWL2 ontologies, which are becoming more and more important in Semantic Web and Enterprise applications. The goal of the paper is to present transformations from SBVR specifications to ontologies and to describe conditions for creating "right" vocabularies in order to obtain consistent ontologies without losing information. The need for such approach is caused by several reasons. Concept models rely on the closed world assumption, whereas ontologies rely on the open one where every constraint should be explicitly specified. Both SBVR and OWL2 have terminology related part, desirable being separated from the substantial ontology. We suggest rules that can help creating meaningful SBVR vocabularies regarding consequences of affecting the behavior of ontology reasoners, taking advantages of ontologies and retaining terminological information separately from the main ontology. © 2012 Springer-Verlag.},
   author = {Jaroslav Karpovic and Lina Nemuraite and Milda Stankeviciene},
   doi = {10.1007/978-3-642-33308-8_35/COVER},
   isbn = {9783642333071},
   issn = {18650929},
   journal = {Communications in Computer and Information Science},
   keywords = {ATL,OWL2,SBVR,Semantics,business rules,business vocabulary,consistency,ontology,transformations},
   pages = {420-435},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Requirements for semantic business vocabularies and rules for transforming them into consistent OWL2 ontologies},
   volume = {319 CCIS},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-642-33308-8_35},
   year = {2012},
}
@article{Daclin2016,
   abstract = {Interoperability analysis is highly correlated with interoperability requirements, the ability to grasp, structure, author and verify such requirements has become fundamental to the analytical process. To this end, requirements must be: (1) properly submitted in a suitable and usable repository; (2) written correctly by stakeholders with relevance to the studied domain; and (3) as easily verifiable as possible on various models of the system for which interoperability capabilities are being requested. The purpose of this article is to present both a structured repository for interoperability requirements and a Domain Specific Language to write and verify interoperability requirements - within a collaborative process model - using formal verification techniques. The interoperability requirements repository, which serves to structure interoperability requirements and make them available, is itself structured through abstraction levels, views and interoperability life cycle dimensions. Additional parameters detailing the requested information and the known impacts of requirements on behavior of the studied system have also been included. The Domain Specific Language provides the means for writing interoperability requirements. Afterwards, these requirements - more specifically the temporal requirements - are re-written into properties by transforming the temporal logic TCTL to allow for their effective verification by using the model checker UPPAAL. The overall approach is illustrated in a case study based on a collaborative drug circulation process. The article also draws conclusions and offers an outlook for future research and application efforts.},
   author = {N. Daclin and S. Mallek Daclin and V. Chapurlat and B. Vallespir},
   doi = {10.1016/J.COMPIND.2016.04.001},
   issn = {0166-3615},
   journal = {Computers in Industry},
   keywords = {Domain Specific Language,Interoperability requirements,Repository for interoperability requirements,Requirements verification},
   month = {10},
   pages = {1-18},
   publisher = {Elsevier},
   title = {Writing and verifying interoperability requirements: Application to collaborative processes},
   volume = {82},
   year = {2016},
}
@article{Rojas2013,
   abstract = {KPIs (key performance indicators) are sets of measures for assessing organizational performance and fulfilling organizational strategic objectives. Such measures are useful for achieving organizational success. Therefore, a good representation of KPIs is required; a good knowledge representation should exhibit the following features: (i) proficiency in computational tractability; (ii) clear and accurate syntax; (iii) clear and accurate semantics; (iv) stakeholder understandability; and (v) extensibility. Several approaches have been proposed for representing KPIs; however, they are incomplete; and they exhibit some of the aforementioned drawbacks. In this paper we propose an executable pre-conceptual schema to achieve an appropriate knowledge representation of KPIs. This work demonstrates the benefits of using such a schema by developing a case study, and by providing a comparison of our proposal against relevant studies aimed at representing KPIs. © 2013 IEEE.},
   author = {Luis Fernando Castro Rojas and Carlos Mario Zapata Jaramillo},
   doi = {10.1109/COLOMBIANCC.2013.6637526},
   isbn = {9781479910564},
   journal = {2013 8th Computing Colombian Conference, 8CCC 2013},
   keywords = {KPI,indicator,knowledge representation,measure,metric,performance,pre-conceptual schemas,programming and software engineering},
   publisher = {IEEE Computer Society},
   title = {Executable pre-conceptual schemas for representing key performance indicators},
   year = {2013},
}
@article{Graml2008,
   abstract = {A problem of today's standard business process (BP) automation systems is that they are too rigid to cope with changing business demands, especially for long running BPs. A solution to overcome this problem is to combine BPs with business rules (BRs). State-of-the-art BP automation systems use WS composition languages and BR services. Often these BRs are used to make calculations and to adapt simple decisions to business users without full integration into a BP automation system. The authors show that BP execution and BR functionality can be integrated properly in a standard service-oriented architecture. This finding is applied in a new approach of configuring BPs through using BRs. The assumption is that if one considers BRs already while modelling a BP, more advanced BP aspects like decisions, data constraints and control flow can be made agile and adaptive during run-time. BP modelling patterns are presented that demonstrate how BRs can be used to obtain different aspects of BP agility. Furthermore, different implementational aspects of bringing BPs and BRs together are discussed and it is shown how to implement these patterns using the IBM WebSphere integration developer and the IBM WebSphere process server.},
   author = {Tobias Graml and Ralf Bracht and Marcus Spies},
   doi = {10.1109/EDOC.2007.35},
   month = {4},
   pages = {365-365},
   publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
   title = {Patterns of Business Rules to Enable Agile Business Processes},
   year = {2008},
}
@article{Franch2016,
   abstract = {i* is a widespread framework in the software engineering field that supports goal-oriented modeling of socio-technical systems and organizations. At its heart lies a language offering concepts such as actor, dependency, goal and decomposition. i* models resemble a network of interconnected, autonomous, collaborative and dependable strategic actors. Around this language, several analysis techniques have emerged, e.g., goal satisfaction analysis and metrics computation. In this work, we present a consolidated version of the i* language based on the most adopted versions of the language. We define the main constructs of the language and we articulate them in the form of a metamodel. Then, we implement this version and a concrete technique, goal satisfaction analysis based on goal propagation, using ADOxx. Throughout the chapter, we used an example based on open source software adoption to illustrate the concepts and test the implementation.},
   author = {Xavier Franch and Lidia López and Carlos Cares and Daniel Colomer},
   doi = {10.1007/978-3-319-39417-6_22},
   isbn = {9783319394176},
   journal = {Domain-Specific Conceptual Modeling: Concepts, Methods and Tools},
   keywords = {Goal-oriented modeling,Goal-oriented requirements engineering,Satisfaction analysis techniques,i*-Framework,i-Star},
   month = {7},
   pages = {485-506},
   publisher = {Springer International Publishing},
   title = {The i* framework for goal-oriented modeling},
   year = {2016},
}
@article{,
   abstract = {It is important to count on tools to help software professionals to evaluate the software process and how it may be affected by factors related to its deployment. Simulation models are a valuable means to illustrate the behaviour of such a process since scenario generation supports the prediction of potential outcomes and the prevention of undesired scenarios which are harmful to the process and the company in charge of the project to be developed. This work explores the effectiveness of introducing system dynamics (SD) models in the software engineers’ process of understanding, from a management perspective, the software process dynamics. The used SD simulation model of the software process emphasises the representation of an iterative process. The COCOMO II model drivers and their main attributes were used, providing a set of reference factors that affect the software process, the estimation of project cost and the effort required. A set of 59 junior software professionals with no previous knowledge about SD participated in a validation study. For simple predictive scenarios, there was no important improvement effect, while for more complex predictive scenarios SD helped them to guess better and provide a rationale for the expected behaviour of the software process performance.},
   author = {German Lenin Dugarte-Peña and María Isabel Sánchez-Segura and Antonio de Amescua and Fuensanta Medina-Domínguez and Stefano Armenia},
   doi = {10.1049/SFW2.12031},
   issn = {17518814},
   issue = {6},
   journal = {IET Software},
   keywords = {project management,software cost estimation,software development management,software metrics,software process improvement,software quality},
   month = {12},
   pages = {351-364},
   publisher = {John Wiley and Sons Inc},
   title = {Using system dynamics to teach about dependencies, correlation and systemic thinking on the software process workflows},
   volume = {15},
   year = {2021},
}
@article{Oran2017,
   abstract = {Effective requirements communication is essential in software development projects due to the importance of understanding the requirements throughout the software development cycle. Software requirements can be specified in different formats, for instance using free texts or more structured forms, such as use cases and user stories used in Behavior Driven Development (BDD). We present a comparative analysis on the requirements communication dynamics using use case specification and user stories as the basis for mockups creation. We carried out an exploratory empirical study involving 16 students. The study comprised 3 steps: requirements specification, mockups construction, and inspection to investigate whether the mockups were in accordance with the specifications. Results show that there is no significant difference in using use case specification or user stories to communicate software requirements. Our findings suggest that different specification formats can provide similar results while communicating requirements, nonetheless the human factor should not be neglected.},
   author = {Ana Carolina Oran and Elizamary Nascimento and Gleison Santos and Tayana Conte},
   doi = {10.1145/3131151.3131166},
   isbn = {9781450353267},
   journal = {ACM International Conference Proceeding Series},
   keywords = {Behavior Driven Development,Experimental study,Requirements communication,Requirements specification,Use case},
   month = {9},
   pages = {214-223},
   publisher = {Association for Computing Machinery},
   title = {Analysing Requirements Communication Using Use Case Specification and User stories},
   year = {2017},
}
@article{Bjarnason2016,
   abstract = {Context It is an enigma that agile projects can succeed ‘without requirements’ when weak requirements engineering is a known cause for project failures. While agile development projects often manage well without extensive requirements test cases are commonly viewed as requirements and detailed requirements are documented as test cases. Objective We have investigated this agile practice of using test cases as requirements to understand how test cases can support the main requirements activities, and how this practice varies. Method We performed an iterative case study at three companies and collected data through 14 interviews and two focus groups. Results The use of test cases as requirements poses both benefits and challenges when eliciting, validating, verifying, and managing requirements, and when used as a documented agreement. We have identified five variants of the test-cases-as-requirements practice, namely de facto, behaviour-driven, story-test driven, stand-alone strict and stand-alone manual for which the application of the practice varies concerning the time frame of requirements documentation, the requirements format, the extent to which the test cases are a machine executable specification and the use of tools which provide specific support for the practice of using test cases as requirements. Conclusions The findings provide empirical insight into how agile development projects manage and communicate requirements. The identified variants of the practice of using test cases as requirements can be used to perform in-depth investigations into agile requirements engineering. Practitioners can use the provided recommendations as a guide in designing and improving their agile requirements practices based on project characteristics such as number of stakeholders and rate of change.},
   author = {Elizabeth Bjarnason and Michael Unterkalmsteiner and Markus Borg and Emelie Engström},
   doi = {10.1016/J.INFSOF.2016.03.008},
   issn = {09505849},
   journal = {Information and Software Technology},
   keywords = {Acceptance test,Agile development,Behaviour-driven development,Case study,Empirical software engineering,Requirements,Test-driven development,Test-first development,Testing},
   month = {9},
   pages = {61-79},
   publisher = {Elsevier B.V.},
   title = {A multi-case study of agile requirements engineering and the use of test cases as requirements},
   volume = {77},
   year = {2016},
}
@article{Tiwari2015,
   abstract = {Context Use cases have been widely accepted and acknowledged as a specification tool for specifying the functional requirements of a software system. Many variations of use cases exist which tries to address the issues such as their completeness, degree of formalism, automated information extraction, usability, and pertinence. Objective The aim of this systematic review is to examine the existing literature for the evolution of the use cases, their applications, quality assessments, open issues, and the future directions. Method We perform keyword-based extensive search to identify the relevant studies related to use case specifications research reported in journal articles, conference papers, workshop papers, bulletins and book chapters. Results The specified search process resulted 119 papers, which were published between 1992 and February 2014. This included, 54 journal articles, 42 conference papers, 2 ACM/IEEE bulletins, 12 book chapters, 6 workshop papers and 3 white papers. We found that as many as twenty use case templates have been proposed and applied for various software specification problems ranging from informal descriptions with paragraph-style text to more formal keyword-oriented templates. Conclusion Use cases have been evolved from initial plain, semi-formal textual descriptions to a more formal template structure facilitating automated information extraction in various software development life cycle activities such as requirement documentation, requirement analysis, requirement validation, domain modeling, test case generation, planning and estimation, and maintenance. The issues that remain to be sorted out are (1) the right degree of formalism, (2) the efficient change management, (3) the industrial relevance, and (4) assessment of the quality of the specification. Additionally, its synergy with other software models that are used in the development processes is an issue that needs to be addressed.},
   author = {Saurabh Tiwari and Atul Gupta},
   doi = {10.1016/J.INFSOF.2015.06.004},
   issn = {09505849},
   journal = {Information and Software Technology},
   keywords = {Evolution,Guidelines,Quality,Systematic reviews,Use case specifications,Use case templates},
   month = {11},
   pages = {128-158},
   publisher = {Elsevier B.V.},
   title = {A systematic literature review of use case specifications research},
   volume = {67},
   year = {2015},
}
@article{Rahy2022,
   abstract = {Non-functional requirements define qualities of the software system that ensure effectiveness while embedding any constraints and restrictions on the design. A challenge rises with agile implementation in handling non-function requirements in regulated environments. Thus, a practitioner's perceptions of agile method tailoring are described in relation to inter-team boundaries and non-functional requirements. The research comprises 18 practitioner interviews from two multinational agile software development companies. Interviews were recorded, transcribed, and analysed using an approach informed by grounded theory and information flow models were used to compare and contrast interactions of processes. It was discovered that one of the case study companies managed non-functional requirements as artefacts in their agile methodology, while the other company reverts to conventional plan-based software development practices of documentation, timeline estimations, and safety critical requirements. This research creates a detailed comparison of these contrasting approaches. The main contribution of this study is a set of proposed recommendations to deal with non-functional requirements in a regulated environment using agile techniques. The introduction of two new artefacts, Documentation Work Item and Safety Critical Work Item, is recommended and it is accompanied with an illustrative example, to transform the handling of documentation and safety critical requirements in a more agile way.},
   author = {Scarlet Rahy and Julian M. Bass},
   doi = {10.1049/SFW2.12037},
   issn = {17518806},
   issue = {1},
   journal = {IET Software},
   month = {2},
   pages = {60-72},
   publisher = {John Wiley and Sons Inc},
   title = {Managing non-functional requirements in agile software development},
   volume = {16},
   year = {2022},
}
@article{,
   abstract = {Requirements Engineering (RE) has received much attention in research and practice due to its importance to software project success. Its interdisciplinary nature, the dependency to the customer, and its inherent uncertainty still render the discipline difficult to investigate. This results in a lack of empirical data. These are necessary, however, to demonstrate which practically relevant RE problems exist and to what extent they matter. Motivated by this situation, we initiated the Naming the Pain in Requirements Engineering (NaPiRE) initiative which constitutes a globally distributed, bi-yearly replicated family of surveys on the status quo and problems in practical RE. In this article, we report on the qualitative analysis of data obtained from 228 companies working in 10 countries in various domains and we reveal which contemporary problems practitioners encounter. To this end, we analyse 21 problems derived from the literature with respect to their relevance and criticality in dependency to their context, and we complement this picture with a cause-effect analysis showing the causes and effects surrounding the most critical problems. Our results give us a better understanding of which problems exist and how they manifest themselves in practical environments. Thus, we provide a first step to ground contributions to RE on empirical observations which, until now, were dominated by conventional wisdom only.},
   author = {D. Méndez Fernández and S. Wagner and M. Kalinowski and M. Felderer and P. Mafra and A. Vetrò and T. Conte and M. T. Christiansson and D. Greer and C. Lassenius and T. Männistö and M. Nayabi and M. Oivo and B. Penzenstadler and D. Pfahl and R. Prikladnicki and G. Ruhe and A. Schekelmann and S. Sen and R. Spinola and A. Tuzcu and J. L. de la Vara and R. Wieringa},
   doi = {10.1007/S10664-016-9451-7},
   issn = {15737616},
   issue = {5},
   journal = {Empirical Software Engineering},
   keywords = {Requirements engineering,Survey research},
   month = {10},
   pages = {2298-2338},
   publisher = {Springer New York LLC},
   title = {Naming the pain in requirements engineering: Contemporary problems, causes, and effects in practice},
   volume = {22},
   year = {2017},
}
@article{Oran2019,
   abstract = {Requirements communication is essential in software development projects since customer needs must be communicated to the development team clearly and effectively. Although the use case (UC) specifications are used to communicate requirements in detail, developers do not always follow them. This study presents an empirical study carried out to understand the reasons why developers do not follow UC specifications and their difficulties using UC specifications in generating prototypes. Results show that the four reasons why developers fail to follow UC specifications are the existence of specification errors, ambiguous information, lack of detailed specification, or incomplete information, and due to improvement suggestions. Also, the specification defects types that impacted prototypes creation the most were an omission, ambiguity, and incorrect fact. The authors noted that 6 out of 25 (24%) defects in the UC specifications caused discrepancies in prototypes, 12 (48%) were corrected during the prototypes creation, and 7 (28%) were not propagated to the prototypes.},
   author = {Ana Carolina Oran and Natasha Valentim and Gleison Santos and Tayana Conte},
   doi = {10.1049/IET-SEN.2018.5239},
   issn = {1751-8814},
   issue = {6},
   journal = {IET Software},
   keywords = {UC specifications,detailed specification,development team,generating prototypes,project management,requirements communication,software development management,software development projects,software engineering,software prototyping,specification defects types,specification errors,use case specifications},
   month = {12},
   pages = {510-517},
   publisher = {The Institution of Engineering and Technology},
   title = {Why use case specifications are hard to use in generating prototypes?},
   volume = {13},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1049/iet-sen.2018.5239 https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-sen.2018.5239 https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/iet-sen.2018.5239},
   year = {2019},
}
@article{Boyer2011,
   abstract = {Target audience (Must) business analyst; (optional) project manager, application architect, rule author In this chapter you will learn What are the different types of rules, and why it is important to understand them How to set in place the rule harvesting process...},
   author = {Jérôme Boyer and Hafedh Mili},
   doi = {10.1007/978-3-642-19041-4_4},
   journal = {Agile Business Rule Development},
   pages = {73-113},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Rule Harvesting},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-19041-4_4},
   year = {2011},
}
@article{Zacharias2009,
   abstract = {Recently, with the large-scale practical use of business rule systems and the interest of the Semantic Web community in rule languages, there is an increasing need for methods and tools supporting the development of rule-based systems. Existing methodologies fail to...},
   author = {Valentin Zacharias},
   doi = {10.1007/978-0-387-68772-8_8},
   journal = {Information Systems Development},
   pages = {93-104},
   publisher = {Springer, Boston, MA},
   title = {The Agile Development of Rule Bases},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-0-387-68772-8_8},
   year = {2009},
}
@article{,
   abstract = {Modelling languages are frequently extended to include new constructs to be used together with the original syntax. New constructs may be proposed by adding textual information, such as UML stereotypes, or by creating new graphical representations. Thus, these new symbols need to be expressive and proposed in a careful way to increase the extension’s adoption. A method to create symbols for the original constructs of a modelling language was proposed and has been used to create the symbols when a new modelling language is designed. We argue this method can be used to recommend new symbols for the extension’s constructs. However, it is necessary to make some adjustments since the new symbols will be used with the existing constructs of the modelling language original syntax. In this paper, we analyse the usage of this adapted method to propose symbols to mitigate the occurrence of overloaded symbols in the existing iStar extensions. We analysed the existing iStar extensions in an SLR and identified the occurrence of symbol overload among the existing constructs. We identified a set of fifteen overloaded symbols in existing iStar extensions. We used these concepts with symbol overload in a multi-stage experiment that involved users in the visual notation design process. The study involved 262 participants, and its results revealed that most of the new graphical representations were better than those proposed by the extensions, with regard to semantic transparency. Thus, the new representations can be used to mitigate this kind of conflict in iStar extensions. Our results suggest that next extension efforts should consider user-generated notation design techniques in order to increase the semantic transparency.},
   author = {Enyo Gonçalves and Camilo Almendra and Miguel Goulão and João Araújo and Jaelson Castro},
   doi = {10.1007/S10270-019-00770-9/FIGURES/10},
   issn = {16191374},
   issue = {3},
   journal = {Software and Systems Modeling},
   keywords = {Experiment,Model-based engineering,Modelling language extensions,Semiotic clarity principle,Symbol overload,iStar},
   month = {5},
   pages = {763-784},
   publisher = {Springer},
   title = {Using empirical studies to mitigate symbol overload in iStar extensions},
   volume = {19},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/article/10.1007/s10270-019-00770-9},
   year = {2020},
}
@article{Dardenne1993,
   abstract = {Requirements analysis includes a preliminary acquisition step where a global model for the specification of the system and its environment is elaborated. This model, called requirements model, involves concepts that are currently not supported by existing formal specification languages, such as goals to be achieved, agents to be assigned, alternatives to be negotiated, etc. The paper presents an approach to requirements acquisition which is driven by such higher-level concepts. Requirements models are acquired as instances of a conceptual meta-model. The latter can be represented as a graph where each node captures an abstraction such as, e.g., goal, action, agent, entity, or event, and where the edges capture semantic links between such abstractions. Well-formedness properties on nodes and links constrain their instances-that is, elements of requirements models. Requirements acquisition processes then correspond to particular ways of traversing the meta-model graph to acquire appropriate instances of the various nodes and links according to such constraints. Acquisition processes are governed by strategies telling which way to follow systematically in that graph; at each node specific tactics can be used to acquire the corresponding instances. The paper describes a significant portion of the meta-model related to system goals, and one particular acquisition strategy where the meta-model is traversed backwards from such goals. The meta-model and the strategy are illustrated by excerpts of a university library system. © 1993.},
   author = {Anne Dardenne and Axel van Lamsweerde and Stephen Fickas},
   doi = {10.1016/0167-6423(93)90021-g},
   issn = {01676423},
   issue = {1-2},
   journal = {Sci. Comput. Program.},
   keywords = {Requirements engineering,conceptual modeling,domain analysis,meta-level inference,nonfunctional requirements,specification acquisition,specification reuse},
   pages = {3-50},
   title = {Goal-directed requirements acquisition},
   volume = {20},
   year = {1993},
}
@article{Liskin2014,
   abstract = {User stories are a widespread instrument for representing requirements. They describe small user-oriented parts of the system and guide the daily work of developers. Often however, user stories are too coarse, so that misunderstandings or dependencies remain unforeseeable. Granularity of user stories needs to be investigated more, but at the same time is a hard-to-grasp concept. This paper investigates Expected Implementation Duration (EID) of a user story as a characteristic of granularity. We want to find out, whether it is suitable as a quality aspect and can help software teams improve their user stories. We have conducted a study with software engineering practitioners. There, many user stories had a relatively high EID of four or more days. Many developers state to have experienced certain problems to occur more often with such coarse user stories. Our findings emphasize the importance to reflect on granularity when working with user stories. © Springer International Publishing Switzerland 2014.},
   author = {Olga Liskin and Raphael Pham and Stephan Kiesling and Kurt Schneider},
   doi = {10.1007/978-3-319-06862-6_8/COVER},
   isbn = {9783319068619},
   issn = {18651348},
   journal = {Lecture Notes in Business Information Processing},
   keywords = {Requirements quality,User requirements,User stories},
   pages = {110-125},
   publisher = {Springer Verlag},
   title = {Why we need a granularity concept for user stories},
   volume = {179 LNBIP},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-319-06862-6_8},
   year = {2014},
}
@article{,
   abstract = {In agile methods the user stories are widely used to describe requirements. However, the user stories are an artifact too narrow to represent and detail the requirements. Issues like software context and dependencies between stories are also limited with the use of only this artifact. The lack of documentation in agile development environment is identified as one of the main challenges of the methodology. This work proposes the use of i* model that aims to reduce this lack of existing documentation in agile methods. We propose a set of heuristics to perform the mapping of the requirements presented as user stories in i* models. The i* models are used as a form of documentation in agile environment, thus the user stories can be viewed more broadly and with their proper relationships according to the business environment that they will meet.},
   author = {Aline Jaqueira and Márcia Lucena and Eduardo Aranha and Fernanda Alencar and Jaelson Castro},
   journal = {Citeseer},
   keywords = {Agile Requirements,User Stories,i* Models},
   title = {Using i* Models to Enrich User Stories},
   url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.363.5799&rep=rep1&type=pdf},
}
@inproceedings{,
   abstract = {User stories are widely used to capture the desires of the users in agile development. A set of user stories is easy to read and write but incapable of representing the hierarchical relations and synergies among the user stories. By contrast, goal models are uncommon in industrial projects however they can express the structure and other relations am...},
   author = {Tuğçe Günes and Cahid Arda Öz and Fatma Başak Aydemir},
   journal = {2021 IEEE 29th International Requirements Engineering Conference (RE)},
   publisher = {IEEE},
   title = {ArTu: A Tool for Generating Goal Models from User Stories},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/9604615/},
   year = {2021},
}
@inproceedings{Grundy2020,
   abstract = {Many current software systems suffer from a lack of consideration of the human differences between end users. This includes age, gender, language, culture, emotions, personality, education, physical and mental challenges, and so on. We describe our work looking to consider these characteristics by incorporation of human centric-issues throughout the model-driven engineering process lifecycle. We propose the use of the co-creational "living lab" model to better collect human-centric issues in the software requirements. We focus on modelling these human-centric factors using domain-specific visual languages, themselves humancentric modelling artefacts. We describe work to incorporate these human-centric issues into model-driven engineering design models, and to support both code generation and run-time adaptation to different user human factors. We discuss continuous evaluation of such human-centric issues in the produced software and feedback of user reported defects to requirements and model refinement.},
   author = {John Grundy and Hourieh Khalajzadeh and Jennifer Mcintosh},
   doi = {10.5220/0009806002290238},
   isbn = {978-989-758-421-3},
   journal = {Proceedings of the 15th International Conference on Evaluation of Novel Approaches to Software Engineering},
   keywords = {Human Factors,Human-centric Software Engineering,Model-driven Engineering},
   pages = {229-238},
   publisher = {SCITEPRESS - Science and Technology Publications},
   title = {Towards Human-centric Model-driven Software Engineering},
   url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0009806002290238},
   year = {2020},
}
@article{Pissierssens2019,
   abstract = {Data is omnipresent in the modern, digital world and a significant number of people need to make sense of data as part of their everyday social and professional life. Therefore, together with the rise of data, the design of graphical representations has gained importance and attention. Yet, although a large body of procedural knowledge about effective visualization exists, the quality of representations is often reported to be poor, proposedly because these guidelines are scattered, unstructured and sometimes perceived as contradictive. Therefore, this paper describes a literature research addressing these problems. The research resulted in the collection and structuring of 81 guidelines and 34 underlying propositions, as well as in the derivation of 7 foundational principles about graphical representation design, called the “Physics of Diagrams”, which are illustrated with concrete, practical examples throughout the paper.},
   author = {Sarah Pissierssens and Jan Claes and Geert Poels},
   journal = {arXiv},
   keywords = {Conceptual modeling,Graphical representation design,Human factors,Knowledge representation},
   pages = {1-42},
   title = {The “Physics of Diagrams”: Revealing the scientific basis of graphical representation design},
   url = {https://arxiv.org/abs/1903.05941},
   year = {2019},
}
@article{Dittrich2010,
   abstract = {Software is created by people -- software engineers in cooperation with domain experts, users and other stakeholders--in varied environments, under various conditions. Thus understanding cooperativ...},
   author = {Yvonne Dittrich and Helen Sharp and Heike Winshiers Theophilus and Cleidson De Souza and Mikko Korpela and Janice Singer},
   doi = {10.1145/1838687.1838693},
   issn = {0163-5948},
   issue = {5},
   journal = {ACM SIGSOFT Software Engineering Notes},
   keywords = {cooperative and human factors,software engineering},
   month = {10},
   pages = {27-29},
   publisher = {
		ACM
		PUB27
		New York, NY, USA
	},
   title = {Cooperative and human aspects of software engineering},
   volume = {35},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/1838687.1838693},
   year = {2010},
}
@article{Jakumeit2014,
   abstract = {Model transformation is one of the key tasks in model-driven engineering and relies on the efficient matching and modification of graph-based data structures; its sibling graph rewriting has been used to successfully model problems in a variety of domains. Over the last years, a wide range of graph and model transformation tools have been developed - all of them with their own particular strengths and typical application domains. In this paper, we give a survey and a comparison of the model and graph transformation tools that participated at the Transformation Tool Contest 2011. The reader gains an overview of the field and its tools, based on the illustrative solutions submitted to a Hello World task, and a comparison alongside a detailed taxonomy. The article is of interest to researchers in the field of model and graph transformation, as well as to software engineers with a transformation task at hand who have to choose a tool fitting to their needs. All solutions referenced in this article provide a SHARE demo. It supported the peer-review process for the contest, and now allows the reader to test the tools online. © 2013 Elsevier B.V.},
   author = {Edgar Jakumeit and Sebastian Buchwald and Dennis Wagelaar and Li Dan and Ábel Hegedüs and Markus Herrmannsdörfer and Tassilo Horn and Elina Kalnina and Christian Krause and Kevin Lano and Markus Lepper and Arend Rensink and Louis Rose and Sebastian Wätzoldt and Steffen Mazanek},
   doi = {10.1016/J.SCICO.2013.10.009},
   issn = {01676423},
   issue = {PART A},
   journal = {Science of Computer Programming},
   keywords = {Graph rewriting,Model transformation,Tool survey,Transformation tool contest},
   pages = {41-99},
   publisher = {Elsevier B.V.},
   title = {A survey and comparison of transformation tools based on the transformation tool contest},
   volume = {85},
   year = {2014},
}
@article{Iung2020,
   abstract = {Domain-specific languages (DSL) are programming or modeling languages devoted to a given application domain. There are many tools used to support the implementation of a DSL, making hard the decision-making process for one or another. In this sense, identifying and mapping their features is relevant for decision-making by academic and industrial initiative on DSL development. Objective: The goal of this work is to identify and map the tools, Language Workbenches (LW), or frameworks that were proposed to develop DSLs discussed and referenced in publications between 2012 and 2019. Method: A Systematic Mapping Study (SMS) of the literature scoping tools for DSL development. Results: We identified 59 tools, including 9 under a commercial license and 41 with non-commercial licenses, and analyzed their features from 230 papers. Conclusion: There is a substantial amount of tools that cover a large number of features. Furthermore, we observed that usually, the developer adopts one type of notation to implement the DSL: textual or graphical. We also discuss research gaps, such as a lack of tools that allow meta-meta model transformations and that support modeling tools interoperability.},
   author = {Aníbal Iung and João Carbonell and Luciano Marchezan and Elder Rodrigues and Maicon Bernardino and Fabio Paulo Basso and Bruno Medeiros},
   doi = {10.1007/S10664-020-09872-1/TABLES/9},
   issn = {15737616},
   issue = {5},
   journal = {Empirical Software Engineering},
   keywords = {DSL,DSL-supporting tools,Domain-specific language,Language workbench,Model driven engineering,Systematic mapping study,Systematic review},
   month = {9},
   pages = {4205-4249},
   publisher = {Springer},
   title = {Systematic mapping study on domain-specific language development tools},
   volume = {25},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/article/10.1007/s10664-020-09872-1},
   year = {2020},
}
@article{Horkoff2019,
   abstract = {Over the last two decades, much attention has been paid to the area of goal-oriented requirements engineering (GORE), where goals are used as a useful conceptualization to elicit, model, and analyze requirements, capturing alternatives and conflicts. Goal modeling has been adapted and applied to many sub-topics within requirements engineering (RE) and beyond, such as agent orientation, aspect orientation, business intelligence, model-driven development, and security. Despite extensive efforts in this field, the RE community lacks a recent, general systematic literature review of the area. In this work, we present a systematic mapping study, covering the 246 top-cited GORE-related conference and journal papers, according to Scopus. Our literature map addresses several research questions: we classify the types of papers (e.g., proposals, formalizations, meta-studies), look at the presence of evaluation, the topics covered (e.g., security, agents, scenarios), frameworks used, venues, citations, author networks, and overall publication numbers. For most questions, we evaluate trends over time. Our findings show a proliferation of papers with new ideas and few citations, with a small number of authors and papers dominating citations; however, there is a slight rise in papers which build upon past work (implementations, integrations, and extensions). We see a rise in papers concerning adaptation/variability/evolution and a slight rise in case studies. Overall, interest in GORE has increased. We use our analysis results to make recommendations concerning future GORE research and make our data publicly available.},
   author = {Jennifer Horkoff and Fatma Başak Aydemir and Evellin Cardoso and Tong Li and Alejandro Maté and Elda Paja and Mattia Salnitri and Luca Piras and John Mylopoulos and Paolo Giorgini},
   doi = {10.1007/S00766-017-0280-Z},
   issn = {1432010X},
   issue = {2},
   journal = {Requirements Engineering},
   keywords = {GORE,Goal model,Goal-oriented requirements engineering,Systematic mapping study},
   month = {6},
   pages = {133-160},
   publisher = {Springer London},
   title = {Goal-oriented requirements engineering: an extended systematic mapping study},
   volume = {24},
   year = {2019},
}
@article{Siena2008,
   abstract = {This paper evaluates the effectiveness of an extension to i* modelling - normative i* modelling - during the requirements analysis for new socio-technical systems for food traceability. The i* focus on modelling systems as networks of heterogeneous, inter-dependent actors provides limited support for modelling system-wide properties and norms, such as laws and regulations, that also influence the specification of socio-technical systems. In this paper we introduce an extension to i* to model and analyse norms, then apply it to model laws and regulations applicable to European food traceability systems. We report an analysis of the relative strengths and weaknesses of this extended form of i* with its traditional forms, and use results to answer two research questions about the usefulness and usability of the i* modelling extension. © 2008 Springer-Verlag Berlin Heidelberg.},
   author = {Alberto Siena and Neil Maiden and James Lockerbie and Kristine Karlsen and Anna Perini and Angelo Susi},
   doi = {10.1007/978-3-540-69534-9_15},
   isbn = {3540695338},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {182-196},
   title = {Exploring the effectiveness of normative i* modelling: Results from a case study on food chain traceability},
   volume = {5074 LNCS},
   year = {2008},
}
@article{Watson2017,
   abstract = {The human user is important to consider during system design. However, common system design models, such as the system modeling language, typically represent human users and operators as external actors, rather than as internal to the system. This research presents a method for integrating human considerations into system models through human-centered design. A specific system is selected to serve as the case study for demonstrating the methodology. The sample system is analyzed to identify the task and information flow. Then, both system- and human-centered diagrams are separately created to represent different viewpoints of the system. These diagrams are compared and analyzed, and new diagrams are created that incorporate both system and human considerations into one concordant representation of the system model. These new views allow systems engineers and human factors engineers to effectively communicate the role of the user during early system design trades.},
   author = {Michael E. Watson and Christina F. Rusnock and John M. Colombi and Michael E. Miller},
   doi = {10.1177/1555343417705255},
   issn = {21695032},
   issue = {3},
   journal = {Journal of Cognitive Engineering and Decision Making},
   keywords = {command and control,design methods,human systems integration,military,robotics,system dynamic analysis},
   month = {9},
   pages = {252-269},
   publisher = {SAGE Publications Inc.},
   title = {Human-Centered Design Using System Modeling Language},
   volume = {11},
   year = {2017},
}
@article{Zhang2018,
   abstract = {Generating meaningful layout of iStar models is a challenging task, which currently requires significant manual efforts. However, it is time-consuming when dealing with large-scale iStar modeling, rising the need of having an automatic iStar layout tool. Previously, we have proposed an algorithm for laying out iStar SD models and have implemented a corresponding prototype tool. In this paper, we report our ongoing empirical work which aims to evaluate the effectiveness and usability of the prototype tool. In particular, we present a research design which is applied to compare manual layout and automatic layout in terms of efficiency and model comprehensibility. Based on such a design, we are planning to carry out empirical studies accordingly in the near future.},
   author = {Haoyuan Zhang and Tong Li and Yunduo Wang},
   doi = {10.1007/978-3-030-01391-2_26/COVER},
   isbn = {9783030013905},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Automatic layout,Controlled experiment,Prototype tool,iStar models},
   pages = {206-211},
   publisher = {Springer Verlag},
   title = {Design of an Empirical Study for Evaluating an Automatic Layout Tool},
   volume = {11158 LNCS},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-030-01391-2_26},
   year = {2018},
}
@article{,
   abstract = {Although goal modelling is a recognised research area, only few empirical studies are reported. In this work we present an experiment where the quality of two goal languages - i* and KAOS - is investigated by means of the semiotic quality framework. We believed that a high quality language would contribute to effective and efficient modelling, and result in high quality models. But the experiment showed that model quality much depends on the particular language characteristics with respect to a given context. The experiment indicated weak and strong properties of goal modelling languages. For researchers, the findings point out possible language improvements. For practitioners, they can facilitate decisions about language selection and use. © Springer-Verlag Berlin Heidelberg 2007.},
   author = {Raimundas Matulevičius and Patrick Heymans},
   doi = {10.1007/978-3-540-73031-6_2},
   isbn = {9783540730309},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {18-32},
   publisher = {Springer Verlag},
   title = {Comparing goal modelling languages: An experiment},
   volume = {4542 LNCS},
   year = {2007},
}
@article{Kamthan2021,
   abstract = {The use of user stories for expressing software requirements, in agile software projects and beyond, has only increased over the years. However, the results on the quality of user stories have been mixed. This paper proposes a semiotics-based, technology-and-tool-independent, semi-formal framework, comprising interrelated conceptual (meta-)models that provide an understanding to the concept of user story, user story context, user story quality, and violations of user story quality. In doing so, it elicits the unique nature of user story, highlights the challenges in modeling and addressing user story quality, presents the results of a preliminary survey of students and professionals on the use of user story, and, through various real-world examples, illustrates violations of user story quality.},
   author = {Pankaj Kamthan and Nazlie Shahmir},
   doi = {10.1007/978-3-030-65796-3_40/COVER},
   isbn = {9783030657956},
   issn = {23673389},
   journal = {Lecture Notes in Networks and Systems},
   keywords = {Agile methodology,Conceptual modeling,Continuous requirements engineering,Interactive system,Requirements debt,Standard},
   pages = {413-422},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {A Framework for the Semiotic Quality of User Stories},
   volume = {182},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-030-65796-3_40},
   year = {2021},
}
@article{Taibi2017,
   abstract = {Context: Eliciting requirements from customers is a complex task. In Agile processes, the customer talks directly with the development team and often reports requirements in an unstructured way. The requirements elicitation process is up to the developers, who split it into user stories by means of different techniques. Objective: We aim to compare the requirements decomposition process of an unstructured process and three Agile processes, namely XP, Scrum, and Scrum with Kanban. Method: We conducted a multiple case study with a replication design, based on the project idea of an entrepreneur, a designer with no experience in software development. Four teams developed the project independently, using four different development processes. The requirements were elicited by the teams from the entrepreneur, who acted as product owner and was available to talk with the four groups during the project. Results: The teams decomposed the requirements using different techniques, based on the selected development process. Conclusion: Scrum with Kanban and XP resulted in the most effective processes from different points of view. Unexpectedly, decomposition techniques commonly adopted in traditional processes are still used in Agile processes, which may reduce project agility and performance. Therefore, we believe that decomposition techniques need to be addressed to a greater extent, both from the practitioners’ and the research points of view.},
   author = {Davide Taibi and Valentina Lenarduzzi and Andrea Janes and Kari Liukkunen and Muhammad Ovais Ahmad},
   doi = {10.1007/978-3-319-57633-6_5},
   isbn = {9783319576329},
   issn = {18651348},
   journal = {Lecture Notes in Business Information Processing},
   pages = {68-83},
   publisher = {Springer Verlag},
   title = {Comparing requirements decomposition within the Scrum, Scrum with Kanban, XP, and Banana development processes},
   volume = {283},
   year = {2017},
}
@article{Wautelet2018,
   abstract = {[Context and Motivation] User Stories (US) are often used as requirement representation artifacts within agile projects. Within US sets, the nature, granularity and inter-dependencies of the elements constituting each US is not or poorly represented. To deal with these drawbacks, previous research allowed to build a unified model for tagging the elements of the WHO, WHAT and WHY dimensions of a US; each tag representing a concept with an inherent nature and defined granularity. Once tagged, the US elements can be graphically represented with an icon and the modeler can define the inter-dependencies between the elements to build one or more so-called Rationale Trees (RT). [Question/Problem] RT and their benefits have been illustrated on case studies but the ability to easily build a RT in a genuine case for software modelers not familiar with the concepts needs to be evaluated. [Principal ideas/results] This paper presents the result of a double exercise aimed to evaluate how well novice and experienced modelers were able to build a RT out of an existing US set. The experiment explicitly forces the test subjects to attribute a concept to US elements and to link these together. [Contribution] On the basis of the conducted experiment, we highlight the encountered difficulties that the lambda modeler faces when building a RT with basic support. Overall, the test subjects have produced models of satisfying quality. Also, we highlight these necessary conditions that need to be provided to the lambda modeler to build a consistent RT.},
   author = {Yves Wautelet and Mattijs Velghe and Samedi Heng and Stephan Poelmans and Manuel Kolp},
   doi = {10.1007/978-3-319-77243-1_13/COVER},
   isbn = {9783319772424},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Granularity,Modeling experiment,Rationale tree,User story},
   pages = {209-226},
   publisher = {Springer Verlag},
   title = {On modelers ability to build a visual diagram from a user story set: A goal-oriented approach},
   volume = {10753 LNCS},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-319-77243-1_13},
   year = {2018},
}
@article{Wautelet2016,
   abstract = {Requirements representation in agile methods is often done on the basis of User Stories (US) which are short sentences relating a WHO, WHAT and (possibly) WHY dimension. They are by nature very operational and simple to understand thus very efficient. Previous research allowed to build a unified model for US templates associating semantics to a set of keywords based on templates collected over the web and scientific literature. Since the semantic associated to these keywords is mostly issued of the i∗ framework we overview in this paper how to build a custom rationale diagram on the basis of a US set tagged using that unified template. The rationale diagram is strictly speaking not an i∗ strategic rationale diagram but uses parts of its constructs and visual notation to build various trees of relating US elements in a single project. Indeed, the benefits of editing such a rationale diagram is to identify depending US, identifying EPIC ones and group them around common Themes. The paper shows the feasibility of building the rationale diagram, then points to the use of these consistent sets of US for iteration content planning. To ensure the US set and the rationale diagram constitute a consistent and not concurrent whole, an integrated Computer-Aided Software Engineering (CASE) tool supports the approach.},
   author = {Yves Wautelet and Samedi Heng and Manuel Kolp and Isabelle Mirbel and Stephan Poelmans},
   doi = {10.1109/RCIS.2016.7549299},
   isbn = {9781479987092},
   issn = {21511357},
   journal = {Proceedings - International Conference on Research Challenges in Information Science},
   keywords = {Agile Requirements Modeling,Rationale Diagram,SCRUM,User Story,User Story Template,eXtreme Programming},
   month = {8},
   publisher = {IEEE Computer Society},
   title = {Building a rationale diagram for evaluating user story sets},
   volume = {2016-August},
   year = {2016},
}
@article{Wautelet2014,
   abstract = {Within Agile methods, User Stories (US) are mostly used as primary requirements artifacts and units of functionality of the project. The idea is to express requirements on a low abstraction basis using natural language. Most of them are exclusively centered on the final user as only stakeholder. Over the years, some templates (in the form of concepts relating the WHO, WHAT and WHY dimensions into a phrase) have been proposed by agile methods practitioners or academics to guide requirements gathering. Using these templates can be problematic. Indeed, none of them define any semantic related to a particular syntax precisely or formally leading to various possible interpretations of the concepts. Consequently, these templates are used in an ad-hoc manner, each modeler having idiosyncratic preferences. This can nevertheless lead to an underuse of representation mechanisms, misunderstanding of a concept use and poor communication between stakeholders. This paper studies templates found in literature in order to reach unification in the concepts' syntax, an agreement in their semantics as well as methodological elements increasing inherent scalability of US-based projects. © 2014 Springer International Publishing.},
   author = {Yves Wautelet and Samedi Heng and Manuel Kolp and Isabelle Mirbel},
   doi = {10.1007/978-3-319-07881-6_15},
   isbn = {9783319078809},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Agile Requirements Modeling,Scrum,User Story Template,eXtreme Programming},
   pages = {211-225},
   publisher = {Springer Verlag},
   title = {Unifying and extending user story models},
   volume = {8484 LNCS},
   year = {2014},
}
@article{Wautelet2013,
   abstract = {[Context and Motivation] Business modeling is nowadays a common approach in huge enterprise software developments. It notably allows to align business processes and supporting IT solutions at best, to produce a documentation of the company's "savoir-faire" and to look for possible optimizations. The business modeling discipline of the Rational Unified Process (RUP) has enriched the semantic of the Unified Modeling Language's (UML) use case diagrams for the special purpose of representing the organization's processes with accurate elements. [Question/Problem] RUP/UML business use case scemantics are nevetheless only intended to further stereotype use case models and not to be used for reasoning. In parallel and in line with artificial intelligence concepts, researchers have developed the i* framework enabling the evaluation and decomposition of multiple design opportunities. RUP/UML business use case scemantics could be used more efficiently to integrate the latter benefits. [Principal ideas/results] Through a systematic mapping of elements from i* on the one side and of the RUP/UML business use case model on the other, we have set up a RUP/UML graphical notation for i* elements. Applicability has been shown on an illustrative example. [Contribution] The main contribution of the framework is allowing to model in an i* fashion using CASE-tools meant for RUP/UML and proposing an interface for forward engineering the produced model in a classical UML requirements model. Future work is required to fully validate the proposal, notably to measure the method's efficacy. © 2013 Springer-Verlag.},
   author = {Yves Wautelet and Manuel Kolp},
   doi = {10.1007/978-3-642-37422-7_17},
   isbn = {9783642374210},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Business Modeling,RUP/UML Business Use Case Model,i*},
   pages = {237-252},
   title = {Mapping i* within UML for business modeling},
   volume = {7830 LNCS},
   year = {2013},
}
@article{Yu2009,
   abstract = {Many different types of models are used in various scientific and engineering fields, reflecting the subject matter and the kinds of understanding that is sought in each field. Conceptual modeling techniques in software and information systems engineering have in the past focused mainly on describing and analyzing behaviours and structures that are implementable in software. As software systems become ever more complex and densely intertwined with the human social environment, we need models that reflect the social characteristics of complex systems. This chapter reviews the approach taken by the i*framework, highlights its application in several areas, and outlines some open research issues. © 2009 Springer Berlin Heidelberg.},
   author = {Eric S. Yu},
   doi = {10.1007/978-3-642-02463-4_7},
   isbn = {3642024629},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {99-121},
   title = {Social modeling and i*},
   volume = {5600 LNCS},
   year = {2009},
}
@article{Lucassen2016,
   abstract = {User stories are a widely adopted requirements notation in agile development. Yet, user stories are too often poorly written in practice and exhibit inherent quality defects. Triggered by this observation, we propose the Quality User Story (QUS) framework, a set of 13 quality criteria that user story writers should strive to conform to. Based on QUS, we present the Automatic Quality User Story Artisan (AQUSA) software tool. Relying on natural language processing (NLP) techniques, AQUSA detects quality defects and suggest possible remedies. We describe the architecture of AQUSA, its implementation, and we report on an evaluation that analyzes 1023 user stories obtained from 18 software companies. Our tool does not yet reach the ambitious 100 % recall that Daniel Berry and colleagues require NLP tools for RE to achieve. However, we obtain promising results and we identify some improvements that will substantially improve recall and precision.},
   author = {Garm Lucassen and Fabiano Dalpiaz and Jan Martijn E.M. van der Werf and Sjaak Brinkkemper},
   doi = {10.1007/S00766-016-0250-X},
   issn = {1432010X},
   issue = {3},
   journal = {Requirements Engineering},
   keywords = {AQUSA,Multi-case study,Natural language processing,QUS framework,Requirements quality,User stories},
   month = {9},
   pages = {383-403},
   publisher = {Springer London},
   title = {Improving agile requirements: the Quality User Story framework and tool},
   volume = {21},
   year = {2016},
}
@article{,
   abstract = {User stories are commonly used to capture user needs in agile methods due to their ease of learning and understanding. Yet, the simple structure of user stories prevents us from capturing relations among them. Such relations help the developers to better understand and structure the backlog items derived from the user stories. One solution to this problem is to build goal models that provide explicit relations among goals but require time and effort to build. This paper presents a pipeline to automatically generate a goal model from a set of user stories by applying natural language processing (NLP) techniques and our initial heuristics to build realistic goal models. We first parse and identify the dependencies in the user stories, and store the results in a graph database to maintain the relations among the roles, actions, and objects mentioned in the set of user stories. By applying NLP techniques and several heuristics, we generate goal models that resemble human-built models. Automatically generating models significantly decreases the time spent on this tedious task. Our research agenda includes calculating the similarity between the automatically generated models and the expert-built models. Our overarching research goals are to provide i. an NLP-powered framework that generates goal models from a set of user stories, ii. several heuristics to generate goal models that resemble human-built models, and iii. a repository that includes sets of user stories, with corresponding human-built and automatically generated goal models.},
   author = {Tuğçe Güneş and Fatma Başak Aydemir and Tu˘ Gçe and Güne¸s Güne¸s and Fatma Ba¸sak and Ba¸sak Aydemir},
   journal = {ceur-ws.org},
   keywords = {agile develop-ment,goal models,model driven development,natural language processing,requirements engi-neering,user stories},
   title = {US2StarTool: Generating i* Models From User Stories.},
   url = {http://ceur-ws.org/Vol-1402/paper19.pdf},
}
@article{Wang2021,
   abstract = {The utility and practicality of iStar framework can be affected by various factors, such as its conceptual model, ways of modeling, and supporting tools. In this paper, we report our ongoing empirical work that aims to investigate the experience of iStar beginners, based on which we explore how beginners can learn and use iStar framework more easily. In particular, we present a research design of this work and discuss initial results from a specific case. In the future, we plan to conduct a series of case studies, based on which we want to propose a practically efficient iStar modeling procedure and a corresponding CASE tool.},
   author = {Yunduo Wang and Jinlian Du and Tong Li},
   keywords = {Case study,Learner-friendly,iStar 20},
   title = {A Learner-Friendly Approach for Using the iStar Modeling Framework: an Ongoing Study},
   url = {http://ceur-ws.org/Vol-2983/iStar21_paper_7.pdf},
   year = {2021},
}
@article{,
   abstract = {The comprehensive syntax of iStar modeling language allows requirements analysts to clearly capture stakeholder's needs, as well as dependencies among stakeholders. However, such iStar models cannot be automatically laid out using typical layout algorithms, such as hierarchical layout and circular layout. Thus, constructing and adjusting iStar models are laborious tasks, especially when dealing with large-scale models. In this paper, we propose a tentative approach to automatically lay out iStar models using a force-based layout algorithm. In particular, our approach has been designed by taking into account the syntax of iStar models in order to ensure both neatness and understandability of resulting models.},
   author = {X Du and T Li and D Wang - iStar and undefined 2017},
   journal = {ceur-ws.org},
   title = {An Automatic Layout Approach for iStar Models.},
   url = {http://ceur-ws.org/Vol-1829/iStar17_paper_14.pdf},
}
@article{Agra2015,
   author = {C Agra and A Sousa and J Melo and M Lucena - Proceedings of the Eighth … and undefined 2015},
   journal = {ceur-ws.org},
   title = {Specifying guidelines to transform i* Model into User Stories: an overview},
   url = {http://ceur-ws.org/Vol-1402/paper21.pdf},
   year = {2015},
}
@article{Li2016,
   abstract = {Many iStar-based modeling and analysis tools have been developed for specific iStar-related purposes (e.g., tools enumerated in iStar wiki). Despite the proliferation of tools, new tools keep being built, taking resources and time and possibly "reinventing the wheel" in terms of tool functionality. To gain an in-depth understanding of this situation , we are interested in the challenges and tradeoffs in creating iStar tools. In this paper, we examine three diverse tools as case studies, illustrating the challenges and tradeoffs from the developer's perspective. Based on such studies, we establish a goal model to capture the collected knowledge, which can help researchers to make informed decisions and optimize their tool development.},
   author = {Tong Li and Alicia M Grubb and Jennifer Horkoff},
   title = {Understanding challenges and tradeoffs in istar tool development},
   url = {https://scholarworks.smith.edu/csc_facpubs/217/},
   year = {2016},
}
@article{,
   abstract = {Since its first proposal in the nineties, the i* framework has been used to requirements specification in many domains, such as healthcare, tele-communication, and air traffic control. After the modeling of different examples and case studies, it has been observed that i* models become dramatically more difficult to understand and analyze as they grow larger. This issue has led us to investigate scalability in the context of the i* framework, by means of a systematic mapping study. A total of 119 papers were analyzed, in order to understand how scalability is perceived by the i* research community, which proposals have considered this topic, and what open issues still need to be addressed. We found that scalability issues are indeed perceived as relevant and that further work is still required, even though many potential solutions have already been proposed. This study can be a starting point for researchers aiming to further advance the treatment of scalability in social goal models.},
   author = {Paulo Lima¹ and Jéssyka Vilela¹ and Enyo Gonçalves¹² and João Pimentel and Ana Holanda and Jaelson Castro¹ and Fernanda Alencar¹ and Maria Lencastre},
   keywords = {Goal Models,Scalability,Systematic Mapping study,i*,iStar},
   title = {Scalability of istar: a systematic mapping study},
   url = {http://www.inf.puc-rio.br/wer/WERpapers/artigos/artigos_WER16/WER_2016_paper_45.pdf},
   year = {2016},
}
@article{Wautelet2018,
   abstract = {[Context and Motivation] User Stories (US) are often used as requirement representation artifacts within agile projects. Within US sets, the nature , granularity and inter-dependencies of the elements constituting each US is not or poorly represented. To deal with these drawbacks, previous research allowed to build a unified model for tagging the elements of the WHO, WHAT and WHY dimensions of a US; each tag representing a concept with an inherent nature and defined granularity. Once tagged, the US elements can be graphically represented with an icon and the modeler can define the inter-dependencies between the elements to build one or more so-called Rationale Trees (RT). [Ques-tion/Problem] RT and their benefits have been illustrated on case studies but the ability to easily build a RT in a genuine case for software modelers not familiar with the concepts needs to be evaluated. [Principal ideas/results] This paper presents the result of a double exercise aimed to evaluate how well novice and experienced modelers were able to build a RT out of an existing US set. The experiment explicitly forces the test subjects to attribute a concept to US elements and to link these together. [Contribution] On the basis of the conducted experiment , we highlight the encountered difficulties that the lambda modeler faces when building a RT with basic support. Overall, the test subjects have produced models of satisfying quality. Also, we highlight these necessary conditions that need to be provided to the lambda modeler to build a consistent RT.},
   author = {Yves Wautelet and K U Leuven and Samedi Heng and Stephan Poelmans and Manuel Kolp and Mattijs Velghe},
   doi = {10.1007/978-3-319-77243-1_13},
   journal = {ksiresearch.org},
   keywords = {Granularity,Modeling Experiment,Rationale Tree,User Story},
   pages = {209-226},
   publisher = {Springer Verlag},
   title = {A Node-Merging based Approach for Generating iStar Models from User Stories},
   volume = {10753 LNCS},
   url = {http://ksiresearch.org/seke/seke22paper/paper176.pdf},
   year = {2018},
}
@article{Araujo2016,
   abstract = {Although goal modeling using i* provides several benefits to requirements analysis, it may be difficult to use it with agile methods. This paper proposes a method that combines i* with Scrum, integrating the initial phases of Tropos with Scrum practices. In summary, goals in a Strategic Rationale model are prioritized to a release and then refined into user stories. The stories are implemented in Sprints, following the Scrum activities. This paper also reports the use of this method to develop a mobile app that brings political decisions closer to the Brazilian electorate. 1 Introduction User story [1] is a popular requirements representation used in agile software development projects. Even though its textual template represents the goal behind a feature (in the "so that" part), it is not possible to reason about goals, especially higher-level goals, or reason about how goals are refined into requirements. This information, for example, is important when creating user stories, or when a product owner selects the features to be implemented in an iteration. Even if some agile methods, such as Scrum [2], do not impose using user stories, it is not clear how to use goal modeling in agile projects, with changing requirements and continuous delivery of software. This paper proposes a method that combines goal modeling, using i* models, with agile software development. We integrate the initial phases of Tropos, Early Requirements and Late Requirements, with Scrum practices. Differently from works that propose transforming i* models into user stories [3], transforming user stories into i* models [4], or obtaining a goal net model from user stories [5], our method involves refining a partial i* model into user stories that will be used to guide the software development.},
   author = {LB de Araujo and FL Siqueira - iStar and undefined 2016},
   journal = {ceur-ws.org},
   keywords = {agile,i-star,planning,requirements,scrum,tropos,user story},
   title = {Using i* with Scrum: An Initial Proposal.},
   url = {http://ceur-ws.org/Vol-1674/iStar16_pp19-24.pdf},
   year = {2016},
}
@article{Alaasam2021,
   abstract = {Digital twins of processes and devices use information from sensors to synchronize their state with the entities of the physical world. The concept of stream computing enables effective processing of events generated by such sensors. However, the need to track the state of an instance of the object leads to the impossibility of organizing instances of digital twins as stateless services. Another feature of digital twins is that several tasks implemented on their basis require the ability to respond to incoming events at near-real-time speed. In this case, the use of cloud computing becomes unacceptable due to high latency. Fog computing manages this problem by moving some computational tasks closer to the data sources. One of the recent solutions providing the development of loosely coupled distributed systems is a Microservice approach, which implies the organization of the distributed system as a set of coherent and independent services interacting with each other using messages. The microservice is most often isolated by utilizing containers to overcome the high overheads of using virtual machines. The main problem is that microservices and containers together are stateless by nature. The container technology still does not fully support live container migration between physical hosts without data loss. It causes challenges in ensuring the uninterrupted operation of services in fog computing environments. Thus, an essential challenge is to create a containerized stateful stream processing based microservice to support digital twins in the fog computing environment. Within the scope of this article, we study live stateful stream processing migration and how to redistribute computational activity across cloud and fog nodes using Kafka middleware and its Stream DSL API.},
   author = {Ameer Basim Abdulameer Alaasam and Gleb Igorevich Radchenko and Andrei Nikolaevitch Tchernykh and José Luis González-Compeán González-Compeán},
   issn = {20798156},
   issue = {1},
   journal = {Proceedings of the Institute for System Programming of the RAS},
   pages = {65-80},
   publisher = {Institute for System Programming of the Russian Academy of Sciences},
   title = {Stateful Stream Processing Containerized as Microservice to Support Digital Twins in Fog Computing},
   volume = {33},
   year = {2021},
}
@article{Kashyap2006,
   abstract = {We present an approach and architecture for implementing scalable and maintainable clinical decision support at the Partners HealthCare System. The architecture integrates a business rules engine that executes declarative if-then rules stored in a rule-base referencing objects and methods in a business object model. The rules engine executes object methods by invoking services implemented on the clinical data repository. Specialized inferences that support classification of data and instances into classes are identified and an approach to implement these inferences using an OWL based ontology engine is presented. Alternative representations of these specialized inferences as if-then rules or OWL axioms are explored and their impact on the scalability and maintenance of the system is presented. Architectural alternatives for integration of clinical decision support functionality with the invoking application and the underlying clinical data repository; and their associated trade-offs are discussed and presented.},
   author = {Vipul Kashyap and Alfredo Morales and Tonya Hongsermeier},
   issn = {15594076},
   journal = {AMIA ... Annual Symposium proceedings / AMIA Symposium. AMIA Symposium},
   pages = {414-418},
   pmid = {17238374},
   title = {On implementing clinical decision support: achieving scalability and maintainability by combining business rules and ontologies.},
   year = {2006},
}
@inproceedings{,
   abstract = {During the 1980 and 1990's decades, relational database management systems arose as an alternative to implement and store application business logic due to its robustness. Many of those legacy systems suffer from several problems such as low scalability, database vendor lock-in, and complex maintenance and evolution. With the success of lightweight virtualization techniques and new distributed architectures, mainly the microservices, companies are migrating legacy systems to this architectural style. Although several studies have proposed migration processes and reported migration experience to microservices, to the best of our knowledge, none of them has addressed systems whose business rules are implemented in database artifacts, particularly stored procedures. Therefore, this paper presents a process to identify microservice candidates from application business rules implemented in stored procedures. We applied the process to a real large scale system, for which 357 business rules were mapped and 13 microservices were identified. In addition, the process helped to find out many duplicated pieces of code, thus also improving the system maintainability.},
   author = {Marx Haron Gomes Barbosa and Paulo Henrique M. Maia},
   doi = {10.1109/ICSA-C50368.2020.00015},
   isbn = {9781728174150},
   journal = {Proceedings - 2020 IEEE International Conference on Software Architecture Companion, ICSA-C 2020},
   keywords = {legacy systems,microservice,process,stored procedure},
   month = {3},
   pages = {41-48},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Towards Identifying Microservice Candidates from Business Rules Implemented in Stored Procedures},
   year = {2020},
}
@misc{Malakuti2019,
   abstract = {The promise of a digital twin is to make asset lifecycle information accessible by providing a single access point to the information. Thereby, it reduces the required time and effort and enables new data-intensive use cases. This paper provides an abstract four-layer architecture pattern to construct digital twins and to incorporate information from various kinds of sources. The pattern is designed to be flexibly extensible with new information sources and can flexibly support new kinds of proprietary or standard information. We discuss various alternatives to implement the pattern and provide an example realization based on microservices and OPC UA.},
   author = {Somayeh Malakuti and Johannes Schmitt and Marie Platenius-Mohr and Sten Grüner and Ralf Gitzel and Prerna Bihani},
   isbn = {9783030299828},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Digital twin,Information model,Microservice,OPC UA},
   pages = {231-246},
   publisher = {Springer Verlag},
   title = {A four-layer architecture pattern for constructing and managing digital twins},
   volume = {11681 LNCS},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-030-29983-5_16},
   year = {2019},
}
@article{Choi2015,
   author = {Ilwoo Choi},
   doi = {10.1016/j.psra.2015.11.005},
   issn = {24058823},
   issue = {2},
   journal = {Pacific Science Review A: Natural Science and Engineering},
   month = {7},
   pages = {51-60},
   publisher = {Elsevier BV},
   title = {A study on rule separation based on AOP for an efficient service system},
   volume = {17},
   year = {2015},
}
@misc{Carvalho2019,
   abstract = {Microservices is an emerging industrial technique to promote better modularization and management of small and autonomous services. Microservice architecture is widely used to overcome the limitations of monolithic legacy systems, such as limited maintainability and reusability. Migration to a microservice architecture is increasingly becoming the focus of academic research. However, there is little knowledge on how microservices are extracted from legacy systems in practice. Among these limitations, there is a lack of understanding if variability is considered useful along the microservice extraction from a configurable system. In order to address this gap, we performed an exploratory study composed of two phases. Firstly, we conducted an online survey with 26 specialists that contributed to the migration of existing systems to a microservice architecture. Secondly, we performed individual interviews with seven survey participants. A subset of the participants (13 out of 26) dealt with systems with variability during the extraction, which stated that variability is a key criterion for structuring the microservices. Moreover, variability in the legacy system is usually implemented with simple mechanisms. Finally, initial evidence points out that microservices extraction can increase software customization.},
   author = {Luiz Carvalho and Alessandro Garcia and Wesley K.G. Assunção and Rodrigo Bonifácio and Leonardo P. Tizzei and Thelma Elita Colanzi},
   doi = {10.1145/3336294.3336319},
   isbn = {9781450371384},
   journal = {ACM International Conference Proceeding Series},
   keywords = {Architecture migration,Microservice architecture,Microservice customization,Software variability},
   month = {9},
   publisher = {Association for Computing Machinery},
   title = {Extraction of configurable and reusable microservices from legacy systems: An exploratory study},
   volume = {A},
   url = {https://doi.org/10.1145/3336294.3336319},
   year = {2019},
}
@article{Landre2006,
   abstract = {In this paper we present the experience gained and lessons learned when the IT department at Statoil ASA, a large Oil and Gas company in Norway, extended their Enterprise Architecture with strategic level Domain-Driven design techniques and used the extended Enterprise Architecture to improve the software architecture of a large enterprise system. Traditionally, Enterprise Architecture has been prescribed as the key tool to conquer complexity and align IT development with business priorities and strategies, but we found our Enterprise Architecture too coarse to be practical useful at the software level. By extending our Enterprise Architecture with context maps and the process of context mapping valuable insight was gained, insight that enabled better scoping of new projects and architectural improvement of existing software in a controlled way. In addition, use of responsibility layers combined with context maps reduces the perceived complexity of the architecture. Use of other techniques such as distillation and identification of the core domain looks promising at the tactical level of a single project, but its value is more uncertain at the strategic level. The key issue is that large enterprise systems do not have a single core. On the other hand, at the project level, there should always be a core, and the project is best of by knowing its core domain and aim its best resources to work with the core.},
   author = {Einar Landre and Harald Wesenberg and Harald Rønneberg},
   city = {New York, New York, USA},
   doi = {10.1145/1176617},
   isbn = {159593491X},
   journal = {Companion to the 21st ACM SIGPLAN conference on Object-oriented programming systems, languages, and applications  - OOPSLA '06},
   keywords = {Complexity,Context map,D211 [Software Engineering]: Software Architecture,Distillation,Domain-driven design,Enterprise architecture,Management Keywords Domain-Driven design,Responsibility layer,Theory,complexity,context map,distillation,enterprise architecture,responsibility layer},
   pages = {809-814},
   publisher = {ACM Press},
   title = {Architectural Improvement by use of Strategic Level Domain-Driven Design},
   volume = {2006},
   year = {2006},
}
@misc{Fontana2016,
   author = {Francesca Arcelli Fontana and Riccardo Roveda and Marco Zanoni and Claudia Raibulet and Rafael Capilla},
   doi = {10.1109/WICSA.2016.37},
   isbn = {9781509021314},
   month = {7},
   pages = {21-30},
   title = {An experience report on detecting and repairingsoftware architecture erosion},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/7516808/},
   year = {2016},
}
@article{Mumtaz2021,
   abstract = {The recognition of the need for high-quality software architecture is evident from the increasing trend in investigating architectural smells. Detection of architectural smells is paramount because they can seep through to design and implementation stages if left unidentified. Many architectural smells detection techniques and tools are proposed in the literature. The diversity in the detection techniques and tools suggests the need for their collective analysis to identify interesting aspects for practice and open research areas. To fulfill this, in this paper, we unify the knowledge about the detection of architectural smells through a systematic mapping study. We report on the existing detection techniques and tools for architectural smells to identify their limitations. We find there has been limited investigation of some architectural smells (e.g., micro-service smells); many architectural smells are not detected by tools yet; and there are limited empirical validations of techniques and tools. Based on our findings, we suggest several open research problems, including the need to (1) investigate undetected architectural smells (e.g., Java package smells), (2) improve the coverage of architectural smell detection across architecture styles (e.g., service-oriented and cloud), and (3) perform empirical validations of techniques and tools in industry across different languages and project domains.},
   author = {Haris Mumtaz and Paramvir Singh and Kelly Blincoe},
   doi = {10.1016/J.JSS.2020.110885},
   journal = {Journal of Systems and Software},
   keywords = {Antipatterns,Architectural debt,Architectural smells,Smell detection techniques,Systematic mapping study},
   month = {3},
   pages = {110885},
   title = {A systematic mapping study on architectural smells detection},
   volume = {173},
   year = {2021},
}
@article{Laukkanen2017,
   abstract = {Context: Continuous delivery is a software development discipline in which software is always kept releasable. The literature contains instructions on how to adopt continuous delivery, but the adoption has been challenging in practice. Objective: In this study, a systematic literature review is conducted to survey the faced problems when adopting continuous delivery. In addition, we identify causes for and solutions to the problems. Method: By searching five major bibliographic databases, we identified 293 articles related to continuous delivery. We selected 30 of them for further analysis based on them containing empirical evidence of adoption of continuous delivery, and focus on practice instead of only tooling. We analyzed the selected articles qualitatively and extracted problems, causes and solutions. The problems and solutions were thematically synthesized into seven themes: build design, system design, integration, testing, release, human and organizational and resource. Results: We identified a total of 40 problems, 28 causal relationships and 29 solutions related to adoption of continuous delivery. Testing and integration problems were reported most often, while the most critical reported problems were related to testing and system design. Causally, system design and testing were most connected to other themes. Solutions in the system design, resource and human and organizational themes had the most significant impact on the other themes. The system design and build design themes had the least reported solutions. Conclusions: When adopting continuous delivery, problems related to system design are common, critical and little studied. The found problems, causes and solutions can be used to solve problems when adopting continuous delivery in practice.},
   author = {Eero Laukkanen and Juha Itkonen and Casper Lassenius},
   doi = {10.1016/J.INFSOF.2016.10.001},
   issn = {09505849},
   journal = {Information and Software Technology},
   keywords = {Continuous delivery,Continuous deployment,Continuous integration,Systematic literature review},
   month = {2},
   pages = {55-79},
   publisher = {Elsevier B.V.},
   title = {Problems, causes and solutions when adopting continuous delivery—A systematic literature review},
   volume = {82},
   year = {2017},
}
@article{Carvallo2019,
   abstract = {Early phases of information systems engineering include the understanding of the enterprise’s context and the construction of models at different levels of decomposition, required to design the system architecture. These time-consuming activities are usually conducted by relatively large teams, composed of groups of non-technical stakeholders playing mostly an informative role (i.e. not involved in documentation and even less in modelling), led by few experienced technical consultants performing most of the documenting and modelling effort. This paper evaluates the ability of non-technical stakeholders to create strategic dependency diagrams written with the i* language in the design of the context model of a system architecture, and find out which difficulties they may encounter and what the quality of the models they build is. A case study involving non-technical stakeholders from 11 organizational areas in an Ecuadorian university held under the supervision and coordination of the two authors acting as consultants. The non-technical stakeholders identified the majority of the dependencies that should appear in the case study’s context model, although they experienced some difficulties in declaring the type of dependency, representing such dependencies graphically and applying the description guidelines provided in the training. Managers were observed to make more mistakes than other more operational roles. From the observations of these results, a set of methodological advices were compiled for their use in future, similar endeavours. It is concluded that non-technical stakeholders can take an active role in the construction of the context model. This conclusion is relevant for both researchers and practitioners involved in technology transfer actions with use of i*.},
   author = {Juan Pablo Carvallo and Xavier Franch},
   issn = {1432010X},
   issue = {1},
   journal = {Requirements Engineering},
   keywords = {Dependency,Empirical study,Enterprise architecture,Requirements engineering,System architecture,i* Framework,iStar},
   month = {3},
   pages = {27-53},
   publisher = {Springer London},
   title = {An empirical study on the use of i* by non-technical stakeholders: the case of strategic dependency diagrams},
   volume = {24},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/article/10.1007/s00766-018-0300-7},
   year = {2019},
}
@article{Alaasam2020,
   abstract = {Abstract: Digital twins of processes and devices use information from sensors to synchronize their state with the entities of the physical world. The concept of stream computing enables effective processing of events generated by such sensors. However, the need to track the state of an instance of the object leads to the impossibility of organizing instances of digital twins as stateless services. Another feature of digital twins is that several tasks implemented on their basis require the ability to respond to incoming events at near-real-time speed. In this case, the use of cloud computing becomes unacceptable due to high latency. Fog computing manages this problem by moving some computational tasks closer to the data sources. One of the recent solutions providing the development of loosely coupled distributed systems is a Microservice approach, which implies the organization of the distributed system as a set of coherent and independent services interacting with each other using messages. The microservice is most often isolated by utilizing containers to overcome the high overheads of using virtual machines. The main problem is that microservices and containers together are stateless by nature. The container technology still does not fully support live container migration between physical hosts without data loss. It causes challenges in ensuring the uninterrupted operation of services in fog computing environments. Thus, an essential challenge is to create a containerized stateful stream processing based microservice to support digital twins in the fog computing environment. Within the scope of this article, we study live stateful stream processing migration and how to redistribute computational activity across cloud and fog nodes using Kafka middleware and its Stream DSL API.},
   author = {Ameer B.A. Alaasam and G. Radchenko and A. Tchernykh and J. L. González Compeán},
   doi = {10.1134/S0361768820080083/FIGURES/12},
   issn = {16083261},
   issue = {8},
   journal = {Programming and Computer Software},
   keywords = {Artificial Intelligence,Computer Science,Operating Systems,Software Engineering,Software Engineering/Programming and Operating Sys,general},
   month = {12},
   pages = {511-525},
   publisher = {Pleiades journals},
   title = {Analytic Study of Containerizing Stateful Stream Processing as Microservice to Support Digital Twins in Fog Computing},
   volume = {46},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/article/10.1134/S0361768820080083},
   year = {2020},
}
@inproceedings{Alaasam2019,
   author = {Ameer B.A. A. Alaasam and Gleb Radchenko and Andrey Tchernykh},
   doi = {10.1109/SIBIRCON48586.2019.8958367},
   isbn = {9781728144016},
   keywords = {Apache Kafka,Digital Twin,Event-Driven,Microservice,Sensors,Stream processing},
   month = {10},
   pages = {804-809},
   title = {Stateful Stream Processing for Digital Twins: Microservice-Based Kafka Stream DSL},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/8958367/},
   year = {2019},
}
@misc{Siqueira2017,
   author = {Fabio Levy Siqueira and Thiago C. de Sousa and Paulo S. Muniz Silva and Fabio Levy Siqueira and Thiago C de Sousa and Paulo S Muniz Silva},
   doi = {10.1109/FormaliSE.2017..5},
   isbn = {9781538604229},
   journal = {2017 IEEE/ACM 5th International FME Workshop on Formal Methods in Software Engineering (FormaliSE)},
   keywords = {BDD,Event-B,SBVR,formal methods,method,requirements},
   publisher = {IEEE},
   title = {Using BDD and SBVR to refine business goals intoan Event-B model: a research idea},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/7967990/},
   year = {2017},
}
@inproceedings{Ahmad2018,
   abstract = {Audit logs serve as a critical component in the enterprise business systems that are used for auditing, storing, and tracking changes made to the data. However, audit logs are vulnerable to a series of attacks, which enable adversaries to tamper data and corresponding audit logs. In this paper, we present BlockAudit: a scalable and tamper-proof system that leverages the design properties of audit logs and security guarantees of blockchains to enable secure and trustworthy audit logs. Towards that, we construct the design schema of BlockAudit, and outline its operational procedures. We implement our design on Hyperledger and evaluate its performance in terms of latency, network size, and payload size. Our results show that conventional audit logs can seamlessly transition into BlockAudit to achieve higher security, integrity, and fault tolerance.},
   author = {Ashar Ahmad and Muhammad Saad and Mostafa Bassiouni and Aziz Mohaisen},
   doi = {10.1145/3286978.3286985},
   isbn = {9781450360937},
   journal = {ACM International Conference Proceeding Series},
   keywords = {Audit log,Blockchain,Distributed systems,Hyperledger},
   month = {11},
   pages = {443-448},
   publisher = {Association for Computing Machinery},
   title = {Towards blockchain-driven, secure and transparent audit logs},
   year = {2018},
}
@article{Grady2001,
   abstract = {Big data analytic (BDA) systems leverage data distribution and parallel processing across a cluster of resources. This introduces a number of new challenges specifically for analytics. The analytics portion of the complete lifecycle has typically followed a waterfall process-completing one step before beginning the next. While efforts have been made to map different types of analytics to an agile methodology, the steps are often described as breaking activities into smaller tasks while the overall process is still consistent with step-by-step waterfall. BDA changes a number of the activities in the analytics lifecycle, as well as their ordering. The goal of agile analytics-to reach a point of optimality between generating value from data and the time spent getting there. This paper discusses the implications of an agile process for BDA in cleansing, transformation, and analytics.},
   author = {NW Grady and JA Payne and H Parker - on big data (big data) and undefined 2017},
   journal = {ieeexplore.ieee.org},
   keywords = {AnalyticsOps,Deep Learning,DevOps,Knowledge Discovery in Data Science, machine learning,advanced analytics,agile development,analytics lifecycle,big data analytics,data science,data science process models},
   title = {Agile big data analytics: AnalyticsOps for data science},
   url = {https://ieeexplore.ieee.org/abstract/document/8258187/},
   year = {2001},
}
@article{Juuso2018,
   abstract = {Increasing volumes of data, referred as big data, require massive scale and complex computing. Artificial intelligence, deep learning, internet of things and cloud computing are proposed for heterogeneous datasets in hierarchical analytics to manage with the volume, variety, velocity and value of the big data. These solutions are not sufficient in technical systems where measurements, waveform signals, spectral data, images and sparse performance indicators require specific methods for the feature extraction before interactions can be properly analysed. In practical applications, the data analysis, knowledge-based methodologies and optimization need to be combined. The solutions require compact calculation units which can be adaptively modified. The artificial intelligence is extended with various methodologies of computational intelligence. The advanced deep learning approach proposed in this paper uses generalized norms in feature generation, nonlinear scaling in developing compact indicators and linear interactions in model-based systems. The intelligent temporal analysis is available for all indices, including for stress, condition and quality indicators. The service and automation solutions combine these data-driven solutions with the domain expertise by using fuzzy logic for case-based systems. The applications are developed gradually in connections, conversion, cyber, cognition and configuration layers. The advanced methodology is based on the integration of features, scaling functions and interaction models specified by parameters. All the sub-systems and different combinations of them can be recursively updated and optimized with evolutionary computing. The systems adapt to the changing operating conditions and provide situation awareness for the risk analysis. The approach supports different levels of the smart adaptive systems.},
   author = {Esko K. Juuso},
   doi = {10.1515/ENG-2018-0043/HTML},
   issn = {23915439},
   issue = {1},
   journal = {Open Engineering},
   keywords = {artificial intelligence,big data analysis,computational intelligence,deep learning,fuzzy logic,recursive analysis,smart adaptation,statistical methods,temporal analysis},
   month = {2},
   pages = {403-416},
   publisher = {De Gruyter},
   title = {Smart Adaptive Big Data Analysis with Advanced Deep Learning},
   volume = {8},
   year = {2018},
}
@article{Kalan2016,
   abstract = {In 2015, number of people plugging to Internet rise to 3 billion. Beside human activity in social networks, billions of connected devices, sensor and instrument uninterrupted make amount of data. Irrespective of how data created, a natural question comes to mind is: How we could manage data analytics and what sort of tools should be adapted for handling this task in effective way? This paper discusses approaches and environments about big data. It revolves around three important areas of big data tools and talent, namely (i) data management (ii) infrastructure and technology, and (iii) query performance. The goal of big data management is to ensure a high level of data quality and accessibility which strongly depend on data transforming and storing. Using index improve query performance, however, many traditional indexing approaches in big data trinity involve a significant upfront cost for index creation. The key contribution of this paper is that using right tools over right data, and providing effective index to minimize upfront cost.},
   author = {RS Kalan and İ Kocabaş - challenge and undefined 2016},
   issue = {1},
   journal = {jmest.org},
   keywords = {Adaptive Indexing,Analytic,Big Data,Cloud computing,Hadoop,MapReduce},
   pages = {3159-3199},
   title = {Adaptive tools and technology in big data analytics},
   volume = {3},
   url = {http://www.jmest.org/wp-content/uploads/JMESTN42351339.pdf},
   year = {2016},
}
@article{Ali2017,
   abstract = {Due to dynamic nature of current software development methods, changes in requirements are embraced and given proper consideration. However, this triggers the rank reversal problem which involves re-prioritizing requirements based on stakeholders' feedback. It incurs significant cost because of time elapsed in large number of human interactions. To solve this issue, a Semi-Automated Framework for soFtware Requirements priOritizatioN (SAFFRON) is presented in this paper. For a particular requirement, SAFFRON predicts appropriate stakeholders' ratings to reduce human interactions. Initially, item-item collaborative filtering is utilized to estimate similarity between new and previously elicited requirements. Using this similarity, stakeholders who are most likely to rate requirements are determined. Afterwards, collaborative filtering based on latent factor model is used to predict ratings of those stakeholders. The proposed approach is implemented and tested on RALIC dataset. The results illustrate consistent correlation, similar to state of the art approaches, with the ground truth. In addition, SAFFRON requires 13.5-27% less human interaction for re-prioritizing requirements.},
   author = {Syed Ali and Zarif Masud and Rubaida Easmin and Alim Ul},
   doi = {10.14569/ijacsa.2017.081265},
   issn = {2158107X},
   issue = {12},
   journal = {International Journal of Advanced Computer Science and Applications},
   publisher = {The Science and Information Organization},
   title = {SAFFRON: A Semi-Automated Framework for Software Requirements Prioritization},
   volume = {8},
   year = {2017},
}
@article{Bukhsh2020,
   abstract = {[Context and Motivation] Many requirements prioritization approaches have been proposed, however not all of them have been investigated empirically in real-life settings. As a result, our knowledge of their applicability and actual use is incomplete. [Question/problem] A 2007 systematic review on requirements prioritization mapped out the landscape of proposed prioritization approaches and their prioritization criteria. To understand how this sub-field of requirements engineering has developed since 2007 and what evidence has been accumulated through empirical evaluations, we carried out a literature review that takes as input publications published between 2007 and 2019. [Principle ideas/results] We evaluated 102 papers that proposed and/or evaluated requirements prioritization methods. Our results show that the newly proposed requirements prioritization methods tend to use as basis fuzzy logic and machine learning algorithms. We also concluded that the Analytical Hierarchy Process is the most accurate and extensively used requirement prioritization method in industry. However, scalability is still its major limitation when requirements are large in number. We have found that machine learning has shown potential to deal with this limitation. Last, we found that experiments were the most used research method to evaluate the various aspects of the proposed prioritization approaches. [Contribution] This paper identified and evaluated requirements prioritization techniques proposed between 2007 and 2019, and derived some trends. Limitations of the proposals and implications for research and practice are identified as well.},
   author = {Faiza Allah Bukhsh and Zaharah Allah Bukhsh and Maya Daneva},
   doi = {10.1016/J.CSI.2019.103389},
   issn = {0920-5489},
   journal = {Computer Standards & Interfaces},
   keywords = {Empirical research method,Empirical study,Requirements engineering,Requirements prioritization,Systematic literature review},
   month = {3},
   pages = {103389},
   publisher = {North-Holland},
   title = {A systematic literature review on requirement prioritization techniques and their empirical evaluation},
   volume = {69},
   year = {2020},
}
@article{Souza2019,
   abstract = {Context: Software architecture design creates and documents the high-level structure of a software system. Such structure, expressed in architectural models, comprises software elements, relations among them, and properties of these elements and relations. Existing software architecture methods offer ways to derive architectural models from requirements specifications. These models must balance different forces that should be analyzed during this derivation process, such as those imposed by different application domains and quality attributes. Such balance is difficult to achieve, requiring skilled and experienced architects. Object: The purpose of this paper is to provide a comprehensive overview of the existing methods to derive architectural models from requirements specifications and offer a research roadmap to challenge the community to address the identified limitations and open issues that require further investigation. Method: To achieve this goal, we performed a systematic mapping study following the good practices from the Evidence-Based Software Engineering field. Results: This study resulted in 39 primary studies selected for analysis and data extraction, from the 2575 initially retrieved. Conclusion: The major findings indicate that current architectural derivation methods rely heavily on the architects’ tacit knowledge (experience and intuition), do not offer sufficient support for inexperienced architects, and lack explicit evaluation mechanisms. These and other findings are synthesized in a research roadmap which results would benefit researchers and practitioners.},
   author = {Eric Souza and Ana Moreira and Miguel Goulão},
   doi = {10.1016/J.INFSOF.2019.01.004},
   issn = {0950-5849},
   journal = {Information and Software Technology},
   keywords = {Literature review,Mapping study,Software architecture},
   month = {5},
   pages = {26-39},
   publisher = {Elsevier},
   title = {Deriving architectural models from requirements specifications: A systematic mapping study},
   volume = {109},
   year = {2019},
}
@article{Aldave2019,
   abstract = {Agile approaches tend to focus solely on scoping and simplicity rather than on problem solving and discovery. This hampers the development of innovative solutions. Additionally, little has been said about how to capture and represent the real user needs. To fill this gap, some authors argue in favor of the application of “Creative thinking” for requirements elicitation within agile software development. This synergy between creativeness and agility has arisen as a new means of bringing innovation and flexibility to increasingly demanding software. The aim of the present study is therefore to employ a systematic review to investigate the state-of-the-art of those approaches that leverage creativity in requirements elicitation within Agile Software Development, as well as the benefits, limitations and strength of evidence of these approaches. The review was carried out by following the guidelines proposed by Dr. Kitchenham. The search strategy identified 1451 studies, 17 of which were eventually classified as primary studies. The selected studies contained 13 different and unique proposals. These approaches provide evidence that enhanced creativity in requirements elicitation can be successfully implemented in real software projects. We specifically observed that projects related to user interface development, such as those for mobile or web applications, are good candidates for the use of these approaches. We have also found that agile methodologies such as Scrum, Extreme Programming or methodologies based on rapid modelling are preferred when introducing creativity into requirements elicitation. Despite this being a new research field, there is a mixture of techniques, tools and processes that have already been and are currently being successfully tested in industry. Finally, we have found that, although creativity is an important ingredient with which to bring about innovation, it is not always sufficient to generate new requirements because this needs to be followed by user engagement and a specific context in which proper conditions, such as flexibility, time or resources, have to be met.},
   author = {Ainhoa Aldave and Juan M. Vara and David Granada and Esperanza Marcos},
   doi = {10.1016/J.JSS.2019.110396},
   issn = {0164-1212},
   journal = {Journal of Systems and Software},
   keywords = {Agile methodologies,Creative thinking,Requirements elicitation,Software development,Software project management,Systematic review},
   month = {11},
   pages = {110396},
   publisher = {Elsevier},
   title = {Leveraging creativity in requirements elicitation within agile software development: A systematic literature review},
   volume = {157},
   year = {2019},
}
@article{Reynares2014,
   abstract = {Ontology development techniques still constitute an open research area despite its importance in semantic aware information systems. Until now, most methods have used UML in supporting ontology development process. Recent works propose the mapping of business rules expressions to ontology statements as a building technique by means of SBVR language. However, there is still no experimental research comparing such approaches. Aim of this work is to evaluate the feasibility of mapping business domain expressions to ontology statements. An exploratory experiment comparing performance of techniques based on UML and SBVR languages is presented. Comparison is rooted in the quality assessment of the ontologies developed by 10 equally sized groups randomly conformed by 30 undergraduate engineering students and applying such techniques. Developed ontologies largely outperform the minimally acceptable quality, according to the considered quality assessment framework. There is no statistical significant difference between the quality scores of the ontologies developed by means of UML and SBVR techniques, in any of the assessed quality dimensions. The feasibility of mapping business domain expressions to ontology statements is shown: ontologies developed by means of a SBVR based approach at least equate the quality of ontologies developed by using an UML based method. Results confirm previous research about the effectiveness of UML approaches for conceptualizing lightweight ontologies while stressing the potential of the SBVR language to express complex notions of a domain of interest. The potential of SBVR to OWL 2 mappings as an ontology development technique worthy of further study is highlighted. © 2013 Elsevier Ltd. All rights reserved.},
   author = {Emiliano Reynares and María Laura Caliusco and María Rosa Galli},
   doi = {10.1016/J.ESWA.2013.08.054},
   issn = {0957-4174},
   issue = {4},
   journal = {Expert Systems with Applications},
   keywords = {Exploratory experiment,Ontology development,SBVR,UML},
   month = {3},
   pages = {1576-1583},
   publisher = {Pergamon},
   title = {Approaching the feasibility of SBVR as modeling language for ontology development: An exploratory experiment},
   volume = {41},
   year = {2014},
}
@article{Curcio2018,
   abstract = {Context: Requirements engineering in agile software development is a relatively recent software engineering topic and it is not completely explored and understood. The understanding of how this process works on agile world needs a deeper analysis. Objective: The goal of this paper is to map the subject area of requirements engineering in agile context to identify the main topics that have been researched and to identify gaps to develop future researches. It is also intended to identify the obstacles that practitioners face when using agile requirements engineering. Method: A systematic mapping study was conducted and as a result 2171 papers were initially identified and further narrowed to 104 by applying exclusion criteria and analysis. Conclusion: After completing the classification and the analysis of the selected studies it was possible to identify 15 areas (13 based on SWEBOK) where researches were developed. Five of such areas points to the need of future researches, among them are requirements elicitation, change management, measuring requirements, software requirements tools and comparative studies between traditional and agile requirements. In this research, some obstacles that practitioners face dealing with requirements engineering in agile context were also identified. They are related to environment, people and resources.},
   author = {Karina Curcio and Tiago Navarro and Andreia Malucelli and Sheila Reinehr},
   doi = {10.1016/J.JSS.2018.01.036},
   issn = {0164-1212},
   journal = {Journal of Systems and Software},
   keywords = {Agile software development,Requirements engineering,Systematic mapping study},
   month = {5},
   pages = {32-50},
   publisher = {Elsevier},
   title = {Requirements engineering: A systematic mapping study in agile software development},
   volume = {139},
   year = {2018},
}
@article{Bajec2005,
   abstract = {Business rules are evidently important for organisations as they describe how they are doing business. Their value has also been recognised within the information system (IS) domain, mostly because of their ability to make applications flexible and amenable to change. In this paper, we propose a methodology that helps business people and developers to keep business rules at the business level inline with the rules that are implemented at the system level. In contrast to several existing approaches that primarily focus on business rules in the scope of an application, our methodology addresses the entire IS of an organisation. The paper also describes requirements for a tool support that would be appropriate to support the methodology. © 2004 Elsevier B.V. All rights reserved.},
   author = {Marko Bajec and Marjan Krisper},
   doi = {10.1016/J.IS.2004.05.003},
   issn = {0306-4379},
   issue = {6},
   journal = {Information Systems},
   keywords = {Business rule,Business rule management,Enterprise modeling},
   month = {9},
   pages = {423-443},
   publisher = {Pergamon},
   title = {A methodology and tool support for managing business rules in organisations},
   volume = {30},
   year = {2005},
}
@article{Fortineau2019,
   abstract = {Current PLM or BIM based information systems suffer from a lack of checking components for business rules. One reason is the misunderstanding of the role and nature of business rules, and how they should be treated in a product-centric information system. This paper intends to provide both a process and a related model to build such a component and enrich future systems. Rules and requirements process management enables the unambiguous formalization of implicit knowledge contained in business rules, generally expressed in easily understandable language, and leads to the formal expression of requirements. In this paper, the requirements are considered a consequence of the application of a business rule. A conceptual model is then introduced, called DALTON (DAta Linked Through Occurrences Network), which supports this process. In this ontology, concepts and product data, coming for instance from an existing product database, are represented using instances and occurrences, connected together with triples built from business rules and requirements according to previous management processes. An experiment involving a set of SWRL rules is conducted in the Protégé environment that validates the model and the process.},
   author = {Virginie Fortineau and Thomas Paviot and Samir Lamouri},
   doi = {10.1016/J.COMPIND.2018.10.001},
   issn = {0166-3615},
   journal = {Computers in Industry},
   keywords = {BIM,Business rule,Ontology,PLM,Requirements},
   month = {1},
   pages = {22-33},
   publisher = {Elsevier},
   title = {Automated business rules and requirements to enrich product-centric information},
   volume = {104},
   year = {2019},
}
@article{Caballero2022,
   abstract = {Data quality evaluation is built upon data quality measurement results. “Data quality evaluation” uses the “data quality rules” representing the risk appetite of the organization to decide on the usability of the data; “data quality measurement” uses the business rules describing the “data requirements” or “data specifications” to determine the validity of the data. Consequently, to conduct meaningful and useful data quality evaluations, business rules must be first completely identified and captured at the beginning of the evaluation to perform sound measurements. We propose that the evaluation leads to better and more interpretable and useful results when the potential contribution of these business rules to the measurement of the data quality characteristics is first evaluated, avoiding the inclusion in the evaluation of those not having potential contribution and the resulting waste of resources. Considering this, we feel that for a better management of business rules for data quality evaluation, it makes sense to group all business rules having an important contribution to the evaluation of data quality characteristics, something that other business rules management methodologies have not covered yet. Through our experiences in conducting industrial projects of data quality evaluations we identified six problems when collecting and grouping the business rules. These problems make data quality evaluation processes less efficient and more costly. The main contribution of this paper is a methodology to systematically collect, group and validate the business rules to avoid or to alleviate these problems. For the sake of generalization, comparability, and reusability, we propose to do the grouping for data quality characteristics and properties defined in ISO/IEC 25012 and ISO/IEC 25024, respectively. Lastly, we validate the methodology in three case studies of real projects. From this validation, it is possible to raise the conclusion that the methodology is useful, applicable in the real world, and valid to capture and group the business rules used as a basis for data quality evaluation.},
   author = {Ismael Caballero and Fernando Gualo and Moisés Rodríguez and Mario Piattini},
   doi = {10.1016/J.IS.2022.102058},
   issn = {0306-4379},
   journal = {Information Systems},
   keywords = {Business rules,Data quality,Data quality characteristics,Data quality evaluation,Data quality measurement,Data quality properties,ISO/IEC 25012,ISO/IEC 25024},
   month = {11},
   pages = {102058},
   publisher = {Pergamon},
   title = {BR4DQ: A methodology for grouping business rules for data quality evaluation},
   volume = {109},
   year = {2022},
}
@article{Wang2022,
   abstract = {Business process models are widely used in organizations by information systems analysts to represent complex business requirements. They are also used by business users to understand business operations and constraints. This understanding is extracted from graphical process models as well as business rules. Prior research advocated integrating business rules and business process models to improve the effectiveness of various organizational activities, such as developing a shared understanding of practices, process improvement, and mitigating risks of compliance and policy breaches. However, whether such integrated modeling can improve the understanding of business processes, which is a fundamental benefit of integrated modeling, has not been empirically evaluated. In this paper, first, we report on an experiment investigating whether rule linking, a representative integrated modeling method, can improve understanding performance. We use eye tracking technology to understand the cognitive process by which model readers use models to perform understanding tasks. Our results show that rule linking outperforms separated modeling in terms of understanding effectiveness, efficiency, perceived mental effort, and visual attention. Further, cognitive process analysis reveals that the form of rule representation does not affect the extent of deep processing, but rule linking significantly decreases the occurrence of rule scanning and screening processes. Moreover, our results show that rule linking leads to an increase of visual association suggesting improved information integration, leading to improved task performance.},
   author = {Wei Wang and Tianwa Chen and Marta Indulska and Shazia Sadiq and Barbara Weber},
   doi = {10.1016/J.IS.2021.101901},
   issn = {0306-4379},
   journal = {Information Systems},
   keywords = {Business process modeling,Business rule modeling,Cognitive process,Controlled experiment,Eye-tracking,Model understanding},
   month = {2},
   pages = {101901},
   publisher = {Pergamon},
   title = {Business process and rule integration approaches—An empirical analysis of model understanding},
   volume = {104},
   year = {2022},
}
@article{,
   abstract = {Process modeling plays a central role in the development of today's process-aware information systems both on the management level (e.g., providing input for requirements elicitation and fostering communication) and on the enactment level (providing a blue-print for process execution and enabling simulation). The literature comprises a variety of process modeling approaches proposing different modeling languages (i.e., imperative and declarative languages) and different types of process artifact support (i.e., process models, textual process descriptions, and guided simulations). However, the use of an individual modeling language or a single type of process artifact is usually not enough to provide a clear and concise understanding of the process. To overcome this limitation, a set of so-called “hybrid” approaches combining languages and artifacts have been proposed, but no common grounds have been set to define and categorize them. This work aims at providing a fundamental understanding of these hybrid approaches by defining a unified terminology, providing a conceptual framework and proposing an overarching overview to identify and analyze them. Since no common terminology has been used in the literature, we combined existing concepts and ontologies to define a “Hybrid Business Process Representation” (HBPR). Afterwards, we conducted a Systematic Literature Review (SLR) to identify and investigate the characteristics of HBPRs combining imperative and declarative languages or artifacts. The SLR resulted in 30 articles which were analyzed. The results indicate the presence of two distinct research lines and show common motivations driving the emergence of HBPRs, a limited maturity of existing approaches, and diverse application domains. Moreover, the results are synthesized into a taxonomy classifying different types of representations. Finally, the outcome of the study is used to provide a research agenda delineating the directions for future work.},
   author = {Amine Abbad Andaloussi and Andrea Burattin and Tijs Slaats and Ekkart Kindler and Barbara Weber},
   doi = {10.1016/J.IS.2020.101505},
   issn = {0306-4379},
   journal = {Information Systems},
   keywords = {Business process modeling,Declarative process modeling,Hybrid process model,Process flexibility,Understandability of process models},
   month = {7},
   pages = {101505},
   publisher = {Pergamon},
   title = {On the declarative paradigm in hybrid business process representations: A conceptual framework and a systematic literature study},
   volume = {91},
   year = {2020},
}
@article{Spijkman2021,
   abstract = {Context: Requirements engineering and software architecture are tightly linked disciplines. The Twin Peaks model suggests that requirements and architectural components should stay aligned while the system is designed and as the level of detail increases. Unfortunately, this is hardly the case in practical settings. Objective: We surmise that a reason for the absence of conjoint evolution is that existing models, such as the Twin Peaks, do not provide concrete guidance for practitioners. We propose the Requirements Engineering for Software Architecture (RE4SA) model to assist in analyzing the alignment and the granularity of functional requirements and architectural components. Methods: After detailing the RE4SA model in notation-independent terms, we propose a concrete instance, called RE4SA-Agile, that connects common artifacts in agile development, such as user stories and features. We introduce metrics that measure the alignment between the requirements and architecture, and we define granularity smells to pinpoint situation in which the granularity of one high-level requirement or high-level component is not uniform with the norm. We show two applications of RE4SA-Agile, including the use of the metrics, to real-world case studies. Results: Our applications of RE4SA-Agile, which were discussed with representatives from the development teams, prove to be able to pinpoint problematic situations regarding the relationship between functional requirements and architecture. Conclusion: RE4SA and its metrics can be seen as a first attempt to provide a concrete approach for supporting the application of the Twin Peaks model. We expect future research to apply our metrics to additional cases and to provide variants for RE4SA that support different concrete notations, and extend the perspective beyond the functional perspective of this research, similar to what we did with RE4SA-Agile in this paper.},
   author = {Tjerk Spijkman and Sabine Molenaar and Fabiano Dalpiaz and Sjaak Brinkkemper},
   doi = {10.1016/J.INFSOF.2021.106535},
   issn = {09505849},
   journal = {Information and Software Technology},
   keywords = {Agile development,Alignment,Case study,Granularity,Requirements engineering,Software architecture,Twin Peaks},
   month = {5},
   publisher = {Elsevier B.V.},
   title = {Alignment and granularity of requirements and architecture in agile development: A functional perspective},
   volume = {133},
   year = {2021},
}
@article{,
   abstract = {This paper presents a research and its results in the domain of higher education's pedagogical patterns for remote assessments - precisely in the computer science, software engineering and informatics-related courses. This research was motivated by the COVID-19 crisis, which separated teachers, teaching assistants, and students physically. During this period, remote knowledge assessment was one of the most challenging among all educational activities. The lack of available resources and advice on remote knowledge assessment revealed a need for a specialized assessment pattern catalog. The main result of the research is the assessment pattern catalog that started to grow organically at the Institute of Informatics, where we teach IT-related courses. We started with the initial set of patterns, identified by analyzing recurring practices, applied by teaching staff for remote assessments in the period from March 2020 till December 2020. The patterns were aggregated and gradually refined using a systematic approach. In addition to guided workshops, a systematic literature review was employed, followed by catalog refinements, and, finally, an extensive survey was carried out among teachers and teaching assistants. The latter was used as a validation of the correctness of the novel assessment pattern catalog, as well as the presented patterns’ suitability and popularity among users. The resulting assessment pattern catalog presented in this paper boasts 47 patterns, classified into four main categories, that support the whole process of (remote) assessment. It is organized and documented systematically. It also boasts several indicators per each pattern to demonstrate its suitability for distant assessments, popularity rankings among teachers, teaching assistants, and top picks in every category per teachers and teaching assistants. The survey that we performed revealed a subset of patterns that are important for a successful remote assessment, validated in the IT-related courses. Based on the results, the presented assessment pattern catalog showed itself to be useful not only for the remote assessment but also for judging knowledge in the classroom successfully.},
   author = {Luka Pavlič and Tina Beranič and Lucija Brezočnik and Marjan Heričko},
   doi = {10.1016/J.COMPEDU.2022.104470},
   issn = {0360-1315},
   journal = {Computers & Education},
   keywords = {Distance education and online learning,Improving classroom teaching,Pedagogical issues,Post-secondary educations,Teacher professional development},
   month = {6},
   pages = {104470},
   publisher = {Pergamon},
   title = {Towards a novel catalog of assessment patterns for distant education in the information technology domain},
   volume = {182},
   year = {2022},
}
@article{Kasauli2021,
   abstract = {Context: Agile methods have become mainstream even in large-scale systems engineering companies that need to accommodate different development cycles of hardware and software. For such companies, requirements engineering is an essential activity that involves upfront and detailed analysis which can be at odds with agile development methods. Objective: This paper presents a multiple case study with seven large-scale systems companies, reporting their challenges, together with best practices from industry. We also analyze literature about two popular large-scale agile frameworks, SAFe® and LeSS, to derive potential solutions for the challenges. Methods: Our results are based on 20 qualitative interviews, five focus groups, and eight cross-company workshops which we used to both collect and validate our results. Results: We found 24 challenges which we grouped in six themes, then mapped to solutions from SAFe®, LeSS, and our companies, when available. Conclusion: In this way, we contribute a comprehensive overview of RE challenges in relation to large-scale agile system development, evaluate the degree to which they have been addressed, and outline research gaps. We expect these results to be useful for practitioners who are responsible for designing processes, methods, or tools for large scale agile development as well as guidance for researchers.},
   author = {Rashidah Kasauli and Eric Knauss and Jennifer Horkoff and Grischa Liebel and Francisco Gomes de Oliveira Neto},
   doi = {10.1016/J.JSS.2020.110851},
   issn = {0164-1212},
   journal = {Journal of Systems and Software},
   keywords = {Large-scale agile,Requirements engineering,Systems engineering},
   month = {2},
   pages = {110851},
   publisher = {Elsevier},
   title = {Requirements engineering challenges and practices in large-scale agile system development},
   volume = {172},
   year = {2021},
}
@article{Inayat2015,
   abstract = {Unlike traditional software development methods, agile methods are marked by extensive collaboration, i.e. face-to-face communication. Although claimed to be beneficial, the software development community as a whole is still unfamiliar with the role of the requirements engineering practices in agile methods. The term "agile requirements engineering" is used to define the "agile way" of planning, executing and reasoning about requirements engineering activities. Moreover, not much is known about the challenges posed by collaboration-oriented agile way of dealing with requirements engineering activities. Our goal is to map the evidence available about requirements engineering practices adopted and challenges faced by agile teams in order to understand how traditional requirements engineering issues are resolved using agile requirements engineering. We conducted a systematic review of literature published between 2002 and June 2013 and identified 21 papers, that discuss agile requirements engineering. We formulated and applied specific inclusion and exclusion criteria in two distinct rounds to determine the most relevant studies for our research goal. The review identified 17 practices of agile requirements engineering, five challenges traceable to traditional requirements engineering that were overcome by agile requirements engineering, and eight challenges posed by the practice of agile requirements engineering. However, our findings suggest that agile requirements engineering as a research context needs additional attention and more empirical results are required to better understand the impact of agile requirements engineering practices e.g. dealing with non-functional requirements and self-organising teams.},
   author = {Irum Inayat and Siti Salwah Salim and Sabrina Marczak and Maya Daneva and Shahaboddin Shamshirband},
   doi = {10.1016/J.CHB.2014.10.046},
   issn = {0747-5632},
   journal = {Computers in Human Behavior},
   keywords = {Agile requirements engineering,Agile software development methods,Collaboration,Systematic review,Traditional requirements engineering},
   month = {10},
   pages = {915-929},
   publisher = {Pergamon},
   title = {A systematic literature review on agile requirements engineering practices and challenges},
   volume = {51},
   year = {2015},
}
@article{Mishra2021,
   abstract = {Over the last decade, migration methodologies from Legacy to SOA design have been broadly explored. Language migration, database migration, and platform migration were the primary issues with legacy systems. Legacy systems are a time-consuming and costly process for maintaining application and programming source code. It is difficult to change a single module of the software system without affecting other modules. SOA, on the other hand, enables organizations to manage business processes, business rules, enterprise content management, and management security. In this study A survey is carried out on the SOA maintainability challenges namely Analyzability, Changeability, Stability and Testability.},
   author = {Arvind Kumar Mishra and Renuka Nagpal and Kirti Seth and Rajni Sehgal},
   doi = {10.1109/ICRITO51393.2021.9596285},
   isbn = {9781665417037},
   journal = {2021 9th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions), ICRITO 2021},
   keywords = {ANZ(Analizability),Ability(AB),CIA (ComplexInformationSystem),LS (Legacy System),SOA (Service Oriented Architecture)},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A Critical Review on Service Oriented Architecture and its Maintainability},
   year = {2021},
}
@article{Cahyono2019,
   abstract = {i ∗ framework is a socio-technical goal-based modeling framework and models the actors in the project/system environment. In 2016 iStar 2.0 was proposed to further evolve i∗ basic concepts to be more acceptable for wider users. Therefore, it motivates us to propose a formal rule for validating iStar 2.0 in XML-based modelling standard similar to iStarML, called iStarML 2.0. In addition to validation process, this paper proposes formal methods for translating i∗ to iStar 2.0 model and iStar 2.0 to class diagram. iStarService is a tool developed for iStar 2.0 modelling based on iStarML 2.0 with functionalities such as iStar 2.0 model validation, i∗ to iStar 2.0 model translation, and iStar 2.0 model to class diagram translation. It is implemented in form of web API using Java and had been tested with various models from multiple iStar proceedings.},
   author = {Francisco Kenandi Cahyono and Bayu Hendradjaya and Hari Purnama},
   doi = {10.1109/ICODSE48700.2019.9092607},
   isbn = {9781728149929},
   journal = {Proceedings of 2019 International Conference on Data and Software Engineering, ICoDSE 2019},
   keywords = {class diagram,iStar 2.0,iStarML,i∗,tool,translation,validation},
   month = {11},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {An iStar 2.0 Syntax Validation Formal Rules and Its Implementation on a New Translator},
   year = {2019},
}
@article{Boyer2011,
   abstract = {Target audience All; nontechnical audiences can skip Sects. 3.3 and 3.4 In this chapter you will learn What is the Agile Business Rule Development (ABRD See agile business rule development) methodology, and what are its core principles.Why develop business rule...},
   author = {Jérôme Boyer and Hafedh Mili},
   doi = {10.1007/978-3-642-19041-4_3},
   journal = {Agile Business Rule Development},
   pages = {49-71},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Agile Business Rule Development},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-642-19041-4_3},
   year = {2011},
}
@article{Jarzebowicz2020,
   abstract = {Agile software development stresses the importance of providing the customer with a product of a maximized business value. To achieve that, requirements prioritization is used. Agile development methods like Scrum define guidelines for prioritization, however practitioners do not necessarily have to follow them. Our goal was to investigate the industry practice related to requirements prioritization process, including its timing, participants, criteria used and prioritization techniques applied. We designed an on-line questionnaire (based on literature review) and conducted a survey involving practitioners from Polish IT industry. We received 69 valid responses indicating requirements prioritization practices in industrial Agile projects. We found out that despite the fact that business value is the most common criterion used to prioritize requirements, other criteria like complexity, stability and mutual interdependencies are considered as well. Other findings indicate that consideration of such multiple criteria requires different viewpoints, thus making requirements prioritization a process that has to involve many participants of different roles and competencies.},
   author = {Aleksander Jarzebowicz and Natalia Sitko},
   doi = {10.1016/J.PROCS.2020.09.052},
   issn = {1877-0509},
   journal = {Procedia Computer Science},
   keywords = {Agile Requirements Engineering,Business Value,Requirements Prioritization,Survey,Value-Based Software Engineering},
   month = {1},
   pages = {3446-3455},
   publisher = {Elsevier},
   title = {Agile Requirements Prioritization in Practice: Results of an Industrial Survey},
   volume = {176},
   year = {2020},
}
@article{Rasheed2021,
   abstract = {Agile software development has large success rate due to its benefits and promising nature but natively where the size of the project is small. Requirement engineering (RE) is crucial as in each software development life cycle, "Requirements"play a vital role. Though agile provides values to customer's business needs, changing requirement, and interaction, we also have to face impediments in agile, many of which are related to requirement challenges. This article aims to find out the challenges being faced during requirement engineering of agile projects. Many research studies have been conducted on requirement challenges which are somehow biased, no suggestions are given to improve the agile development process, and the research does not highlight large-scale agile development challenges. Hence, this article covers all the challenges discussed above and presents a comprehensive overview of agile models from requirement engineering perspective. The findings and results can be very helpful for software industry to improve development process as well as for researchers who want to work further in this direction.},
   author = {Aqsa Rasheed and Bushra Zafar and Tehmina Shehryar and Naila Aiman Aslam and Muhammad Sajid and Nouman Ali and Saadat Hanif Dar and Samina Khalid},
   doi = {10.1155/2021/6696695},
   issn = {15635147},
   journal = {Mathematical Problems in Engineering},
   publisher = {Hindawi Limited},
   title = {Requirement Engineering Challenges in Agile Software Development},
   volume = {2021},
   year = {2021},
}
@article{Gnoyke2021,
   abstract = {If software quality assurance is postponed or even abandoned for a software system, maintenance and evolution become harder or even impossible. One widely known symptom for the degradation of system quality are Architecture Smells (ASs), which violate fundamental principles of software design. In this paper, we present a study on the evolution of ASs as well as on how and when they foster system degradation. Thus, we provide valuable insights regarding what ASs are meaningful to assure system quality. To this end, we analyzed the evolution of three types of ASs in 14 open-source systems with a total of 485 versions. We adapted indicators used in previous studies to assess the severity of ASs (e.g., growth, lifetime), and relate ASs to technical debt as another established indicator. Our results indicate that 1) ASs remain mostly stable compared to the code size of a system, 2) certain types of ASs, such as cyclic dependencies, have a greater impact on system degradation, and 3) certain properties determine how much an AS contributes to software degradation. These findings are valuable for practitioners to identify and tackle system degeneration, as well as for researchers to scope new research on managing ASs and technical debt.},
   author = {Philipp Gnoyke and Sandro Schulze and Jacob Kruger},
   doi = {10.1109/ICSME52107.2021.00043},
   isbn = {9781665428828},
   journal = {Proceedings - 2021 IEEE International Conference on Software Maintenance and Evolution, ICSME 2021},
   keywords = {architecture smells,empirical study,software evolution,software maintenance,software quality,technical debt},
   pages = {413-424},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {An Evolutionary Analysis of Software-Architecture Smells},
   year = {2021},
}
@article{Biesialska2021,
   abstract = {Context: Over the last decade, Agile methods have changed the software development process in an unparalleled way and with the increasing popularity of Big Data, optimizing development cycles through data analytics is becoming a commodity. Objective: Although a myriad of research exists on software analytics as well as on Agile software development (ASD) practice on itself, there exists no systematic overview of the research done on ASD from a data analytics perspective. Therefore, the objective of this work is to make progress by linking ASD with Big Data analytics (BDA). Method: As the primary method to find relevant literature on the topic, we performed manual search and snowballing on papers published between 2011 and 2019. Results: In total, 88 primary studies were selected and analyzed. Our results show that BDA is employed throughout the whole ASD lifecycle. The results reveal that data-driven software development is focused on the following areas: code repository analytics, defects/bug fixing, testing, project management analytics, and application usage analytics. Conclusions: As BDA and ASD are fast-developing areas, improving the productivity of software development teams is one of the most important objectives BDA is facing in the industry. This study provides scholars with information about the state of software analytics research and the current trends as well as applications in the business environment. Whereas, thanks to this literature review, practitioners should be able to understand better how to obtain actionable insights from their software artifacts and on which aspects of data analytics to focus when investing in such initiatives.},
   author = {Katarzyna Biesialska and Xavier Franch and Victor Muntés-Mulero},
   doi = {10.1016/J.INFSOF.2020.106448},
   issn = {0950-5849},
   journal = {Information and Software Technology},
   keywords = {Agile software development,Artificial intelligence,Data analytics,Literature review,Machine learning,Software analytics},
   month = {4},
   pages = {106448},
   publisher = {Elsevier},
   title = {Big Data analytics in Agile software development: A systematic mapping study},
   volume = {132},
   year = {2021},
}
@inproceedings{,
   abstract = {Digital data are critical to people and companies, acting as a crucial element in the decision-making process in different areas. Accordingly, logs can track how this data changes over time and are essential to enable auditing. The traditional approach to ensure log integrity is to store them on well-kept servers, both from a physical and a digital security standpoint. However, this ap...},
   author = {Bruno de Azevedo Mendonça and Paulo Matias},
   journal = {2021 11th IFIP International Conference on New Technologies, Mobility and Security (NTMS)},
   publisher = {IEEE},
   title = {Auditchain: a mechanism for ensuring logs integrity based on proof of existence in a public blockchain},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/9432639/},
   year = {2021},
}
@inproceedings{Pawar2021,
   abstract = {Audit logs serve as a crucial constituent for every organization that has a digital presence and are used to audit, store, and track modifications made to the data for efficient business intelligence. However, the audit logs are unfortified to attacks that delegate the adversaries to alter data and corresponding audit logs without being disclosed. Two prom...},
   author = {Ashish Pawar and Dhanshree Barthare and Naveen Rawat and Manish Yadav and Mahesh Shirole},
   journal = {2021 5th International Conference on Information Systems and Computer Networks (ISCON)},
   publisher = {IEEE},
   title = {BlockAudit 2.0: PoA blockchain based solution for secure Audit logs},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/9702378/},
   year = {2021},
}
@inproceedings{Kanhere2015,
   abstract = {Database audit logs contain the information about database operations which are helpful to verify accuracy, lawfulness and to report risks. In financial systems, the audit logs should be monitored on continuous basis in order to detect and take action against any reasonably abnormal behavior. Outlier detection is a very important concept in the data mining...},
   author = {Pradnya Kanhere and H.K. Khanuja},
   journal = {2015 International Conference on Computing Communication Control and Automation},
   publisher = {IEEE},
   title = {A Methodology for Outlier Detection in Audit Logs for Financial Transactions},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/7155965/},
   year = {2015},
}
@article{Margulies2015,
   abstract = {Modern enterprises centrally monitor their systems by collecting logs using audit reduction tools that can search, sort, and alert. The author describes how developers can support such monitoring by writing logging mechanisms that account for the strengths and weaknesses of audit reduction tools.},
   author = {Jonathan Margulies},
   issue = {3},
   journal = {IEEE Security & Privacy},
   publisher = {IEEE},
   title = {A Developer's Guide to Audit Logging},
   volume = {13},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/7118074/},
   year = {2015},
}
@inproceedings{,
   abstract = {Information-algebraic models have been shown as effective semantic frameworks in the specification of audit logging requirements. These semantic frameworks underlie implementation models that formally guarantee correctness of audit logs. Recently, one such implementation model has been proposed for concurrent systems. In this paper, we study the deployment of an instr...},
   author = {Sepehr Amir-Mohammadian and Afsoon Yousefi Zowj},
   journal = {2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)},
   publisher = {IEEE},
   title = {Towards Concurrent Audit Logging in Microservices},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/9529762/},
   year = {2021},
}
@inproceedings{Aslan2021,
   abstract = {Audit log files can contain both personal data and system actions. Therefore, storing audit log files in accordance with the General Data Protection Regulation (GDPR) has become a legal obligation. In this paper, we propose a GDPR-compliant log storage system called Solid Log Chain (SLC). The goal of Solid Log Chain is to store audit logs and ensure the confidentiality and immutability of log data while meeting GDPR obligations. We have developed SLC by combining existing technologies with blockchain and using it in innovative ways. We have developed a data structure that allows only personal data to be deleted without destroying data integrity. Solid Log Chain is intended to be an alternative to expensive hardware-based solutions. We describe the design concept and architecture of the SLC and evaluate its performance in terms of latency and payload size.},
   author = {Ulas Aslan and Baha Sen},
   doi = {10.1109/UYMS54260.2021.9659700},
   isbn = {9781665410700},
   journal = {2021 Turkish National Software Engineering Symposium, UYMS 2021 - Proceedings},
   keywords = {Ethereum,GDPR,audit log,blockchain,data security,smart contracts},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {GDPR Compliant Audit Log Management System with Blockchain: GDPR Uyumlu Denetim Günlüǧü Yönetim Sistemi},
   year = {2021},
}
@inproceedings{Jesus2015,
   abstract = {The use of business rules technologies to develop information systems has been advocated as a good strategy to provide more flexibility and keeping systems up to date. However, designing an architecture that allows systems to be rapidly adapted as business rules change over time remains a challenging activity. Based on the publish-subscribe pattern, we propose a flexible, scalable and loose-couple...},
   author = {Jandisson S. De Jesus and Ana C.V. de Melo},
   journal = {2015 IEEE 17th Conference on Business Informatics},
   publisher = {IEEE},
   title = {An Architectural Pattern to Implement Business Rules in Information Systems},
   volume = {2},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/7264772/},
   year = {2015},
}
@inproceedings{Chittimalli2019,
   abstract = {An enterprise system operates business by providing various services that are guided by set of certain business rules (BR) and constraints. These BR are usually written using plain Natural Language in operating procedures, terms and conditions, and other documents or in source code of legacy enterprise systems. For implementing the BR in a software system, expressing them as UML use-case specifica...},
   author = {Pavan Kumar Chittimalli and Kritika Anand and Shrishti Pradhan and Sayandeep Mitra and Chandan Prakash and Rohit Shere and Ravindra Naik},
   journal = {2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
   publisher = {IEEE},
   title = {BuRRiTo: A Framework to Extract, Specify, Verify and Analyze Business Rules},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/8952315/},
   year = {2019},
}
@inproceedings{Moschoyiannis2018,
   abstract = {The service choreography approach has been proposed for describing the global ordering constraints on the observable message exchanges between participant services in service oriented architectures. Recent work advocates the use of structured natural language, in the form of Semantics of Business Vocabulary and Rules (SBVR), for specifying and validating choreographies. This paper addresses ...},
   author = {Sotiris Moschoyiannis and Leandros Maglaras and Nurulhuda A. Manaf},
   journal = {2018 IEEE 11th Conference on Service-Oriented Computing and Applications (SOCA)},
   publisher = {IEEE},
   title = {Trace-Based Verification of Rule-Based Service Choreographies},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/8599596/},
   year = {2018},
}
@inproceedings{Natali2015,
   abstract = {Data are the most important asset of an organization. Even though the application or business rules change, consistent data stored in the database must be kept. This research is intended to propose a framework for checking the consistency of an existing database to the business rules in which the data must comply. The research focuses on the situation where the documents of database and the applic...},
   author = {Vania Natali and Inggriani Liem},
   journal = {2015 International Conference on Data and Software Engineering (ICoDSE)},
   publisher = {IEEE},
   title = {Automated data consistency checking using SBVR: Case study: Academic data in a University},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/7436971/},
   year = {2015},
}
@inproceedings{Mitra2018,
   abstract = {In the modern age, the need for automation has led to Business Organizations representing their functionality as structured Business Rules. SBVR has come up as an universally popular format for representation of Business Rules. The presence of different Business Organizations working in a particular real life domain results in generation of different rules for each of the organization. Due t...},
   author = {Sayandeep Mitra and Chandan Prakash and Shayak Chakraborty and Pavan Kumar Chittimalli},
   journal = {2018 25th Asia-Pacific Software Engineering Conference (APSEC)},
   publisher = {IEEE},
   title = {MatGap: A Systematic Approach to Perform Match and Gap Analysis among SBVR-Based Domain Specific Business Rules},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/8719543/},
   year = {2018},
}
@inproceedings{Hnatkowska2020,
   abstract = {Business rules are used to define or limit certain aspects of business. While they should be understandable for different interested parties, including business analysts, end-users, programmers, or testers, it is also highly recommendable that computers efficiently process them. Business rules can be expressed using different languages (styles), potentially influencing their quality. The main aim ...},
   author = {Bogumila Hnatkowska and Anna Hnatkowska},
   journal = {2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
   publisher = {IEEE},
   title = {Usability of the Business Rules Specification Languages},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/9283400/},
   year = {2020},
}
@inproceedings{Manaf2017,
   abstract = {We present a compilation tool SBVR2Alloy which is used to automatically generate as well as validate service choreographies specified in structured natural language. The proposed approach builds on a model transformation between Semantics of Business Vocabulary and Rules (SBVR), an OMG standard for specifying business models in structured English, and the Alloy Analyzer which is a SAT based ...},
   author = {Nurulhunda A Manaf and Andreas Antoniades and Sotiris Moschoyiannis},
   journal = {2017 IEEE 10th Conference on Service-Oriented Computing and Applications (SOCA)},
   publisher = {IEEE},
   title = {SBVR2Alloy: An SBVR to Alloy Compiler},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/8241527/},
   year = {2017},
}
@inproceedings{Iqbal2016,
   abstract = {UML is actually a standard that is used in the production of software models. Its function is to facilitate the working of visuals of software artifacts. In order to produce a UML diagram, manufacturer has to gather all the software requirements in a natural like English r semi formal like SBVR language. After that he analyzes and produces the activity diagram manually in an available case t...},
   author = {Usama Iqbal and Imran Sarwar Bajwa},
   journal = {2016 Sixth International Conference on Innovative Computing Technology (INTECH)},
   publisher = {IEEE},
   title = {Generating UML activity diagram from SBVR rules},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/7845094/},
   year = {2016},
}
@article{Grubb2018,
   abstract = {Our previous work presented the Evolving Intentions framework, which specified how evolving qualitative goal models can be modeled and analyzed. Recent improvements to the framework allow for precise semantics of goal relationships with propagation of both evidence for and evidence against a goal's satisfaction (as in Tropos), and enables evaluation of evolution with absolute time (in addition to relative time). The reasoning is expressed as a constraint satisfaction problem. In this paper, we present BloomingLeaf, a new web-based tool that implements the new semantics. We showcase how the implementation and architecture of BloomingLeaf can be used to answer time-based questions.},
   author = {Alicia M. Grubb and Marsha Chechik},
   doi = {10.1109/RE.2018.00067},
   isbn = {9781538674185},
   journal = {Proceedings - 2018 IEEE 26th International Requirements Engineering Conference, RE 2018},
   keywords = {Evolving requirements,Goal oriented requirements engineering (GORE),Tool demo},
   month = {10},
   pages = {490-491},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {BloomingLeaf: A formal tool for requirements evolution over time},
   year = {2018},
}
@article{Mellado2014,
   abstract = {Security and requirements engineering are two of the most important factors of success in the development of a software product line (SPL). Goal-driven security requirements engineering approaches, such as Secure Tropos, have been proposed as a suitable paradigm for elicitation of security requirements and their analysis on both a social and a technical dimension. Nevertheless, goal-driven security requirements engineering methodologies are not appropriately tailored to the specific demands of SPL, while on the other hand specific proposals of SPL engineering have traditionally ignored security requirements. This paper presents work that fills this gap by proposing "SecureTropos-SPL" framework. © 2014 Elsevier B.V.},
   author = {Daniel Mellado and Haralambos Mouratidis and Eduardo Fernández-Medina},
   doi = {10.1016/J.CSI.2013.12.006},
   issn = {0920-5489},
   issue = {4},
   journal = {Computer Standards & Interfaces},
   keywords = {Product lines,Requirements engineering,Secure Tropos,Security requirement engineering,Security requirements},
   month = {6},
   pages = {711-722},
   publisher = {North-Holland},
   title = {Secure Tropos framework for software product lines requirements engineering},
   volume = {36},
   year = {2014},
}
@article{Kavallieratos2020,
   abstract = {The growing convergence of information technology with operational technology and the accordant proliferation of interconnected cyber-physical systems (CPSs) has given rise to several security and safety challenges. One of these refers to systematically identifying coherent, consistent, and non-conflicting security and safety requirements. This paper proposes an integrated method for safety and security requirements engineering for CPSs at the design stage of the system lifecycle. The method identifies security and safety objectives, it systematically elicits a comprehensive list of requirements, and it links these requirements to objectives, thus facilitating the process of resolving conflicts. To provide insight into the operations of the method, we demonstrate its use to the most vulnerable CPSs on board the Cyber-Enabled Ship (C-ES). By utilizing the proposed method, the safety and security objectives of these systems were defined, and their safety and security requirements were identified.},
   author = {Georgios Kavallieratos and Sokratis Katsikas and Vasileios Gkioulos},
   doi = {10.1016/J.CSI.2020.103429},
   issn = {0920-5489},
   journal = {Computer Standards & Interfaces},
   keywords = {Cyber physical systems,Requirements elicitation,Safety,Security,maritime ecosystem},
   month = {6},
   pages = {103429},
   publisher = {North-Holland},
   title = {SafeSec Tropos: Joint security and safety requirements elicitation},
   volume = {70},
   year = {2020},
}
@article{Hadar2013,
   abstract = {Context Over the years, several modeling languages for requirements have been proposed. These languages employ different conceptual approaches, including scenario-based and goal-oriented ones. Empirical studies providing evidence about requirements model comprehensibility are rare, especially when addressing languages that belong to different modeling approaches. Objective This work aims to compare the comprehensibility of requirements models expressed in different but comparable modeling approaches from a requirements analysts' perspective. In particular, in this paper we compare the comprehensibility of requirements models expressed in two visual modeling languages: Use Case, which is scenario-based, and Tropos, which exploits goal-oriented modeling. We further compare the effort required for comprehending the different models, and the derived productivity in each case. Method Requirements model comprehensibility is measured here in the context of three types of tasks that analysts usually perform, namely mapping between textual description and the model elements, reading and understanding the model irrespectively of the original textual description, and modifying the model. This experimental evaluation has been conducted within a family of controlled experiments aiming at comparing the comprehensibility of Use Case and Tropos requirements models. Three runs of the experiment were performed, including a first experiment and two replications, involving 79 subjects overall (all of which were information systems students). The data for each experiment was separately analyzed, followed by a meta-analysis of the three experiments. Results The experimental results show that Tropos models seem to be more comprehensible with respect to the three types of requirements analysis tasks, although more time consuming than Use Case models. Conclusions Measuring model comprehensibility by means of controlled experiments is feasible and provides a basis for comparing Tropos and Use Case models, although these languages belong to different modeling approaches. Specifically, Tropos outperformed Use Case in terms of comprehensibility, but required more effort leading to a similar productivity of the two languages. © 2013 Elsevier B.V. All rights reserved.},
   author = {Irit Hadar and Iris Reinhartz-Berger and Tsvi Kuflik and Anna Perini and Filippo Ricca and Angelo Susi},
   doi = {10.1016/J.INFSOF.2013.05.003},
   issn = {0950-5849},
   issue = {10},
   journal = {Information and Software Technology},
   keywords = {Controlled experiment,Empirical studies,Model comprehension,Requirements models,Tropos,UML use case},
   month = {10},
   pages = {1823-1843},
   publisher = {Elsevier},
   title = {Comparing the comprehensibility of requirements models expressed in Use Case and Tropos: Results from a family of experiments},
   volume = {55},
   year = {2013},
}
@article{Dalpiaz2016,
   abstract = {The i* modeling language was introduced to fill the gap in the spectrum of conceptual modeling languages, focusing on the intentional (why?), social (who?), and strategic (how? how else?) dimensions. i* has been applied in many areas, e.g., healthcare, security analysis, eCommerce. Although i* has seen much academic application, the diversity of extensions and variations can make it difficult for novices to learn and use it in a consistent way. This document introduces the iStar 2.0 core language, evolving the basic concepts of i* into a consistent and clear set of core concepts, upon which to build future work and to base goal-oriented teaching materials. This document was built from a set of discussions and input from various members of the i* community. It is our intention to revisit, update and expand the document after collecting examples and concrete experiences with iStar 2.0.},
   author = {Fabiano Dalpiaz and Xavier Franch and Jennifer Horkoff},
   month = {5},
   title = {iStar 2.0 Language Guide},
   url = {http://arxiv.org/abs/1605.07767},
   year = {2016},
}
@article{Wautelet2017,
   abstract = {Agile software development methods are mostly built as a set of managerial guidelines and development concepts on how to handle a software development but are not bounded to software development paradigms like object or agent orientation. Some methods, like eXtreme Programming and SCRUM are driven by operational requirements representation models called User Stories. These User Stories can be used as an anchoring point to agile methods; this means that we could take a User Stories set to drive a software transformation approach embedded in a particular development paradigm. This paper presents a process fragment for Multi-Agent Systems development with agile methods based on User Stories sets. The process fragment indeed takes advantage of an initial set of User Stories to build a reasoning model (called the Rationale Tree; typically several of these are built for a single project) that documents decompositions and means-end alternatives in scenarios for requirements realization. A Rationale Tree can then be aligned with a Multi-Agent design and implemented in an agent-oriented development language. In this paper the transformation is targeted to the JAVA Agent DEvelopment (JADE) framework. The process fragment (at least partially) covers the Requirements Analysis, Multi-Agent System Design and Multi-Agent System Implementation phases. Transformation from one phase to the other is overseen and illustrated on an example.},
   author = {Yves Wautelet and Samedi Heng and Soreangsey Kiv and Manuel Kolp},
   doi = {10.1016/J.CL.2017.06.007},
   issn = {1477-8424},
   journal = {Computer Languages, Systems & Structures},
   keywords = {Agent software engineering,Agile development,JADE,JAVA Agent DEvelopment framework,Multi-agent system,Process fragment,Rationale tree,User story,i*-based software process modeling},
   month = {12},
   pages = {159-176},
   publisher = {Pergamon},
   title = {User-story driven development of multi-agent systems: A process fragment for agile methods},
   volume = {50},
   year = {2017},
}
@article{Bruel2021,
   abstract = {A major determinant of the quality of software systems is the quality of their requirements, which should be both understandable and precise. Most requirements are written in natural language, whic...},
   author = {Jean Michel Bruel and Sophie Ebersold and Florian Galinier and Manuel Mazzara and Alexandr Naumchev and Bertrand Meyer},
   doi = {10.1145/3448975},
   issn = {15577341},
   issue = {5},
   journal = {ACM Computing Surveys (CSUR)},
   keywords = {Formal,requirement,seamless,software,specification},
   month = {5},
   publisher = {
		ACM
		PUB27
		New York, NY, USA
	},
   title = {The Role of Formalism in System Requirements},
   volume = {54},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3448975},
   year = {2021},
}
@article{Horkoff2015,
   abstract = {The i* Framework, facilitating goal-oriented information systems modeling, has received much attention in research since its introduction. As the i* and related frameworks (e.g., GRL and Tropos) have been in existence for more than 20 years, researchers around the world have accumulated experience in teaching such languages, at both the undergraduate and graduate levels, to students with a wide variety of backgrounds. It is our aim to begin to collect and share these experiences. As such, we organized the first International iStar Teaching Workshop (iStarT'15), a focused workshop covering topics related to i* and goal-oriented pedagogy held at the Conference on Advanced Information Systems Engineering (CAiSE'15). In this report we summarize the presentations, discussion and future plans made as part of iStarT'15.},
   author = {Jennifer Horkoff and James Lockerbie and Xavier Franch and Eric Yu and John Mylopoulos},
   doi = {10.1145/2830719},
   issn = {01635948},
   issue = {6},
   journal = {ACM SIGSOFT Software Engineering Notes},
   keywords = {Conceptual Modeling,D21 [Languages]: Modeling Languages General Terms Human Factors,Goal-oriented Modeling,Languages Keywords i*,Pedagogy,Standardization,Teaching,iStar},
   title = {Report on the First International i* Teaching Workshop (iStarT) June 9th, Co-located with the Conference on Advanced Information Systems Engineering (CAiSE'15), Stockholm, Sweden},
   volume = {40},
   url = {http://doi.acm.org/10.1145/2830719.2830735},
   year = {2015},
}
@article{Kordon1998,
   abstract = {Software engineering methodologies rely on various and complex graphical representations and are more useful when associated to CASE (Computer Aided Software Engineering) tools designed to take care of constraints that have to be respected. Now, CASE tools gave way to CASE environments (a set of tools that have a strong coherence in their us). This concept provides enhanced solutions for software reusability while the environment may be adapted to a specific understanding of a design methodology.This paper describes FrameKit, an Ada based framework dedicated to the quick implementation of CASE environments. We summarize first the concepts implemented in FrameKit and illustrate them using a detailed example of a simple tool implementation and integration.},
   author = {Fabrice Kordon and Jean-Luc Mounier},
   doi = {10.1145/291712.291755},
   issn = {1094-3641},
   issue = {5},
   journal = {ACM SIGAda Ada Letters},
   month = {9},
   pages = {57-66},
   publisher = {Association for Computing Machinery (ACM)},
   title = {FrameKit, an Ada framework for a fast implementation of CASE enviroments},
   volume = {XVIII},
   year = {1998},
}
@article{Abad2017,
   abstract = {Several notations have been proposed in the last decades to support information system architecting, design and implementation. Although some of them have been widely adopted, their practical application remains cumbersome. Reasons are manifold: ambiguous semantics, confusing graphical representation, lack of safe guidelines, etc. In this paper, we explored the use of the i∗ framework in industry for modeling organizational context. We review the models resulting from 36 industrial collaborations conducted in the last five years, where i∗ has been intensively used by novice modellers, without previous exposure to i∗, acting as junior consultants in the organizations. We identify and categorize the main problems that they faced and as a result, we propose a set of guidelines to improve the adoption and practical application of the framework.},
   author = {Karina Abad and Wilson Pérez and Juan Pablo Carvallo and Xavier Franch},
   doi = {10.1145/3019612.3019754},
   isbn = {9781450344869},
   journal = {Proceedings of the ACM Symposium on Applied Computing},
   keywords = {Context models,Enterprise architecture,Goal-oriented modeling,Industrial validation,Social dependencies,iStar},
   month = {1},
   pages = {1122-1129},
   publisher = {Association for Computing Machinery},
   title = {I∗ in practice: Identifying frequent problems in its application},
   volume = {Part F128005},
   year = {2017},
}
@article{Bastos2005,
   abstract = {There is a clear relationship between requirements and architectures. However, evolving and elaborating system requirements into a software architecture satisfying those requirements is still a difficult task, mainly based on intuition. To narrow these problems, we are investigating techniques for deriving architectural model in concert with Multi-Agent System (MAS) requirement specifications. Our framework advocates that a multi-agent system corresponds to the organisational structure, in which actors are members of a group in order to perform specific roles. An organisation comprises groups, goals, members, roles and interactions.},
   author = {Lucia R.D. Bastos and Jaelson F.B. Castro},
   doi = {10.1145/1082960.1082980},
   isbn = {1595931163},
   journal = {SELMAS 2005 - Proceedings of the 4th International Workshop on Software Engineering for Large-Scale Multi-Agent Systems},
   month = {5},
   publisher = {Association for Computing Machinery, Inc},
   title = {From requirements to multi-agent architecture using organisational concepts},
   year = {2005},
}
@article{Dowson1987,
   abstract = {ISTAR is a full, commercially available, integrated project support environment. It includes a comprehensive and extendible set of tools covering every aspect of the software and system development process.},
   author = {Mark Dowson},
   doi = {10.1145/24208.24212},
   isbn = {0897912128},
   journal = {Proceedings of the 2nd ACM SIGSOFT/SIGPLAN Software Engineering Symposium on Practical Software Development Environments, SDE 1986},
   month = {1},
   pages = {27-33},
   publisher = {Association for Computing Machinery, Inc},
   title = {ISTAR - An integrated project support environment},
   year = {1987},
}
@article{,
   abstract = {iStar is a popular goal-based modelling language that has been used in different domains. The language has been extended to incorporate new constructs related to an application domain or to adjust it to practical situations during requirements modelling. However, often iStar extensions are proposed in an ad-hoc way resulting in several problems such as incompleteness, inconsistency and conflicts. This work aims to propose a process to be used by those interested in proposing systematic iStar extensions. A systematic literature review and a qualitative study were conducted previously to identify not only the current iStar extensions, but also to understand what the researchers feel that are important when proposing them. In this paper, we propose a process to support the proposal of new iStar extensions. It is driven by model-based development concepts, reuse of existing iStar extensions and the experts' best practices. It has already been used to propose several iStar extensions. Furthermore, the feedback on the proposal provided by several iStar experts was quite positive. Our process can avoid several problems in current iStar extensions, such as the absence of the constructs meaning, lack of metamodel definition, inconsistencies between abstract and concrete syntaxes and conflicts in the concrete syntax.},
   author = {Enyo Gonçalves and João Araujo and Jaelson Castro},
   doi = {10.1145/3341105.3375761},
   isbn = {9781450368667},
   journal = {Proceedings of the ACM Symposium on Applied Computing},
   keywords = {Extension,Goal modelling,IStar,Modelling language,Process},
   month = {3},
   pages = {1363-1370},
   publisher = {Association for Computing Machinery},
   title = {A process to support the creation of iStar extensions},
   year = {2020},
}
@article{Deligiannakis2007,
   abstract = {Sensor nodes are small devices that "measure" their environment and communicate feeds of low-level data values to a base station for further processing and archiving. Dissemination of these multi-valued feeds is challenging because of the limited resources (processing, bandwidth, energy) available in the nodes of the network. In this paper, we first describe the SBR algorithm for compressing multi-valued feeds containing historical data from each sensor. The key to our technique is the base signal, a series of values extracted from the real measurements that is used to provide piece-wise approximation of the measurements. While our basic technique exploits correlations among measurements taken on a single node, we further show how it can be adapted to exploit correlations among multiple nodes in a localized setting. Sensor nodes may form clusters and, within a cluster, a group leader identifies and coalesces similar measurements taken by different nodes. This localized mode of operation further improves the accuracy of the approximation, typically by a factor from 5 to 15. We provide detailed experiments of our algorithms and make direct comparisons against standard approximation techniques like Wavelets, Histograms and the Discrete Cosine Transform, on a variety of error metrics and for real data sets from different domains. © 2006 Springer-Verlag.},
   author = {Antonios Deligiannakis and Yannis Kotidis and Nick Roussopoulos},
   doi = {10.1007/S00778-005-0173-5},
   issn = {10668888},
   issue = {4},
   journal = {VLDB Journal},
   keywords = {Compression,Sensor Networks},
   month = {10},
   pages = {439-461},
   title = {Dissemination of compressed historical information in sensor networks},
   volume = {16},
   year = {2007},
}
@article{Mangisengi1998,
   abstract = {The paper introduces the concept of quotient relations to model and query OLAP data. The use of quotient relation inherits the advantages of the original relational model as it was introduced by Codd. Drill-down and roll-up operations can be performed by the powerful partitioning and de-partitioning operators on quotient relations. The proposed approach fulfils the requirements for a formal data model, namely the existence of an implementation independent formalism, the separation of structure and content and the existence of a declarative query language.},
   author = {O. Mangisengi and A. M. Tjoa},
   doi = {10.1145/294260.294270},
   isbn = {1581131208},
   journal = {DOLAP: Proceedings of the ACM International Workshop on Data Warehousing and OLAP},
   month = {11},
   pages = {40-46},
   publisher = {Association for Computing Machinery},
   title = {A multidimensional modeling approach for olap within the framework of the relational model based on quotient relations},
   volume = {Part F129242},
   year = {1998},
}
@article{Hilts2010,
   abstract = {Social media can be employed as powerful tools for enabling broad participation in public policy making. However, variations in the design of a social media technology system can lead to different levels or kinds of engagement, including low participation or polarized interchanges. The complex motivations, expectations, and actions among various actors in political communication need to be considered as 'make-or-break' factors in the design of such systems. This paper uses the i* modeling framework to analyse the impact that alternative configurations of a social media technology can have on the goals and relationships of the actors involved. The framework is applied to a website devoted to the deliberation of climate change issues. The intentional qualities that guide the actors in these discussions are derived from a review of the literature on e-government, on means for eliciting citizen opinion, and on the social aspects of online discussion. © 2010 ACM.},
   author = {Andrew Hilts and Eric Yu},
   doi = {10.1145/1835980.1835983},
   isbn = {9781450302296},
   journal = {Proceedings of the International Workshop on Modeling Social Media, MSM '10},
   keywords = {Collaborative filters,Conceptual modeling,E-government,Istar,Social media},
   title = {Modeling social media support for the elicitation of citizen opinion},
   year = {2010},
}
@article{Vilela2016,
   abstract = {Context: The activities related to Requirements engineering (RE) are some of the most important steps in software development, since the requirements describe what will be provided in a software system in order to fulfill the stakeholders' needs. In this context, the ACM Symposium on Applied Computing (SAC) has been a primary gathering forum for many RE activities. When studying a research area, it is important to identify the most active groups, topics, the research trends and so forth. Objective: In a previous paper, we investigated how the SAC RE-Track is evolving, by analyzing the papers published in its 8 previous editions. In this paper, we extended the analysis including the papers of the last edition (2016) and a brief resume of all papers published in the nine editions of SAC-RE track. Method: We adopted a research strategy that combines scoping study and systematic review good practices. Results: We investigated the most active countries, institutions and authors, the main topics discussed, the types of the contributions, the conferences and journals that have most referenced SAC RE-Track papers, the phases of the RE process supported by the contributions, the publications with the greatest impact, and the trends in RE. Conclusions: We found 86 papers over the 9 previous SAC RETrack editions, which were analyzed and discussed.},
   author = {Jessyka Vilela and Enyo Goncalves and Ana Carla Holanda and Jaelson Castro and Bruno Figueiredo},
   doi = {10.1145/2993231.2993234},
   issn = {1559-6915},
   issue = {2},
   journal = {ACM SIGAPP Applied Computing Review},
   month = {8},
   pages = {26-41},
   publisher = {Association for Computing Machinery (ACM)},
   title = {A retrospective analysis of SAC requirements},
   volume = {16},
   year = {2016},
}
@inproceedings{Laureano2021,
   abstract = {Since Portugal joined the European Union (EU) that it has been receiving incentives/funds to reduce disparities with other EU countries. Despite this goal, disparities between European regions still exist and the impact of such funds is questionable. What if it is possible to predict the success of such incentives when the funds are awarded to the beneficiaries? Using data from the database of The...},
   author = {Raul M. S. Laureano and Graça Trindade and Luis M. S. Laureano},
   journal = {2021 16th Iberian Conference on Information Systems and Technologies (CISTI)},
   publisher = {IEEE},
   title = {Predictive models for managing financial incentives oriented to companies: Application to Portugal 2020},
   url = {https://ieeexplore.ieee.org/document/9476655/},
   year = {2021},
}
@inproceedings{Rodrigues2021,
   abstract = {The growing use of digital media has been accompanied by an increase of the risks associated with the use of information systems, notably cybersecurity risks. In turn, the increasing use of information systems has an impact on users' media and information literacy. This research aims to address the relationship between media and information literacy, and the adoption of risky cybersecurity behavio...},
   author = {André Filipe Rodrigues and Bruno Miguel Monteiro and Isabel Pedrosa},
   journal = {2021 16th Iberian Conference on Information Systems and Technologies (CISTI)},
   publisher = {IEEE},
   title = {Cybersecurity risks : A behavioural approach through the influence of media and information literacy},
   url = {https://ieeexplore.ieee.org/document/9476383/},
   year = {2021},
}
@inproceedings{Cretan2018,
   abstract = {The cooperation of multiple networked microgrids can be one of the best solutions for solving inconsistency between energy production and consumption, which is essential for a more sustainable operation for the manufacturing business. In this paper, we are interested in the collaborative interactions among autonomous microgrids grouped into an alliance to optimise their energy exchange and schedul...},
   author = {Adina Cretan and Carlos Coutinho and Ben Bratu and Ricardo Jardim-Goncalves},
   journal = {2018 International Conference on Intelligent Systems (IS)},
   publisher = {IEEE},
   title = {A Negotiation Cloud-based Solution to Support Interoperability among Interconnected Autonomous Microgrids},
   url = {https://ieeexplore.ieee.org/document/8710579/},
   year = {2018},
}
@article{Pedro2021,
   abstract = {Virtual Reality scenarios where emitters convey information to receptors can be used as a tool for distance learning and to enable virtual visits to company physical headquarters. However, immersive Virtual Reality setups usually require visualization interfaces such as Head-mounted Displays, Powerwalls or CAVE systems, supported by interaction devices (Microsoft Kinect, Wii Motion, among others),...},
   author = {Tiago M. Silva Pedro and José Luís Silva},
   journal = {IEEE Access},
   publisher = {IEEE},
   title = {Towards Higher Sense of Presence: A 3D Virtual Environment Adaptable to Confusion and Engagement},
   volume = {9},
   url = {https://ieeexplore.ieee.org/document/9312589/},
   year = {2021},
}
@inproceedings{Haddad2016,
   abstract = {This article aims at presenting and summarizing the main concepts of citizenship, digital citizenship and digital inclusion, within the concept of citizenship construction. The methodological approach is essentially explicative, bibliographical and documental. We begin with a state-of-the art of the topic, which is necessary to prepare the well-documented empirical investigation that will be carri...},
   author = {Samir Rodrigues Haddad and Abílio Oliveira and Gustavo Cardoso},
   journal = {2016 11th Iberian Conference on Information Systems and Technologies (CISTI)},
   publisher = {IEEE},
   title = {Framework for evaluation of digital citizenship among less favored population in Brazil},
   url = {https://ieeexplore.ieee.org/document/7521448/},
   year = {2016},
}
@inproceedings{Nhabomba2021,
   abstract = {Several studies in the area of Frameworks for Information Systems Architectures recommend carrying out additional research on the support of Information Systems Architectures to the organization's businesses, using models of reference frameworks. The subject is so important mainly with regard to the support of the architectural maturity model in National Organizations for Official Statistics. Howe...},
   author = {Arlindo B. P. Nhabomba and Bráulio Alturas and Isabel Machado Alexandre},
   journal = {2021 16th Iberian Conference on Information Systems and Technologies (CISTI)},
   publisher = {IEEE},
   title = {Information Systems Architecture Framework for National Official Statistics Organizations},
   url = {https://ieeexplore.ieee.org/document/9476481/},
   year = {2021},
}
@inproceedings{Piteira2019,
   abstract = {Artificial intelligence (AI) has in recent times assumed a relevant role in the most diverse sectors of our society. We are at a no return point, and our future will incorporate artificial intelligence into our everyday life, professional or personal. The idea of “thinking” machines existence, making decisions by Humans raises several ethical questions. It is fundamental to study and investigate t...},
   author = {Martinha Piteira and Manuela Aparicio and Carlos J. Costa},
   journal = {2019 14th Iberian Conference on Information Systems and Technologies (CISTI)},
   publisher = {IEEE},
   title = {Ethics of Artificial Intelligence: Challenges},
   url = {https://ieeexplore.ieee.org/document/8760826/},
   year = {2019},
}
@inproceedings{Piteira2017,
   abstract = {Gamification has attracted the attention of researchers from different areas such as marketing, health, sports and education. Gamification integrates elements of game design in non-game context, with the purpose of engaging a person in a certain activity. This integration should follow a formal and clear design process. However, these gamification design processes for specific contexts aren't stil...},
   author = {Martinha Piteira and Carlos J. Costa},
   journal = {2017 12th Iberian Conference on Information Systems and Technologies (CISTI)},
   publisher = {IEEE},
   title = {Gamification: Conceptual framework to online courses of learning computer programming},
   url = {https://ieeexplore.ieee.org/document/7975695/},
   year = {2017},
}
@inproceedings{Franch2012,
   abstract = {Actors, dependencies, goal satisfaction, ..., in a word, strategic modeling and reasoning, are recurrent matters in many software engineering and information system activities and disciplines. Standing out among other initiatives, i* soon became the preferred strategic analysis framework to many research groups that have adopted it and shaped it to their particular interests. A quick look to...},
   author = {Xavier Franch},
   journal = {2012 Sixth International Conference on Research Challenges in Information Science (RCIS)},
   publisher = {IEEE},
   title = {The i∗ framework: The way ahead},
   url = {https://ieeexplore.ieee.org/document/6240418/},
   year = {2012},
}
@inproceedings{Guizzardi2012,
   abstract = {The i* community has raised several main dialects and dozens of variations in the definition of the i* language. Differences may be found related not just to the representation of new concepts but to the very core of the i* language. If on the one hand this is caused by large adoption of the framework in the academic setting, on the other hand, it also poses some threats. For example, novice...},
   author = {Renata Guizzardi and Xavier Franch and Giancarlo Guizzardi},
   journal = {2012 Sixth International Conference on Research Challenges in Information Science (RCIS)},
   publisher = {IEEE},
   title = {Applying a foundational ontology to analyze means-end links in the i∗ framework},
   url = {https://ieeexplore.ieee.org/document/6240425/},
   year = {2012},
}
@article{Penha2018,
   abstract = {Complex systems are inherent to modern society, in which individuals, organizations, and computational elements relate with each other to achieve a predefined purpose, which transcends individual goals. In this context, these systems’ complexity is originated by the large number of parts interacting in a non-simple way, given the properties of these parts and the laws, as well as by the wishes that govern these interactions. Also, in organizations, there is a need for additional information to understand this universe considering the already consolidated static and dynamic dimensions. With this purpose, the iStar framework was developed to capture and represent intentional and social information in two views: Strategic Dependency (SD) and Strategic Rationale (SR). This framework, however, does not offer alternatives to deal with the complexity that is inherent to modern society systems, which is related to a large number of parts interacting, when modeled from their views. The problem is present in monolithic languages because they do not consist of building blocks, such as subprocesses or modules. Despite this problem, the iStar framework provides modeling versatility by combining goal-oriented paradigms and agents. Another positive point is the focus on intentional and social properties, thus providing expressiveness aligned with the modern society’s demand, in which everything is related. Therefore, the objective of this research was to provide ways for the iStar framework to deal with the complexity presented by complex systems and, consequently, make iStar models understandable to be used, in a given context. The proposal is based on a state of the art review to create an interdependente part for the iStar models and will make the construction of views as a composition of these parts possible. To make it happen, and considering its benefits, a textual notation (SMiLe - Scalable Modular iStar Language) was conceived and applied to support the architecture within this social modeling scenario. The proposal and its artifacts were submitted to a proof of concept, and then, through adjustments, an evaluation was carried by the users through a case study. The results pointed to evidence of the possible management of iStar model and an improvement in the understanding of this model, suggesting that the proposed solution is a feasible alternative for the established objective.},
   author = {Fábio Penha and Erica Miranda and Márcia Lucena and Leonardo Lucena and Fernanda Alencar and Celso Sá Filho},
   doi = {10.1186/S40411-018-0055-3},
   issue = {1},
   journal = {Journal of Software Engineering Research and Development},
   month = {12},
   publisher = {Sociedade Brasileira de Computacao - SB},
   title = {Actor’s social complexity: a proposal for managing the iStar model},
   volume = {6},
   year = {2018},
}
@article{Yu2013,
   abstract = {i* is a goal-oriented and agent-oriented modeling framework that focuses on the analysis of intentional and strategic relationships among actors. In this mini-tutorial, we highlight a number of recent applications in practical industrial and business settings. © 2013 IEEE.},
   author = {Eric Yu and Daniel Amyot and Gunter Mussbacher and Xavier Franch and Jaelson Castro},
   doi = {10.1109/RE.2013.6636754},
   isbn = {9781467357654},
   journal = {2013 21st IEEE International Requirements Engineering Conference, RE 2013 - Proceedings},
   keywords = {GRL,Goal-oriented requirements engineering,IStar social modeling},
   pages = {366-367},
   publisher = {IEEE Computer Society},
   title = {Practical applications of i* in industry: The state of the art},
   year = {2013},
}
@article{,
   abstract = {The i* (i-star) framework is one of the most widely adopted modelling approaches by several communities (business modelling, requirements engineering, ...). Probably due to its highly strategic nature, the definition of the modelling language offered by the framework does not make explicit the full behaviour of some basic constructs, leaving them thus open to several interpretations. This looseness may not be important in some contexts, even it may be beneficial since it leaves room for researchers to customize the framework to their needs. However, it becomes an obstacle in other situations, e.g., model interoperability and model-driven development. In this paper we identify ambiguities and silences in the i* language definition in a systematic manner, and then we propose an interpretation to deal with them. In some cases, the proposal may include the addition of some annotation into some language construct. The result is a formal definition taking the form of a UML conceptual data diagram (a metamodel) with several important integrity constraints. © 2011 Springer-Verlag.},
   author = {Lidia López and Xavier Franch and Jordi Marco},
   doi = {10.1007/978-3-642-24606-7_6/COVER},
   isbn = {9783642246050},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {ambiguity,i* framework,i-star,iStar,silence},
   pages = {62-77},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Making explicit some implicit i* language decisions},
   volume = {6998 LNCS},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-642-24606-7_6},
   year = {2011},
}
@article{Teruel2019,
   abstract = {Collaborative business intelligence (BI) is widely embraced by
enterprises as a way of making the most of their business processes.
However, decision makers usually work in isolation without the knowledge
or the time needed to obtain and analyze all the available information
for making decisions. Unfortunately, collaborative BI is currently based
on exchanging e-mails and documents between participants. As a result,
information may be lost, participants may become disoriented, and the
decision-making task may not yield the needed results. The authors
propose a modeling language aimed at modeling and eliciting the goals
and information needs of participants of collaborative BI systems. This
approach is based on innovative methods to elicit and model
collaborative systems and BI requirements. A controlled experiment was
performed to validate this language, assessing its understandability,
scalability, efficiency, and user satisfaction by analyzing two
collaborative BI systems. By using the framework proposed in this work,
clear guideless can be provided regarding: (1) collaborative tasks, (2)
their participants, and (3) the information to be shared among them. By
using the approach to design collaborative BI systems, practitioners may
easily trace every element needed in the decision processes, avoiding
the loss of information and facilitating the collaboration of the
stakeholders of such processes.},
   author = {Miguel A Teruel and Alejandro Mate and Elena Navarro and Pascual Gonzalez and Juan C Trujillo},
   city = {ABRAHAM-LINCOLN STASSE 46, WIESBADEN, 65189, GERMANY},
   doi = {10.1007/s12599-019-00578-3},
   issn = {2363-7005},
   issue = {5},
   journal = {BUSINESS & INFORMATION SYSTEMS ENGINEERING},
   keywords = {Collaborative systems; Business intelligence; Goal-oriented
requirements; CASE support; I-Star; Controlled experiment},
   month = {10},
   pages = {615-634},
   publisher = {SPRINGER VIEWEG-SPRINGER FACHMEDIEN WIESBADEN GMBH},
   title = {The New Era of Business Intelligence Applications: Building from a
Collaborative Point of View},
   volume = {61},
   year = {2019},
}
@article{Champollion2015,
   abstract = {Why can I tell you that I ran for five minutes but not that I (star)ran
to the store for five minutes? Why can we talk about five pounds of
books but not about (star)five pounds of book? What keeps you from
saying (star)sixty degrees Celsius of water when you can say sixty
inches of water? And what goes wrong when I complain that (star)all the
ants in my kitchen are numerous? The constraints on these constructions
involve concepts that are generally studied separately: aspect, plural
and mass reference, measurement, distributivity, and collectivity. This
paper provides a unified perspective on these domains and gives a single
answer to the questions above in the framework of algebraic event
semantics.},
   author = {Lucas Champollion},
   city = {GENTHINER STRASSE 13, D-10785 BERLIN, GERMANY},
   doi = {10.1515/tl-2015-0008},
   issn = {0301-4428},
   issue = {3-4},
   journal = {THEORETICAL LINGUISTICS},
   keywords = {algebraic semantics; aspect; boundedness; collectivity; distributivity;
mass; measurement; mereology; monotonicity; plural; partitives; telicity},
   month = {10},
   pages = {109-149},
   publisher = {WALTER DE GRUYTER GMBH},
   title = {Stratified reference: the common core of distributivity, aspect, and
measurement},
   volume = {41},
   year = {2015},
}
@inproceedings{Lopez2014,
   abstract = {Increasing adoption of Open Source Software (OSS) in information system
engineering has led to the emergence of different OSS business
strategies that affect and shape organizations' business models. In this
context, organizational modeling needs to reconcile efficiently OSS
adoption strategies with business strategies and models. In this paper,
we propose to embed all the knowledge about each OSS adoption strategy
into an i* model that can be used in the intentional modeling of the
organization. These models describe the consequences of adopting one
such strategy or another: which are the business goals that are
supported, which are the resources that emerge, etc. To this aim, we
first enumerate the main existing OSS adoption strategies, next we
formulate an ontology that comprises the activities and resources that
characterise these strategies, then based on the experience of 5
industrial partners of the RISCOSS EU-funded project, we explore how
these elements are managed in each strategy and formulate the
corresponding model using the i* framework.},
   author = {Lidia Lopez and Dolors Costal and Claudia P Ayala and Ruediger Franch Xavier
and Glott and Kirsten Haaland},
   city = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
   doi = {10.1007/978-3-319-12206-9_29},
   editor = {E Yu and G Dobbie and M Jarke and S Purao},
   isbn = {978-3-319-12206-9; 978-3-319-12205-2},
   issn = {0302-9743},
   journal = {CONCEPTUAL MODELING},
   keywords = {OSS; Open Source Software; OSS adoption strategy; OSS ontology; i*
framework; i-star},
   note = {33rd International Conference on Conceptual Modeling (ER), Atlanta, GA,
OCT 27-29, 2014},
   pages = {349-362},
   publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
   title = {Modelling and Applying OSS Adoption Strategies},
   volume = {8824},
   year = {2014},
}
@article{Pimentel2012,
   abstract = {Some quality attributes are known to have an impact on the overall
architecture of a system, so that they are required to be properly
handled from the early beginning of the software development. For
example, adaptability is a key concern for autonomic and adaptive
systems, which brings to them the capability to alter their behavior in
response to changes on their surrounding environments. In this paper, we
propose a Strategy for Transition between Requirements and Architectural
Models for Adaptive systems (STREAM-A). In particular, we use goal
models based on the i* (i-Star) framework to support the design and
evolution of systems that require adaptability. To obtain software
architectures for such systems, the STREAM-A approach uses model
transformations from i* models to architectural models expressed in
Acme. Both the requirements and the architectural model are refined to
accomplish the adaptability requirement.},
   author = {Joao Pimentel and Marcia Lucena and Jaelson Castro and Emanuel Silva Carla
and Santos and Fernanda Alencar},
   city = {ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES},
   doi = {10.1007/s00766-011-0126-z},
   issn = {0947-3602},
   issue = {4, SI},
   journal = {REQUIREMENTS ENGINEERING},
   keywords = {Requirements engineering; Architectural design; Mapping between
requirements model and architectural model; Model-driven engineering;
Adaptive systems},
   month = {11},
   pages = {259-281},
   publisher = {SPRINGER},
   title = {Deriving software architectural models from requirements models for
adaptive systems: the STREAM-A approach},
   volume = {17},
   year = {2012},
}
@article{Ruiz2015,
   abstract = {Context: Organisational reengineering, continuous process improvement,
alignment among complementary analysis perspectives, and information
traceability are some current motivations to promote investment and
scientific effort for integrating goal and business process
perspectives. Providing support to integrate information systems
analysis becomes a challenge in this complex setting.
Objective: The GoBIS framework integrates two goal and business process
modelling approaches: i* (a goal-oriented modelling method) and
Communication Analysis (a communication-oriented business process
modelling method).
Method: In this paper, we describe the methodological integration of
both methods with the aim of fulfilling several criteria: i) to rely on
appropriate theories; ii) to provide abstract and concrete syntaxes;
iii) to provide scenarios of application; iv) to develop tool support;
v) to provide demonstrable benefits to potential adopters.
Results: We provide guidelines for using the two modelling methods in a
top-down analysis scenario. The guidelines are validated by means of a
comparative experiment and a focus-group session with students.
Conclusions: From a practitioner viewpoint (modeller and/or analyst),
the guidelines facilitate the traceability between goal and business
process models, the experimental results highlight the benefits of GoBIS
in performance and usability perceptions, and demonstrate an improvement
on the completeness of the latter having an impact on efficiency. From a
researcher perspective, the validation has produced useful feedback for
future research. (C) 2015 Elsevier Ltd. All rights reserved.},
   author = {Marcela Ruiz and Dolors Costal and Sergio Espana and Oscar Franch Xavier
and Pastor},
   city = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND},
   doi = {10.1016/j.is.2015.03.007},
   issn = {0306-4379},
   journal = {INFORMATION SYSTEMS},
   keywords = {Modelling language; Requirements engineering; Goal modelling; Business
process modelling; Ontological analysis; Metamodel integration;
Performance and usability analysis; i*; iStar; Communication Analysis},
   month = {10},
   note = {26th International Conference on Advanced Information Systems
Engineering (CAiSE), Thessaloniki, GREECE, 2014},
   pages = {330-345},
   publisher = {PERGAMON-ELSEVIER SCIENCE LTD},
   title = {GoBIS: An integrated framework to analyse the goal and business process
perspectives in information systems},
   volume = {53},
   year = {2015},
}
@inproceedings{Liu2002,
   abstract = {Virtual enterprise as a temporary alliance to respond to the rapidly
changing requirements of market has become an important application
field for multi-agent systems. In a virtual enterprise, partners are
selected according to their goals to jointly fulfill the market
requirement that otherwise can't be achieved or not easily be achieved
for the limitation of individual's capabilities. This paper proposes an
idea paradigm to model virtual enterprise with strategic dependency
model in the i* framework. Based on the modeling approach, the
partners are treated as strategic actors and cooperation relationships
among them are depicted as dependency relationships. Through dependency
relationships, the actors can form a hierarchical architecture and the
dependencies will be propagated over the network. This modeling approach
will implement agent-oriented analysis and bridge the gap between the
requirement analysis and the architecture design in multi-agent virtual
enterprise. Example from an air-separator virtual enterprise is used to
illustrate.},
   author = {Z Liu and M Lai and L Liu},
   city = {8820 SIX FORKS ROAD, RALEIGH, NC 27615 USA},
   editor = {S R Subramanya},
   isbn = {1-880843-45-5},
   journal = {COMPUTER APPLICATIONS IN INDUSTRY AND ENGINEERING},
   note = {15th International Conference on Computer Applications in Industry and
Engineering (CAINE-2002), SAN DIEGO, CA, NOV 07-09, 2002},
   pages = {166-169},
   publisher = {INTERNATIONAL SOCIETY COMPUTER S & THEIR APPLICATIONS (ISCA)},
   title = {Modeling virtual enterprise based on i(star) framework},
   year = {2002},
}
@inproceedings{Willis2013,
   abstract = {The modelling of the Automatic Target Detection, Recognition and
Identification performance in systems of multiple sensors and/or
platforms is important in many respects. For example, in the selection
of sensors or sensor combinations of sufficient effectiveness to achieve
operational requirements, or for understanding how the system might be
best exploited. It is possible that a sensing system may be comprised of
sensors of several different types, including active and passive
approaches in the radio frequency and optical portions of the spectrum.
Some may have well-understood performance, whereas others may be only
poorly characterised.
A simulation framework has been developed examining sensor options
across different sensor types, parameterisations, search strategies, and
applications. The framework is based around Bayesian Decision Theoretic
principles along with simple sensor models and search environment. It
uses Monte-Carlo simulation to derive statistical measures of
performance for systems. The framework has been designed to encompass
detection, recognition and identification problems and also to treat
sensor characterisation.
The modelling framework has been applied to a number of illustrative
problems. These range from simple target detection scenarios using
sensors of differing performance or of different regional search
schemes, through to examinations of: the number of measurements required
to reach threshold performance; the effects of sensor measurement cost;
issues relating to the poor characterisation of sensors within the
system, and; the performance of combined detection and recognition
sensor systems. Results are presented illustrating these effects. These
generally show that the method is able to quantify qualitative
expectations of performance, and is sufficiently powerful to highlight
some unexpected aspects of operation.},
   author = {Chris J Willis},
   city = {1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA},
   doi = {10.1117/12.2027582},
   editor = {K L Lewis and R C Hollins and T J Merlet and M T Gruneisen and M Dusek and J G Rarity and E M Carapezza},
   isbn = {978-0-8194-9768-0},
   issn = {0277-786X},
   journal = {EMERGING TECHNOLOGIES IN SECURITY AND DEFENCE; AND QUANTUM SECURITY II;
AND UNMANNED SENSOR SYSTEMS X},
   keywords = {performance modelling; sensor management; sensor system;
surveillance/reconnaissance; ISTAR; target
detection/recognition/identification; Monte-Carlo simulation; weak
characterisation},
   note = {Conference on Emerging Technologies in Security and Defence; and Quantum
Security II; and Unmanned Sensor Systems X, Dresden, GERMANY, SEP 23-26,
2013},
   publisher = {SPIE-INT SOC OPTICAL ENGINEERING},
   title = {Meeting performance and sensing-cost requirements for detection and
recognition systems},
   volume = {8899},
   year = {2013},
}
@inproceedings{Costal2015,
   abstract = {Increasing adoption of Open Source Software (OSS) requires a change in
the organizational culture and reshaping IT decision-makers mindset.
Adopting OSS software components introduces some risks that can affect
the adopter organization's business goals, therefore they need to be
considered. To assess these risks, it is required to understand the
socio-technical structures that interrelate the stakeholders in the OSS
ecosystem, and how these structures may propagate the potential risks to
them. In this paper, we study the connection between OSS adoption risks
and OSS adopter organizations' business goals. We propose a model-based
approach and analysis framework that combines two existing frameworks:
the i* framework to model and reason about business goals, and the
RiskML notation to represent and analyse OSS adoption risks. We
illustrate our approach with data drawn from an industrial partner
organization in a joint EU project.},
   author = {Dolors Costal and Lidia Lopez and Mirko Morandini and Maria Carmela Siena Alberto
and Annosi and Daniel Gross and Lucia Mendez and Xavier Franch and Angelo Susi},
   city = {GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND},
   doi = {10.1007/978-3-319-25264-3_3},
   editor = {P Johannesson and M L Lee and S W Liddle and A L Opdahl and O P Lopez},
   isbn = {978-3-319-25264-3; 978-3-319-25263-6},
   issn = {0302-9743},
   journal = {CONCEPTUAL MODELING, ER 2015},
   keywords = {Risk analysis; Open source software; i* framework; i-star},
   note = {34th International Conference on Conceptual Modeling (ER), Stockholm,
SWEDEN, OCT 19-22, 2015},
   pages = {35-49},
   publisher = {SPRINGER INTERNATIONAL PUBLISHING AG},
   title = {Aligning Business Goals and Risks in OSS Adoption},
   volume = {9381},
   year = {2015},
}
@inproceedings{,
   abstract = {In Web Engineering (WE), several Goal-oriented Requirements Engineering
(GORE) approaches have emerged using its advantages, such as the
representation of actors, their intentions, goals and the tasks needed
to achieve the goal, for requirements specification with promising
results. Regrettably, the use of GORE approaches has one, among others,
gap detected, the scalability. In these modeling frameworks, when the
designer performs the requirements specification, the requirements
diagram (model) trends to rapidly grow, becoming very difficult to use
in projects with a considerable amount of requirements changing and
growing constantly. In this paper, we propose an association form for
the i* goal-oriented modeling framework in order to define the
creation of two type of modules: Navigational and Service modules, since
these are the two types of functional requirements more used for
requirements specification in our proposal. Furthermore, we provide an
example of application. Finally, with this approach, the benefits are:
firstly, the scalability of the Web requirements model will be
increased, therefore the model will be less complex and easier to
understand and maintain, and secondly, the construction of modeling
tools improving the user experience, the maintainability of the models
and its reuse.},
   author = {Jose Alfonso Aguilar and Anibal Zaldivar and Carolina Tripp and Sanjay Misra and Salvador Sanchez and Miguel Martinez and Omar Vicente
Garcia},
   city = {HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY},
   editor = {B Murgante and S Misra and AMAC Rocha and C Torre and J G Rocha and M I Falcao and D Taniar and B O Apduhan and O Gervasi},
   isbn = {978-3-319-09156-3; 978-3-319-09155-6},
   issn = {0302-9743},
   journal = {COMPUTATIONAL SCIENCE AND ITS APPLICATIONS - ICCSA 2014, PT V},
   keywords = {Web Engineering; Goal-oriented Requirements Engineering; i-star;
Scalability; Requirements Modeling},
   note = {14th International Conference on Computational Science and Its
Applications (ICCSA), Guimaraes, PORTUGAL, JUN 30-JUL 03, 2014},
   pages = {135+},
   publisher = {SPRINGER-VERLAG BERLIN},
   title = {A Solution Proposal for Complex Web Application Modeling with the I-Star
Framework},
   volume = {8583},
   year = {2014},
}
@inproceedings{,
   abstract = {Modern enterprise engineering (EE) requires deep understanding of
organizations and their interaction with their context. Because of this,
in early phases of the EE process, enterprise context models are often
built and used to reason about organizational needs with respects to
actors in their context and vice versa. However, far from simple, this
task is usually cumbersome because of knowledge and communication gaps
among technical personnel performing EE activities and their
administrative counterparts. In this paper, we propose the use of
strategic patterns expressed with the i* language aimed to help
bridging this gap. Patterns emerged from several industrial applications
of our DHARMA method, and synthesize knowledge about common enterprise
strategies, e. g. CRM. Patterns have been constructed based on the
well-known Porter's model of the 5 market forces and built upon i*
strategic dependency models. In this way technical and administrative
knowledge and skills are synthesized in a commonly agreeable framework.
The use of patterns is illustrated with an industrial example in the
telecom field.},
   author = {Juan Pablo Carvallo and Xavier Franch},
   city = {HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY},
   editor = {S Aier and M Ekstedt and F Matthes and E Proper and J L Sanz},
   isbn = {978-3-642-34163-2; 978-3-642-34162-5},
   issn = {1865-1348},
   journal = {TRENDS IN ENTERPRISE ARCHITECTURE RESEARCH AND PRACTICE-DRIVEN RESEARCH
ON ENTERPRISE TRANSFORMATION},
   keywords = {enterprise pattern; enterprise context model; market forces; strategic
dependencies; i* framework; iStar},
   note = {7th Workshop on Trends in Enterprise Architecture Research (TEAR) / 5th
Working on Practice-DrivenResearch on Enterprise Transformation (PRET)
Held at the Open Group Conference on Enterprise Architecture, Cloud
Computing, Security, Barcelona, SPAIN, OCT 23-24, 2012},
   pages = {40-59},
   publisher = {SPRINGER-VERLAG BERLIN},
   title = {Building Strategic Enterprise Context Models with i*: A Pattern-Based
Approach},
   volume = {131},
   year = {2012},
}
@inproceedings{Mate2011,
   abstract = {The success rate of Data Warehouse (DW) development is improved by
performing a requirements elicitation stage in which the users' needs to
be fulfilled by the DW are modeled. Currently, among the different
proposals for modeling requirements, there is a special focus on
Goal-Oriented models, and in particular on the i* framework. In order
to adapt this framework for DW development, a UML profile for DWs was
proposed. However, as the general i* framework, the proposal lacks
modularity. This has a specially negative impact for DW development,
since the business strategy plans tend to include a huge number of
elements with many crossed relationships between them. In turn, the
readability of the models is decreased, harming their utility and
increasing the error rates and development time. In this paper, we
propose an extension of the i* profile for DWs considering the
modularization of goals. We also provide a set of guidelines in order to
correctly apply our proposal. Furthermore, we have performed an
experiment in order to validate our proposal. The great benefits of our
proposal are an increase in the modularity and scalability of the models
which, in turn, increases the error correction capability, and makes
complex models easier to understand by both DW developers and non expert
users.},
   author = {Alejandro Mate and Juan Trujillo and Xavier Franch},
   city = {HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY},
   editor = {M A Jeusfeld and L Delcambre and T W Ling},
   isbn = {978-3-642-24605-0},
   issn = {0302-9743},
   journal = {CONCEPTUAL MODELING - ER 2011},
   keywords = {Data Warehouses; modules; user requirements; i-star},
   note = {30th International Conference on Conceptual Modeling (ER), Brussels,
BELGIUM, OCT 31-NOV 03, 2011},
   pages = {421+},
   publisher = {SPRINGER-VERLAG BERLIN},
   title = {A Modularization Proposal for Goal-Oriented Analysis of Data Warehouses
Using I-Star},
   volume = {6998},
   year = {2011},
}
@inproceedings{Eridaputra2014,
   abstract = {In this research, we propose generic requirement model for Big Data
application. The model offer the use of i(star) Framework and Knowledge
Acquisition autOmated System (KAOS), which are part of Goal Oriented
Requirement Engineering (GORE). Big Data applications handle flood of
data that occurs from anything such as climate data, genomes, even just
software logs or facebook status. To build such applications demands
gathering special requirements specific for Big Data. A generic
requirement model is proposed using i(star) and KAOS model. The model is
constructed from analyzing the requirements based on the characteristics
of Big Data and its challenges. This generic model is then applied to a
case study in an Indonesian's government agency for development planning
of West Java (UPTB Pusdalisbang Jawa Barat). The result of this
application has demonstrated that the model can be used to generate a
valid software requirement specification.},
   author = {Hanif Eridaputra and Bayu Hendradjaya and Wikan Danar Sunindyo},
   city = {345 E 47TH ST, NEW YORK, NY 10017 USA},
   isbn = {978-1-4799-7996-7},
   journal = {2014 International Conference on Data and Software Engineering (ICODSE)},
   keywords = {Big Data; GORE; i(star); KAOS; requirement},
   note = {International Conference on Data and Software Engineering ICODSE, Inst
Teknol Bandung ITB Campus, Bandung, INDONESIA, NOV 26-27, 2014},
   publisher = {IEEE},
   title = {Modeling The Requirements for Big Data Application Using Goal Oriented
Approach},
   year = {2014},
}
@inproceedings{Yu2015,
   abstract = {Identifying features and creating a feature model during the process of
software product line engineering is time-consuming and requires
substantial effort from modelers. In recent years, a number of
approaches that map goal models to feature models have been proposed so
as to avoid creating the feature model from scratch. However, these
approaches can only map part of the elements, such as the goals. The
question of how to map the dependencies among actors to the feature
model remains an open problem. In this paper, we exploit the standard
goal-oriented framework, i.e., the i(star) framework, for constructing a
more complete domain feature model. In particular, we propose mapping
rules as well as an algorithm for mapping dependencies among actors to
the feature model. In addition, we seek to customize the application
products using the i(star) model evaluation approach during the
application engineering phase. The case shows that our approach is
practical and efficient.},
   author = {Dongjin Yu and Zhenli Chen and Yifei Zhang},
   city = {345 E 47TH ST, NEW YORK, NY 10017 USA},
   doi = {10.1109/APSEC.2015.22},
   editor = {J Sun and Y R Reddy and A Bahulkar and A Pasala},
   isbn = {978-1-4673-9644-8},
   issn = {1530-1362},
   journal = {2015 22ND ASIA-PACIFIC SOFTWARE ENGINEERING CONFERENCE (APSEC 2015)},
   keywords = {goal models; feature models; i(star) framework; software product line;
application product customization},
   note = {22nd Asia-Pacific Software Engineering Conference (APSEC), New Delhi,
INDIA, DEC 01-04, 2015},
   pages = {277-284},
   publisher = {IEEE},
   title = {From Goal Models to Feature Models: a Rule-based Approach for Software
Product Lines},
   year = {2015},
}
@inproceedings{Subramanian2015,
   abstract = {Requirement analysis involves elicitation of suitable functions or
operations and relevant data to support software. A requirement analyst
examines different alternative options to decide on an optimal
alternative option that benefits the stakeholders of the system. The
decision making of alternative design option is complicated by the
unavailable or incomplete and imprecise input data. Optimisation, an
operation research technique, can be used as a method to solve this
problem. The goal-oriented framework, such as i(star) is used to present
social models for the analysis of a software system during the early
phase of the requirement's engineering process. This paper aims to
develop an optimisation model for the i(star) goal models, using
multi-objective optimisation. The optimisation model aims to fully
automate the goal analysis and to handle large goal models. A simulation
for the proposed approach was developed by integrating Visual C++ with
Matlab and was evaluated with case studies from the existing literature.
The evaluation results show that the proposed approach is feasible and
offers guidance in the decision making of alternative options.},
   author = {Chitra M Subramanian and Aneesh Krishna and Arshinder Kaur},
   city = {345 E 47TH ST, NEW YORK, NY 10017 USA},
   doi = {10.1109/APSEC.2015.55},
   editor = {J Sun and Y R Reddy and A Bahulkar and A Pasala},
   isbn = {978-1-4673-9644-8},
   issn = {1530-1362},
   journal = {2015 22ND ASIA-PACIFIC SOFTWARE ENGINEERING CONFERENCE (APSEC 2015)},
   keywords = {Requirements Engineering; Goal model; Multi-objective Optimisation;
i(star) Framework},
   note = {22nd Asia-Pacific Software Engineering Conference (APSEC), New Delhi,
INDIA, DEC 01-04, 2015},
   pages = {346-353},
   publisher = {IEEE},
   title = {Optimal Reasoning of Goals in the i(star) Framework},
   year = {2015},
}
@inproceedings{,
   abstract = {Owing to the dynamicity of business environments in which organizations
must quickly adapt to changes, the information systems have recently had
to adapt to new situations so that they can keep adding value
efficiently and effectively. In the light of this scenario, a new
discipline called Service-oriented Systems Engineering has emerged in
the academic scene and this has highlighted the disciplined, systematic
and quantified development of Service-oriented Computing systems. This
discipline is divided into other sub-disciplines; one of these
sub-disciplines is Service-oriented Requirements Engineering (SORE) and
it is concerned with defining processes and methodologies to address the
question of services requirements from two different perspectives:
service consumer and service provider. In the SORE context, this paper
proposes the WS&i(&STAR;)-RGPS approach that explores some
alternatives to the descriptions proposed by the Role, Goal, Process and
Service (RGPS) metamodels - an approach in SORE. This involves a new
definition that seeks to incorporate the benefits of other methodologies
-established in the literature -with RGPS. Accordingly, this new
approach outlines the use of the i(star) Framework to describe the Role
and Goal layers and shows the use of WS-BPEL/WSDL to describe the
process and service layers. The use of the i(star) Framework sets out
the goals for managing different aspects of the systems specification.
Moreover, this paper presents a comparison among WS& i(star)-RGPS and
other SORE approaches based in three parameters presented in SORE
literature.},
   author = {Keith de Souza and Marcelo Fantinato and Marcelo Medeiros Eler},
   city = {345 E 47TH ST, NEW YORK, NY 10017 USA},
   doi = {10.1109/SCC.2014.20},
   editor = {E Ferrari and R Kaliappa and P Hung},
   isbn = {978-1-4799-5066-9},
   journal = {2014 IEEE INTERNATIONAL CONFERENCE ON SERVICES COMPUTING (SCC 2014)},
   keywords = {service-oriented requirement engineering; SORE; RGPS; i(star) Framework;
WS-BPEL},
   note = {11th IEEE International Conference on Services Computing (SCC),
Anchorage, AK, JUN 27-JUL 02, 2014},
   pages = {83-90},
   publisher = {IEEE},
   title = {Systematizing the Service-oriented Requirements Engineering Through
i(star) Framework and Web Services},
   year = {2014},
}
@inproceedings{Saito2021,
   abstract = {Software innerSourcing has been attracting attention over the past
decade in terms of procuring human resources. It can make any employee
become a member of all projects in the company by adopting an Open
Source Software (OSS) development style. For encouraging healthy
employee participation on software innerSourcing, it is important to
understand what motivates them. This report creates a visual model using
the iStar framework, a goal and actor-oriented modeling methodology, to
develop an understanding of employee motivation for software
innerSourcing. We interview six employees who participated in one web
application development project via software innerSourcing. We then
create iStar models based on the interview results. They can be used to
explain what motivates employees to use their time for other projects in
software innerSourcing.},
   author = {Shinobu Saito and Yukako Iimura},
   city = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA},
   doi = {10.1109/ICSSP-ICGSE52873.2021.00013},
   isbn = {978-1-6654-1401-2},
   journal = {2021 IEEE/ACM JOINT 15TH INTERNATIONAL CONFERENCE ON SOFTWARE AND SYSTEM
PROCESSES AND 16TH ACM/IEEE INTERNATIONAL CONFERENCE ON GLOBAL SOFTWARE
ENGINEERING (ICSSP/ICGSE 2021)},
   keywords = {Motivation; Visualization; InnerSourcing},
   note = {IEEE/ACM Joint 15th International Conference on Software and System
Processes / 16th ACM/IEEE International Conference on Global Software
Engineering: (ICSSP/ICGSE), ELECTR NETWORK, MAY 22-30, 2021},
   pages = {33-38},
   publisher = {IEEE COMPUTER SOC},
   title = {Toward Understanding of Employee Motivation for Software InnerSourcing
Industrial Experience Report},
   year = {2021},
}
@inproceedings{Schuetz2018,
   abstract = {In strategic management, strategy analysis includes an analysis of a
company's strategic position. In particular, the PESTEL framework serves
to analyze a company's macroenvironment, i.e., the political, economic,
social, technological, ecological, and legal factors that influence a
company's strategic position. In this paper, we demonstrate a modeling
tool for PESTEL analysis implemented in MetaEdit+ using iStar 2.0
notation. Semantic technologies then serve to infer new knowledge from
PESTEL models, namely the opportunities and threats that a company
faces.},
   author = {Christoph Schuetz and Eva Mair and Michael Schrefl},
   city = {345 E 47TH ST, NEW YORK, NY 10017 USA},
   doi = {10.1109/EDOCW.2018.00040},
   isbn = {978-1-5386-4141-5},
   issn = {2325-6583},
   journal = {2018 IEEE 22ND INTERNATIONAL ENTERPRISE DISTRIBUTED OBJECT COMPUTING
CONFERENCE WORKSHOPS (EDOCW 2018)},
   keywords = {strategic management; environmental analysis; SWOT; business modeling;
metaCASE tools; CASE tools},
   note = {IEEE 22nd International Enterprise Distributed Object Computing
Conference (EDOC), Stockholm, SWEDEN, OCT 16-19, 2018},
   pages = {216-219},
   publisher = {IEEE},
   title = {PESTEL Modeler: Strategy Analysis Using MetaEdit+, iStar 2.0, and
Semantic Technologies},
   year = {2018},
}
@inproceedings{Liu2021,
   abstract = {[Context] Establishing requirements models is an effective way to
analyze them, which is typically dealt with in a graphical manner (i.e.,
the drag-and-draw fashion). However, as the size of models increases,
the scalability issue has become an unignorable challenge, hindering the
practical adoption of requirements modeling approach. Some researchers
have recently proposed and promoted textual modeling approaches,
mitigating these issues of requirements modeling. [Objective] In this
paper, we aim at evaluating the two modeling methods, i.e., a graphical
modeling method VS. a textual modeling method. In particular, we apply
these two methods to iStar modeling language, which has been widely
recognized as an effective means to model and analyze requirements.
[Methods] We have systematically designed and conducted a controlled
experiment with 38 participants to compare two iStar modeling methods
(graphical and textual) using two corresponding modeling tools (piStar
and T-Star). The experimental results reveal that the numbers of iStar
model nodes and relationships built by the participants had no
significant difference, regardless of the modeling method adopted.
[Conclusions] First, the results show that the textual modeling method
is as usable as the graphical modeling method when creating iStar
models. Second, we have identified a number of issues that contribute to
improving the utility and practicality of the iStar modeling method.},
   author = {Wenxing Liu and Yunduo Wang and Qixiang Zhou and Tong Li},
   city = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA},
   doi = {10.1109/COMPSAC51774.2021.00117},
   editor = {W K Chan and B Claycomb and H Takakura and J J Yang and Y Teranishi and D Towey and S Segura and H Shahriar and S Reisman and S I Ahamed},
   isbn = {978-1-6654-2463-9},
   issn = {0730-3157},
   journal = {2021 IEEE 45TH ANNUAL COMPUTERS, SOFTWARE, AND APPLICATIONS CONFERENCE
(COMPSAC 2021)},
   keywords = {Requirements modeling; iStar modeling framework; Controlled experiment;
Scalability},
   note = {45th Annual International IEEE-Computer-Society Computers, Software, and
Applications Conference (COMPSAC), ELECTR NETWORK, JUL 12-16, 2021},
   pages = {844-853},
   publisher = {IEEE COMPUTER SOC},
   title = {Graphical Modeling VS. Textual Modeling: An Experimental Comparison
Based on iStar Models},
   year = {2021},
}
@article{Goncalves2019,
   abstract = {iStar is a goal-based requirements modelling language, being used in
both industrial and academic projects of different domains. Often the
language is extended to incorporate new constructs related to a
particular application domain or to adjust it to practical situations
during requirements modelling. Currently, the language is undergoing
standardisation, and several studies have focused on the analysis of
iStar variations to identify similarities and to define a core. This
does not imply or constrain the need for iStar to continue to be
extended. This paper contributes to the understanding of how iStar is
extended by analysing how iStar researchers perform iStar extensions. To
address this question, we followed a qualitative approach based on
interviews involving 20 researchers from different research groups that
proposed iStar extensions. The analysis revealed a good understanding
about what extending a modelling language means and pointed out
differences about how extensions are proposed. We discovered categories
that impact positively on iStar extensions (such as reusing existing
extensions, proposing extensions in abstract and concrete syntaxes, and
creating new modelling tools), and other categories that impact
negatively (such as modifying representations of the original
constructs, proposing extensions in an ad hoc fashion and not carefully
choosing graphical representations). We also evaluated the findings of
interviews through an online survey answered by 30 iStar researchers.
Finally, we proposed a set of guidelines to support the proposal for
better future iStar extensions.},
   author = {Enyo Goncalves and Marcos Antonio de Oliveira and Ingrid Monteiro and Jaelson Castro and Joao Araujo},
   city = {233 SPRING ST, NEW YORK, NY 10013 USA},
   doi = {10.1007/s00766-018-0302-5},
   issn = {0947-3602},
   issue = {1},
   journal = {REQUIREMENTS ENGINEERING},
   keywords = {Goal-based modelling; iStar; Extensions; Qualitative study; Survey},
   month = {3},
   pages = {55-84},
   publisher = {SPRINGER},
   title = {Understanding what is important in iStar extension proposals: the
viewpoint of researchers},
   volume = {24},
   year = {2019},
}
@inproceedings{Chen2019,
   abstract = {iStar framework is an effective means for modeling and analyzing goals
and interactions among social agents. Most of existing iStar modeling
tools build iStar models via a graphical manner, which has a steep
learning curve and suffer from scalability issues. Especially when
dealing with large-scale models in industrial settings, it is very time
consuming to play with the layout of models. In this paper, we present a
tool which allows users to build iStar models from textual descriptions
of systems. And the established models are then visualized with
reasonable layouts. In particular, our tool supports both SD (Strategic
Dependency) view and SR (Strategic Rationale) view of iStar models.},
   author = {Yiwen Chen and Yuanpeng Wang and Yixuan Hou and Yunduo Wang},
   city = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA},
   doi = {10.1109/RE.2019.00069},
   editor = {D Damian and A Perini and S W Lee},
   isbn = {978-1-7281-3912-8},
   issn = {2332-6441},
   journal = {2019 27TH IEEE INTERNATIONAL REQUIREMENTS ENGINEERING CONFERENCE (RE
2019)},
   keywords = {iStar; textual modeling; textual template; prototype},
   note = {IEEE 27th International Requirements Engineering Conference (RE), SOUTH
KOREA, SEP 23-27, 2019},
   pages = {490-491},
   publisher = {IEEE COMPUTER SOC},
   title = {T-Star: A Text-based iStar Modeling Tool},
   year = {2019},
}
@article{Marinos2009,
   abstract = {We propose a framework that can be used to produce functioning web applications from SBVR models. To achieve this, we begin by discussing the concept of declarative application generation and examining the commonalities between SBVR and the RESTful architectural style of the web. We then show how a relational database schema and RESTful interface can be generated from an SBVR model. In this context, we discuss how SBVR can be used to semantically describe hypermedia on the Web and enhance its evolvability and loose coupling properties. Finally, we show that this system is capable of exhibiting process-like behaviour without requiring explicitly defined processes. © 2009 Springer-Verlag Berlin Heidelberg.},
   author = {Alexandros Marinos and Paul Krause},
   doi = {10.1007/978-3-642-04985-9_15/COVER},
   isbn = {3642049842},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Business Rules,Declarative Programming,REST,SBVR,SQL,Web-based Applications},
   pages = {144-158},
   publisher = {Springer, Berlin, Heidelberg},
   title = {An SBVR framework for RESTful web applications},
   volume = {5858 LNCS},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-642-04985-9_15},
   year = {2009},
}
@article{Putrycz2007,
   abstract = {By using several reverse engineering tools and techniques, it is possible to extract business rules from the legacy source code that are easy to understand by the non-IT experts. These business rules can be used at different stages of system modernization. System maintainers can use the rules to locate in the code parts affected by a change in a rule. Business analysts can use those rules as means to aide understanding of the system at a business level. The extracted rules can serve as source of documentation and possible input for configuring a new system. This paper presents a novel approach for extracting business rules from legacy source code and application of the results at different stages of system modernization. © Springer- Verlag Berlin Heidelberg 2007.},
   author = {Erik Putrycz and Anatol W. Kark},
   doi = {10.1007/978-3-540-75975-1_9/COVER},
   isbn = {9783540759744},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Business rules,Information retrieval,Reverse engineering,System modernization},
   pages = {107-118},
   publisher = {Springer Verlag},
   title = {Recovering business rules from legacy source code for system modernization},
   volume = {4824 LNCS},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-540-75975-1_9},
   year = {2007},
}
@article{Kamada2010,
   abstract = {The main source of changing requirements of the dynamic business environment is response to changes in regulations and contracts towards which businesses are obligated to comply. At the same time, many organizations have their business processes specified independently of their business obligations (which include adherence to contracts laws and regulations). Thus, the problem of mapping business changes into computational systems becomes much more complicated. In this paper we address the problem by providing an automated transformation of business rules into a formal language capable of directly mapping onto executable specifications. The model transformation is consistent with MDA/MOF/QVT concepts using ATL to perform the mapping. Business rules are compliant to SBVR metamodel, and are transformed into FCL, a logic based formalism, known to have a direct mapping onto executable specifications. Both, source and target rules are based on principles of deontic logic, the core of which are obligations, permissions and prohibitions. © 2010 Springer-Verlag.},
   author = {Aqueo Kamada and Guido Governatori and Shazia Sadiq},
   doi = {10.1007/978-3-642-16289-3_14/COVER},
   isbn = {3642162886},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Business Contract,Business Rule Transformation,FCL,MDA,SBVR},
   pages = {153-161},
   publisher = {Springer Verlag},
   title = {Transformation of SBVR compliant business rules to executable FCL rules},
   volume = {6403 LNCS},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-642-16289-3_14},
   year = {2010},
}
@article{,
   abstract = {Business rules define and constrain various aspects of the business, such as vocabulary, control-flow and organizational issues. Although the presence of many languages for expressing business rules that differ in expressivenes, knowledge representation mechanism and execution model, none of these cover all the necessary business aspects. In this paper, we show how business rules, not only vocabulary rules, but also control-flow rules and organizational rules can be expressed in SBVR and translated using patterns into a more uniform event mechanism, such that the event handling could provide an integrated enforcement of business rules of many kinds. As a proof of concept a prototype tool integrates this pattern mechanism and provides an execution environment in which these rules are enforced. © 2012 Springer-Verlag.},
   author = {Willem De Roover and Filip Caron and Jan Vanthienen},
   doi = {10.1007/978-3-642-28108-2_43/COVER},
   isbn = {9783642281075},
   issn = {18651348},
   issue = {PART 1},
   journal = {Lecture Notes in Business Information Processing},
   keywords = {SBVR,business processes,business rules,declarative process modeling,event coordination,prototype},
   pages = {446-457},
   publisher = {Springer Verlag},
   title = {A prototype tool for the event-driven enforcement of SBVR business rules},
   volume = {99 LNBIP},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-642-28108-2_43},
   year = {2012},
}
@article{Mishra2015,
   abstract = {Business Process Modeling Notation (BPMN) is a technique for graphically drawing and illustrating business processes in diagramtic form. Semantic of Business Vocabulary and Business Rules (SBVR) is a declarative language used to define business vocabulary, rules and policy. Several times inconsistencies occur between BPMN and SBVR as they are independently maintained. Our aim is to investigate techniques for automatically detecting inconsistencies between business process and rules. We present a method for inconsistency detection (between BPMN and SBVR) based on converting SBVR rules to graphical representation and apply sub graph-isomorphism to detect instances of inconsistencies between BPMN and SBVR models. We propose a multi-step process framework for identification of instances of inconsistencies between the two models. We first generate an XML of BPMN diagram and apply parsing and tag extraction. We then apply Stanford NLP Parser to generate parse tree of rules. The detailed information about the parse tree is stored in the form of Typed Dependency which represent grammatical relation between words of a sentence. We utilize the grammatical relation extract triplet (actor-action-object) of a sentence. We find node-induced sub-graph of all possible length of nodes of a graph and apply VF2 Algorithm to detect instances of inconsistency between sub graphs. Finally, we evaluate the proposed research framework by conducting experiments on synthetic dataset to validate the accuracy and effectiveness of our approach.},
   author = {Akanksha Mishra and Ashish Sureka},
   doi = {10.1007/978-3-319-26832-3_12/TABLES/4},
   isbn = {9783319268316},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Business Process Modeling,Business process intelligence,Business rule modeling,Graph matching algorithms,Inconsistency detection},
   pages = {115-129},
   publisher = {Springer Verlag},
   title = {A graph processing based approach for automatic detection of semantic inconsistency between BPMN process model and SBVR rules},
   volume = {9468},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-319-26832-3_12},
   year = {2015},
}
@article{,
   abstract = {The Semantics of Business Vocabulary and Business Rules (SBVR) is a specification created by the Object Management Group (OMG) to provide a way to semantically describe business concepts and specify business rules. However, reasoning with SBVR is still an open subject, and current efforts to provide reasoning are done through the Web Ontology Language (OWL), by providing a mapping between SBVR and OWL. In this paper we focus on the problem of mapping SBVR vocabulary and rulebook to OWL 2, but unlike previous mappings described in the literature, we provide a novel and unorthodox mapping that allows to describe legal rules which have their own intricate anatomy.},
   author = {Firas Al Khalil and Marcello Ceci and Kosala Yapa and Leona O’Brien},
   doi = {10.1007/978-3-319-42019-6_17/TABLES/1},
   isbn = {9783319420189},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Legal,OWL,Rule,SBVR},
   pages = {258-266},
   publisher = {Springer Verlag},
   title = {SBVR to OWL 2 mapping in the domain of legal rules},
   volume = {9718},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-319-42019-6_17},
   year = {2016},
}
@article{Kluza2016,
   abstract = {The same business concepts can be expressed in various knowledge representations like processes or rules. This paper presents an interoperability solution for transforming a subset of the SBVR rules into the BPMN and DMN models. The translation algorithm describes how to translate the SBVR vocabulary, structural and operational rules into particular BPMN and DMN elements. The result is a combined process and decision model, which can be used for validating SBVR rules by people aware of BPMN and DMN notations.},
   author = {Krzysztof Kluza and Krzysztof Honkisz},
   doi = {10.1007/978-3-319-39384-1_39/FIGURES/3},
   isbn = {9783319393834},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {453-462},
   publisher = {Springer Verlag},
   title = {From SBVR to BPMN and DMN models. Proposal of translation from rules to process and decision models},
   volume = {9693},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-319-39384-1_39},
   year = {2016},
}
@article{Normantas2012,
   abstract = {As software systems evolve, it becomes increasingly complex for maintainers to keep them aligned with rapidly changing business requirements. Therefore the cost of software maintenance often exceeds the cost of its initial development or adaptation. As a result, automated approaches for software comprehension emerge providing valuable improvements and cost-savings for the software maintenance. This paper presents an approach that facilitates software comprehension by enabling traceability of implementation of business rules and business scenarios in the software system. It also describes a case study on application of this approach for comprehension of business logic implemented in the enterprise content management system and reports obtained results. © 2012 Springer-Verlag.},
   author = {Kestutis Normantas and Olegas Vasilecas},
   doi = {10.1007/978-3-642-33308-8_40/COVER},
   isbn = {9783642333071},
   issn = {18650929},
   journal = {Communications in Computer and Information Science},
   keywords = {Architecture-Driven Modernization,Business Knowledge Extraction,Business Rules Discovery,Knowledge Discovery Meta-Model,Model-Driven Reverse Engineering},
   pages = {482-496},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Extracting business rules from existing enterprise software system},
   volume = {319 CCIS},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-642-33308-8_40},
   year = {2012},
}
@article{Hnatkowska2016,
   abstract = {Business rules are very important assets of any enterprise. Very often they are directly coded in existing software systems. As business rules evolve during a time, the software itself becomes the only valuable source of the rules applied. The aim of the paper is to present an approach to automatic business rules extraction from existing system written in C#. Considerations are limited to structural business rules. The proposed approach was implemented in a tool which usefulness was confirmed by examples. In comparison with existing solutions for reverse-engineering it gives better results, characterized by high correctness, and accuracy.},
   author = {Bogumila Hnatkowska and Marcin Ważeliński},
   doi = {10.1007/978-3-662-49381-6_22/TABLES/1},
   isbn = {9783662493809},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Business rules,C#,Extraction,ORM,Reverse engineering,Structural business rules},
   pages = {225-234},
   publisher = {Springer Verlag},
   title = {Extraction of structural business rules from C#},
   volume = {9621},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-662-49381-6_22},
   year = {2016},
}
@article{Moschoyiannis2010,
   abstract = {Declarative technologies have made great strides in expressivity between SQL and SBVR. SBVR models are more expressive that SQL schemas, but not as imminently executable yet. In this paper, we complete the architecture of a system that can execute SBVR models. We do this by describing how SBVR rules can be transformed into SQL DML so that they can be automatically checked against the database using a standard SQL query. In particular, we describe a formalization of the basic structure of an SQL query which includes aggregate functions, arithmetic operations, grouping, and grouping on condition. We do this while staying within a predicate calculus semantics which can be related to the standard SBVR-LF specification and equip it with a concrete semantics for expressing business rules formally. Our approach to transforming SBVR rules into standard SQL queries is thus generic, and the resulting queries can be readily executed on a relational schema generated from the SBVR model. © 2010 Springer-Verlag.},
   author = {Sotiris Moschoyiannis and Alexandros Marinos and Paul Krause},
   doi = {10.1007/978-3-642-16289-3_12/COVER},
   isbn = {3642162886},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Business Rules,Declarative Programming,Formal Semantics,Predicate Calculus,SBVR,SQL},
   pages = {128-143},
   publisher = {Springer Verlag},
   title = {Generating SQL queries from SBVR rules},
   volume = {6403 LNCS},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-642-16289-3_12},
   year = {2010},
}
@article{Cheng2011,
   abstract = {In the current heavily regulated business world, organisations struggle to establish a consistent view of their policies and operating procedures. These are generally captured through process modeling and business rule modeling, resulting in separate representations of rules and actual practice. The separation of the two models leads to increased risk of noncompliance, as well as reduced benefits from process improvement initiatives. So far, little guidance exists for organisations who struggle to consolidate their existing process models and business rules into one holistic model. In this paper we propose a preliminary framework that provides an overarching scope for the consolidation of existing business process models and business rule descriptions. The framework is supported by a collection of mapping methods (currently based on BPMN and SBVR) that assist business modelers in identifying inconsistencies between their existing business process models and business rule repositories, and thus helps to establish a coherent view of organizational policies and procedures. © 2011 Springer-Verlag.},
   author = {Ran Cheng and Shazia Sadiq and Marta Indulska},
   doi = {10.1007/978-3-642-21863-7_2/COVER},
   isbn = {9783642218293},
   issn = {18651348},
   journal = {Lecture Notes in Business Information Processing},
   keywords = {BPMN,SBVR,business process,business rule,integration},
   pages = {13-24},
   publisher = {Springer Verlag},
   title = {Framework for business process and rule integration: A case of BPMN and SBVR},
   volume = {87 LNBIP},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-642-21863-7_2},
   year = {2011},
}
@article{,
   abstract = {This paper presents an original use of SBVR to help building a set of business rules out of regulatory documents. The formalization is analyzed as a three-step process, in which SBVR-SE stands in an intermediate position between the Natural Language on the one hand and the formal language on the other hand. The rules are extracted, clarified and simplified at the general regulatory level (expert task) before being refined according to the business application (engineer task). A methodology for these first two steps is described, with different operations composing each step. It is illustrated with examples from the literature and from the Ontorule use cases. © 2013 Springer-Verlag Berlin Heidelberg.},
   author = {François Lévy and Adeline Nazarenko},
   doi = {10.1007/978-3-642-39617-5_5/COVER},
   isbn = {9783642396168},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   month = {1},
   pages = {19-33},
   publisher = {Springer Verlag},
   title = {Formalization of natural language regulations through sbvr structured english (tutorial)},
   volume = {8035 LNCS},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-642-39617-5_5},
   year = {2013},
}
@article{,
   abstract = {Combination capabilities of BPMN and SBVR are analyzed in this paper. In order to combine these two standards we have to analyze current proposals. Process modeling focuses on visualization of process with specific notation. However, today there is a need to have business rules separately from process in order to reduce the size of the process model and avoid misunderstandings and miscommunications between analysts and domain experts or between organizations. Therefore, there is a need to combine business process management and business rule management in one user-friendly environment. © Springer-Verlag Berlin Heidelberg 2013.},
   author = {Egle Mickevičiute and Rimantas Butleris},
   doi = {10.1007/978-3-642-41947-8_11/COVER},
   isbn = {9783642419461},
   issn = {18650929},
   journal = {Communications in Computer and Information Science},
   keywords = {BPMN,SBVR,business process,business rule,integration,transformation},
   pages = {114-121},
   publisher = {Springer Verlag},
   title = {Towards the Combination of BPMN Process Models with SBVR Business Vocabularies and Rules},
   volume = {403},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-642-41947-8_11},
   year = {2013},
}
@article{,
   abstract = {SBVR is becoming more and more popular as the metamodel for defining vocabulary based business rules. In an extended form SBVR can be used to declare a whole spectrum of business rules including control-flow and organizational rules. Enforcing the rules of the business in information systems is however not straightforward. SBVR leaves open the gap between defining business rules and actually enforcing them. In this paper, we examine if and how business rules can be expressed in SBVR and translated using patterns into a more uniform event mechanism, such that the event handling could provide an integrated enforcement of the defined business rules. © 2011 Springer-Verlag.},
   author = {Willem De Roover and Jan Vanthienen},
   doi = {10.1007/978-3-642-22760-8_19/COVER},
   isbn = {9783642227592},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {SBVR,business processes,business rules,declarative process modeling,event coordination},
   pages = {172-179},
   publisher = {Springer, Berlin, Heidelberg},
   title = {A transformation from SBVR business rules into event coordinated rules by means of SBVR patterns},
   volume = {6569 LNCS},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-642-22760-8_19},
   year = {2011},
}
@article{,
   abstract = {This paper addresses the problem of extracting formal statements, in the form of business rules, from free text descriptions of financial products or services. This automatic process is integrated in the banking software factory, permitting business analysts the formal specification, direct implementation and fast deployment of new products. This system is fully integrated with the typical software methodologies and architectures used in the banking industry for conventional development of backoffice or online applications. © 2008 Springer-Verlag Berlin Heidelberg.},
   author = {José L. Martínez-Fernández and José C. González and Julio Villena and Paloma Martínez},
   doi = {10.1007/978-3-540-69858-6_29/COVER},
   isbn = {3540698574},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Banking industry,Business rules,Financial ontologies,Natural language processing},
   pages = {299-310},
   publisher = {Springer, Berlin, Heidelberg},
   title = {A preliminary approach to the automatic extraction of business rules from unrestricted text in the banking industry},
   volume = {5039 LNCS},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-540-69858-6_29},
   year = {2008},
}
@article{Mickeviciute2014,
   abstract = {Information system modelling starts from business vocabulary, the second step is business process modelling, which is closely related with modelling of business rules. Modelling of business rules and business processes should be considered as the complementary approaches based on business vocabulary. However, business vocabularies still are not used in CASE tools. The goal of the paper is to present the principles how the business vocabulary can be used for modelling business processes and business rules in such a way that it would be possible to transform business process models to business rules for validating business process by domain experts. In order to achieve this goal we present requirements for defining business processes in line with business vocabularies, which allow transforming business process models to business rules without linguistic processing techniques.},
   author = {Egle Mickeviciute and Lina Nemuraite and Rimantas Butleris},
   doi = {10.1007/978-3-319-11460-6_10/FIGURES/5},
   issn = {18651348},
   journal = {Lecture Notes in Business Information Processing},
   keywords = {BPMN,Business process model,Business rule,Business vocabulary,SBVR,Transformation},
   pages = {105-116},
   publisher = {Springer Verlag},
   title = {Applying SBVR business vocabulary and business rules for creating BPMN process models},
   volume = {183},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-319-11460-6_10},
   year = {2014},
}
@article{Skersys2014,
   abstract = {The main purpose of this paper is to explore the possibilities to extract well-structured business vocabularies and rules from the formalized requirements specifications expressed via use case diagrams; Object Management Group's (OMG) standards, namely Semantics of Business Vocabularies and Business Rules (SBVR) and Unified Modeling Language (UML), are used for this purpose. The paper concentrates on a semi-automatic extraction approach by proposing UML2SBVR mapping matrix, extraction algorithm and implementation prototype. An experiment and the evaluation of its results are discussed to prove the usability of the presented approach. © Springer International Publishing Switzerland 2014.},
   author = {Tomas Skersys and Paulius Danenas and Rimantas Butleris},
   doi = {10.1007/978-3-319-06505-2_13/COVER},
   isbn = {9783319065045},
   issn = {18651348},
   journal = {Lecture Notes in Business Information Processing},
   keywords = {Model extraction,Model-to-model transformation,SBVR business vocabulary and business rules,UML use case diagram},
   pages = {182-196},
   publisher = {Springer Verlag},
   title = {Approach for semi-automatic extraction of business vocabularies and rules from use case diagrams},
   volume = {174 LNBIP},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-319-06505-2_13},
   year = {2014},
}
@article{Bernotaityte2013,
   abstract = {Semantics of Business Vocabulary and Business Rules (SBVR) is OMG adopted metamodel allowing defining noun concepts, verb concepts and business rules of a problem domain in structured natural language based on formal logics. SBVR business vocabulary and business rules are capable of representing ontologies. There are some research works devoted to transforming SBVR into Web Ontology Language OWL2. The reverse way of representing ontology concepts with SBVR structured language was not investigated though there are much more ontologies than SBVR vocabularies. Our research is concentrated on methodology for creating SBVR vocabularies and rules from OWL2 ontologies without a loss of the expressive power, characteristic for ontologies, as some ontology-specific concepts have no direct representation in SBVR. The particular attention is devoted to applying SBVR vocabulary in semantic search. © Springer-Verlag Berlin Heidelberg 2013.},
   author = {Gintare Bernotaityte and Lina Nemuraite and Rita Butkiene and Bronius Paradauskas},
   doi = {10.1007/978-3-642-41947-8_13/COVER},
   isbn = {9783642419461},
   issn = {18650929},
   journal = {Communications in Computer and Information Science},
   keywords = {OWL 2,SBVR,business rules,business vocabulary,domain ontology,lexical ontology},
   pages = {134-145},
   publisher = {Springer Verlag},
   title = {Developing SBVR vocabularies and business rules from OWL2 ontologies},
   volume = {403},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-642-41947-8_13},
   year = {2013},
}
@article{Haj2019,
   abstract = {In the early phases of software development, both of business analysts and IT architects collaborate to define the business needs in a consistent and unambiguous format before exploiting them to produce a software solution to the problem have been defined. Given the divergence of the interest areas of each intervenor, the natural language remains the most adequate format to define the business needs in order to avoid misunderstanding. This informal support suffers from ambiguity leading to inconsistencies, which will affect the reliability of the final solution. Accordingly, the Object Management Group (OMG) has proposed the “Semantic Business Vocabulary and Rules” (SBVR) standard which offers the opportunity to gather business rules in a natural language format having a formal logic aspect, letting the possibility to be understood by not only the different stakeholders but also directly processed by the machine. Since the SBVR standard is born to represent business rules by combining business vocabulary, it would be wise to give a great attention to the latter. In this paper we present an approach to extract business vocabulary according to SBVR Structured English as one of possibly notation that can map to the SBVR Meta-Model, with a view to provide a relevant resource for the next software deployment steps.},
   author = {Abdellatif Haj and Youssef Balouki and Taoufiq Gadi},
   doi = {10.1007/978-3-030-11914-0_19/COVER},
   issn = {23673389},
   journal = {Lecture Notes in Networks and Systems},
   keywords = {Business rules,Business vocabulary,Natural Language Processing,SBVR,SBVR structured English,Semantic of Business Vocabulary and Rule},
   pages = {170-182},
   publisher = {Springer},
   title = {Automatic extraction of SBVR based business vocabulary from natural language business rules},
   volume = {66},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-030-11914-0_19},
   year = {2019},
}
@article{Jesus2013,
   author = {Jandisson Soares de Jesus},
   city = {São Paulo},
   doi = {10.11606/D.45.2013.TDE-22012014-144303},
   institution = {Universidade de São Paulo},
   month = {11},
   title = {Um método para a implementação de regras de negócio à partir da semântica SBVR},
   url = {http://www.teses.usp.br/teses/disponiveis/45/45134/tde-22012014-144303/},
   year = {2013},
}
@inproceedings{Abdelmoez2012,
   abstract = {Software architecture is a key discipline in software engineering as it performs a central role in many modern software development paradigms. For an evolving complex architecture, assessing the change impact for the components considering all maintenance scenarios is a difficult problem. In this paper, we present a methodology to conduct sensitivity analysis...},
   author = {W. Abdelmoez and M. Ibrahim and M. A. Omar and H. H. Ammar},
   journal = {2012 8th International Conference on Informatics and Systems (INFOS)},
   publisher = {IEEE},
   title = {Sensitivity analysis of maintainability-based risk factors for software architectures},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/6236577/},
   year = {2012},
}
@article{Rajapakse2022,
   abstract = {Context: DevOps (Development and Operations) has become one of the fastest-growing software development paradigms in the industry. However, this trend has presented the challenge of ensuring secure software delivery while maintaining the agility of DevOps. The efforts to integrate security in DevOps have resulted in the DevSecOps paradigm, which is gaining significant interest from both industry and academia. However, the adoption of DevSecOps in practice is proving to be a challenge. Objective: This study aims to systemize the knowledge about the challenges faced by practitioners when adopting DevSecOps and the proposed solutions reported in the literature. We also aim to identify the areas that need further research in the future. Method: We conducted a Systematic Literature Review of 54 peer-reviewed studies. The thematic analysis method was applied to analyze the extracted data. Results: We identified 21 challenges related to adopting DevSecOps, 31 specific solutions, and the mapping between these findings. We also determined key gap areas in this domain by holistically evaluating the available solutions against the challenges. The results of the study were classified into four themes: People, Practices, Tools, and Infrastructure. Our findings demonstrate that tool-related challenges and solutions were the most frequently reported, driven by the need for automation in this paradigm. Shift-left security and continuous security assessment were two key practices recommended for DevSecOps. People-related factors were considered critical for successful DevSecOps adoption but less studied. Conclusions: We highlight the need for developer-centered application security testing tools that target the continuous practices in DevSecOps. More research is needed on how the traditionally manual security practices can be automated to suit rapid software deployment cycles. Finally, achieving a suitable balance between the speed of delivery and security is a significant issue practitioners face in the DevSecOps paradigm.},
   author = {Roshan N. Rajapakse and Mansooreh Zahedi and M. Ali Babar and Haifeng Shen},
   doi = {10.1016/J.INFSOF.2021.106700},
   issn = {09505849},
   journal = {Information and Software Technology},
   keywords = {Continuous Software Engineering,DevOps,DevSecOps,Security,Systematic Literature Review},
   month = {1},
   publisher = {Elsevier B.V.},
   title = {Challenges and solutions when adopting DevSecOps: A systematic review},
   volume = {141},
   year = {2022},
}
@article{Rubert2022,
   abstract = {Continuous delivery has been adopted by organizations to make software available to their users at any time. The transition from traditional software delivery methodologies to continuous delivery can impact on the results generated by organizations, e.g., the quality of source code and products. Although widely adopted, little is known about its effects. To account for this, this article reports a case study on the effects of continuous delivery on the quality of source code and products produced. Our case study was carried out for 12 months within a software development company in Brazil. Our findings indicate that the adoption of continuous delivery practices improved the quality of delivered products, mainly considering the number of defects reported by customers, the number of demands delivered per month, and user satisfaction. However, the adoption of continuous delivery did not favor the quality of source code, including the number of bugs, security vulnerabilities, code smells, duplicated code, and code complexity. Researchers and practitioners may benefit from our findings typically when delivering software products, designing and seeking to improve deployment pipeline practices. Finally, our study draws up some implications and shows the potential of adopting continuous delivery for developing enterprise applications that are constantly evolving.},
   author = {Maluane Rubert and Kleinner Farias},
   doi = {10.1016/J.CSI.2021.103588},
   issn = {09205489},
   journal = {Computer Standards and Interfaces},
   keywords = {Continuous delivery,Enterprise resource planning,Software development,Software engineering},
   month = {4},
   publisher = {Elsevier B.V.},
   title = {On the effects of continuous delivery on code quality: A case study in industry},
   volume = {81},
   year = {2022},
}
@article{Deiters2009,
   abstract = {Modern enterprise application systems are parts of complex IT landscapes. The architecture of such a landscape may impose constraints upon the design of single applications, for example by the mandatory use of enterprise-wide reference architectures. It is of great importance for the sake of smooth operation and easy maintaining that single applications are com-pliant to the reference architectures. Checking this compliance is highly important for the architecture management to assure the quality of application systems. Unfortunately, current tool support is not flexible enough to easily check different aspects of architectural compliance. This paper proposes a rule-based approach based upon logic programming concepts towards a formalism for architectural compliance checking. In this approach, the architecture and design are represented as logical knowledge base that can be queried for architectural compliance. Furthermore, the paper presents a case study, in which the approach was prototypically implemented and applied in an industrial context. © 2009 IEEE.},
   author = {Constanze Deiters and Patrick Dohrmann and Sebastian Herold and Andreas Rausch},
   doi = {10.1109/EDOC.2009.15},
   isbn = {9780769537856},
   journal = {Proceedings - 13th IEEE International Enterprise Distributed Object Computing Conference, EDOC 2009},
   keywords = {Architectural compliance,Architectural rules,Enterprise architecture management,Software architecture},
   pages = {183-192},
   title = {Rule-based architectural compliance checks for enterprise architecture management},
   year = {2009},
}
@inproceedings{Gerdes2018,
   abstract = {For software architects involved in maintaining existing software systems, understanding the architecture is a crucial task. They have to comprehend the evolution of a system and the rationale behind design decisions to avoid architecture erosion. This is challenging due to frequently outdated documentation and incomplete knowledge about the sy...},
   author = {Sebastian Gerdes and Tobias Fechner and Matthias Riebisch},
   journal = {2018 25th Australasian Software Engineering Conference (ASWEC)},
   publisher = {IEEE},
   title = {Identification of Technology Features to Understand and Maintain Software Architectures},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/8587306/},
   year = {2018},
}
@inproceedings{Pruijt2013,
   abstract = {Architecture Compliance Checking (ACC) is an approach to verify the conformance of implemented program code to high-level models of architectural design. ACC is used to prevent architectural erosion during the development and evolution of a software system. Static ACC, based on static software analysis techniques, focuses on the modular architecture and especially on ...},
   author = {Leo Pruijt and Christian Köppe and Sjaak Brinkkemper},
   journal = {2013 IEEE International Conference on Software Maintenance},
   publisher = {IEEE},
   title = {Architecture Compliance Checking of Semantically Rich Modular Architectures: A Comparative Study of Tool Support},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/6676893/},
   year = {2013},
}
@inproceedings{Lilienthal2009,
   abstract = {Today companies apply software systems with more than 14 million Lines of Code (LOC), which are still maintainable and expandable in spite of their size. On the other hand, you will find smaller software systems (> 200,000 LOC), which can either only be adapted with considerable effort, or which cannot be adapted at all. Why are some software systems fraught wit...},
   author = {Carola Lilienthal},
   journal = {2009 13th European Conference on Software Maintenance and Reengineering},
   publisher = {IEEE},
   title = {Architectural Complexity of Large-Scale Software Systems},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/4812735/},
   year = {2009},
}
@inproceedings{Ding2001,
   abstract = {During the past decade (1991-2001), object-orientation (OO) has become the dominant software development methodology, accompanied by a number of modeling notations, programming languages, and development environments. OO applications of today are increasingly complex and user driven. They are also developed more rapidly and evolved more frequently than was the case with software system...},
   author = {Lei Ding and N. Medvidovic},
   journal = {Proceedings Working IEEE/IFIP Conference on Software Architecture},
   publisher = {IEEE},
   title = {Focus: a light-weight, incremental approach to software architecture recovery and evolution},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/948429/},
   year = {2001},
}
@article{Besker2018,
   abstract = {Large Software Companies need to support the continuous and fast delivery of customer value in both the short and long term. However, this can be impeded if the evolution and maintenance of existing systems is hampered by what has been recently termed Technical Debt (TD). Specifically, Architectural TD has received increased attention in the last few years due to its significant impact on system success and, left unchecked, it can cause expensive repercussions. It is therefore important to understand the underlying factors of architectural TD. With this as background, there is a need for a descriptive model to illustrate and explain different architectural TD issues. The aim of this study is to synthesize and compile research efforts with the goal of creating new knowledge with a specific interest in the architectural TD field. The contribution of this paper is the presentation of a novel descriptive model, providing a comprehensive interpretation of the architectural TD phenomenon. This model categorizes the main characteristics of architectural TD and reveals their relations. The results show that, by using this model, different stakeholders could increase the system's success rate, and lower the rate of negative consequences, by raising awareness about architectural TD.},
   author = {Terese Besker and Antonio Martini and Jan Bosch},
   doi = {10.1016/J.JSS.2017.09.025},
   issn = {0164-1212},
   journal = {Journal of Systems and Software},
   keywords = {Architectural technical debt,Software architecture,Software maintenance,Systematic literature review},
   month = {1},
   pages = {1-16},
   publisher = {Elsevier},
   title = {Managing architectural technical debt: A unified model and systematic literature review},
   volume = {135},
   year = {2018},
}
@inproceedings{Martini2015,
   abstract = {Architectural Technical Debt is a metaphor for representing sub-optimal architectural solutions that might cause an interest, in terms of effort or quality, to be paid by the organization in the long run. Such metaphor has been regarded as useful for communicating risks of suboptimal solutions between technical and non-technical stakeholders. However, it's fundamental to un...},
   author = {Antonio Martini and Jan Bosch},
   journal = {2015 41st Euromicro Conference on Software Engineering and Advanced Applications},
   publisher = {IEEE},
   title = {Towards Prioritizing Architecture Technical Debt: Information Needs of Architects and Product Owners},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/7302484/},
   year = {2015},
}
@inproceedings{Kazman2015,
   abstract = {Our recent research has shown that, in large-scale software systems, defective files seldom exist alone. They are usually architecturally connected, and their architectural structures exhibit significant design flaws which propagate bugginess among files. We call these flawed structures the architecture roots, a type of technical debt that incurs high maintenance penalties....},
   author = {Rick Kazman and Yuanfang Cai and Ran Mo and Qiong Feng and Lu Xiao and Serge Haziyev and Volodymyr Fedak and Andriy Shapochka},
   journal = {2015 IEEE/ACM 37th IEEE International Conference on Software Engineering},
   publisher = {IEEE},
   title = {A Case Study in Locating the Architectural Roots of Technical Debt},
   volume = {2},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/7202962/},
   year = {2015},
}
@article{,
   abstract = {Background. Architectural smells and code smells are symptoms of bad code or design that can cause different quality problems, such as faults, technical debt, or difficulties with maintenance and evolution. Some studies show that code smells and architectural smells often appear together in the same file. The correlation between code smells and architectural smells, however, is not clear yet; some studies on a limited set of projects have claimed that architectural smells can be derived from code smells, while other studies claim the opposite. Objective. The goal of this work is to understand whether architectural smells are independent from code smells or can be derived from a code smell or from one category of them. Method. We conducted a case study analyzing the correlations among 19 code smells, six categories of code smells, and four architectural smells. Results. The results show that architectural smells are correlated with code smells only in a very low number of occurrences and therefore cannot be derived from code smells. Conclusion. Architectural smells are independent from code smells, and therefore deserve special attention by researchers, who should investigate their actual harmfulness, and practitioners, who should consider whether and when to remove them.},
   author = {Francesca Arcelli Fontana and Valentina Lenarduzzi and Riccardo Roveda and Davide Taibi},
   doi = {10.1016/J.JSS.2019.04.066},
   issn = {01641212},
   journal = {Journal of Systems and Software},
   keywords = {Architectural smells,Code smells,Empirical analysis,Technical debt},
   month = {8},
   pages = {139-156},
   publisher = {Elsevier Inc.},
   title = {Are architectural smells independent from code smells? An empirical study},
   volume = {154},
   year = {2019},
}
@article{Pigazzini2021,
   abstract = {Design patterns are recommended solutions for typical software design problems, with an extensively studied and documented impact on various quality factors. Flaws in design at a higher levels of abstraction are manifested in architectural smells. Some of those smells, similarly to code smells, can reduce the expected advantages of design patterns or even prevent their proper implementation. In this paper we study if and how design patterns and architectural smells are related, and how this knowledge could be exploited in practice. We present an empirical study with an analysis of 16 design patterns and 3 architectural smells in 60 open source Java systems. We analyze their diffuseness and correlation, and we extract association rules that describe their presence and dependencies. We demonstrate that there exist relationships between architectural smells and design patterns, both at the class and package levels. Some smells appear falsely positive, as they result from conscious decisions made by programmers, while the application of some patterns can be a cause of certain smells. Our results provide evidence that design patterns and architectural smells are related and affect each other. With knowledge about the relationships, programmers can avoid the side effects of applying some design patterns.},
   author = {Ilaria Pigazzini and Francesca Arcelli Fontana and Bartosz Walter},
   doi = {10.1016/J.JSS.2021.110984},
   issn = {01641212},
   journal = {Journal of Systems and Software},
   keywords = {Architectural smells,Architectural smells and design patterns relationships,Design patterns},
   month = {8},
   publisher = {Elsevier Inc.},
   title = {A study on correlations between architectural smells and design patterns},
   volume = {178},
   year = {2021},
}
@article{,
   abstract = {Design erosion is a common problem in software engineering. We have found that invariably, no matter how ambitious the intentions of the designers were, software designs tend to erode over time to the point that redesigning from scratch becomes a viable alternative compared to prolonging the life of the existing design. In this paper, we illustrate how design erosion works by presenting the evolution of the design of a small software system. In our analysis of this example, we show how design decisions accumulate and become invalid because of new requirements. Also it is argued that even an optimal strategy for designing the system (i.e. no compromises with respect to e.g. cost are made) does not lead to an optimal design because of unforeseen requirement changes that invalidate design decisions that were once optimal. © 2002 Elsevier Science Inc. All rights reserved.},
   author = {Jilles Van Gurp and Jan Bosch},
   doi = {10.1016/S0164-1212(01)00152-2},
   issn = {0164-1212},
   issue = {2},
   journal = {Journal of Systems and Software},
   month = {3},
   pages = {105-119},
   publisher = {Elsevier},
   title = {Design erosion: problems and causes},
   volume = {61},
   year = {2002},
}
@inproceedings{Lindvall2002,
   abstract = {Software systems undergo constant change causing the architecture of the system to degenerate over time. Redirecting development effort toward reversing system degeneration takes extra effort and delays the release of the next version. The value of an improved architecture is clear to technical staff, but it is often difficult to convince upper management that the extra eff...},
   author = {M. Lindvall and R. Tesoriero and P. Costa},
   journal = {Proceedings Eighth IEEE Symposium on Software Metrics},
   publisher = {IEEE},
   title = {Avoiding architectural degeneration: an evaluation process for software architecture},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/1011327/},
   year = {2002},
}
@inproceedings{Li2011,
   abstract = {It is considered that software architecture usually degrades as the system evolves. This phenomenon is termed as architectural degeneration. This paper describes a defect perspective for measuring architectural degeneration. Several metrics are defined for the measurement performed on two versions of a commercial compiler system. The main results are: (1) ...},
   author = {Zude Li and Jun Long},
   journal = {2011 18th Asia-Pacific Software Engineering Conference},
   publisher = {IEEE},
   title = {A Case Study of Measuring Degeneration of Software Architectures from a Defect Perspective},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/6130693/},
   year = {2011},
}
@article{Hochstein2005,
   abstract = {As software systems evolve over time, they invariably undergo changes that can lead to a degeneration of the architecture. Left unchecked, degeneration may reach a level where a complete redesign is necessary, a task that requires significant effort. In this paper, we present a survey of technologies developed by researchers that can be used to combat degeneration, that is, technologies that can be employed in identifying, treating and researching degeneration. We also discuss the various causes of degeneration and how it can be prevented. © 2004 Elsevier B.V. All rights reserved.},
   author = {Lorin Hochstein and Mikael Lindvall},
   doi = {10.1016/J.INFSOF.2004.11.005},
   issn = {0950-5849},
   issue = {10},
   journal = {Information and Software Technology},
   keywords = {Architecture compliance,Architecture recovery,Software architecture},
   month = {7},
   pages = {643-656},
   publisher = {Elsevier},
   title = {Combating architectural degeneration: a survey},
   volume = {47},
   year = {2005},
}
@article{Halkidis2008,
   abstract = {The importance of software security has been profound, since most attacks to software systems are based on vulnerabilities caused by poorly designed and developed software. Furthermore, the enforcement of security in software systems at the design phase can reduce the high cost and effort associated with the introduction of security during implementation. For this purpose, security patterns that offer security at the architectural level have been proposed in analogy to the well-known design patterns. The main goal of this paper is to perform risk analysis of software systems based on the security patterns that they contain. The first step is to determine to what extent specific security patterns shield from known attacks. This information is fed to a mathematical model based on the fuzzy-set theory and fuzzy fault trees in order to compute the risk for each category of attacks. The whole process has been automated using a methodology that extracts the risk of a software system by reading the class diagram of the system under study. © 2008 IEEE.},
   author = {Spyros T. Halkidis and Nikolaos Tsantalis and Alexander Chatzigeorgiou and George Stephanides},
   doi = {10.1109/TDSC.2007.70240},
   issn = {15455971},
   issue = {3},
   journal = {IEEE Transactions on Dependable and Secure Computing},
   keywords = {Dependability analysis,Design patterns,Fuzzy risk analysis,Security architecture,Security patterns,Software architecture,Software security},
   pages = {129-142},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Architectural risk analysis of software systems based on security patterns},
   volume = {5},
   year = {2008},
}
@article{Ionita2004,
   abstract = {We present a quantitative method for scenariodriven value, risk, and cost analysis when proposing new system architectures for innovation projects. The method helps to articulate the relative benefits and/or disadvantages of the proposed set of scenarios in the early architecting phases of a new system. It provides the arguments on which to base an informed decision to select the final architectural scenarios for further consideration in the design. In this paper we present a case study in which we applied the proposed method.},
   author = {Mugurel T. Ionita and Pierre America and Dieter K. Hammer and Henk Obbink and Jos J.M. Trienekens},
   doi = {10.1109/WICSA.2004.1310709},
   isbn = {076952172X},
   journal = {Proceedings - Fourth Working IEEE/IFIP Conference on Software Architecture (WICSA 2004)},
   pages = {277-280},
   title = {A scenario-driven approach for value, risk, and cost analysis in system architecting for innovation},
   year = {2004},
}
@article{Iyapparaja2012,
   abstract = {Software development has nowadays evolved into a extreme change that uses the best modules being run in various closed and open source software. Extracting the best component and fit them into an ongoing component based development software poses great challenges to run the software error free with desired outcomes. In this proposal we have introduced an 'Intelligent Risk Analysis Model (IRAM) for reusability in developing component service oriented software, which are developed using eminent object oriented programming language paradigm Java. This model performs surfing operation into Java object oriented programming modules warehouse, which consists of several availability modules of various projects, and gives a list of suitable module. Those modules are tested on cohesion /coupling testing analysis to determine the individual strength and binding capacity of modules before performing Regression test. Final desirable outcomes are tested on performing regression test while integrating successful level module with domain being developed and proper deployment is made on successful expected outcome of the product. Risk analysis of software project is analyzed for transformation of reusability components risk before transformation Analysis, adaptable risk Analysis, and reusability risk Analysis for package in Java programming. In this paper IRAM model to focus Java package the domain and size of package, integration and dependency relationship of package, measuring the coupling and cohesion in risk metric for development and implementation of reusability package without any risk factor.},
   author = {M. Iyapparaja and S. Sureshkumar},
   doi = {10.1049/CP.2012.2189},
   isbn = {9781849197977},
   issue = {624 CP},
   journal = {IET Conference Publications},
   keywords = {Cohesion & coupling,IRAM model,Packages,Risk analysis,Software reusability},
   pages = {52-57},
   title = {Coupling and cohesion metrics in Java for adaptive reusability risk reduction},
   volume = {2012},
   year = {2012},
}
@article{Zhang2006,
   abstract = {This paper presents an automated software tool SQUARE (Software QUality and ARchitecture modelling Environment). It is designed and implemented to support the analysis of software quality from software architectural designs. The tool is based on a model-based method and follows a structured process to systematically derive quality models from software architectural designs by adapting and applying the principles of system hazard analysis. Through identification of potential quality hazards and their consequences, the quality related properties of the components and connectors and the causal relationships between them are derived and then translated into a quality model represented in a graphical notation. The tool enables automated analysis of the quality models in the graphical notation to recognize a number of types of software quality features including quality sensitive components, quality risks and quality trade-off points, etc. A case study with a real e-commerce system is also reported. © 2006 IEEE.},
   author = {Qian Zhang and Jian Wu and Hong Zhu},
   doi = {10.1109/COMPSAC.2006.82},
   isbn = {0769526551},
   issn = {07303157},
   journal = {Proceedings - International Computer Software and Applications Conference},
   keywords = {Analysis of software architecture,Automated software tools,Software architecture design,Software quality models},
   pages = {121-128},
   title = {Tool support to model-based quality analysis of software architecture},
   volume = {1},
   year = {2006},
}
@article{Yacoub2002,
   abstract = {Risk assessment is an essential process of every software risk management plan. Several risk assessment techniques are based on the subjective judgement of domain experts. Subjective risk assessment techniques are human intensive and error-prone. Risk assessment should be based on product attributes that we can quantitatively measure using product metrics. This paper presents a methodology for reliability risk assessment at the early stages of the development lifecycle, namely, the architecture level. We describe a heuristic risk assessment methodology that is based on dynamic metrics. The methodology uses dynamic complexity and dynamic coupling metrics to define complexity factors for the architecture elements (components and connectors). Severity analysis is performed using Failure Mode and Effect Analysis (FMEA) as applied to architecture models. We combine severity and complexity factors to develop heuristic risk factors for the architecture components and connectors. Based on analysis scenarios, we develop a risk assessment model that represents components, connectors, component risk factors, connector risk factors, and probabilities of component interactions. We also develop a risk analysis algorithm that aggregates risk factors of components and connectors to the architectural level. Using the risk aggregation and the risk analysis model, we show how to analyze the overall risk factor of the architecture as the function of the risk factors of its constituting components and connectors. A case study of a pacemaker architecture is used to illustrate the application of the methodology. The methodology is used to identify critical components and connectors and to investigate the sensitivity of the architecture risk factor to changes in the heuristic risk factors of the architecture elements.},
   author = {Sherif M. Yacoub and Hany H. Ammar},
   doi = {10.1109/TSE.2002.1010058},
   issn = {00985589},
   issue = {6},
   journal = {IEEE Transactions on Software Engineering},
   keywords = {Component-dependency graphs,Dynamic metrics,Reliability risk analysis,Risk assessment,Risk modeling,Severity measures,Software architecture},
   month = {6},
   pages = {529-547},
   title = {A methodology for architecture-level reliability risk analysis},
   volume = {28},
   year = {2002},
}
@misc{,
   abstract = {Title from content provider.},
   isbn = {978-1-5044-5904-4},
   journal = {ISO/IEC/IEEE 42030:2019(E)},
   publisher = {IEEE},
   title = {42030-2019 - ISO/IEC/IEEE International Standard - Software, systems and enterprise -- Architecture evaluation framework},
   year = {2019},
}
@article{,
   abstract = {Resilience benchmarking aims to assess a software system's and an organization's ability to cope with failures, e.g., by injecting faults and observing their effects in both, testing and production environments. However, existing resilience benchmarks are ad-hoc and based on randomly injected faults. In this paper, we give an overview of the vision and the current state of our ORCAS approach for a more efficient resilience benchmarking for microservice architectures. ORCAS leverages the following characteristics: i) relationship between resilience patterns, antipatterns, and fault injections; ii) automatically extracted architectural knowledge to generate and refine resilience benchmarks; iii) use of simulations to further reduce the number of benchmarks to execute in testing and production systems.},
   author = {Andre Van Hoorn and Aldeida Aleti and Thomas F. Dullmann and Teerat Pitakrat},
   doi = {10.1109/ISSREW.2018.00-10},
   isbn = {9781538694435},
   journal = {Proceedings - 29th IEEE International Symposium on Software Reliability Engineering Workshops, ISSREW 2018},
   keywords = {chaos engineering,fault injection,microservices,resilience benchmarking},
   month = {11},
   pages = {146-147},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {ORCAS: Efficient Resilience Benchmarking of Microservice Architectures},
   year = {2018},
}
@article{Strasser2014,
   abstract = {Most automobile manufacturers maintain many vehicle types to keep a successful position on the market. Through the further development all vehicle types gain a diverse amount of new functionality. Additional features have to be supported by the car's software. For time efficient accomplishment, usually the existing electronic control unit (ECU) code is extended. In the majority of cases this evolutionary development process is accompanied by a constant decay of the software architecture. This effect known as software erosion leads to an increasing deviation from the requirements specifications. To counteract the erosion it is necessary to continuously restore the architecture in respect of the specification. Automobile manufacturers cope with the erosion of their ECU software with varying degree of success. Successfully we applied a methodical and structured approach of architecture restoration in the specific case of the brake servo unit (BSU). Software product lines from existing BSU variants were extracted by explicit projection of the architecture variability and decomposition of the original architecture. After initial application, this approach was capable to restore the BSU architecture recurrently. © 2014 Springer International Publishing Switzerland.},
   author = {Arthur Strasser and Benjamin Cool and Christoph Gernert and Christoph Knieke and Marco Körner and Dirk Niebuhr and Henrik Peters and Andreas Rausch and Oliver Brox and Stefanie Jauns-Seyfried and Hanno Jelden and Stefan Klie and Michael Krämer},
   doi = {10.1007/978-3-319-04298-5_43/COVER/},
   isbn = {9783319042978},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Architecture design,Automotive,Engineering methodologies,Model driven development,Reuse,Software erosion,Software product lines},
   pages = {491-502},
   publisher = {Springer Verlag},
   title = {Mastering erosion of software architecture in automotive software product lines},
   volume = {8327 LNCS},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-319-04298-5_43},
   year = {2014},
}
@article{Herold2014,
   abstract = {Keeping the software architecture of a system and its implementation consistent can be tough. The larger and more complex a software system is, the more likely software architecture erosion occurs. This effect can lead to a decrease of quality with respect to adaptability, maintainability, or reusability. Refactorings can help to reverse software architecture erosion through systematically applying them to resolve architecture violations. However, it can be difficult in complex systems to manually resolve all violations in an efficient way due to the complex interdependencies between them. In this paper, we propose a new approach to the automatic recommendation of refactorings to resolve architecture violations based on a meta-heuristic search for an efficient set of refactorings. The approach is applied to resolve architectural dependency violations using the "move class" refactoring. © 2014 Springer International Publishing Switzerland.},
   author = {Sebastian Herold and Matthias Mair},
   doi = {10.1007/978-3-319-09970-5_33/COVER/},
   isbn = {9783319099699},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {390-397},
   publisher = {Springer Verlag},
   title = {Recommending refactorings to re-establish architectural consistency},
   volume = {8627 LNCS},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-319-09970-5_33},
   year = {2014},
}
@inproceedings{Olsson2017,
   abstract = {We motivate and describe why erosion modeling from a static architecture conformance checking point of view is important as well as the impact we think it can have on industry. Our research goal is to get a better understanding of erosion and develop smarter and simpler methods to assess erosion. We describe preliminary results from a retrospective, longitudinal case study ...},
   author = {Tobias Olsson and Morgan Ericsson and Anna Wingkvist},
   journal = {2017 IEEE International Conference on Software Architecture Workshops (ICSAW)},
   publisher = {IEEE},
   title = {Motivation and Impact of Modeling Erosion Using Static Architecture Conformance Checking},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/7958488/},
   year = {2017},
}
@article{Koziolek2013,
   abstract = {It's difficult to express a software architecture's sustainability in a single metric: relevant information is spread across requirements, architecture design documents, technology choices, source code, system context, and software architects' implicit knowledge. Many aspects influence economic sustainability, including design decisions facilitating evolutionary changes, ad...},
   author = {Heiko Koziolek and Dominik Domis and Thomas Goldschmidt and Philipp Vorst},
   issue = {6},
   journal = {IEEE Software},
   publisher = {IEEE},
   title = {Measuring Architecture Sustainability},
   volume = {30},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/6576748/},
   year = {2013},
}
@inproceedings{Terra2012,
   abstract = {Architectural erosion is a recurrent problem faced by software architects. Despite this fact, the process is usually tackled in ad hoc way, without adequate tool support at the architecture level. To address this issue, we describe the preliminary design of a recommendation system whose main purpose is to provide refactoring guidelines for developers and maintainers during the ta...},
   author = {Ricardo Terra and Marco Tulio Valente and Krzysztof Czarnecki and Roberto S. Bigonha},
   journal = {2012 16th European Conference on Software Maintenance and Reengineering},
   publisher = {IEEE},
   title = {Recommending Refactorings to Reverse Software Architecture Erosion},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/6178900/},
   year = {2012},
}
@inproceedings{Wang2019,
   abstract = {When the erosion of software architecture occurs, there is an increase in software maintenance costs, a decrease in software quality, and degradation of software performance, etc. Therefore, it is particularly crucial to find a feasible way to evaluate software architecture to detect and avoid the erosion of software architecture in...},
   author = {Tong Wang and Bixin Li},
   journal = {2019 IEEE 19th International Conference on Software Quality, Reliability and Security (QRS)},
   publisher = {IEEE},
   title = {Analyzing Software Architecture Evolvability Based on Multiple Architectural Attributes Measurements},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/8854729/},
   year = {2019},
}
@inproceedings{Zahid2017,
   abstract = {Software architecture is an important part of the software systems showing its components and connection between the components. Small release time and quick delivery has become the main objective of the software industry, these days. This has led to the negligence of the development of the software architecture resulting software architecture ero...},
   author = {Maryam Zahid and Zahid Mehmmod and Irum Inayat},
   journal = {2017 13th International Conference on Emerging Technologies (ICET)},
   publisher = {IEEE},
   title = {Evolution in software architecture recovery techniques — A survey},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/8281704/},
   year = {2017},
}
@inproceedings{Caracciolo2015,
   abstract = {Software erosion can be controlled by periodically checking for consistency between the de facto architecture and its theoretical counterpart. Studies show that this process is often not automated and that developers still rely heavily on manual reviews, despite the availability of a large number of tools. This is partially due to the high cost involved in setting up and maintain...},
   author = {Andrea Caracciolo and Mircea Filip Lungu and Oscar Nierstrasz},
   journal = {2015 12th Working IEEE/IFIP Conference on Software Architecture},
   publisher = {IEEE},
   title = {A Unified Approach to Architecture Conformance Checking},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/7158502/},
   year = {2015},
}
@article{Baabad2020,
   abstract = {Software architecture (SA) has a prominent role in all stages of system development. Given the persistent evolution of software systems over time, SA tends to be eroded or degraded. Such phenomenon is called architectural degradation. In light of this phenomenon, the current study focuses on problems of architectural erosion in the open-source software (OSS). There ha...},
   author = {Ahmed Baabad and Hazura Binti Zulzalil and Sa’adah Hassan and Salmi Binti Baharom},
   journal = {IEEE Access},
   publisher = {IEEE},
   title = {Software Architecture Degradation in Open Source Software: A Systematic Literature Review},
   volume = {8},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/9200327/},
   year = {2020},
}
@article{Bouwers2010,
   abstract = {Software architecture has been loosely defined as the organizational structure of a software system, including the components, connectors, constraints, and rationale.1 Evaluating a system's software architecture helps stakeholders to check whether the architecture complies with their interests. Additionally, the evaluation can result in a common understanding of...},
   author = {Eric Bouwers and Arie van Deursen},
   issue = {4},
   journal = {IEEE Software},
   publisher = {IEEE},
   title = {A Lightweight Sanity Check for Implemented Architectures},
   volume = {27},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/5440164/},
   year = {2010},
}
@inproceedings{Herold2013,
   abstract = {Reference architecture can help in enterprise architecture management to develop and operate standardized and maintainable software landscapes. Similar to the software architectures of single systems, however, they are threatened by architecture erosion, i.e. the continuous divergence between intended architectures and their actual realizations. A...},
   author = {Sebastian Herold and Matthias Mair and Andreas Rausch and Ingrid Schindler},
   journal = {2013 17th IEEE International Enterprise Distributed Object Computing Conference},
   publisher = {IEEE},
   title = {Checking Conformance with Reference Architectures: A Case Study},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/6658265/},
   year = {2013},
}
@inproceedings{Bandara2018,
   abstract = {Software architecture erosion, or commonly known as the as-implemented architecture violating the as-intended architecture, is one of the major problems faced by the software development industry today. There is no easy way to trace design decisions, tracking back or reconstructing those decisions by looking at the source code level elements. This is one of the ...},
   author = {Vidudaya Bandara and Indika Perera},
   journal = {2018 18th International Conference on Advances in ICT for Emerging Regions (ICTer)},
   publisher = {IEEE},
   title = {Identifying Software Architecture Erosion Through Code Comments},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/8615560/},
   year = {2018},
}
@article{Mair2014,
   abstract = {Uncontrolled software architecture erosion can lead to a degradation of the quality of a software system. It is hence of great importance to repair erosion efficiently. Refactorings can help to systematically reverse software architecture erosion through applying them in the system where architectural violations have been detected. However, existing refactoring approaches do not address architecture erosion holistically. In this paper, we describe and formalize the theoretical problem of repairing eroded software systems by finding optimal repair sequences. Furthermore, we investigate the applicability and limitations of existing refactoring approaches. We argue, true to the motto "more knowledge means less search" that using formalized and explicit knowledge of software engineers-modeled as fault patterns and repair strategies-combined with heuristic search techniques could overcome those limitations. This paper outlines a new approach-analog to a patient history in medicine-we have been starting to investigate in our recent research and also aims at stimulating a discussion about further research challenges in repairing eroded software systems. Copyright © 2014 ACM 978-1-4503-2523-3/14/ 04⋯$15.00.},
   author = {Matthias Mair and Sebastian Herold and Andreas Rausch},
   doi = {10.1145/2578128.2578231},
   isbn = {9781450325233},
   journal = {ACM International Conference Proceeding Series},
   keywords = {Fault pattern,Heuristic search,Repairing architecture erosion,Software architecture erosion,fault pattern,heuristic search,repairing ar-chitecture erosion,reverse engineering},
   publisher = {Association for Computing Machinery},
   title = {Towards flexible automated software architecture erosion diagnosis and treatment},
   url = {http://dx.doi.org/10.1145/2578128.2578231.},
   year = {2014},
}
@inproceedings{Herold2013,
   abstract = {Detecting software architecture erosion is an important task during the development and maintenance of software systems. Even in model-driven approaches in which consistency between artifacts can partially be established by construction and consistency issues have been intensively investigated, the intended architecture and its realization may diverge with negative effects on software quality. In this article, we describe an approach to flexible architecture erosion detection for model-driven development approaches. Consistency constraints expressed by architectural aspects called architectural rules are specified as formulas on a common ontology, and models are mapped to instances of that ontology. A knowledge representation and reasoning system is then utilized to check whether these architectural rules are satisfied for a given set of models. We describe three case studies in which this approach has been used to detect architecture erosion flexibly and argue that the negative effects of architecture erosion can be minimized effectively. © 2013 IEEE.},
   author = {Sebastian Herold and Andreas Rausch},
   doi = {10.1109/MISE.2013.6595292},
   isbn = {9781467364478},
   issn = {21572305},
   journal = {ICSE Workshop on Software Engineering for Adaptive and Self-Managing Systems},
   keywords = {architecture conformance checking,inter-model consistency,software architecture,software architecture erosion},
   pages = {24-30},
   title = {Complementing model-driven development for the detection of software architecture erosion},
   year = {2013},
}
@article{Mair2013,
   abstract = {Software architecture erosion can reduce the quality of software systems significantly. It is hence of great importance to repair erosion efficiently, for example, by means of refactoring. However, existing refactoring approaches do not address architecture erosion holistically. In this paper, we describe the problem of optimally repairing software architecture erosion and investigate the applicability and limitations of current refactoring approaches. We argue that a heuristic search for adequate repairs using formalized and explicit knowledge of software engineers could overcome those limitations. This paper outlines an approach we have been starting to investigate in our recent research and also aims at stimulating a discussion about further research challenges in repairing software architecture erosion. © 2013 Springer-Verlag.},
   author = {Matthias Mair and Sebastian Herold},
   doi = {10.1007/978-3-642-39031-9_25/COVER/},
   isbn = {9783642390302},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {refactoring,software architecture,software architecture erosion,software maintenance},
   pages = {299-306},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Towards extensive software architecture erosion repairs},
   volume = {7957 LNCS},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-642-39031-9_25},
   year = {2013},
}
@inproceedings{Mitra2022,
   author = {Dipra Mitra and Mohit Arora and Manik Rakhra and Challa Rushith Kumar and Mallu Leeladhar Reddy and S. Praveen Kumar Reddy and Chandan Kumar and Mohammad Shabaz},
   journal = {Annals of the Romanian Society for Cell Biology},
   pages = {2974-2021},
   title = {A Hybrid Framework to Control Software Architecture Erosion for Addressing Maintenance Issues | Annals of the Romanian Society for Cell Biology},
   url = {https://www.annalsofrscb.ro/index.php/journal/article/view/2839},
   year = {2022},
}
@article{Baabad2022,
   abstract = {Software architecture is crucial in determining success or failure in a variety of software development and design fields. Typically, as a system evolves, software architecture deteriorates. This phenomenon is known as architectural erosion. Several studies have addressed architectural erosion based on different solutions. As a result, the metrics technique is the mos...},
   author = {Ahmed Baabad and Hazura Binti Zulzalil and Sa’adah Hassan and Salmi Binti Baharom},
   journal = {IEEE Access},
   publisher = {IEEE},
   title = {Characterizing the Architectural Erosion Metrics: A Systematic Mapping Study},
   volume = {10},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/9709798/},
   year = {2022},
}
@inproceedings{Knieke2021,
   abstract = {Architecture erosion is a big challenge in modern architectures leading to a deterioration of the quality properties of these systems. Today, no comprehensive approach for regaining architecture consistency in eroded software systems exists and architecture consistency is essentially achieved by repairing the implementation level only. In this paper, we propose a novel approach enabling a joint architecture and implementation repairing for tackling software architecture erosion. By using a holistic view on violation causes and suitable repair actions in combination with learning mechanisms we build up a system specific knowledge-base improving accuracy and efficiency in consolidation of architecture and implementation over time.},
   author = {Christoph Knieke and Andreas Rausch and Mirco Schindler},
   doi = {10.1109/APR52552.2021.00011},
   isbn = {9781665444729},
   journal = {Proceedings - 2021 IEEE/ACM International Workshop on Automated Program Repair, APR 2021},
   keywords = {Machine Learning,Program repair,Software Architecture Degradation,Software Evolution},
   month = {6},
   pages = {19-20},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Tackling Software Architecture Erosion: Joint Architecture and Implementation Repairing by a Knowledge-based Approach},
   year = {2021},
}
@article{Li2022,
   abstract = {Architecture erosion (AEr) can adversely affect software development and has received significant attention in the last decade. However, there is an absence of a comprehensive understanding of the state of research about the reasons and consequences of AEr, and the countermeasures to address AEr. This work aims at systematically investigating, identifying, and analyzing the reasons, consequences, and ways of detecting and handling AEr. With 73 studies included, the main results are as follows: (1) AEr manifests not only through architectural violations and structural issues but also causing problems in software quality and during software evolution; (2) non-technical reasons that cause AEr should receive the same attention as technical reasons, and practitioners should raise awareness of the grave consequences of AEr, thereby taking actions to tackle AEr-related issues; (3) a spectrum of approaches, tools, and measures has been proposed and employed to detect and tackle AEr; and (4) three categories of difficulties and five categories of lessons learned on tackling AEr were identified. The results can provide researchers a comprehensive understanding of AEr and help practitioners handle AEr and improve the sustainability of their architecture. More empirical studies are required to investigate the practices of detecting and addressing AEr in industrial settings.},
   author = {Ruiyin Li and Peng Liang and Mohamed Soliman and Paris Avgeriou},
   doi = {10.1002/SMR.2423},
   issn = {2047-7481},
   issue = {3},
   journal = {Journal of Software: Evolution and Process},
   keywords = {architecture erosion,software architecture,systematic mapping study},
   month = {3},
   pages = {e2423},
   publisher = {John Wiley & Sons, Ltd},
   title = {Understanding software architecture erosion: A systematic mapping study},
   volume = {34},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1002/smr.2423 https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2423 https://onlinelibrary.wiley.com/doi/10.1002/smr.2423},
   year = {2022},
}
@article{,
   abstract = {Software architectures capture the most significant properties and design constraints of software systems. Thus, modifications to a system that violate its architectural principles can degrade system performance and shorten its useful lifetime. As the potential frequency and scale of software adaptations increase to meet rapidly changing requirements and business conditions, controlling such architecture erosion becomes an important concern for software architects and developers. This paper presents a survey of techniques and technologies that have been proposed over the years either to prevent architecture erosion or to detect and restore architectures that have been eroded. These approaches, which include tools, techniques and processes, are primarily classified into three generic categories that attempt to minimise, prevent and repair architecture erosion. Within these broad categories, each approach is further broken down reflecting the high-level strategies adopted to tackle erosion. These are: process-oriented architecture conformance, architecture evolution management, architecture design enforcement, architecture to implementation linkage, self-adaptation and architecture restoration techniques consisting of recovery, discovery and reconciliation. Some of these strategies contain sub-categories under which survey results are presented. We discuss the merits and weaknesses of each strategy and argue that no single strategy can address the problem of erosion. Further, we explore the possibility of combining strategies and present a case for further work in developing a holistic framework for controlling architecture erosion. © 2011 Elsevier Inc.},
   author = {Lakshitha De Silva and Dharini Balasubramaniam},
   doi = {10.1016/J.JSS.2011.07.036},
   issn = {0164-1212},
   issue = {1},
   journal = {Journal of Systems and Software},
   keywords = {Architecture erosion,Controlling architecture erosion,Design erosion,Software architecture,Software decay,Survey},
   month = {1},
   pages = {132-151},
   publisher = {Elsevier},
   title = {Controlling software architecture erosion: A survey},
   volume = {85},
   year = {2012},
}
@article{,
   abstract = {Title from content provider.},
   author = {Institute of Electrical and Electronics Engineers.},
   isbn = {978-1-5044-8332-2},
   journal = {IEEE Software},
   publisher = {IEEE},
   title = {29119-4-2021 - IEEE/ISO/IEC International Standard - Software and systems engineering--Software testing--Part 4 : Test techniques - Redline.},
   year = {2021},
}
@inproceedings{Lagerstedt2014,
   abstract = {In software development the code often must comply to a number of non-functional requirements, like architectural requirements. These requirements are often communicated and verified by writing guidelines and creating reports of the non-compliance. This way of communicating and verifying non-functional requirements is very costly since al...},
   author = {Robert Lagerstedt},
   journal = {2014 IEEE 1st International Workshop on Requirements Engineering and Testing (RET)},
   publisher = {IEEE},
   title = {Using automated tests for communicating and verifying non-functional requirements},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/6908675/},
   year = {2014},
}
@article{Sarwar2013,
   abstract = {Testing is the method that assures that the developed system conforms to the specification and no error arises during system usage. No system is deployed without testing as testing produces confidence in the system. This paper presents the Requirements Based Testing (RBT) technique for testing the Usability and Functional Requirements of a software. The main emphasis of this paper is to test the usability of a software, as being a non-functional requirement that is perceived subjectively it is difficult to measure it. A generic template has been proposed for testing usability. For the purpose of evaluation, this method is applied to test a software named "Programming Teaching Aid", a system that helps students in understanding the programming language C++. © 2013 IEEE.},
   author = {Tabinda Sarwar and Wajiha Habib and Fahim Arif},
   doi = {10.1109/ICOIA.2013.6650281},
   isbn = {9781467352550},
   journal = {2013 2nd International Conference on Informatics and Applications, ICIA 2013},
   keywords = {Non-functional Requirements,Requirements,Requirements Based Testing,Software Testing,Usability},
   pages = {347-352},
   publisher = {IEEE Computer Society},
   title = {Requirements based testing of software},
   year = {2013},
}
@article{Eckhardt2016,
   abstract = {Non-functional requirements (NFRs) are commonly distinguished from functional requirements by differentiating how the system shall do something in contrast to what the system shall do. This distinction is not only prevalent in research, but also inuences how requirements are handled in practice. NFRs are usually documented separately from functional requirements, without quantitative measures, and with relatively vague descriptions. As a result, they remain diffcult to analyze and test. Several authors argue, however, that many so-called NFRs actually describe behavioral properties and may be treated the same way as functional requirements. In this paper, we empirically investigate this point of view and aim to increase our understanding on the nature of NFRs addressing system properties. We report on the classification of 530 NFRs extracted from 11 industrial requirements specifications and analyze to which extent these NFRs describe system behavior. Our results suggest that most\non-functional" requirements are not non-functional as they describe behavior of a system. Consequently, we argue that many so-called NFRs can be handled similarly to functional requirements.},
   author = {Jonas Eckhardt and Andreas Vogelsang and Daniel Méndez Fernández},
   doi = {10.1145/2884781.2884788},
   isbn = {9781450339001},
   issn = {02705257},
   journal = {Proceedings - International Conference on Software Engineering},
   keywords = {Classification,Empirical studies,Model-based development,Non-functional requirements},
   month = {5},
   pages = {832-842},
   publisher = {IEEE Computer Society},
   title = {Are non-functional requirements really non-functional? an investigation of non-functional requirements in practice},
   volume = {14-22-May-2016},
   year = {2016},
}
@article{,
   abstract = {"ISO/IEC/IEEE 29119-2:2013(E)." "2013-09-01." Abstract: The purpose of the ISO/IEC/IEEE 29119 series of software testing standards is to define an internationally-agreed set of standards for software testing that can be used by any organization when performing any form of software testing. ISO/IEC/IEEE 29119-2 comprises test process descriptions that define the software testing processes at the organizational level, test management level and dynamic test levels. It supports dynamic testing, functional and non-functional testing, manual and automated testing, and scripted and unscripted testing. The processes defined in ISO/IEC/IEEE 29119-2 can be used in conjunction with any software development lifecycle model. Since testing is a key approach to risk-mitigation in software development, ISO/IEC/IEEE 29119-2 follows a risk-based approach to testing. Risk-based testing is a common industry approach to strate gizing and managing testing. Risk-based testing allows testing to be prioritized and focused on the most important features and functions. Keywords: 29119, 29119-1, software testing, Test Planning Process, Test Plan, verification and validation.},
   author = {International Organization for Standardization. and International Electrotechnical Commission. and Institute of Electrical and Electronics Engineers. and IEEE-SA Standards Board.},
   isbn = {978-0-7381-8599-6},
   journal = {IEEE Software},
   title = {29119-2-2013 - ISO/IEC/IEEE International Standard - Software and systems engineering —Software testing —Part 2:Test processes},
   year = {2013},
}
@misc{Zimmerer2018,
   author = {Peter Zimmerer},
   journal = {2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)},
   pages = {532-533},
   title = {Strategy for Continuous Testing in iDevOps | IEEE Conference Publication | IEEE Xplore},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/8449648},
   year = {2018},
}
@article{Ribeiro2016,
   abstract = {Software testing aims to reveal failures due to the lack of conformity (defects) among functional and non-functional requirements and the implemented system. Thus, defects can be identified and fixed, improving software quality. However, despite several works emphasizing the importance of non-functional requirements (NFRs), there is an insufficient amount of software testing approaches dealing with them. The lack of NFR evaluation may be the cause of low-quality software that does not meet users need, influencing software project success. Goal: To organize a body of knowledge regarding NFRs and software testing approaches available in the technical literature and reveal the gaps between testable NFRs and software testing approaches. Method: To perform structured literature reviews to identify NFRs and software testing approaches dealing with testable NFRs. To combine both results, reveal research opportunities and organize a body of knowledge regarding NFRs and software testing approaches. Results: From 224 identified NFRs, 87 were described, and 47 software testing approaches observed. Only eight approaches are empirically evaluated. No testing approaches were identified for 11 testable NFRs. Furthermore, regarding the testing process, we did not observe any testing approach covering the test planning phase. Conclusion: Despite their importance, many testable NFRs seem not be tested due to the lack of appropriate software testing approaches yet. Also, the existing testing approaches do not cover all testing processes activities and, in general, lack empirical evidence about their feasibility and performance, making their use in software projects risky.},
   author = {Victor V. Ribeiro and Guilherme H. Travassos},
   doi = {10.5753/SBQS.2016.15137},
   issn = {0000-0000},
   journal = {Anais do Simpósio Brasileiro de Qualidade de Software (SBQS)},
   month = {10},
   pages = {226-240},
   publisher = {SBC},
   title = {Testing Non-Functional Requirements: Lacking of Technologies or Researching Opportunities?},
   url = {https://sol.sbc.org.br/index.php/sbqs/article/view/15137},
   year = {2016},
}
@article{Soni2016,
   abstract = {In modern environment, delivering innovative idea in a fast and reliable manner is extremely significant for any organizations. In the existing scenario, Insurance industry need to better respond to dynamic market requirements, faster time to market for new initiatives and services, and support innovative ways of customer interaction. In past few years, the transition to cloud platforms has given benefits such as agility, scalability, and lower capital costs but the application lifecycle management practices are slow with this disruptive change. DevOps culture extends the agile methodology to rapidly create applications and deliver them across environment in automated manner to improve performance and quality assurance. Continuous Integration (CI) and Continuous delivery (CD) has emerged as a boon for traditional application development and release management practices to provide the capability to release quality artifacts continuously to customers with continuously integrated feedback. The objective of the paper is to create a proof of concept for designing an effective framework for continuous integration, continuous testing, and continuous delivery to automate the source code compilation, code analysis, test execution, packaging, infrastructure provisioning, deployment, and notifications using build pipeline concept.},
   author = {Mitesh Soni},
   doi = {10.1109/CCEM.2015.29},
   isbn = {9781467385664},
   journal = {Proceedings - 2015 IEEE International Conference on Cloud Computing in Emerging Markets, CCEM 2015},
   keywords = {Automation,Cloud Computing,Configuration Management,Continuous Delivery,Continuous Integration,Continuous Testing,DevOps},
   month = {3},
   pages = {85-89},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {End to End Automation on Cloud with Build Pipeline: The Case for DevOps in Insurance Industry, Continuous Integration, Continuous Testing, and Continuous Delivery},
   year = {2016},
}
@article{Mascheroni2021,
   abstract = {Continuous Delivery is a practice where high-quality software is built in a way that it can be released into production at any time. However, systematic literature reviews and surveys performed as part of this Doctoral Research report that both the literature and the industry are still facing problems related to testing using practices like Continuous Delivery or Continuous Deployment. Thus, we propose Continuous Testing Improvement Model (CTIM) as a solution to the testing problems in continuous software development environments. It brings together proposals and approaches from different authors which are presented as good practices grouped by type of tests and divided into four levels. These levels indicate an improvement hierarchy and an evolutionary path in the implementation of Continuous Testing. Also, an application called EvalCTIM was developed to support the appraisal of a testing process using the proposed model. Finally, to validate the model, an action-research methodology was employed through an interpretive theoretical evaluation followed by case studies conducted in real software development projects. After several improvements made as part of the validation outcomes, the results demonstrate that the model can be used as a solution for implementing Continuous Testing gradually at companies using Continuous Deployment or Continuous Delivery and measuring its progress.},
   author = {Maximiliano Agustin Mascheroni and Emanuel Irrazabal and Gustavo Rossi},
   doi = {10.1109/AST52587.2021.00020},
   isbn = {9781665435673},
   journal = {Proceedings - 2021 IEEE/ACM International Conference on Automation of Software Test, AST 2021},
   keywords = {continuous delivery,continuous deployment,continuous testing,doctoral thesis},
   month = {5},
   pages = {109-112},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Continuous Testing Improvement Model},
   year = {2021},
}
@inproceedings{Silva2015,
   abstract = {Software architecture erosion is a problem faced by many organizations in the software industry. It happens when `as-implemented' architecture does not conform to the `as-intended' architecture, which results in low quality, complex, hard to maintain software. Architecture conformance checking refers to assessing the conformity of the imp...},
   author = {Mahesh De Silva and Indika Perera},
   city = {Peradeniya, Sri Lanka},
   journal = {International Conference on Industrial and Information Systems},
   pages = {43-48},
   publisher = {IEEE},
   title = {Preventing software architecture erosion through static architecture conformance checking},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/7398983/},
   year = {2015},
}
@article{Khayami2011,
   abstract = {Constant changes in information technology (IT) and business environments have made the demand for a powerful management for IT systems more pressing. Enterprise architecture is a framework to develop and maintain IT, to achieve organizational goals and to manage resources of this technology. Enterprise Architecture (EA) quality is a multi-dimensional content which is not easily distinguishable and measurable. To determine this content more exact, the qualitative models have been presented in which different aspects of this matter are investigated. This paper attempts to introduce mentioned about determine EA qualification and its qualitative characteristics more clearly. This article can be used as a reference to investigate EA qualification and its models. Also, it can help stakeholders to explain the qualitative requirements more exactly. © 2010 Published by Elsevier Ltd.},
   author = {Raouf Khayami},
   doi = {10.1016/J.PROCS.2011.01.004},
   issn = {1877-0509},
   journal = {Procedia Computer Science},
   keywords = {Enterprise architecture,Enterprise architecture characteristics,Quality model of enterprise architecture},
   month = {1},
   pages = {1277-1282},
   publisher = {Elsevier},
   title = {Qualitative characteristics of enterprise architecture},
   volume = {3},
   year = {2011},
}
@article{Hitchins2003,
   author = {DK Hitchins},
   title = {Advanced systems thinking, engineering, and management},
   url = {https://books.google.com.br/books?hl=pt-BR&lr=&id=SlxWiudf2g0C&oi=fnd&pg=PR15&dq=Advanced+Systems,+thinking,+engineering+and+management&ots=qWxnXU_sqi&sig=qoqyXzPqtAXlehG7Erohal8Ddkk},
   year = {2003},
}
@article{Emes2012,
   abstract = {The UK Chapter of the International Council on Systems Engineering (INCOSE UK) has commissioned research to illustrate the variety of usage of the terms architecture and architecting in the systems engineering community. These terms, though widely used, are rarely strictly defined, and the meaning attributed to the terms is not consistent even in formal publications. Using soft systems methodology, this research has analysed three published sources (MODAF, The Art of Systems Architecting by Maier and Rechtin, and ISO/IEC 42010), and conducted a series of interviews with systems architecting practitioners. Twelve contentious questions in systems architecting are discussed, and six perspectives on systems architecting presented, including three basic worldviews of the relationship between systems engineering and systems architecting. One model sees systems architecting as simply a rebranding of systems engineering to broaden its appeal with no change in content. Another model sees systems engineering restricted to its traditional processes, with systems architecting adding to systems engineering through external processes. The final model, and the most popular amongst the systems engineering community, sees systems architecting addressing shortcomings in traditional sequential lifecycle models by stretching the content of systems engineering to include new elements under the banner of systems architecting.},
   author = {M R Emes and P A Bryant and M K Wilkinson and P King and A M James and S Arnold},
   doi = {10.1002/sys.21202},
   issue = {4},
   journal = {Wiley Online Library},
   keywords = {Interpreting 'Systems Architecting',architecture,belief systems,soft systems 2 Emes et al,systems architecting,systems terminology},
   month = {12},
   pages = {369-395},
   title = {Interpreting “systems architecting”},
   volume = {15},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sys.21202},
   year = {2012},
}
@article{Maier2009,
   abstract = {The design of complex systems is viewed as a blend of the art of architecture with the science of engineering. The principles of systems are discussed, with examples to illustrate each point. The artistic elements of architecture are then considered, and four methods in common use, depending on the nature and phase of the project, the particular problem to be solved, and the style of the architect, are described. The role of architecting throughout the system development cycle is examined. Architecting and concurrent engineering are compared and seem to be complementary strategies},
   author = {Mark W. Maier},
   doi = {10.1201/9781420079142/ART-SYSTEMS-ARCHITECTING-MARK-MAIER},
   journal = {The Art of Systems Architecting},
   month = {1},
   publisher = {CRC Press},
   title = {The Art of Systems Architecting},
   year = {2009},
}
@inproceedings{Kaisler2005,
   abstract = {An enterprise architecture (EA) identifies the main components of the organization, its information systems, the ways in which these components work together in order to achieve defined business objectives, and the way in which the information systems support the business processes of the organization. The components include staff, business processes, technology, information, financial and o...},
   author = {S.H. Kaisler and F. Armour and M. Valivullah},
   journal = {Proceedings of the 38th Annual Hawaii International Conference on System Sciences},
   publisher = {IEEE},
   title = {Enterprise Architecting: Critical Problems},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/1385698/},
   year = {2005},
}
@article{Vural2018,
   abstract = {In cloud computing, the microservices has become the mostly used architectural style. However, there is still an ongoing debate about how big a microservice should be. In this case study, a monolith application is measured using Common Software Measurement International Consortium (COSMIC) Function Points. The same application is divided into pieces by following the Domain Driven Design (DDD) principles. The resulting cloud friendly microservices are measured again using COSMIC Function Points and the obtained results are compared.},
   author = {Hulya Vural and Murat Koyuncu and Sanjay Misra},
   doi = {10.1007/978-3-319-95174-4_36/COVER/},
   isbn = {9783319951737},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {COSMIC function points,Cloud computing,Microservices,Web services},
   pages = {454-463},
   publisher = {Springer Verlag},
   title = {A case study on measuring the size of microservices},
   volume = {10964 LNCS},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-319-95174-4_36},
   year = {2018},
}
@inproceedings{Cabrera2020,
   abstract = {The Internet of Things (IoT) represents the new industrial revolution, in which physical and virtual objects are interconnected. On the other hand, microservices architectures have broken the monolithic and centralized way to build software, and provide systems with high-quality characteristics (e.g., resilience, availability, modularity, and portability). Therefore, the id...},
   author = {Edwin Cabrera and Paola Cárdenas and Priscila Cedillo and Paola Pesántez-Cabrera},
   journal = {2020 IEEE International Conference on Services Computing (SCC)},
   publisher = {IEEE},
   title = {Towards a Methodology for creating Internet of Things (IoT) Applications based on Microservices},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/9284589/},
   year = {2020},
}
@inproceedings{Singjai2021,
   abstract = {Microservice API design is a critical aspect in crafting a microservice architecture. While API design in general has been studied, the specific relation of API design to design practices and models commonly used in microservice architectures is yet understudied. In particular, practitioners frequently use Domain-Driven Design (DDD) in their [:...},
   author = {Apitchaka Singjai and Uwe Zdun and Olaf Zimmermann},
   journal = {2021 IEEE 18th International Conference on Software Architecture (ICSA)},
   publisher = {IEEE},
   title = {Practitioner Views on the Interrelation of Microservice APIs and Domain-Driven Design: A Grey Literature Study Based on Grounded Theory},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/9426760/},
   year = {2021},
}
@inproceedings{Farsi2021,
   abstract = {Building software using a microservice architecture is a means that gives more options to scale up applications, deploy them independently and limit their "blast radius" of failure. Microservices tend to solve the complexity and the increasing development problems by applying the functional decomposition principle. This main feature can become a point of weakness in the absence of a fu...},
   author = {Hassan Farsi and Driss Allaki and Abdeslam En-nouaary and Mohamed Dahchour},
   journal = {2021 IEEE/ACS 18th International Conference on Computer Systems and Applications (AICCSA)},
   publisher = {IEEE},
   title = {Following Domain Driven Design principles for Microservices decomposition: is it enough?},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/9686947/},
   year = {2021},
}
@article{Kapferer2020,
   abstract = {Service-oriented architectures and microservices have gained much attention in recent years; companies adopt these concepts and supporting technologies in order to increase agility, scalability, and maintainability of their systems. Decomposing an application into multiple independently deployable, appropriately sized services and then integrating such services is challenging. With strategic patterns such as Bounded Context and Context Map, Domain-driven Design (DDD) can support business analysts, (enterprise) architects, and microservice adopters. However, existing architecture description languages do not support the strategic DDD patterns sufficiently; modeling tools for DDD primarily focus on its tactical patterns. As a consequence, different opinions on how to apply strategic DDD exist, and it is not clear how to combine its patterns. Aiming for a clear and concise interpretation of the patterns and their combinations, this paper distills a meta-model of selected strategic DDD patterns from the literature. It then introduces Context Mapper, an open source project that a) defines a Domain-specific Language (DSL) expressing the strategic DDD patterns and b) provides editing, validation, and transformation tools for this DSL. As a machine-readable description of DDD, the DSL provides a modeling foundation for (micro-)service design and integration. The models can be refactored and transformed within an envisioned tool chain supporting the continuous specification and evolution of Context Maps. Our validation activities (prototyping, action research, and case studies) suggest that the DDD pattern clarification in our meta-model and the Context Mapper tool indeed can benefit the target audience.},
   author = {Stefan Kapferer and Olaf Zimmermann},
   doi = {10.5220/0008910502990306},
   isbn = {9789897584008},
   journal = {MODELSWARD 2020 - Proceedings of the 8th International Conference on Model-Driven Engineering and Software Development},
   keywords = {DSL,Enterprise Application Integration,Model-driven Software Engineering,Patterns,Service Design},
   pages = {299-306},
   publisher = {SciTePress},
   title = {Domain-specific language and tools for strategic domain-driven design, context mapping and bounded context modeling},
   year = {2020},
}
@article{Vural2021,
   abstract = {Information systems are moving into the cloud. The new requirements enforced by cloud standards are high availability, high scalability, and a reduced mean time to recovery. Due to these new requirements, information system architecture styles are also evolving. Microservice architecture is becoming the de facto standard for developing highly modular cloud information systems. Since [:...},
   author = {Hulya Vural and Murat Koyuncu},
   journal = {IEEE Access},
   publisher = {IEEE},
   title = {Does Domain-Driven Design Lead to Finding the Optimal Modularity of a Microservice?},
   volume = {9},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/9359794/},
   year = {2021},
}
@article{Yale2017,
   abstract = {In the service oriented, customer experience centric and customer changing demand driven market environment, ICT is becoming the leading enabler and partner of the modern enterprise business. More and more companies are transformed into more or pure digital style and virtual organized enterprises that are enabled and supported by a group of business oriented and microservice based applications and modules that are new to most of the companies and enterprises, both business and technical solution staff. This paper presents a microservice based reference architecture model with clear understanding and definition of the concept of microservice and key architectural components and building blocks for implementing and managing enterprise microservices in the context of enterprise architecture. This reference architecture model can be used as the pragmatic guidance for both business and IT professionals when they develop the enterprise IT transformation architecture solutions so that the API and microservice relevant technologies could be used properly at enterprise level without confusion. This paper also highlights a set of key architectural issues and provides corresponding recommendations when APIs and microsevices are leveraged within a company at enterprise level.},
   author = {Yu Yale and Haydn Silveira and Max Sundaram},
   doi = {10.1109/IMCEC.2016.7867539},
   isbn = {9781467396127},
   journal = {Proceedings of 2016 IEEE Advanced Information Management, Communicates, Electronic and Automation Control Conference, IMCEC 2016},
   keywords = {API,API gateway,API platform,Domain architecture,Domain driven design,Enterprise architecture,Microservice,Service oriented architecture},
   month = {2},
   pages = {1856-1860},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A microservice based reference architecture model in the context of enterprise architecture},
   year = {2017},
}
@article{Parnas1972,
   abstract = {This paper discusses modularization as a mechanism for improving the flexibility and comprehensibility of a system while allowing the shortening of its development time. The effectiveness of a modu...},
   author = {D. L. Parnas},
   doi = {10.1145/361598.361623},
   issn = {15577317},
   issue = {12},
   journal = {Communications of the ACM},
   keywords = {KWIC index,modularity,modules,software,software design,software engineering},
   month = {12},
   pages = {1053-1058},
   publisher = {
		ACM
		PUB27
		New York, NY, USA
	},
   title = {On the criteria to be used in decomposing systems into modules},
   volume = {15},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/abs/10.1145/361598.361623},
   year = {1972},
}
@inproceedings{Ding2020,
   abstract = {At present, each industry has invested huge energy in the development of its own information construction, and has also derived numerous high-quality enterprise information forms. At the same time, a variety of new information construction concepts are emerging. Among them, in order to solve the problems of the rapid development of information technology, such as difficult to express the bus...},
   author = {Yu Ding and LiLing Wang and Shaokun Li and XuTong Wang and JianWei Zhang},
   journal = {2020 2nd International Conference on Information Technology and Computer Application (ITCA)},
   publisher = {IEEE},
   title = {Enterprise service application architecture based on Domain Driven Model Design},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/9422078/},
   year = {2020},
}
@inproceedings{Yu2016,
   abstract = {In the service oriented, customer experience centric and customer changing demand driven market environment, ICT is becoming the leading enabler and partner of the modern enterprise business. More and more companies are transformed into more or pure digital style and virtual organized enterprises that are enabled and supported by a group of business oriented and microservic...},
   author = {Yale Yu and Haydn Silveira and Max Sundaram},
   journal = {2016 IEEE Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)},
   publisher = {IEEE},
   title = {A microservice based reference architecture model in the context of enterprise architecture},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/7867539/},
   year = {2016},
}
@inproceedings{Petrasch2017,
   abstract = {Microservices need to communicate via inter-service communication mechanism. This leads to service integration that requires the aspect of messaging and event handling to be considered during the specification of microservices. Therefore, the application of Enterprise Integration Patterns (EIP) in the context of the design of microservice architectures seems to ...},
   author = {Roland Petrasch},
   journal = {2017 14th International Joint Conference on Computer Science and Software Engineering (JCSSE)},
   publisher = {IEEE},
   title = {Model-based engineering for microservice architectures using Enterprise Integration Patterns for inter-service communication},
   url = {https://ieeexplore-ieee-org.ez67.periodicos.capes.gov.br/document/8025912/},
   year = {2017},
}
@article{Braun2021,
   abstract = {More and more data-intensive systems have emerged lately. Big Data, Artificial Intelligence, or cloud-native applications all require high scalability and availability. Data is no longer persisted in one central relational database with serialized and transactional access, but rather distributed and replicated among different nodes running only under eventual consistency. This poses a number of design challenges for software architects, as they cannot rely on a single system to mask the concurrency anomalies of concurrent access to distributed and replicated data. Based on three case studies, we developed a theory regarding how practitioners handle synchronization and consistency design challenges in distributed data-intensive applications. We also identified the "white spots"of missing design guidance needed by practitioners to handle the aforementioned challenges appropriately. We are currently evaluating our theory in the context of an action research study. In this study, we are also evaluating the novel design guidelines we are proposing in this regard, which, according to our theory, meet the needs of practitioners. Our design guidelines integrate with Domain-Driven Design, which is widely used in practice. Following the idea of multilevel serializability, we investigate the compatibility of business operations beyond commutativity. We provide concrete practical design guidance to achieve compatibility of non-commutative business operations. We also describe the basic infrastructure guarantees our design guidelines require from replication frameworks.},
   author = {Susanne Braun and Annette Bieniusa and Frank Elberzhager},
   doi = {10.1145/3447865.3457969},
   isbn = {9781450383387},
   journal = {Proceedings of the 8th Workshop on Principles and Practice of Consistency for Distributed Data, PaPoC 2021},
   keywords = {Data-intensive systems,Domain-driven design,Eventual consistency},
   month = {4},
   publisher = {Association for Computing Machinery, Inc},
   title = {Advanced domain-driven design for consistency in distributed data-intensive systems},
   url = {https://doi.org/10.1145/3447865.3457969},
   year = {2021},
}
@article{Wesenberg2006,
   abstract = {Purchasing a Commercial-Off-The-Shelf (COTS) package solution can be a complex and daunting task. Selecting and evaluating the right candidate is difficult, especially when the solution aims at the heart of company business. The company's competitive edge must be maintained, while at the same time ensuring the intended goals such as reduced costs and better functional coverage. A good Enterprise Architecture should be a prime tool when evaluating several solutions against the company's needs.In this paper we will recount the experience and lessons learned when we evaluated three COTS systems to replace a set of legacy oil trading and operations systems. Based on weaknesses in our Enterprise Architecture, we applied strategic domain-driven design principles to extend our Enterprise Architecture during the evaluation. We found that these techniques enabled us to thoroughly analyse our domain with the domain experts and provide answers based on tacit domain knowledge, without going through the cost and effort of performing a full-scale architectural analysis. At the same time, the tacit domain knowledge became explicit and shared, easing the communication with various stakeholders.},
   author = {Harald Wesenberg and Einar Landre and Harald Rønneberg},
   doi = {10.1145/1176617.1176730},
   isbn = {159593491X},
   journal = {Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA},
   keywords = {Context map,Domain-driven design,Enterprise architecture,Information architecture,Responsibility layer,information architecture,responsibility layer},
   pages = {824-829},
   title = {Using domain-driven design to evaluate commercial off-the-shelf software},
   volume = {2006},
   year = {2006},
}
@article{Landre2007,
   abstract = {In this paper we present the experience gained and lessons learned when the IT department at Statoil ASA, a large Oil and Gas company in Norway, applied Domain-Driven design techniques in combination with agile software development practices to assess the software architecture of our next generation oil trading and supply chain application. Our hypothesis was that the use of object oriented techniques, Domain-Driven design and a proper object-relational mapping tool would significantly improve the performance and reduce the code base compared with current legacy systems. The legacy system is based on several Oracle databases serving a variety of clients written in Java, Gupta Centura Team Developer and HTML. The databases have a layer of business logic written in PL/SQL offering various system services to the clients. To validate our new object oriented software architecture, we re-implemented one of the most computationally heavy and data intensive services using Test First and DomainDriven design techniques. The resulting software was then tested on a set of servers with a representative subset of data from the production environment. We found that using these techniques improved our software architecture with respect to performance as well as code quality when running on top of our Oracle databases. We also tested the switch to an object database from Versant and achieved additional performance gains.},
   author = {Einar Landre and Harald Wesenberg and Jørn Ølmheim},
   doi = {10.1145/1297846.1297967},
   isbn = {9781595938657},
   journal = {Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA},
   keywords = {Agile software development,Domain-driven design,Object databases,Object-relational mapping,Oracle,PL/SQL,Relational databases,Test driven design,Test first,Versant,object databases,relational databases},
   pages = {983-993},
   title = {Agile enterprise software development using domain-driven design and test first},
   year = {2007},
}
@article{Zachman1987,
   abstract = {With increasing size and complexity of the implementations of information systems, it is necessary to use some logical construct (or architecture) for defining and controlling the interfaces and the integration of all of the components of the system. This paper defines information systems architecture by creating a descriptive framework from disciplines quite independent of information systems, then by analogy specifies information systems architecture based upon the neutral, objective framework. Also, some preliminary conclusions about the implications of the resultant descriptive framework are drawn. The discussion is limited to architecture and does not include a strategic planning methodology.},
   author = {J. A. Zachman},
   doi = {10.1147/sj.263.0276},
   issn = {0018-8670},
   issue = {3},
   journal = {IBM System Journal},
   month = {4},
   pages = {276-292},
   publisher = {IBM},
   title = {A framework for information systems architecture},
   volume = {26},
   year = {1987},
}
@article{Balabko2006,
   abstract = {Enterprise Architecture (EA) is a relatively new domain that is rapidly developing. “The primary reason for developing EA is to support business by providing the fundamental technology and process structure for an IT strategy” [TOGAF]. EA models have to model enterprises facets that span from marketing to IT. As a result, EA models tend to become large. Large EA models create a problem for model management. Concern-based design methods (CBDMs) aim to solve this problem by considering EA models as a composition of smaller, manageable parts—concerns. There are dozens of different CBDMs that can be used in the context of EA: from very generic methods to specific methods for business modeling or IT implementations. This variety of methods can cause two problems for those who develop and use innovative CBDMs in the field of Enterprise Architecture (EA). The first problem is to choose specific CBDMs that can be used in a given EA methodology: this is a problem for researchers who develop their own EA methodology. The second problem is to find similar methods (with the same problem domain or with similar frameworks) in order to make a comparative analysis with these methods: this is a problem of researchers who develop their own CBDMs related to a specific problem domain in EA (such as business process modeling or aspect oriented programming). We aim to address both of these problems by means of a definition of generic Requirements for CBDMs based on the system inquiry. We use these requirements to classify twenty CBDMs in the context of EA. We conclude with a short discussion about trends that we have observed in the field of concern-based design and modeling.},
   author = {Pavel Balabko and Alain Wegmann},
   doi = {10.1007/S10796-006-7976-9},
   issn = {1572-9419},
   issue = {2},
   journal = {Information Systems Frontiers 2006 8:2},
   keywords = {Control,IT in Business,Management of Computing and Information Systems,Operations Research/Decision Theory,Systemic inquiry,Systems Theory},
   month = {2},
   pages = {115-131},
   publisher = {Springer},
   title = {Systemic classification of concern-based design methods in the context of enterprise architecture},
   volume = {8},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/article/10.1007/s10796-006-7976-9},
   year = {2006},
}
@article{Bala2018,
   abstract = {Search-based software testing (SBST) is considered an effective process in the generation of non-functional test cases. The SBST employs metaheuristic search techniques to evaluate the best-case and worst-case execution times of real-time scenarios. However, these search techniques suffer software remodularization for software systems. In this paper, an exploratory review on some of these techniques such as Genetic Algorithms, Harmony Search and Simulated Annealing is presented by highlighting the fitness function employed, non - functional testing and the challenges observed. The review also investigates each technique based on different applications employed in a white box, black-box, and gray-box testing. It shows that Harmony Search-Based Algorithm is the effective technique to address the problem of software remodularization.},
   author = {N. M. Bala and S. Suhailan},
   doi = {10.14419/IJET.V7I4.28.22617},
   issn = {2227524X},
   issue = {4},
   journal = {International Journal of Engineering and Technology(UAE)},
   keywords = {Genetic Algorithms,Harmony Search,Non-functional testing and simulated annealing},
   pages = {368-391},
   publisher = {Science Publishing Corporation Inc},
   title = {Effective search-based approach for testing non-functional properties in software system: An empirical review},
   volume = {7},
   year = {2018},
}
@article{,
   abstract = { Context: The mobile app market is continually growing offering solutions to almost all aspects of people’s lives, e.g., healthcare, business, entertainment, as well as the stakeholders’ demand for apps that are more secure, portable, easy to use, among other non-functional requirements (NFRs). Therefore, manufacturers should guarantee that their mobile apps achieve high-quality levels. A good strategy is to include software testing and quality assurance activities during the whole life cycle of such solutions.  Problem: Systematically warranting NFRs is not an easy task for any software product. Software engineers must take important decisions before adopting testing techniques and automation tools to support such endeavors.  Proposal: To provide to the software engineers with a broad overview of existing dynamic techniques and automation tools for testing mobile apps regarding NFRs.  Methods: We planned and conducted a Systematic Mapping Study (SMS) following well-established guidelines for executing secondary studies in software engineering.  Results: We found 56 primary studies and characterized their contributions based on testing strategies, testing approaches, explored mobile platforms, and the proposed tools.  Conclusions: The characterization allowed us to identify and discuss important trends and opportunities that can benefit both academics and practitioners. },
   author = {Misael C. Júnior and Domenico Amalfitano and Lina Garcés and Anna Rita Fasolino and Stevão A. Andrade and Márcio Delamaro},
   doi = {10.1145/3507903},
   issn = {0360-0300},
   journal = {ACM Computing Surveys},
   month = {1},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Dynamic Testing Techniques of Non-Functional Requirements in Mobile Apps: A Systematic Mapping Study},
   year = {2022},
}
@article{,
   abstract = {Container applications are complex in nature as they are a collection of micro-services running in sync to achieve the application's desired functionality. This application build structure makes the Non-Functional characteristics of the container applications more relevant and a focus area of testing for the application. In this paper, the authors have tried to investigate all the possible non-functional areas of container-based application. The paper further delves into the nuances of non-functional testing in general which would become an integral part of the Non-Functional Testing (NFT) of container-based applications.},
   author = {Shadab Alam Siddiqui},
   doi = {10.17577/IJERTV9IS070290},
   issue = {07},
   journal = {International Journal of Engineering Research and},
   month = {7},
   publisher = {ESRSA Publications Pvt. Ltd.},
   title = {Non-Functional Characteristics and NFT Testing of Container Applications},
   volume = {V9},
   year = {2020},
}
@article{Afzal2009,
   abstract = {Search-based software testing is the application of metaheuristic search techniques to generate software tests. The test adequacy criterion is transformed into a fitness function and a set of solutions in the search space are evaluated with respect to the fitness function using a metaheuristic search technique. The application of metaheuristic search techniques for testing is promising due to the fact that exhaustive testing is infeasible considering the size and complexity of software under test. Search-based software testing has been applied across the spectrum of test case design methods; this includes white-box (structural), black-box (functional) and grey-box (combination of structural and functional) testing. In addition, metaheuristic search techniques have also been applied to test non-functional properties. The overall objective of undertaking this systematic review is to examine existing work into non-functional search-based software testing (NFSBST). We are interested in types of non-functional testing targeted using metaheuristic search techniques, different fitness functions used in different types of search-based non-functional testing and challenges in the application of these techniques. The systematic review is based on a comprehensive set of 35 articles obtained after a multi-stage selection process and have been published in the time span 1996-2007. The results of the review show that metaheuristic search techniques have been applied for non-functional testing of execution time, quality of service, security, usability and safety. A variety of metaheuristic search techniques are found to be applicable for non-functional testing including simulated annealing, tabu search, genetic algorithms, ant colony methods, grammatical evolution, genetic programming (and its variants including linear genetic programming) and swarm intelligence methods. The review reports on different fitness functions used to guide the search for each of the categories of execution time, safety, usability, quality of service and security; along with a discussion of possible challenges in the application of metaheuristic search techniques. © 2009 Elsevier B.V. All rights reserved.},
   author = {Wasif Afzal and Richard Torkar and Robert Feldt},
   doi = {10.1016/J.INFSOF.2008.12.005},
   issn = {09505849},
   issue = {6},
   journal = {Information and Software Technology},
   keywords = {Non-functional system properties,Search-based software testing,Systematic review},
   month = {6},
   pages = {957-976},
   title = {A systematic review of search-based testing for non-functional system properties},
   volume = {51},
   year = {2009},
}
@misc{,
   title = {"Testing" and "non-functional" | Mendeley},
   url = {https://www.mendeley.com/search/?page=1&query=%22Testing%22%20and%20%20%22non-functional%22&sortBy=relevance},
   year = {2009},
}
@misc{,
   title = {Microservices Architecture},
   url = {https://publications.opengroup.org/w169},
   year = {2016},
}
@article{Karthikeyan2012,
   abstract = {The software productivity and quality can be improved by the systematic reuse of software. Reuse and reusability are the two important factors in the software development. Reusability is a prime principle in service oriented architecture. For realizing the reuse of service, the adequacy of ability of service reuse should be properly quantified. The quantification of reuse needs a suite of metrics and many metrics are also available. Each research and publication focuses on different qualities of service to propose a set of metrics for service reusability. This paper studies the service reusability to understand the concept and survey the reusability metrics. The purpose of the paper is exploring the available service reusability metrics for further research.},
   author = {T Karthikeyan and J Geetha - International Journal of Information and undefined 2012},
   doi = {10.5815/ijitcs.2012.05.04},
   journal = {mecs-press.net},
   pages = {25-31},
   title = {A Study and Critical Survey on Service Reusability Metrics},
   volume = {5},
   url = {http://www.mecs-press.net/ijitcs/ijitcs-v4-n5/IJITCS-V4-N5-4.pdf},
   year = {2012},
}
@article{Ciavotta2017,
   abstract = {In recent years a considerable effort has been spent by research and industrial communities in the digitalization of production environments with the main objective of achieving a new automation paradigm, more flexible, responsive to changes, and safe. This paper presents the architecture, and discusses the benefits, of a distributed middleware prototype supporting a new generation of smart-factory-enabled applications with special attention paid to simulation tools. Devised within the scope of MAYA EU project, the proposed platform aims at being the first solution capable of empowering shop-floor Cyber-Physical-Systems (CPSs), providing an environment for their Digital Twin along the whole plant life-cycle. The platform implements a microservice IoT-Big Data architecture supporting the distributed publication of multidisciplinary simulation models, managing in an optimized way streams of data coming from the shop-floor for real-digital synchronization, ensuring security and confidentiality of sensible data.},
   author = {Michele Ciavotta and Marino Alge and Silvia Menato and Diego Rovere and Paolo Pedrazzoli},
   doi = {10.1016/J.PROMFG.2017.07.197/A_MICROSERVICE_BASED_MIDDLEWARE_FOR_THE_DIGITAL_FACTORY.PDF},
   issn = {23519789},
   journal = {Procedia Manufacturing},
   keywords = {Cyber Physical Systems,Data Stream Analysis,Industrie 4.0,Microservices,Real-digital synchronization,Smart Factory},
   pages = {931-938},
   publisher = {Elsevier B.V.},
   title = {A Microservice-based Middleware for the Digital Factory},
   volume = {11},
   year = {2017},
}
@article{Ortiz2022,
   abstract = {Major advances in telecommunications and the Internet of Things have given rise to numerous smart city scenarios in which smart services are provided. What was once a dream for the future has now become reality. However, the need to provide these smart services quickly, efficiently, in an interoperable manner and in real time is a cutting-edge technological challenge. Although some software architectures offer solutions in this area, these are often limited in terms of reusability and maintenance by independent modules —involving the need for system downtime when maintaining or evolving, as well as by a lack of standards in terms of the interoperability of their interface. In this paper, we propose a fully reusable microservice architecture, standardized through the use of the Web of things paradigm, and with high efficiency in real-time data processing, supported by complex event processing techniques. To illustrate the proposal, we present a fully reusable implementation of the microservices necessary for the deployment of the architecture in the field of air quality monitoring and alerting in smart ports. The performance evaluation of this architecture shows excellent results.},
   author = {Guadalupe Ortiz and Juan Boubeta-Puig and Javier Criado and David Corral-Plaza and Alfonso Garcia-de-Prado and Inmaculada Medina-Bulo and Luis Iribarne},
   doi = {10.1016/J.CSI.2021.103604},
   issn = {0920-5489},
   journal = {Computer Standards & Interfaces},
   keywords = {Complex event processing,Internet of things,Microservices,Smart ports,Web of things},
   month = {4},
   pages = {103604},
   publisher = {North-Holland},
   title = {A microservice architecture for real-time IoT data processing: A reusable Web of things approach for smart ports},
   volume = {81},
   year = {2022},
}
@misc{,
   title = {Microservice Testing Approaches: A Systematic Literature Review | International Journal of Integrated Engineering},
   url = {https://publisher.uthm.edu.my/ojs/index.php/ijie/article/view/3856},
   year = {2019},
}
@inproceedings{Essebaa2020,
   abstract = {Software Testing is a process of investigating a software product to identify possible mismatches between expected and implemented system requirements. One of the main motivations of software testing is to ensure the correctness of a software system. Thus, due to the continuous quest for better quality of software system, the efforts spent on testing phase are enormous and costly toward both the user and the developer. Many tools and approaches were proposed to solve this problem. Model-Based Testing (MBT) is one of the new technologies to meet the challenges imposed on software testing. Compared to traditional testing methods, MBT is able to manage and accomplish testing tasks in a cheaper and more efficient way. MBT relies on models of the system under test and/or its environment to derive test cases for the system. Models can be used in different manners and for different purposes such as improving specifications quality, code generation, and test generation. In this paper, we use models as the core element of our approach that aims to generate test cases for the different abstraction levels of MDA development life cycle. Our approach uses UML diagrams and Semantic Business Vocabulary and Business Rules (SBVR) to model MDA levels then we propose transformations to generate test cases in order to evaluate if the generated code fulfills system requirements. We aim to implement transformations in an Eclipse plugin.},
   author = {Imane Essebaa and Salima Chantit and Mohammed Ramdani},
   doi = {10.1007/978-3-030-53187-4_66},
   isbn = {9783030531867},
   issn = {18761119},
   journal = {Lecture Notes in Electrical Engineering},
   keywords = {Model driven architecture,Model transformations,Model-based testing,Test generation},
   pages = {600-609},
   publisher = {Springer},
   title = {Model-based testing from model driven architecture: A novel approach for automatic test cases generation},
   volume = {684 LNEE},
   year = {2020},
}
@article{Ashfaq2021,
   abstract = {Natural Language (NL) is the root cause of ambiguity in the SRS document. The quality of the software development process can be improved by mitigating the risk with the use of semantically controlled representation. A possible solution to handle ambiguity can be the use of a mathematical formal logic representation in place of NL to capture software requirements. However, the use of formal logic is a complex task. A wrongly written formal logic will be difficult to handle and it will create serious problems in later stages of software development. Furthermore, stakeholders are typically not able to understand mathematical logic. Hence, this solution does not look feasible. Another possible way of addressing above discussed ambiguity problem is the use of controlled natural languages (CNL). It can work as a bridge between NL and formal representation. Since Requirement Analysis is based on communication and the analyst’s experience, it can be modeled up to a certain limit. This limit gives birth to controlled language. If the document is written in a controlled language, it will be feasible for the development team to use a simpler and less costly linguistic tool. The CNLs are syntactically unambiguous, semantically consistent and, controlled. Several CNLs could be found in literature such as ACE, PENG, CPL, Formalized-English, and Semantics of Business Vocabulary and Rules (SBVR), etc. We aim to use an SBVR based CNL to capture stakeholder’s requirements and prepare an SRS document using SBVR. Such software requirements will not only be syntactically clear but also semantically consistent.},
   author = {Fariha Ashfaq and Imran Sarwar Bajwa},
   doi = {10.1007/s10515-021-00291-0},
   issn = {15737535},
   issue = {2},
   journal = {Automated Software Engineering},
   keywords = {Ambiguity Resolution,SBVR,Semantic annotation,Software requirements},
   month = {11},
   publisher = {Springer},
   title = {Natural language ambiguity resolution by intelligent semantic annotation of software requirements},
   volume = {28},
   year = {2021},
}
@inproceedings{Bachras2020,
   abstract = {As microservices become one of the predominant architectural styles for distributed enterprise computing, there is a need to devise frameworks which allow for the goal driven composition and coordination of such highly granular service components. Even though a number of service composition and orchestration techniques have been proposed over the past decade, these do not take into account stakeholders' intents as well as data, control, and temporal interdependencies between actions microservices can perform. In this paper, we present extensions to goal models with respect to data, logical and temporal dependencies exhibited between tasks and actions among microservices, and we propose a framework based on a graph transformation approach which, when applied to the extended goal models, can yield service invocation plans that achieve the desired requirements and constraints denoted by the specific goal models being considered.},
   author = {Michalis Bachras and Kostas Kontogiannis},
   doi = {10.1109/EDOC49727.2020.00014},
   journal = {Proceedings - 2020 IEEE 24th International Enterprise Distributed Object Computing Conference, EDOC 2020},
   title = {Goal Modelling Meets Service Choreography: A Graph Transformation Approach},
   year = {2020},
}
@article{Rodrigues2016,
   abstract = {The gamification software development gave emphasis to the role played by the users to test and improve the software. This study presents a framework for software gamified in e-banking, taking a users' groups and a qualitative research approach, to check the users' design preferences in five cases of banking software gamified (Futebank, Dreams, Galaxy, Olympics, and Warrants). After software presentation, and usage experience, 53 participants, responses to a survey with six open questions. The data were analyzed through a text semantic software, to detect and classify lexical items in, accordance, with standard of software quality characteristics and user experiences. Two primary categories were identified, as well five dimensions in each element and characteristic categories. The results show five characteristic dimensions (design, appearance, functionality, rules, and objectives) and five element dimension (game, product, security, process, and information). These findings provide a framework for web designers and e-business, highlighting the most important software features when dealing with serious applications with game design. It adds value to the current literature on understanding the customer relationship with the game and the financial product, identifying new dimensions (game and product) on the approach of thinking and design gamification in e-banking. Our finding contributes to a better understanding of key elements and characteristics in e-banking software design and has important practical implications for software development and marketing practices. Thoughts on the users' software design preferences identified, should propel increase adoption and attractiveness of online banking.},
   author = {Luís Filipe Rodrigues and Carlos J. Costa and Abílio Oliveira},
   doi = {10.1016/j.chb.2016.04.035},
   issn = {07475632},
   journal = {Computers in Human Behavior},
   keywords = {E-banking,Gamification,Software design games,Software quality,Users experience},
   month = {9},
   pages = {620-634},
   publisher = {Elsevier Ltd},
   title = {Gamification: A framework for designing software in e-banking},
   volume = {62},
   year = {2016},
}
