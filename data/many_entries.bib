@article{Navas-Loro2019,
   abstract = {This paper introduces ContractFrames, a framework able to translate natural language texts referring to the different events related to the status of a purchase contract to logic clauses from a legal reasoning system called PROLEG. Diverse frames and rules have been developed for the extraction and storage of this event-centric information before its conversion to logic clauses. Our framework uses natural language tools and rules to extract relevant information, store it in the form of frames, and return the logic clauses of the input text. Also an ontology, called the Contract Workflow Ontology, has been developed to represent all the relevant information of the events related to a contract. The framework has been tested in a synthetic dataset, and showed promising results.},
   author = {María Navas-Loro and Ken Satoh and Víctor Rodríguez-Doncel},
   doi = {10.1007/978-3-030-31605-1_9/COVER},
   isbn = {9783030316044},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Contract life-cycle,Legal NLP,Legal ontology,PROLEG},
   pages = {101-114},
   publisher = {Springer},
   title = {ContractFrames: Bridging the Gap Between Natural Language and Logics in Contract Law},
   volume = {11717 LNAI},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-31605-1_9},
   year = {2019},
}
@article{Maynard2020,
   abstract = {Traditionally, there has been a disconnect between custom-built applications used to solve real-world information extraction problems in industry, and automated learning-based approaches developed in academia. Despite approaches such as transfer-based learning, adapting these to more customised solutions where the task and data may be different, and where training data may be largely unavailable, is still hugely problematic, with the result that many systems still need to be custom-built using expert hand-crafted knowledge, and do not scale. In the legal domain, a traditional slow adopter of technology, black box machine learning-based systems are too untrustworthy to be widely used. In industrial settings, the fine-grained highly specialised knowledge of human experts is still critical, and it is not obvious how to integrate this into automated classification systems. In this paper, we examine two case studies from recent work combining this expert human knowledge with automated NLP technologies.},
   author = {Diana Maynard and Adam Funk},
   doi = {10.1007/978-3-030-58323-1_1/COVER},
   isbn = {9783030583224},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Information extraction,Natural language processing,Ontologies},
   pages = {3-10},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Combining expert knowledge with NLP for specialised applications},
   volume = {12284 LNAI},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-58323-1_1},
   year = {2020},
}
@article{Soavi2020,
   abstract = {We are interested in semi-automating the process of generating a formal specification from a legal contract in natural language text form. Towards this end, we present a tool, named ContracT, that annotates legal contract text using an ontology for legal contracts. In the last part of the paper, we present results from a preliminary empirical evaluation of the tool that provided encouraging results in identifying contract concepts in text and discuss critical points to be tackled in future studies.},
   author = {Michele Soavi and Nicola Zeni and John Mylopoulos and Luisa Mich},
   doi = {10.1007/978-3-030-63479-7_9/COVER},
   isbn = {9783030634780},
   issn = {18651356},
   journal = {Lecture Notes in Business Information Processing},
   keywords = {Legal contract,Ontology for contracts,Semantic annotation,Structure model},
   pages = {124-137},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {ContracT - from Legal Contracts to Formal Specifications: Preliminary Results},
   volume = {400},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-63479-7_9},
   year = {2020},
}
@article{Goossens2022,
   abstract = {Vanbreda Risk & Benefits, a large Belgian insurance broker and risk consultant, allocates a substantial amount of time and resources to answer contract related questions from customers. This requires employees to manually search the relevant parameters in the contracts. In this paper, a solution is proposed and evaluated that automatically extracts insurance parameters from contracts using regular expressions and Natural Language Processing. While Natural Language Processing has been used in insurance for optimising premiums, detecting fraudulent claims, or underwriting, limited work has been done regarding parameter extraction. The proposed solution has been developed on 127 different contracts and two different contract types in terms of accuracy and time performance. Moreover, the automatic parameter extraction has been compared to manual parameter extraction. We conclude that automatic parameter extraction using regular expressions achieves better accuracy than manual extraction on top of being significantly faster, allowing Vanbreda Risk & Benefits to invest more time into providing better customer service.},
   author = {Alexandre Goossens and Laure Berth and Emilia Decoene and Ziboud Van Veldhoven and Jan Vanthienen},
   doi = {10.1007/978-3-031-04216-4_3/COVER},
   isbn = {9783031042157},
   issn = {18651356},
   journal = {Lecture Notes in Business Information Processing},
   keywords = {Insurance industry,NLP,Regular expressions,Service automation},
   pages = {27-38},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Automatically Extracting Insurance Contract Knowledge Using NLP},
   volume = {444 LNBIP},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-04216-4_3},
   year = {2022},
}
@article{Thonssen2013,
   abstract = {Contract Management becomes increasingly important for companies and public administrations alike. Obligations and liabilities are described in contract clauses that are often buried in documents of a hundred pages and more. Although commercial Contract Management Systems (CMS) are available, with a few exceptions relevant information has to be extracted manually which is time consuming and error prone. But even if information extraction is automated and contracts are managed using a CMS, dealing with obligations is still a challenge. Whereas the CMSs deal well with time triggered obligations like periodical payments by setting up corresponding workflows, they fail to trigger obligations based on events, as this knowledge is out of the systems' scope. We introduce an approach to fill the gap as we relate information about the obligations managed in a CMS with background knowledge modelled in an ontology. The ontology is a formal representation of an enterprise architecture extended by top-level concepts. Motivating scenario for the approach is the contract management of a large company. For proof of concept a prototype has been developed. © Springer-Verlag Berlin Heidelberg 2013.},
   author = {Barbara Thönssen and Jonas Lutz},
   doi = {10.1007/978-3-642-54105-6_23/COVER},
   isbn = {9783642541049},
   issn = {18650929},
   journal = {Communications in Computer and Information Science},
   keywords = {Contract Management,Enterprise Architecture,Enterprise Ontology,Information Extraction,Obligation Management,Risk Management},
   pages = {337-349},
   publisher = {Springer Verlag},
   title = {Semantically Enriched Obligation Management: An Approach for Improving the Handling of Obligations Represented in Contracts},
   volume = {415},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-54105-6_23},
   year = {2013},
}
@article{Andrew2018,
   abstract = {In recent years, the journalists and computer sciences speak to each other to identify useful technologies which would help them in extracting useful information. This is called”computational Journalism”. In this paper, we present a method that will enable the journalists to automatically identifies and annotates entities such as names of people, organizations, role and functions of people in legal documents; the relationship between these entities are also explored. The system uses a combination of both statistical and rule based technique. The statistical method used is Conditional Random Fields and for the rule based technique, document and language specific regular expressions are used.},
   author = {Judith Jeyafreeda Andrew and Xavier Tannier},
   doi = {10.18653/V1/W18-2401},
   issn = {0736587X},
   journal = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
   pages = {1-8},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {Automatic Extraction of Entities and Relation from Legal Documents},
   year = {2018},
}
@article{Aejas2022,
   abstract = {A contract is a binding document between two or more parties for executing any kind of activities that are defined clearly in its clauses. The parties who are assigned specific roles and rights must review and act accordingly until the contract expires. Automation of contract management is an emerging topic in various fields such as supply chain and legal domains. Recognition of various entities related to the document and their extraction are the main tasks to be performed for automating the contract management process. Named Entity Recognition is a well-known task in NLP that deals with the recognition of named entities such as person and date from a text. But traditional NER models perform poorly for domain-specific entity recognition and extraction such as legal and contract documents. For domain-specific entity recognition, we need to train the model with a dataset from the specific domain. In this paper, various approaches in the task of entity extraction from legal and contract documents are reviewed and discussed with the aim of proposing a new automated method for contract management using NLP.},
   author = {Bajeela Aejas and Abdelaziz Bouras and Abdelhak Belhi and Houssem Gasmi},
   doi = {10.1007/978-981-16-2102-4_69/COVER},
   isbn = {9789811621017},
   issn = {23673389},
   journal = {Lecture Notes in Networks and Systems},
   keywords = {Contracts,NER,Natural Language Processing,Supply Chain Management},
   pages = {763-771},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {A Review of Contract Entity Extraction},
   volume = {217},
   url = {https://link.springer.com/chapter/10.1007/978-981-16-2102-4_69},
   year = {2022},
}
@article{Ferneda2012,
   abstract = {In order to avoid ambiguity and to ensure, as far as possible, a strict interpretation of law, legal texts usually define the specific lexical terms used within their discourse by means of normative rules. With an often large amount of rules in effect in a given domain, extracting these definitions manually would be a costly undertaking. This paper presents an approach to cope with this problem based in a variation of an automated technique of natural language processing of Brazilian Portuguese texts. For the sake of generality, the proposed solution was developed to address the more general problem of building a glossary from domain specific texts that contain definitions amongst their content. This solution was applied to a corpus of texts on the telecommunications regulations domain and the results are reported. The usual pipeline of natural language processing has been followed: preprocessing, segmentation, and part-of-speech tagging. A set of feature extraction functions is specified and used along with reference glossary information on whether or not a text fragment is a definition, to train a SVM classifier. At last, the definitions are extracted from the texts and evaluated upon a testing corpus, which also contains the reference glossary annotations on definitions. The results are then discussed in light of other definition extraction techniques. © 2012 Springer-Verlag.},
   author = {Edilson Ferneda and Hércules Antonio Do Prado and Augusto Herrmann Batista and Marcello Sandi Pinheiro},
   doi = {10.1007/978-3-642-31137-6_48/COVER},
   isbn = {9783642311369},
   issn = {03029743},
   issue = {PART 3},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Definition extraction,Information extraction,Natural Language Processing},
   pages = {631-646},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Extracting definitions from Brazilian legal texts},
   volume = {7335 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-31137-6_48},
   year = {2012},
}
@article{Das2020,
   abstract = {Artificial Intelligence (AI) as an emerging technology, facilitates the mapping of human intelligence with computerized mechanism. This capability of thinking like human being provides computer with boundless potential for user interaction and prediction of logical solution (i.e. decision) for any particular event using its knowledge base obtained through previous experience. To generate faster solution, Artificial Intelligence (AI) can be utilized in multivariate service sectors, which are accustomed to generate logical solution in slow pace due to various constraints like, manpower, fund, infrastructure, policy paralysis, etc. In a developing country like India, which is striving hard to have a constant growth rate, application of Artificial Intelligence (AI) based tools have wide window open in various service sectors like judiciary, healthcare, business, education, agriculture, banking, etc. Generally in case of judiciary, end users have to wait for a long time to get their desired justice which directly affects their efficiency, contribution and well-being in this society. Using the concept of Artificial Intelligence (AI), Legal knowledge based tools may accelerate the service delivery of legal professionals from typical searching of related case journals to extraction of precise information in a customized manner. This paper focuses on legal representation of legal knowledge base to study the changing trends using Artificial Intelligence (AI).},
   author = {Tanaya Das and Abhishek Roy and A. K. Majumdar},
   doi = {10.1007/978-3-030-38040-3_73/COVER},
   issn = {23674520},
   journal = {Lecture Notes on Data Engineering and Communications Technologies},
   keywords = {Artificial Intelligence,Legal knowledge base,Ontology},
   pages = {647-653},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {A study on legal knowledge base creation using artificial intelligence and ontology},
   volume = {46},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-38040-3_73},
   year = {2020},
}
@article{Bansal2019,
   abstract = {The Amount of legal information that is being produced on a daily basis in the law courts is increasing enormously and nowadays this information is available in electronic form also. The application of various machine learning and deep learning methods for processing of legal documents has been receiving considerate attention over the last few years. Legal document classification, translation, summarization, contract review, case prediction and information retrieval are some of the tasks that have received concentrated efforts from the research community. In this survey, we have performed a comprehensive study of various deep learning methods applied in the legal domain and classified various legal tasks into three broad categories, viz. legal data search, legal text analytics and legal intelligent interfaces. The proposed study suggests that deep learning models like CNNs, RNNs, LSTM and GRU, and multi-task deep learning models are being used actively to solve wide variety of legal tasks and are giving state-of-the-art performance.},
   author = {Neha Bansal and Arun Sharma and R. K. Singh},
   doi = {10.1007/978-3-030-19823-7_31/TABLES/1},
   isbn = {9783030198220},
   issn = {18684238},
   journal = {IFIP Advances in Information and Communication Technology},
   keywords = {Classification,Deep learning,Legal text analytics,Prediction systems},
   pages = {374-381},
   publisher = {Springer New York LLC},
   title = {A Review on the Application of Deep Learning in Legal Domain},
   volume = {559},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-19823-7_31},
   year = {2019},
}
@article{Isemann2013,
   abstract = {Tasks and difficulties inherent in the largely open problem of temporal information extraction from legal text are outlined. We demonstrate the efficacy of tools and concepts available "off-the-shelf" and suggest refinements for such applications. In particular, the frequent references between regulatory texts have to be addressed as a separate named entity recognition task that bears relevance to an analysis of the temporal ordering of legislation. A regular expression-based approach as a robust first step towards addressing this problem is tested. © 2013 Springer-Verlag.},
   author = {Daniel Isemann and Khurshid Ahmad and Tim Fernando and Carl Vogel},
   doi = {10.1007/978-3-642-41278-3_60},
   isbn = {9783642412776},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {legal information extraction,named entities,temporality},
   pages = {497-504},
   title = {Temporal dependence in legal documents},
   volume = {8206 LNCS},
   year = {2013},
}
@article{Navas-Loro2021,
   abstract = {In this paper we present a suite of tools named TimeLex, that includes different systems able to process temporal information from legal texts. The first tool, called lawORdate, helps preprocessing legal references in texts in Spanish that can be misleading when trying to find dates in texts. The second one, Añotador, is a temporal tagger (this is, a tool that finds temporal expressions, such as dates or durations) that identifies temporal expressions in texts and provides a standard value for each of them. Finally, a third tool, called WhenTheFact, extracts relevant events from judgments, allowing a full processing of the temporal dimension of this kind of texts, and being a first step towards the complete temporal information processing in the legal domain.},
   author = {María Navas-Loro and Víctor Rodríguez-Doncel},
   doi = {10.1007/978-3-030-89811-3_18/COVER},
   isbn = {9783030898106},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Events,Legal texts,Temporal expressions,Timeline generation},
   pages = {260-266},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {TimeLex: A Suite of Tools for Processing Temporal Information in Legal Texts},
   volume = {13048 LNAI},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-89811-3_18},
   year = {2021},
}
@article{Patil2021,
   abstract = {Information retrieval is the process of extracting a pertinent set of facts from a text or a document. The documents are of unstructured format, and thus, information retrieval techniques aim at organizing this data. Named Entity Recognition is one of the information retrieval techniques which classifies a particular word or a phrase in its appropriate class. NER can thus, also be used in extracting entities from legal documents, which would help in providing an effective way to represent these documents. This would reduce the task of a lawyer scrutinizing the document, multiple times, to look for the same set of information. NER systems can be developed with different approaches, one of which is utilizing an NLP library. However, these pretrained NLP libraries may or may not be suitable for a particular use case. Hence, in this paper, we depict an approach to analyze rental documents by custom training spaCy NLP library for tagging named entities such as a person, address, amount, date, etc. The system will provide an interface for the user to upload rent documents, and the result analysis will be stored for quick insights into the document.},
   author = {Chinmay Patil and Sushant Patil and Komal Nimbalkar and Dhiraj Chavan and Sharmila Sengupta and Devesh Rajadhyax},
   doi = {10.1007/978-981-15-7062-9_38/COVER},
   isbn = {9789811570612},
   issn = {21903026},
   journal = {Smart Innovation, Systems and Technologies},
   keywords = {Annotation,Classifier,Cognie,Named entity recognition,spaCy},
   pages = {389-397},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Named Entity Recognition for Rental Documents Using NLP},
   volume = {196},
   url = {https://link.springer.com/chapter/10.1007/978-981-15-7062-9_38},
   year = {2021},
}
@article{Nguyen2022,
   abstract = {General Data Protection Regulation (GDPR) is an important framework for data protection that applies to all European Union countries. Recently, DAPRECO knowledge base (KB) which is a repository of if-then rules written in LegalRuleML as a formal logic representation of GDPR has been introduced to assist compliance checking. DAPRECO KB is, however, constructed manually and the current version does not cover all the articles in GDPR. Looking for an automated method, we present our machine translation approach to obtain a semantic parser translating the regulations in GDPR to their logic representation on DAPRECO KB. We also propose a new version of GDPR Semantic Parsing data by splitting each complex regulation into simple subparagraph-like units and re-annotating them based on published data from DAPRECO project. Besides, to improve the performance of our semantic parser, we propose two mechanisms: Sub-expression intersection and PRESEG. The former deals with the problem of duplicate sub-expressions while the latter distills knowledge from pre-trained language model BERT. Using these mechanisms, our semantic parser obtained a performance of 60.49% F1 in sub-expression level, which outperforms the baseline model by 5.68%.},
   author = {Minh Phuong Nguyen and Thi Thu Trang Nguyen and Vu Tran and Ha Thanh Nguyen and Le Minh Nguyen and Ken Satoh},
   doi = {10.1007/978-3-031-21743-2_35/COVER},
   isbn = {9783031217425},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {GDPR,Logic representation,Semantic parsing},
   pages = {442-454},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Learning to Map the GDPR to Logic Representation on DAPRECO-KB},
   volume = {13757 LNAI},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-21743-2_35},
   year = {2022},
}
@article{Azevedo2022,
   abstract = {Products, services, among many other things in life have a quality standard, are inclusive, or do not harm customers. Regulations required from their manufacturers or providers make it possible. This type of requirement also exists in the finance sector. Governments, international agencies, or civil institutions are responsible for creating, applying, and inspecting these regulations. Regulators from all spheres (federal, state, and municipal) constantly demand changes in the finance sector to meet current needs adequately. This paper presents the constant evolution of a banking compliance application in Brazil. It aims to classify the relevance or irrelevance of regulatory documents published by more than 100 Brazilian regulators, affecting the businesses of more than 40 departments of Banco do Brasil. The application uses a hybrid strategy, combining machine learning and rules for a binary classification challenge involving each company department. This work also presents a particular type of corpus imbalance called The Imbalance Within Class.},
   author = {Rafael Faria de Azevedo and Tiago Nunes Silva and Henrique Tibério Brandão Vieira Augusto and Paulo Oliveira Sampaio Reis and Isadora Bastos Chaves and Samara Beatriz Naka de Vasconcellos and Liliany Aparecida dos Anjos Pereira and Mauro Melo de Souza Biccas and André Luiz Monteiro and Alexandre Rodrigues Duarte},
   doi = {10.1007/978-3-030-98305-5_13/COVER},
   isbn = {9783030983048},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Banking compliance,Finance sector regulation,Imbalance within class,Machine Learning,Rules},
   pages = {137-147},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Banking Regulation Classification in Portuguese},
   volume = {13208 LNAI},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-98305-5_13},
   year = {2022},
}
@article{Batarseh2022,
   abstract = {In this manuscript, we propose an alternative to conventional policy making procedures. The presented policy pipeline leverages intelligent methods that factor for causal relations and economic factors to produce explainable outcomes. Attribution-based methods for analyzing the effects of technology policies are deployed for all American states. Legal codes are analyzed using natural language processing methods to detect similarity, and K-nearest neighbor (Knn) is applied to group laws by influence on state’s technological descriptors, such as broadband and internet use. Additionally, we classify which laws are excitatory and which ones are inhibitory regarding the overall quality of technology services. Our pipeline allows for explaining the ‘goodness of a policy’ using task-based and end-to-end learning; a notion that has not been explored prior. Data are collected from multiple state statutes, intelligent models are developed, experimental work is performed, and the results are presented and discussed.},
   author = {Feras A. Batarseh and Dominick Perini and Qasim Wani and Laura Freeman},
   doi = {10.1007/978-3-031-08421-8_43/COVER},
   isbn = {9783031084201},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Attributions,Data-driven law,Public policy,XAI},
   pages = {624-637},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Explainable Artificial Intelligence for Technology Policy Making Using Attribution Networks},
   volume = {13196 LNAI},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-08421-8_43},
   year = {2022},
}
@article{Benthem2019,
   abstract = {We identify a pervasive contrast between implicit and explicit stances in logical analysis and system design. Implicit systems change received meanings of logical constants and sometimes also the notion of consequence, while explicit systems conservatively extend classical systems with new vocabulary. We illustrate the contrast for intuitionistic and epistemic logic, then take it further to information dynamics, default reasoning, and other areas, to show its wide scope. This gives a working understanding of the contrast, though we stop short of a formal definition, and acknowledge limitations and borderline cases. Throughout we show how awareness of the two stances suggests new logical systems and new issues about translations between implicit and explicit systems, linking up with foundational concerns about identity of logical systems. But we also show how a practical facility with these complementary working styles has philosophical consequences, as it throws doubt on strong philosophical claims made by just taking one design stance and ignoring alternative ones. We will illustrate the latter benefit for the case of logical pluralism and hyper-intensional semantics.},
   author = {Johan van Benthem},
   doi = {10.1007/S10992-018-9485-Y},
   issn = {15730433},
   issue = {3},
   journal = {Journal of Philosophical Logic},
   keywords = {Explicit,Implicit,Logic,Modality,Translation,Vocabulary},
   month = {6},
   pages = {571-601},
   publisher = {Springer Netherlands},
   title = {Implicit and Explicit Stances in Logic},
   volume = {48},
   year = {2019},
}
@article{Wang2022,
   abstract = {Lattice theory has close connections with modal logic via algebraic semantics and lattices of modal logics. However, one less explored direction is to view lattices as relational structures based on partial orders, and study the modal logic over them. In this paper, following the earlier steps of Burgess and van Benthem in the 1980s, we use the basic tense logic and its nominal extensions with binary modalities of infimum and supremum to talk about lattices via standard Kripke semantics. As the main results, we obtain a series of complete axiomatizations of lattices, (un)bounded lattices over partial orders or strict preorders. In particular, we solve an axiomatization problem left open by Burgess (1984).},
   author = {Xiaoyang Wang and Yanjing Wang},
   doi = {10.1007/978-3-031-15298-6_5/COVER},
   isbn = {9783031152979},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {completeness,hybrid logic,lattice,modal logic,polyadic modal logic,step-by-step,tense logic},
   pages = {70-87},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Tense Logics over Lattices},
   volume = {13468 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-15298-6_5},
   year = {2022},
}
@article{Libal2021,
   abstract = {The GDPR is one of many legal texts which can greatly benefit from the support of automated reasoning. Since its introduction, efforts were made to formalize it in order to support various automated operations. Nevertheless, widespread and efficient automated reasoning over the GDPR has not yet been achieved. In this paper, a tool called the NAI suite is being used in order to annotate article 13 of the GDPR. The annotation results in a fully formalized version of the article, which deals with the transparency requirements of data collection and processing. Automated compliance checking is then being demonstrated via several simple queries. By using the public API of the NAI suite, arbitrary tools can use this procedure to support trust management and GDPR compliance functions.},
   author = {Tomer Libal},
   doi = {10.1007/978-3-030-73959-1_1/COVER},
   isbn = {9783030739584},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Automated reasoning,Compliance checking,Data protection},
   pages = {3-19},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Towards Automated GDPR Compliance Checking},
   volume = {12641 LNAI},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-73959-1_1},
   year = {2021},
}
@article{Mauritz2021,
   abstract = {The fourth industrial revolution is driven by Software-enabled automation. To fully realize the potential of this digital transformation in a way that is beneficial to society, automation needs to become programmable by domain experts—the vision being a Software-assisted increase in productivity instead of replacing workers with Software. While domain experts, e.g., workers in production, typically have extensive experience with processes and workflows involving cyber-physical systems, e.g., production machines, they have little to no knowledge of programming and formal logic. In this paper, we present a framework for expressing executable rules in the context of a cyber-physical system at the conceptual level, akin to human reasoning, in almost natural sentences (e.g., if a person is within 1 m of the machine then the light will turn red). These requirements are automatically transformed by our framework into formal logic and can be executed and evaluated by a rule engine without additional input by domain experts. The framework is designed in a modular way that enables domain engineering, i.e., the development of new languages for individual application domains, with minimal effort. Only domain-specific entities and predicates (e.g., is within) need to be defined and implemented for a new domain. We demonstrate our framework in a logistics scenario on a shop floor that requires human-machine collaboration.},
   author = {Malte Mauritz and Moritz Roidl},
   doi = {10.1007/978-3-030-89159-6_11/COVER},
   isbn = {9783030891589},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Domain-specific languages,Language model transformation,Language programming,Logistics,Runtime monitoring},
   pages = {162-177},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {From Requirements to Executable Rules: An Ensemble of Domain-Specific Languages for Programming Cyber-Physical Systems in Warehouse Logistics},
   volume = {13036 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-89159-6_11},
   year = {2021},
}
@article{Ran2023,
   abstract = {Since the launch of Ethereum in 2013, the smart contract has been a momentous part of the blockchain systems due to its character of automatic execution. The generation of smart contracts has also attracted extensive attention from the academic community. However, the preparation and generation of smart contracts are still mainly manual so far, which limits the scalability of the smart contracts. In this paper, we put forward a new method to generate smart contracts automatically based on knowledge extraction and Unified Modeling Language (UML), which can significantly accelerate the generation of smart contracts. We will describe this method in more detail based on the logistics supply chain.},
   author = {Peiyun Ran and Mingsheng Liu and Jianwu Zheng and Zakirul Alam Bhuiyan and Jianhua Li and Gang Li and Shiyuan Yu and Lifeng Wang and Song Tang and Peng Zhao},
   doi = {10.1007/978-3-031-28124-2_44/COVER},
   isbn = {9783031281235},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {automatic code generation,blockchain,knowledge extraction,smart contract,unified modeling language},
   pages = {461-474},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Automatic Smart Contract Generation with Knowledge Extraction and Unified Modeling Language},
   volume = {13828 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-28124-2_44},
   year = {2023},
}
@article{Zhang2021,
   abstract = {The objective of this research is to present an innovative technique of extracting and presenting knowledge in construction documents. A construction project can generate a huge number of documents such as contract, correspondences, meeting minutes, quality and...},
   author = {Qiqi Zhang and Zirui Hong and Xing Su},
   doi = {10.1007/978-981-15-8892-1_59},
   isbn = {978-981-15-8892-1},
   journal = {Proceedings of the 24th International Symposium on Advancement of Construction Management and Real Estate},
   pages = {823-837},
   publisher = {Springer, Singapore},
   title = {Content Analysis Based on Knowledge Graph: A Practice on Chinese Construction Contracts},
   url = {https://link.springer.com/chapter/10.1007/978-981-15-8892-1_59},
   year = {2021},
}
@article{Aejas2023,
   abstract = {Identifying and extracting information from contracts is an important task of contract analysis, which is mostly performed manually by lawyers and legal specialists. This manual analysis is a time-consuming, error-prone task. We can overcome this by automating the task of legal entity extraction using the Natural Language Processing (NLP) techniques. For extracting information from the natural language text, we can use popular NLP methods Named Entity Recognition (NER) and relation extraction (RE). Most NER and RE methods rely on machine learning and deep learning to identify relevant entities in natural language text. The main concern in adapting the AI methods for contract element extraction is the scarcity of annotated datasets in the legal field. Aiming at tackling this challenge, we decided to prepare the contract datasets for NER and RE tasks by manually annotating publicly available English contracts. This work is a part of the research aimed at automating the conversion of natural language contracts into Smart Contracts in the blockchain-based Supply Chain context. This paper explains the implementation and comparison of NER models using the deep learning methods BiLSTM and transformer-based BERT for evaluating the dataset.},
   author = {Bajeela Aejas and Abdelhak Belhi and Abdelaziz Bouras},
   doi = {10.1007/978-981-19-7663-6_70/COVER},
   isbn = {9789811976629},
   issn = {23673389},
   journal = {Lecture Notes in Networks and Systems},
   keywords = {Dataset,Deep learning,Legal domain,NER,NLP,RE},
   pages = {751-759},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Toward an NLP Approach for Transforming Paper Contracts into Smart Contracts},
   volume = {579},
   url = {https://link.springer.com/chapter/10.1007/978-981-19-7663-6_70},
   year = {2023},
}
@article{Balbiani2009,
   abstract = {In nowadays applications of temporal deontic logic to the verification of security policies, an issue arises concerning the temporal inheritance of future directed obligations that have net yet been met. We investigate decision procedures for temporal deontic logics that account for this particular interaction between time and obligation. © 2009 Elsevier B.V. All rights reserved.},
   author = {Philippe Balbiani and Jan Broersen and Julien Brunel},
   doi = {10.1016/J.ENTCS.2009.02.030},
   issn = {15710661},
   issue = {C},
   journal = {Electronic Notes in Theoretical Computer Science},
   keywords = {axiomatization,deontic logic,tableaux method,temporal logic},
   month = {3},
   pages = {69-89},
   title = {Decision Procedures for a Deontic Logic Modeling Temporal Inheritance of Obligations},
   volume = {231},
   year = {2009},
}
@article{Khoja2022,
   abstract = {Contracts in business life, and in particular company purchase agreements, often comprise a large number of provisions and are correspondingly long and complex. In practice, it is therefore a great challenge to keep track of their regulatory context and to identify and avoid inconsistencies in such contracts. Against this background, we propose a semi-formal as well as a formal logical modeling of this type of contracts, using decidable first-order theories. We also present the tool ContractCheck, which performs fully automated inconsistency analyses on the considered contracts using Satisfiability Modulo Theories (SMT) solving.},
   author = {Alan Khoja and Martin Kölbl and Stefan Leue and Rüdiger Wilhelmi},
   doi = {10.1007/978-3-031-15077-7_1/COVER},
   isbn = {9783031150760},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {1-23},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Automated Consistency Analysis for Legal Contracts},
   volume = {13255 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-15077-7_1},
   year = {2022},
}
@article{Bonifacio2021,
   abstract = {Contracts play an important role in business management where relationships among different parties are dictated by legal rules. Electronic contracts have emerged mostly due to technological advances and electronic trading between companies and customers. New challenges have then arisen to guarantee reliability among the stakeholders in electronic negotiations. In this scenario, automatic verification of electronic contracts appeared as an imperative support, specially the conflict detection task of multi-party contracts. The problem of checking contracts has been largely addressed in the literature, but there are few, if any, methods and practical tools that can deal with multi-party contracts using a contract language with deontic and dynamic aspects as well as relativizations, over the same formalism. In this work we present an automatic checker for finding conflicts on multi-party contracts modeled by an extended contract language with deontic operators and relativizations. Moreover a well-known case study of sales contract is modeled and automatically verified by our tool. Further, we performed practical experiments in order to evaluate the efficiency of our method and the practical tool.},
   author = {Adilson Luiz Bonifacio and Wellington Aparecido Della Mura},
   doi = {10.1007/S10506-020-09276-Y},
   issn = {15728382},
   issue = {3},
   journal = {Artificial Intelligence and Law},
   keywords = {Conflict detection,Experiments,Multi-party contracts,Relativized contract language},
   month = {9},
   pages = {287-310},
   publisher = {Springer Science and Business Media B.V.},
   title = {Automatically running experiments on checking multi-party contracts},
   volume = {29},
   year = {2021},
}
@article{Castro2007,
   abstract = {In this paper we present a propositional deontic logic, with the goal of using it to specify fault-tolerant systems, and an axiomatization of it. We prove several results about this logic: completeness, soundness, compactness and decidability. The main technique used during the completeness proof is based on standard techniques for modal logics, but it has some new characteristics introduced for dealing with this logic. In addition, the logic provides several operators which appear useful for use in practice, in particular to model fault-tolerant systems and to reason about their fault tolerance properties. © Springer-Verlag Berlin Heidelberg 2007.},
   author = {Pablo F. Castro and T. S.E. Maibaum},
   doi = {10.1007/978-3-540-75292-9_8},
   isbn = {9783540752905},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Deontic logic,Fault tolerance,Modal logic,Software specification,Temporal logic},
   pages = {109-123},
   publisher = {Springer Verlag},
   title = {A complete and compact propositional deontic logic},
   volume = {4711 LNCS},
   year = {2007},
}
@article{Boley2010,
   abstract = {RuleML is a family of languages, whose modular system of XML schemas permits high-precision Web rule interchange. The family's top-level distinction is deliberation rules vs. reaction rules. Deliberation rules include modal and derivation rules, which themselves include facts, queries (incl. integrity constraints), and Horn rules (incl. Datalog). Reaction rules include Complex Event Processing (CEP), Knowledge Representation (KR), and Event-Condition- Action (ECA) rules, as well as Production (CA) rules. RuleML rules can combine all parts of both derivation and reaction rules. This allows uniform XML serialization across all kinds of rules. After its use in SWRL and SWSL, RuleML has provided strong input to W3C RIF on several levels. This includes the use of 'striped' XML as well as the structuring of rule classes into sublanguages with partial mappings between, e.g., Datalog RuleML and RIF-Core, Hornlog RuleML and RIF-BLD, as well as Production RuleML and RIF-PRD. We discuss the rationale and key features of RuleML 1.0 as the overarching specification of Web rules that encompasses RIF RuleML as a subfamily, and takes into account corresponding OASIS, OMG (e.g., PRR, SBVR), and ISO (e.g., Common Logic) specifications. © Her Majesty the Queen in Right of Canada 2010.},
   author = {Harold Boley and Adrian Paschke and Omair Shafiq},
   doi = {10.1007/978-3-642-16289-3_15},
   isbn = {3642162886},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {162-178},
   publisher = {Springer Verlag},
   title = {RuleML 1.0: The overarching specification of web rules},
   volume = {6403 LNCS},
   year = {2010},
}
@article{Zhao2022,
   abstract = {Due to the imbalance between a large number of litigation cases and the number of judicial personnel, many legal documents to be processed greatly increase the burden of legal practitioners. So the intelligent processing of legal documents is especially important. At present, machine learning and deep learning have made great achievements in the intelligent processing of legal documents, including the elements extraction of legal documents, classification of legal documents, generation of legal documents, abstract extraction of legal documents etc. The main aim of this paper is to present a review of legal documents intelligent processing based on deep learning from legal documents representation, elements extraction of legal documents, classification of legal documents, automatic generation of legal documents.},
   author = {Guolong Zhao and Yuling Liu and E. Erdun},
   doi = {10.1007/978-3-031-06794-5_55/COVER},
   isbn = {9783031067938},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Automatic generation of legal documents,Classification of legal documents,Deep learning,Elements extraction of legal documents,Intelligent justice,Legal documents representation},
   pages = {684-695},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Review on Intelligent Processing Technologies of Legal Documents},
   volume = {13338 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-06794-5_55},
   year = {2022},
}
@article{Nay2016,
   abstract = {We compare policy differences across institutions by embedding representations of the entire legal corpus of each institution and the vocabulary shared across all corpora into a continuous vector space. We apply our method, Gov2Vec, to Supreme Court opinions, Presidential actions, and official summaries of Congressional bills. The model discerns meaningful differences between government branches. We also learn representations for more finegrained word sources: individual Presidents and (2-year) Congresses. The similarities between learned representations of Congresses over time and sitting Presidents are negatively correlated with the bill veto rate, and the temporal ordering of Presidents and Congresses was implicitly learned from only text. With the resulting vectors we answer questions such as: how does Obama and the 113th House differ in addressing climate change and how does this vary from environmental or economic perspectives? Our work illustrates vectorarithmetic- based investigations of complex relationships between word sources based on their texts. We are extending this to create a more comprehensive legal semantic map.},
   author = {John J. Nay},
   doi = {10.18653/v1/w16-5607},
   isbn = {9781945626265},
   journal = {NLP + CSS 2016 - EMNLP 2016 Workshop on Natural Language Processing and Computational Social Science, Proceedings of the Workshop},
   pages = {49-54},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {Gov2Vec: Learning Distributed Representations of Institutions and Their Legal Text},
   year = {2016},
}
@article{Sulis2022,
   abstract = {This paper describes an application of textual similarity techniques in the Legal Informatics domain. In European law, a relevant interest relates to the transposition of EU directives by the Member States, which can be complete, partial, or eventually absent. As part of an European project, legal experts annotated transpositions of six directives on a per-article basis. Following an established NLP pipeline, we explore a similarity-based technique to identify correspondences between transpositions of national implementations. Early results are promising and show the role that Artificial Intelligence may play within the process of harmonization and standardization of domestic legal systems as a result of the adoption of EU legislation.},
   author = {Emilio Sulis and Llio Bryn Humphreys and Davide Audrito and Luigi Di Caro},
   doi = {10.1007/978-3-031-08421-8_13/COVER},
   isbn = {9783031084201},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Harmonization of laws,Legal informatics,Natural language processing,Text similarity},
   pages = {185-197},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Exploiting Textual Similarity Techniques in Harmonization of Laws},
   volume = {13196 LNAI},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-08421-8_13},
   year = {2022},
}
@article{Soavi2022,
   abstract = {Legal contracts have been used for millennia to conduct business transactions world-wide. Such contracts are expressed in natural language, and usually come in written form. We are interested in producing formal specifications from such legal text that can be used to formally analyze contracts, also serve as launching pad for generating smart contracts, information systems that partially automate, monitor and control the execution of legal contracts. We have been developing a method for transforming legal contract documents into specifications, adopting a semantic approach where transformation is treated as a text classification, rather than a natural language processing problem. The method consists of five steps that (a) Identify domain terms in the contract and manually disambiguate them when necessary, in consultation with stakeholders; (b) Semantically annotate text identifying obligations, powers, contracting parties, assets and situations; (c) Identify relationships among the concepts mined in (b); (d) Generate a domain model based on the terms identified in (a), as well as parameters and local variables for the contract; (e) Generate expressions that formalize the conditions of obligations and powers using terms identified in earlier steps in a contract specification language. This paper presents the method through an illustrative example, also reports on a prototype implementation of an environment that supports the method.},
   author = {Michele Soavi and Nicola Zeni and John Mylopoulos and Luisa Mich},
   doi = {10.1007/978-3-031-05760-1_20/COVER},
   isbn = {9783031057595},
   issn = {18651356},
   journal = {Lecture Notes in Business Information Processing},
   keywords = {Domain model,Formal specification,Legal contract,Ontology,Semantic annotation,Smart contract},
   pages = {338-353},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Contratto – A Method for Transforming Legal Contracts into Formal Specifications},
   volume = {446 LNBIP},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-05760-1_20},
   year = {2022},
}
@article{Giri2017,
   abstract = {With crimes increasing at an alarming rate, it becomes essential to impart justice to the victims readily. To come to a final decision, lawyers need to study several previous judgments for research purposes. Reducing the time spent on research can speed up the judicial process drastically. The time consumption mostly happens in two areas - searching for the right document and understanding that document. To start with, being able to get hold of the appropriate judgments or other legal documents is the most essential task for any legal professional, especially lawyers. Once a document is obtained, the next most integral and inevitable task is to read and re-read it, and come to necessary as well as needful conclusions after a comprehensive analysis. To resolve the first issue, there is a need for an efficient search system which can provide searching options based upon multiple views. This system is an effort at improving the search for users by providing them with search options based upon either the semantics of the word or based upon the IPC sections. It is important that laymen can access all the related judgments by entering just one keyword or phrase without bothering about the legal jargon. Post retrieval of documents, the lengthy texts have to be scrutinized for meaningful inferences. To reduce the time spent in reading texts, we intend to present the information in the judgments visually through semantic networks. Lawyers will be benefitted by this system because it will enable them to skip the complexity of the often verbose language of the legal documents. This IR System provides the features of semantic and IPC section based search to users, deriving information from semantic networks that are representative of the documents, so that a more efficient search system on legal documents can be put in place.},
   author = {Rachayita Giri and Yosha Porwal and Vaibhavi Shukla and Palak Chadha and Rishabh Kaushal},
   doi = {10.1109/IC3.2017.8284324},
   isbn = {9781538630778},
   journal = {2017 10th International Conference on Contemporary Computing, IC3 2017},
   month = {7},
   pages = {1-6},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Approaches for information retrieval in legal documents},
   volume = {2018-January},
   year = {2017},
}
@article{Gangemi2005,
   abstract = {The increasing development of legal ontologies seems to offer interesting solutions to legal knowledge formalization, which in past experiences lead to a limited exploitation of legal expert systems for practical use. The paper describes how a constructive approach to ontology can provide useful components to create newly designed legal decision support systems either as local or Web-based semantic services. We describe the relation of our research to AI&Law and legal philosophy, the components of our Core Legal Ontology, the JurWordNet semantic lexicon, and some examples of use of legal ontologies for both norm conformity and compatibility. Our legal ontologies are based on DOLCE+, an extension of the DOLCE foundational ontology developed in the WonderWeb and Metokis EU projects. © 2005 Springer-Verlag.},
   author = {Aldo Gangemi and Maria Teresa Sagri and Daniela Tiscornia},
   doi = {10.1007/978-3-540-32253-5_7},
   isbn = {3540250638},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {97-124},
   title = {A constructive framework for legal ontologies},
   volume = {3369 LNAI},
   year = {2005},
}
@article{Gomes2022,
   abstract = {This work aims to systematize the knowledge on emerging Intelligent Information Retrieval (IIR) practices in scenarios whose context is similar to the field of tax law. It is a part of a project that covers the emerging techniques of IIR and its applicability to the tax law domain. Furthermore, it presents an overview of different approaches for representing legal data and exposes the challenging task of providing quality insights to support decision-making in a dedicated legal environment. It also offers an overview of the related background and prior research referring to the techniques for information retrieval in legal documents, establishing the current state-of-the-art, and identifying its main drawbacks. A summary of the most appropriate technologies and research approaches of the technologies that apply artificial intelligence technology to help legal tasks is also depicted.},
   author = {Marco Gomes and Bruno Oliveira and Cristóvão Sousa},
   doi = {10.1007/978-3-031-16474-3_11/COVER},
   isbn = {9783031164736},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Artificial intelligence,Information Retrieval,Legal domain,Legal knowledge},
   pages = {119-130},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Enriching Legal Knowledge Through Intelligent Information Retrieval Techniques: A Review},
   volume = {13566 LNAI},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-16474-3_11},
   year = {2022},
}
@article{Ashley2013,
   abstract = {Users of commercial legal information retrieval (IR) systems often want argument retrieval (AR): retrieving not merely sentences with highlighted terms, but arguments and argument-related information. Using a corpus of argument-annotated legal cases, we conducted a baseline study of current legal IR systems in responding to standard queries. We identify ways in which they cannot meet the need for AR and illustrate how additional argument-relevant information could address some of those inadequacies. We conclude by indicating our approach to developing an AR system to retrieve arguments from legal decisions. © 2013 The authors and IOS Press.},
   author = {Kevin D. Ashley and Vern R. Walker},
   doi = {10.3233/978-1-61499-359-9-29},
   isbn = {9781614993582},
   issn = {09226389},
   journal = {Frontiers in Artificial Intelligence and Applications},
   keywords = {Argument retrieval,Default-logic framework,Legal information retrieval,Pragmatics,Presuppositional annotation,Semantics},
   pages = {29-38},
   title = {From information retrieval (IR) to argument retrieval (AR) for legal cases: Report on a baseline study},
   volume = {259},
   year = {2013},
}
@article{Dalmonte2022,
   abstract = {We propose a minimal deontic logic, called MIND, based on intuitionistic logic. This logic gives a very simple solution to handling conflicting obligations: the presence of two conflicting obligations does not entail the triviality of the set of norms. Moreover, the logic supports the claim that there may be no obligations at all, so that a logical truth is not obligatory. Like in intuitionistic/constructive modal logic, in this logic, the two deontic modalities Obligation and Permission are not dual, however as a difference with so-called constructive modal logic, it supports distribution of permission over disjunction, but it does not satisfy aggregation of obligations. The logic MIND is a non-normal modal logic based on intuitionistic logic and it is semantically characterised by a suitable neighbourhood semantics. We further present a simple cut-free sequent calculus for this logic. By means of this calculus we show that logic MIND is decidable and that it satisfies the disjunction property.},
   author = {Tiziano Dalmonte and Charles Grellois and Nicola Olivetti},
   doi = {10.1007/978-3-031-15298-6_18/COVER},
   isbn = {9783031152979},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Deontic logic,Intuitionistic modal logic,Neighbourhood semantics,Non-normal modal logic,Sequent calculus},
   pages = {280-294},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Towards an Intuitionistic Deontic Logic Tolerating Conflicting Obligations},
   volume = {13468 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-15298-6_18},
   year = {2022},
}
@article{Athan2015,
   abstract = {This tutorial presents the principles of the OASIS Legal- RuleML applied to the legal domain and discusses why, how, and when LegalRuleML is well-suited for modelling norms. To provide a framework of reference, we present a comprehensive list of requirements for devising rule interchange languages that capture the peculiarities of legal rule modelling in support of legal reasoning. The tutorial comprises syntactic, semantic, and pragmatic foundations, a LegalRuleML primer, as well as use case examples from the legal domain.},
   author = {Tara Athan and Guido Governatori and Monica Palmirani and Adrian Paschke and Adam Wyner},
   doi = {10.1007/978-3-319-21768-0_6},
   isbn = {9783319217673},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Legal rule modeling,LegalRuleML,Meta Model,RuleML},
   pages = {151-188},
   publisher = {Springer Verlag},
   title = {LegalRuleML: Design principles and foundations},
   volume = {9203},
   year = {2015},
}
@article{Steen2022,
   abstract = {LegalRuleML is a comprehensive XML-based representation framework for modeling and exchanging normative rules. The TPTP input and output formats, on the other hand, are general-purpose standards for the interaction with automated reasoning systems. In this paper we provide a bridge between the two communities by (i) defining a logic-pluralistic normative reasoning language based on the TPTP format, (ii) providing a translation scheme between relevant fragments of LegalRuleML and this language, and (iii) proposing a flexible architecture for automated normative reasoning based on this translation. We exemplarily instantiate and demonstrate the approach with three different normative logics.},
   author = {Alexander Steen and David Fuenmayor},
   doi = {10.1007/978-3-031-21541-4_16/COVER},
   isbn = {9783031215407},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Automated reasoning,Deontic logics,LegalRuleML},
   pages = {244-260},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Bridging Between LegalRuleML and TPTP for Automated Normative Reasoning},
   volume = {13752 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-21541-4_16},
   year = {2022},
}
@article{Goossens2022b,
   abstract = {Decision models are increasingly being used in modeling business processes. Hence, extracting decision models automatically from texts would help decision modellers by reducing modeling time and supporting them in their analysis. In this paper, deep learning techniques are investigated to extract decision dependencies and conditional clauses directly from text. By using a large dataset of labeled and tagged sentences and NLP, deep learning techniques (BERT and BI-LSTM-CRF) are trained and tested on the identification of these items. The results show that the performance is sufficiently high to extract decision dependency and logic (semi)-automatically from text which provides a big step towards automatic decision modelling.},
   author = {Alexandre Goossens and Michelle Claessens and Charlotte Parthoens and Jan Vanthienen},
   doi = {10.1007/978-3-030-94343-1_27/COVER},
   isbn = {9783030943424},
   issn = {18651356},
   journal = {Lecture Notes in Business Information Processing},
   keywords = {Decision Model and Notation (DMN),Decision model extraction,Deep learning},
   pages = {349-361},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Extracting Decision Dependencies and Decision Logic from Text Using Deep Learning Techniques},
   volume = {436 LNBIP},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-94343-1_27},
   year = {2022},
}
@article{Novotna2022,
   abstract = {Legal formalization is a necessary step for automated legal reasoning because it aims to capture the meaning of legal texts in a machine-readable form. Therefore, there are many attempts to create better legal formalization methods and to make legal reasoning techniques more efficient. However, these methods are rarely evaluated and thus, it is difficult to recognize the“good” legal formalization method. In this article, the authors provide a categorization of necessary properties of a “good” formalization that is based on a literature review of recent state-of-the-art methods. They conclude in favour of the legal experts’ evaluation method as the most suitable one for assessing the quality of legal formalization.},
   author = {Tereza Novotná and Tomer Libal},
   doi = {10.1007/978-3-031-15565-9_12/COVER},
   isbn = {9783031155642},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Evaluation,Legal formalization,Literature review},
   pages = {189-203},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {An Evaluation of Methodologies for Legal Formalization},
   volume = {13283 LNAI},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-15565-9_12},
   year = {2022},
}
@article{Javed2022,
   abstract = {Process compliance with relevant regulations and de-facto standards is a mandatory requirement for certifying critical systems. However, it is often carried out manually, and therefore perceived as complex and labour-intensive. Ontology-based Natural Language Processing (NLP) provides an efficient support for compliance management with critical software system engineering standards. This, however, has not been considered in the literature. Accordingly, the approach presented in this paper focuses on ontology-based NLP for compliance management of software engineering processes with standard documents. In the developed ontology, the process concerns, such as stakeholders, tasks and work products are captured for better interpretation. The rules are created for extracting and structuring information, in which both syntactic features (captured using NLP tasks) and semantic features (captured using ontology) are encoded. During the planning phase, we supported the generation of requirements, process models and compliance mappings in Eclipse Process Framework (EPF) Composer. In the context of reverse compliance, the gaps with standard documents are detected, potential measures for their resolution are provided, and adaptions are made after the process engineer approval. The applicability of the proposed approach is demonstrated by processing ECSS-E-ST-40C, a space software engineering standard, generating models and mappings, as well as reverse compliance management of extended process model.},
   author = {Muhammad Atif Javed and Faiz Ul Muram and Samina Kanwal},
   doi = {10.1007/978-3-030-96648-5_14/COVER},
   isbn = {9783030966478},
   issn = {18650937},
   journal = {Communications in Computer and Information Science},
   keywords = {Compliance management,EPF composer,Natural language processing,Ontology,Process,Rules,SPEM 2.0,Standards},
   pages = {309-327},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Ontology-Based Natural Language Processing for Process Compliance Management},
   volume = {1556 CCIS},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-96648-5_14},
   year = {2022},
}
@article{Bennett2022,
   abstract = {Standards are a necessary adjunct to many kinds of new technology development. This paper explores an emerging new kind of standard in which business meaning is standardized using formal ontology. Some issues are identified in terms of the characteristics of different ontologies and how these may apply to different intended uses. By way of illustration, an account of the origins, history and intended uses of an industry standard called the Financial Industry Business Ontology (FIBO) is given, along with a summary of other financial industry standards. It is suggested that an understanding of the ontology characterizations described here may be instructive in developing industry standards at the level of semantics and in making use of these standards.},
   author = {Michael G. Bennett},
   doi = {10.1007/978-3-030-89906-6_47/COVER},
   isbn = {9783030899059},
   issn = {23673389},
   journal = {Lecture Notes in Networks and Systems},
   keywords = {FIBO,Finance,Ontology,Standards},
   pages = {717-729},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Standards for Knowledge Graphs in the Financial Sector},
   volume = {358 LNNS},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-89906-6_47},
   year = {2022},
}
@article{Beller2008,
   abstract = {Deontic reasoning is thinking about whether actions are forbidden or allowed, obligatory or not obligatory. It is proposed that social norms, imposing constraints on individual actions, constitute the fundamental concept for the system of these four deontic modalities, and that people reason from such norms flexibly according to deontic core principles. Two experiments are presented, one on deontic conditional reasoning, the other on "pure" deontic reasoning. Both experiments demonstrate people's high deontic competence and confirm the proposed representational and inferential principles. Experiment 1 additionally shows small effects of the conditional formulations. These findings support the dual source approach (Beller & Spada, 2003) that distinguishes between domain-specific and domain-general inferences. Implications for other theories of deontic reasoning are discussed. © 2008 Psychology Press.},
   author = {Sieghard Beller},
   doi = {10.1080/13546780802222258},
   issn = {13546783},
   issue = {4},
   journal = {Thinking and Reasoning},
   keywords = {Conditionals,Content effects,Deontic reasoning,Dual source approach},
   month = {7},
   pages = {305-341},
   title = {Deontic norms, deontic reasoning, and deontic conditionals},
   volume = {14},
   year = {2008},
}
@article{Akaichi2023,
   abstract = {Robust Usage Control (UC) mechanisms are necessary to protect sensitive data and resources, especially when these are distributed across multiple nodes or users. Existing solutions have limitations in expressing and enforcing usage control policies due to difficulties in capturing complex requirements and the lack of formal semantics necessary for automated compliance checking. To address these challenges, we propose GUCON, a generic policy framework that allows for the expression of and reasoning over granular UC policies. This is achieved by leveraging the expressiveness and semantics of graph pattern expressions, as well as the flexibility of deontic concepts. Additionally, GUCON incorporates algorithms for conflict detection, resolution, compliance and requirements checking, ensuring active policy enforcement. We demonstrate the effectiveness of our framework by proposing instantiations using SHACL, OWL and ODRL. We show how instantiations provide a bridge between abstract formalism and concrete implementations, thus allowing existing reasoners and implementations to be leveraged.},
   author = {Ines Akaichi and Giorgos Flouris and Irini Fundulaki and Sabrina Kirrane},
   doi = {10.1007/978-3-031-45072-3_3/COVER},
   isbn = {9783031450716},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Deontic Rules,Enforcement,Policy,Reasoning,Usage Control},
   pages = {34-53},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {GUCON: A Generic Graph Pattern Based Policy Framework for Usage Control Enforcement},
   volume = {14244 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-45072-3_3},
   year = {2023},
}
@article{Francesconi2023,
   abstract = {This paper presents an approach for legal compliance checking in the Semantic Web which can be effectively applied for applications in the Linked Open Data environment. It is based on modeling deontic norms in terms of ontology classes and ontology property restrictions. It is also shown how this approach can handle norm defeasibility. Such methodology is implemented by decidable fragments of OWL 2, while legal reasoning is carried out by available decidable reasoners. The approach is generalised by presenting patterns for modeling deontic norms and norms compliance checking.},
   author = {Enrico Francesconi and Guido Governatori},
   doi = {10.1007/S10506-022-09317-8},
   issn = {15728382},
   issue = {3},
   journal = {Artificial Intelligence and Law},
   keywords = {Legal reasoning,Norm compliance,OWL 2,Semantic Web},
   month = {9},
   pages = {445-464},
   publisher = {Institute for Ionics},
   title = {Patterns for legal compliance checking in a decidable framework of linked open data},
   volume = {31},
   year = {2023},
}
@article{Bonatti2020,
   abstract = {This paper shows how knowledge representation and reasoning techniques can be used to support organizations in complying with the GDPR, that is, the new European data protection regulation. This work is carried out in a European H2020 project called SPECIAL. Data usage policies, the consent of data subjects, and selected fragments of the GDPR are encoded in a fragment of OWL2 called PL (policy language); compliance checking and policy validation are reduced to subsumption checking and concept consistency checking. This work proposes a satisfactory tradeoff between the expressiveness requirements on PL posed by the modeling of the GDPR, and the scalability requirements that arise from the use cases provided by SPECIAL's industrial partners. Real-time compliance checking is achieved by means of a specialized reasoner, called PLR, that leverages knowledge compilation and structural subsumption techniques. The performance of a prototype implementation of PLR is analyzed through systematic experiments, and compared with the performance of other important reasoners. Moreover, we show how PL and PLR can be extended to support richer ontologies, by means of import-by-query techniques. We prove novel tractability and intractability results related to PL, and some negative results about the restrictions posed on ontology import.},
   author = {Piero A. Bonatti and Luca Ioffredo and Iliana M. Petrova and Luigi Sauro and Ida R. Siahaan},
   doi = {10.1016/J.ARTINT.2020.103389},
   issn = {00043702},
   journal = {Artificial Intelligence},
   keywords = {GDPR,Import-by-query,Knowledge compilation,Semantic policy languages,Structural subsumption,Tractable OWL2 fragments},
   month = {12},
   publisher = {Elsevier B.V.},
   title = {Real-time reasoning in OWL2 for GDPR compliance},
   volume = {289},
   year = {2020},
}
@article{Bonatti2020b,
   abstract = {The European General Data Protection Regulation (GDPR) calls for technical and organizational measures to support its implementation. Towards this end, the SPECIAL H2020 project aims to provide a set of tools that can be used by data controllers and processors to automatically check if personal data processing and sharing complies with the obligations set forth in the GDPR. The primary contributions of the project include: (i) a policy language that can be used to express consent, business policies, and regulatory obligations; and (ii) two different approaches to automated compliance checking that can be used to demonstrate that data processing performed by data controllers/processors complies with consent provided by data subjects, and business processes comply with regulatory obligations set forth in the GDPR.},
   author = {Piero A. Bonatti and Sabrina Kirrane and Iliana M. Petrova and Luigi Sauro},
   doi = {10.1007/S13218-020-00677-4},
   issn = {16101987},
   issue = {3},
   journal = {KI - Kunstliche Intelligenz},
   keywords = {Compliance checking,GDPR,Policies},
   month = {9},
   pages = {303-315},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Machine Understandable Policies and GDPR Compliance Checking},
   volume = {34},
   year = {2020},
}
@article{Broersen2004,
   abstract = {Dynamic deontic logics reduce normative assertions about explicit complex actions to standard dynamic logic assertions about the relation between complex actions and violation conditions. We address two general, but related problems in this field. The first is to find a formalization of the notion of 'action negation' that (1) has an intuitive interpretation as an action forming combinator and (2) does not impose restrictions on the use of other relevant action combinators such as sequence and iteration, and (3) has a meaningful interpretation in the normative context. The second problem we address concerns the reduction from deontic assertions to dynamic logic assertions. Our first point is that we want this reduction to obey the free-choice semantics for norms. For ought-to- be deontic logics it is generally accepted that the free-choice semantics is counter-intuitive. But for dynamic deontic logics we actually consider it a viable, if not, the better alternative. Our second concern with the reduction is that we want it to be more liberal than the ones that were proposed before in the literature. For instance, Meyer's reduction does not leave room for action whose normative status is neither permitted nor forbidden. We test the logics we define in this paper against a set of minimal logic requirements. © 2004 Elsevier B.V. All rights reserved.},
   author = {Jan Broersen},
   doi = {10.1016/J.JAL.2004.01.007},
   issn = {15708683},
   issue = {1 SPEC. ISS.},
   journal = {Journal of Applied Logic},
   keywords = {Action negation and refraining,Action theory,Deontic logic,Dynamic logic},
   pages = {153-168},
   publisher = {Elsevier BV},
   title = {Action negation and alternative reductions for dynamic deontic logics},
   volume = {2},
   year = {2004},
}
@article{Su2023,
   abstract = {A normative sentence contains information which is used for either describing some deontic situations or prescribing a new norm. Considering the differences between them, we introduced the notion of relativized conditional obligations based on ideality sequences. On the one hand, each ideality sequence can be treated as a normative system which prescribes the relative ideality of states of affairs. On the other hand, a bare structure provides a factual background. Once these are done, every betterness structure based on a given ideality sequence and a bare structure describes the conditional obligations. Deletion and postfixing are two updates on the normative system which can bring about corresponding obligations successfully or not. Jørgensen’s dilemma can be conceptualized by using the notion of successful updates. A sound and strongly complete axiom system for the logic of relativized conditional obligations PCDL is established.},
   author = {Xingchi Su},
   doi = {10.1007/978-3-031-45558-2_20/COVER},
   isbn = {9783031455575},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Jørgensen’s dilemma,axiomatization,conditional obligation,ideality sequence,normative system,successful update,update normative system},
   pages = {260-268},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Making Norms and Following Norms},
   volume = {14329 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-45558-2_20},
   year = {2023},
}
@article{Yufang2023,
   abstract = {In recent years, massive multi-source heterogeneous South China Sea data have been widely used in the construction of South China Sea digital resources, such as the South China Sea Sovereign Evidence Chain Project. Due to the data sparsity, a large number of isolated data are generated, which seriously affects the analysis effect of the South China Sea Big Data. In this paper, we proposed a novel data association method. We collected data from the South China Sea Library Digital Resources as South China Sea evidence data, which is a sentence or paragraph containing time, place, people, institutions and events can prove the sovereignty of the South China Sea. According to the definition of the evidence weight by the International Court of Justice, the logical relationship of South China Sea evidence data was constructed. Firstly, we randomly selected 3068 data from 21174 evidence data to label the logical relationship. Secondly, we used the BERT pre-training model to extract the logical relationship of evidence data. Finally, the Knowledge Graph technology is used to retrieve and visualize the logical relationship of evidence data. In this paper, we applied the BERT to extract the logical relationships of evidence data with an accuracy of 0.78, which indicates that the model has some feasibility. This paper could help to improve the correlation of the South China Sea Big Data and to enhance the ability of data processing.},
   author = {Peng Yufang and Xu Hao and Jin Weijian and Yang Haiping},
   doi = {10.1007/978-981-99-3300-6_16/COVER},
   isbn = {9789819932993},
   issn = {18650937},
   journal = {Communications in Computer and Information Science},
   keywords = {Deep learning,Knowledge Graph,Logical Relationship,The South China Sea Big Data},
   pages = {219-233},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Logical Relationship Extraction of Multimodal South China Sea Big Data Using BERT and Knowledge Graph},
   volume = {1796 CCIS},
   url = {https://link.springer.com/chapter/10.1007/978-981-99-3300-6_16},
   year = {2023},
}
@article{Beltagy2020,
   abstract = {Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.},
   author = {Iz Beltagy and Matthew E. Peters and Arman Cohan},
   month = {4},
   title = {Longformer: The Long-Document Transformer},
   url = {http://arxiv.org/abs/2004.05150},
   year = {2020},
}
@article{Huang2023,
   abstract = {Legal artificial intelligence (LegalAI) is an emerging field that leverages AI technology to enhance legal services. Similar Case Matching (SCM), which calculates the relevance between a candidate and a target case, is a critical technique in LegalAI to enable diverse legal intelligences. Existing approaches mainly rely the on single query texts or specific keywords for retrieval, yet neglected the domain complexity and multi-faceted nature of queries. Thus, a multi-example matching paradigm is motivated where three inherent challenges reveal. 1) Relevance assessment across multiple examples is complex. 2) The inherent lengthy and structured property of legal documents. 3) Lacking datasets containing golden labels for multi-example-based legal text matching. To address these challenges, this paper develops a novel multi-example dataset, and a Multi-level Correlation Semantic Matching (MCSM) is devised to extract similarity between cases given multi-example inputs. The proposed multi-level scheme can be interpreted as two aspects. Firstly, we consider both content and structure correlations to evaluate the relevance. Secondly, by dividing legal documents into distinctive segments, we can hierarchically learn the intra- and inter-segment dependencies to model the long-term dependencies across components of legal documents. An attention mechanism is employed to capture the complex interconnections among these examples and enable an attentive matching aggregation of content and structure. With multiple examples, the MCSM tackles the intricate and diverse nature of legal queries, providing a comprehensive and multi-dimensional description view. Extensive experimental evaluations show that the proposed MCSM outperforms baseline methods.},
   author = {Ting Huang and Xike Xie and Xiufeng Liu},
   doi = {10.1007/978-981-99-7254-8_48/COVER},
   isbn = {9789819972531},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Legal Artificial Intelligence,Natural Language Processing,Semantic Matching},
   pages = {621-632},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Multi-level Correlation Matching for Legal Text Similarity Modeling with Multiple Examples},
   volume = {14306 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-981-99-7254-8_48},
   year = {2023},
}
@article{Bennett2013,
   abstract = {This article describes the Financial Industry Business Ontology (FIBO) as a set of formal models that define unambiguous shared meaning for financial industry concepts. An account is given of the history and development of the FIBO series of standards and the theoretical underpinnings of these as a business or 'conceptual' model. Some initial proof of concept work is described, demonstrating how in addition to the use of FIBO as a conceptual model, it is possible to derive semantic technology-based applications that may be used to carry out novel types of processing on data. The development roadmap of the FIBO series of standards within the Object Management Group is also described, so that readers can have an idea of what to expect from FIBO and when. © 2013 Macmillan Publishers Ltd.},
   author = {Mike Bennett},
   doi = {10.1057/JBR.2013.13},
   issn = {17456452},
   issue = {3-4},
   journal = {Journal of Banking Regulation},
   keywords = {FIBO,OWL,big data,ontology,semantics,vocabulary},
   month = {7},
   pages = {255-268},
   title = {The financial industry business ontology: Best practice for big data},
   volume = {14},
   year = {2013},
}
@article{Georgoudi2023,
   abstract = {Documents contain textual information, which is of the utmost importance for all the organizations. Document management systems have been used to store vast amounts of unstructured textual data described with minimal metadata, a method that has several limitations. In order to convert hidden knowledge into machine-readable data with rich connections, this paper presents work in progress on the development of the first end-to-end guided approach to construct a Knowledge Graph from Greek governmental documents from the Greek open government portal. The resulted Knowledge graph constitutes a proof-of-concept graph, that illustrates the beneficial semantic relationships between the textual data.},
   author = {Amalia Georgoudi and Nikolaos Stylianou and Ioannis Konstantinidis and Georgios Meditskos and Thanassis Mavropoulos and Stefanos Vrochidis and Nick Bassiliades},
   doi = {10.1007/978-3-031-36819-6_26/COVER},
   isbn = {9783031368189},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Government Documents,Knowledge Graph Construction,Knowledge Representation,Natural Language Processing,Ontologies,RDF Triples},
   pages = {294-299},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Towards Knowledge Graph Creation from Greek Governmental Documents},
   volume = {13925 LNAI},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-36819-6_26},
   year = {2023},
}
@article{Stylianou2022,
   abstract = {Document management systems (DMS) have been used for decades to store large amounts of information in textual form. Their technology paradigm is based on storing vast quantities of textual information enriched with metadata to support searchability. However, this exhibits limitations as it treats textual information as a black box and is based exclusively on user-created metadata, a process that suffers from quality and completeness shortcomings. The use of knowledge graphs in DMS can substantially improve searchability, providing the ability to link data and enabling semantic searching. Recent approaches focus on either creating knowledge graphs from document collections or updating existing ones. In this paper, the authors introduce Doc2KG (Document-to-Knowledge-Graph), an intelligent framework that handles both creation and real-time updating of a knowledge graph, while also exploiting domain-specific ontology standards. They use DIAVGEIA (clarity), an award-winning Greek open government portal, as the case-study and discuss new capabilities for the portal by implementing Doc2KG.},
   author = {Nikolaos Stylianou and Danai Vlachava and Ioannis Konstantinidis and Nick Bassiliades and Vassilios Peristeras},
   doi = {10.4018/IJSWIS.295552},
   issn = {15526291},
   issue = {1},
   journal = {International Journal on Semantic Web and Information Systems},
   keywords = {EGovernment,Government portals,Linked data,Machine learning,Natural language processing,Open data,Semantic web},
   month = {1},
   publisher = {IGI Global},
   title = {Doc2KG: Transforming Document Repositories to Knowledge Graphs},
   volume = {18},
   year = {2022},
}
@article{Izumigawa2023,
   abstract = {Ontologies are human- and machine-readable conceptualizations that define domain concepts and their relationships. The context provided by these representations are essential for advanced reasoning applications and explainable artificial intelligence efforts. Despite their advantages, however, automating ontology generation from text is difficult due to a number of challenges. To bring greater awareness to those challenges and to initiate discussion on possible solutions, we provide a definition of ontologies, the motivation behind our work, a set of key challenges, and an overview of adjacent solutions in recent work.},
   author = {Christianne Izumigawa and Bethany Taylor and Jonathan Sato},
   doi = {10.1007/978-3-031-36004-6_59/COVER},
   isbn = {9783031360039},
   issn = {18650937},
   journal = {Communications in Computer and Information Science},
   keywords = {Natural Language Processing,Ontology Learning},
   pages = {433-438},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Automated Ontology Generation},
   volume = {1836 CCIS},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-36004-6_59},
   year = {2023},
}
@article{Liga2023,
   abstract = {The aim of this work is to employ Tree Kernel algorithms to classify natural language in the legal domain (i.e. deontic sentences and rules). More precisely, an innovative way of extracting labelled legal data is proposed, which combines the information provided by two famous LegalXML formats: Akoma Ntoso and LegalRuleML. We then applied this method on the European General Data Protection Regulation (GDPR) to train a Tree Kernel classifier on deontic and non-deontic sentences which were reconstructed using Akoma Ntoso, and labelled using the LegalRuleML representation of the GDPR. To prove the non-triviality of the task we reported the results of a stratified baseline classifier on two classification scenarios.},
   author = {Davide Liga and Monica Palmirani},
   doi = {10.1007/978-3-031-16072-1_4/COVER},
   isbn = {9783031160714},
   issn = {23673389},
   journal = {Lecture Notes in Networks and Systems},
   keywords = {Akoma Ntoso,Deontic,GDPR,Legal AI,LegalRuleML,NLP},
   pages = {54-73},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Deontic Sentence Classification Using Tree Kernel Classifiers},
   volume = {542 LNNS},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-16072-1_4},
   year = {2023},
}
@article{Chalkidis2018,
   abstract = {We consider the task of detecting contractual obligations and prohibitions. We show that a self-attention mechanism improves the performance of a BILSTM classifier, the previous state of the art for this task, by allowing it to focus on indicative tokens. We also introduce a hierarchical BILSTM, which converts each sentence to an embedding, and processes the sentence embeddings to classify each sentence. Apart from being faster to train, the hierarchical BILSTM outperforms the flat one, even when the latter considers surrounding sentences, because the hierarchical model has a broader discourse view.},
   author = {Ilias Chalkidis and Ion Androutsopoulos and Achilleas Michos},
   doi = {10.18653/v1/p18-2041},
   isbn = {9781948087346},
   journal = {ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
   pages = {254-259},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {Obligation and prohibition extraction using hierarchical RNNs},
   volume = {2},
   year = {2018},
}
@article{Prince-Tritto2024,
   abstract = {The utilization of machine learning methods for the analysis and interpretation of legal documents has been growing over the years, yet their potential and limitations remain under-explored. This study aims to address this gap, using unsupervised machine learning...},
   author = {Philippe Prince-Tritto and Hiram Ponce},
   doi = {10.1007/978-3-031-47640-2_5/COVER},
   isbn = {9783031476396},
   issn = {16113349},
   pages = {52-67},
   publisher = {Springer, Cham},
   title = {Exploring the Challenges and Limitations of Unsupervised Machine Learning Approaches in Legal Concepts Discovery},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-47640-2_5},
   year = {2024},
}
@article{Graham2023,
   abstract = {The contract review process can be a costly and time-consuming task for lawyers and clients alike, requiring significant effort to identify and evaluate the legal implications of individual clauses. To address this challenge, we propose the use of natural language processing techniques, specifically text classification based on deontic tags, to streamline the process. Our research question is whether natural language processing techniques, specifically dense vector embeddings, can help semi-automate the contract review process and reduce time and costs for legal professionals reviewing deontic modalities in contracts. In this study, we create a domain-specific dataset and train both baseline and neural network models for contract sentence classification. This approach offers a more efficient and cost-effective solution for contract review, mimicking the work of a lawyer. Our approach achieves an accuracy of 0.90, showcasing its effectiveness in identifying and evaluating individual contract sentences.},
   author = {S. Georgette Graham and Hamidreza Soltani and Olufemi Isiaq},
   doi = {10.1007/S10506-023-09379-2/METRICS},
   issn = {15728382},
   journal = {Artificial Intelligence and Law},
   keywords = {Annotation,Deep learning,Deontic reasoning,Legal text classification,Natural language processing},
   month = {11},
   pages = {1-22},
   publisher = {Institute for Ionics},
   title = {Natural language processing for legal document review: categorising deontic modalities in contracts},
   url = {https://link.springer.com/article/10.1007/s10506-023-09379-2},
   year = {2023},
}
@article{Liu2023,
   abstract = {Few-shot learning aims to discriminate images from novel categories using only a few available training examples. While existing few-shot methods can adapt quickly and precisely to new categories, they often struggle to retain knowledge of base categories that were used in the training phase. To address this challenge and support lifelong learning, generalized few-shot learning has been introduced to enable few-shot models to classify both base and novel categories. However, as the number of categories increases, few-shot models can lose efficiency due to the limited amount of visual information available for each category. To address this limitation, we propose the knowledge-augmented weight generation (KAWG) method, which incorporates semantic information in addition to visual features. Specifically, KAWG combines textual descriptions and entity relationships extracted from knowledge graphs and visual features to generate more robust classifiers for generalized few-shot learning tasks. Through our meta-training strategy, KAWG can retain the knowledge learned from base categories to the greatest extent when transferring to novel classes. Experiments show that our approach achieves state-of-the-art performance on some generalized few-shot benchmarks.},
   author = {Dianqi Liu and Liang Bai and Tianyuan Yu},
   doi = {10.1007/S11063-023-11278-1/METRICS},
   issn = {1573773X},
   issue = {6},
   journal = {Neural Processing Letters},
   keywords = {Few-shot learning,Image classification,Knowledge graph,Meta-learn},
   month = {12},
   pages = {7649-7666},
   publisher = {Springer},
   title = {Generalized Few-Shot Classification with Knowledge Graph},
   volume = {55},
   url = {https://link.springer.com/article/10.1007/s11063-023-11278-1},
   year = {2023},
}
@article{Abdallah2023,
   abstract = {Answering questions related to the legal domain is a complex task, primarily due to the intricate nature and diverse range of legal document systems. Providing an accurate answer to a legal query typically necessitates specialized knowledge in the relevant domain, which makes this task all the more challenging, even for human experts. Question answering (QA) systems are designed to generate answers to questions asked in human languages. QA uses natural language processing to understand questions and search through information to find relevant answers. QA has various practical applications, including customer service, education, research, and cross-lingual communication. However, QA faces challenges such as improving natural language understanding and handling complex and ambiguous questions. Answering questions related to the legal domain is a complex task, primarily due to the intricate nature and diverse range of legal document systems. Providing an accurate answer to a legal query typically necessitates specialized knowledge in the relevant domain, which makes this task all the more challenging, even for human experts. At this time, there is a lack of surveys that discuss legal question answering. To address this problem, we provide a comprehensive survey that reviews 14 benchmark datasets for question-answering in the legal field as well as presents a comprehensive review of the state-of-the-art Legal Question Answering deep learning models. We cover the different architectures and techniques used in these studies and the performance and limitations of these models. Moreover, we have established a public GitHub repository where we regularly upload the most recent articles, open data, and source code. The repository is available at: \url\{https://github.com/abdoelsayed2016/Legal-Question-Answering-Review\}.},
   author = {Abdelrahman Abdallah and Bhawna Piryani and Adam Jatowt},
   doi = {10.1186/s40537-023-00802-8},
   issue = {1},
   journal = {Journal of Big Data},
   keywords = {Information retrieval,Legal information extraction,Legal question answering,Machine learning,Natural language processing,Transformers},
   month = {4},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Exploring the State of the Art in Legal QA Systems},
   volume = {10},
   url = {http://arxiv.org/abs/2304.06623 http://dx.doi.org/10.1186/s40537-023-00802-8},
   year = {2023},
}
@article{Kim2020,
   abstract = {This study proposes the optimization method of the associative knowledge graph using TF-IDF based ranking scores. The proposed method calculates TF-IDF weights in all documents and generates term ranking. Based on the terms with high scores from TF-IDF based ranking, optimized transactions are generated. News data are first collected through crawling and then are converted into a corpus through preprocessing. Unnecessary data are removed through preprocessing including lowercase conversion, removal of punctuation marks and stop words. In the document term matrix, words are extracted and then transactions are generated. In the data cleaning process, the Apriori algorithm is applied to generate association rules and make a knowledge graph. To optimize the generated knowledge graph, the proposed method utilizes TF-IDF based ranking scores to remove terms with low scores and recreate transactions. Based on the result, the association rule algorithm is applied to create an optimized knowledge model. The performance is evaluated in rule generation speed and usefulness of association rules. The association rule generation speed of the proposed method is about 22 seconds faster. And the lift value of the proposed method for usefulness is about 0.43 to 2.51 higher than that of each one of conventional association rule algorithms.},
   author = {Hyun Jin Kim and Ji Won Baek and Kyungyong Chung},
   doi = {10.3390/app10134590},
   issn = {20763417},
   issue = {13},
   journal = {Appl Sci},
   keywords = {Apriori,Association rule,Associative knowledge graph,FP-tree,TF-IDF},
   month = {7},
   pages = {4590},
   publisher = {MDPI AG},
   title = {Optimization of associative knowledge graph using TF-IDF based ranking score},
   volume = {10},
   year = {2020},
}
@article{Sood2023,
   abstract = {Frauds accounted for significant losses in the financial sector and emerged as the industry’s biggest challenge. Companies invest significant amounts to prevent such fraud. It has been reported that 63.6% of the financial institutions that use Automated Fraud prevention methods successfully prevented frauds before their occurrence. Some estimations suggest that 80% of specialists are confident in cutting down fraud using Artificial Intelligence (AI)-based platforms. Several research studies have also administered AI-based techniques for fraud prevention. This study takes a systematic literature review approach to uncover the emerging areas of fraud detection using AI. The authors have analyzed 241 research articles published in the last 20 years. The Scopus database was the source of the articles in the literature review. The meta-analysis and network analysis were carried out, and the output shows the up trend of this research domain. Author-coauthor network collaboration is analyzed using the VOSviewer tool. K-means clustering was performed to identify the critical research domain, and future research areas were also identified. This research will act as a reference for future scholars who want to perform analysis on the application of AI techniques in financial fraud detection and prevention. We finally conclude the study by identifying the scope of future research and will be a value addition for financial fraud researchers.},
   author = {Pallavi Sood and Chetan Sharma and Shivinder Nijjer and Sumit Sakhuja},
   doi = {10.1007/S13198-023-02043-7/METRICS},
   issn = {09764348},
   issue = {6},
   journal = {International Journal of System Assurance Engineering and Management},
   keywords = {Artificial intelligence,Automated techniques,Financial frauds,KNIME,Latent semantic analysis,Text mining,Topic modelling,Vosviewer},
   month = {12},
   pages = {2120-2135},
   publisher = {Springer},
   title = {Review the role of artificial intelligence in detecting and preventing financial fraud using natural language processing},
   volume = {14},
   url = {https://link.springer.com/article/10.1007/s13198-023-02043-7},
   year = {2023},
}
@article{Greco2023,
   abstract = {Transformer-based language models (TLMs) have widely been recognized to be a cutting-edge technology for the successful development of deep-learning-based solutions to problems and applications that require natural language processing and understanding. Like for other textual domains, TLMs have indeed pushed the state-of-the-art of AI approaches for many tasks of interest in the legal domain. Despite the first Transformer model being proposed about six years ago, there has been a rapid progress of this technology at an unprecedented rate, whereby BERT and related models represent a major reference, also in the legal domain. This article provides the first systematic overview of TLM-based methods for AI-driven problems and tasks in the legal sphere. A major goal is to highlight research advances in this field so as to understand, on the one hand, how the Transformers have contributed to the success of AI in supporting legal processes, and on the other hand, what are the current limitations and opportunities for further research development.},
   author = {Candida M. Greco and Andrea Tagarelli},
   doi = {10.1007/S10506-023-09374-7},
   isbn = {0123456789},
   issn = {1572-8382},
   journal = {Artificial Intelligence and Law 2023},
   keywords = {Artificial Intelligence,IT Law,Information Storage and Retrieval,Intellectual Property,Language models,Legal Aspects of Computing,Legal document review,Legal outcome prediction,Legal search,Media Law,Philosophy of Law,Retrieval,Statutory law data},
   month = {11},
   pages = {1-148},
   publisher = {Springer},
   title = {Bringing order into the realm of Transformer-based language models for artificial intelligence and law},
   url = {https://link.springer.com/article/10.1007/s10506-023-09374-7},
   year = {2023},
}
@article{Szoke2013,
   abstract = {Regulations affect every aspect of our lives. Compliance with the regulations impacts citizens and businesses similarly: they have to find their rights and obligations in the complex legal environment. The situation is more complex when languages and time versions of regulations should be considered. To propose a solution to these demands, we present a semantic enrichment approach which aims at (1) decreasing the ambiguousness of legal texts, (2) increasing the probability of finding the relevant legal materials, and (3) utilizing the application of legal reasoners. Our approach is also implemented both as a service for citizens and businesses and as a modeling environment for legal drafters. To evaluate the usefulness of the approach, a case study was carried out in a large organization and applied to corporate regulations and Hungarian laws. The results suggest this approach can support the previous aims. © 2013 Springer Science+Business Media Dordrecht.},
   author = {Ákos Szőke and András Förhécz and Gábor Kőrösi and György Strausz},
   doi = {10.1007/S10506-013-9145-Z/METRICS},
   issn = {15728382},
   issue = {4},
   journal = {Artificial Intelligence and Law},
   keywords = {Legal xml,Linked data,Ontology,Rdf,SWRL,Semantic enrichment},
   month = {11},
   pages = {485-519},
   publisher = {Kluwer Academic Publishers},
   title = {Versioned linking of semantic enrichment of legal documents: Emerald: An implementation of knowledge-based services in a semantic web approach},
   volume = {21},
   url = {https://link.springer.com/article/10.1007/s10506-013-9145-z},
   year = {2013},
}
@article{McLachlan2023,
   abstract = {Modelling that exploits visual elements and information visualisation are important areas that have contributed immensely to understanding and the computerisation advancements in many domains and yet remain unexplored for the benefit of the law and legal practice. This paper investigates the challenge of modelling and expressing structures and processes in legislation and the law by using visual modelling and information visualisation (InfoVis) to assist accessibility of legal knowledge, practice and knowledge formalisation as a basis for legal AI. The paper uses a subset of the well-defined Unified Modelling Language (UML) to visually express the structure and process of the legislation and the law to create visual flow diagrams called lawmaps, which form the basis of further formalisation. A lawmap development methodology is presented and evaluated by creating a set of lawmaps for the practice of conveyancing and the Landlords and Tenants Act 1954 of the United Kingdom. This paper is the first of a new breed of preliminary solutions capable of application across all aspects, from legislation to practice; and capable of accelerating development of legal AI.},
   author = {Scott McLachlan and Evangelia Kyrimi and Kudakwashe Dube and Norman Fenton and Lisa C. Webley},
   doi = {10.1007/S10506-021-09298-0/FIGURES/14},
   issn = {15728382},
   issue = {1},
   journal = {Artificial Intelligence and Law},
   keywords = {Flowcharts,Lawmaps,Legal process,Legislation,Process visualisation},
   month = {3},
   pages = {169-194},
   publisher = {Institute for Ionics},
   title = {Lawmaps: enabling legal AI development through visualisation of the implicit structure of legislation and lawyerly process},
   volume = {31},
   url = {https://link.springer.com/article/10.1007/s10506-021-09298-0},
   year = {2023},
}
@article{Breuker2004,
   abstract = {In this article we describe two core ontologies of law that specify knowledge that is common to all domains of law. The first one, FOLaw describes and explains dependencies between types of knowledge in legal reasoning; the second one, LRI-Core ontology, captures the main concepts in legal information processing. Although FOLaw has shown to be of high practical value in various applied European ICT projects, its reuse is rather limited as it is rather concerned with the structure of legal reasoning than with legal knowledge itself: as many other "legal core ontologies", FOLaw is therefore rather an epistemological framework than an ontology. Therefore, we also developed LRI-Core. As we argue here that legal knowledge is based to a large extend on common-sense knowledge, LRI-Core is particularly inspired by research on abstract common-sense concepts. The main categories of LRI-Core are: physical, mental and abstract concepts. Roles cover in particular social worlds. Another special category are occurrences; terms that denote events and situations. We illustrate the use of LRI-Core with an ontology for Dutch criminal law, developed in the e-Court European project. © Springer 2006.},
   author = {Joost Breuker and André Valente and Radboud Winkels},
   doi = {10.1007/S10506-006-0002-1/METRICS},
   issn = {09248463},
   issue = {4},
   journal = {Artificial Intelligence and Law},
   keywords = {Common-sense ontology,Legal core ontologies,Legal reasoning},
   month = {12},
   pages = {241-277},
   publisher = {Springer},
   title = {Legal ontologies in knowledge engineering and information management},
   volume = {12},
   url = {https://link.springer.com/article/10.1007/s10506-006-0002-1},
   year = {2004},
}
@article{Sarrafzadeh2017,
   abstract = {In information retrieval and information visualization, hierarchies are a common tool to structure information into topics or facets, and network visualizations such as knowledge graphs link related concepts within a domain. In this paper, we explore a multi-layer extension to knowledge graphs, hierarchical knowledge graphs (HKGs), that combines hierarchical and network visualizations into a unified data representation. .rough interaction logs, we show that HKGs preserve the bene.ts of single-layer knowledge graphs at conveying domain knowledge while incorporating the sensemaking advantages of hierarchies for knowledge seeking tasks. Specially, this paper describes our algorithm to construct these visualizations, analyzes interaction logs to quantitatively demonstrate performance parity with networks and performance advantages over hierarchies, and synthesizes data from interaction logs, interviews, and thinkalouds on a testbed data set to demonstrate the utility of the unified hierarchy+network structure in our HKGs.},
   author = {Bahareh Sarrafzadeh and Edward Lank},
   doi = {10.1145/3077136.3080829},
   isbn = {9781450350228},
   journal = {SIGIR 2017 - Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
   keywords = {Exploratory Search,Hierarchies,Information Seeking,Knowledge Graphs,Representations of Search Results},
   month = {8},
   pages = {145-154},
   publisher = {Association for Computing Machinery, Inc},
   title = {Improving exploratory search experience through hierarchical knowledge graphs},
   year = {2017},
}
@article{Greco2023b,
   abstract = {Transformer-based language models (TLMs) have widely been recognized to be a cutting-edge technology for the successful development of deep-learning-based solutions to problems and applications that require natural language processing and understanding. Like for other textual domains, TLMs have indeed pushed the state-of-the-art of AI approaches for many tasks of interest in the legal domain. Despite the first Transformer model being proposed about six years ago, there has been a rapid progress of this technology at an unprecedented rate, whereby BERT and related models represent a major reference, also in the legal domain. This article provides the first systematic overview of TLM-based methods for AI-driven problems and tasks in the legal sphere. A major goal is to highlight research advances in this field so as to understand, on the one hand, how the Transformers have contributed to the success of AI in supporting legal processes, and on the other hand, what are the current limitations and opportunities for further research development.},
   author = {Candida M. Greco and Andrea Tagarelli},
   doi = {10.1007/S10506-023-09374-7},
   isbn = {0123456789},
   issn = {1572-8382},
   journal = {Artificial Intelligence and Law 2023},
   keywords = {Artificial Intelligence,IT Law,Information Storage and Retrieval,Intellectual Property,Language models,Legal Aspects of Computing,Legal document review,Legal outcome prediction,Legal search,Media Law,Philosophy of Law,Retrieval,Statutory law data},
   month = {11},
   pages = {1-148},
   publisher = {Springer},
   title = {Bringing order into the realm of Transformer-based language models for artificial intelligence and law},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/article/10.1007/s10506-023-09374-7},
   year = {2023},
}
@article{Glogar2023,
   abstract = {Many legal theorists and linguists have addressed the notion of legal language from different perspectives. Despite that, the definitions of legal language vary. Almost all of the approaches conclude that legal language entails several types of communication. Nevertheless, not all of these categories are sufficiently researched. Some types of legal communication seem to be neglected. This lack of interest might be rooted in the uncertainty of whether these texts or utterances even fall under the scope of the concept of legal language. In order to avoid this superficiality in subsequent research, it is first necessary to come to a clear determination of which communicative acts can be considered a part of legal language and which cannot. Accordingly, in this search for the definition of legal language, we should not neglect the fact that language is executed in concrete communicative acts, and the only means to grasp the language is through communication. The aim of this article is therefore to clearly delineate the boundaries of this concept. Based on analysis of how the given term is currently defined, I draw out the common features and trace the characteristics in which they differ. Taking into account these findings, I propose a novel comprehensive demarcation of legal language. This concept argues that the ‘legal’ nature of language should be determined by the context and function of the particular statement or exchange, in connection with the role of participants in the communication. This means that a particular act may be considered a part of legal language not in accordance with a certain form or lexicon used, but mainly by extralinguistic circumstances in the context of which it is being performed.},
   author = {Ondřej Glogar},
   doi = {10.1007/S11196-023-10010-5/FIGURES/2},
   issn = {15728722},
   issue = {3},
   journal = {International Journal for the Semiotics of Law},
   keywords = {Communication,Conceptualization,Legal communication,Legal language,Pragmatics},
   month = {6},
   pages = {1081-1107},
   publisher = {Springer Science and Business Media B.V.},
   title = {The Concept of Legal Language: What Makes Legal Language ‘Legal‘?},
   volume = {36},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/article/10.1007/s11196-023-10010-5},
   year = {2023},
}
@article{Kriz2014,
   abstract = {We present a system that extracts a knowledge base from raw unstructured texts that is designed as a set of entities and their relations and represented in an ontological framework. The extraction pipeline processes input texts by linguistically-aware tools and extracts entities and relations from their syntactic representation. Consequently, the extracted data is represented according to the Linked Data principles. The system is designed both domain and language independent and provides users with data for more intelligent search than full-text search. We present our first case study on processing Czech legal texts.},
   author = {Vincent Kríž and Barbora Hladká and Martin Nečaský and Tomáš Knap},
   doi = {10.1007/978-3-319-13647-9_13},
   isbn = {9783319136462},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {113-124},
   publisher = {Springer Verlag},
   title = {Data extraction using NLP techniques and its transformation to linked data},
   volume = {8856},
   year = {2014},
}
@article{Monroy2009,
   abstract = {Previous work has shown that modeling relationships between articles of a regulation as vertices of a graph network works twice as better than traditional information retrieval systems for returning articles relevant to the question. In this work we experiment by using natural language techniques such as lemmatizing and using manual and automatic thesauri for improving question based document retrieval. For the construction of the graph, we follow the approach of representing the set of all the articles as a graph; the question is split in two parts, and each of them is added as part of the graph. Then several paths are constructed from part A of the question to part B, so that the shortest path contains the relevant articles to the question. We evaluate our method comparing the answers given by a traditional information retrieval system - vector space model adjusted for article retrieval, instead of document retrieval - and the answers to 21 questions given manually by the general lawyer of the National Polytechnic Institute, based on 25 different regulations (academy regulation, scholarships regulation, postgraduate studies regulation, etc.); with the answer of our system based on the same set of regulations. We found that lemmatizing increases performance in around 10%, while the use of thesaurus has a low impact. © Springer-Verlag Berlin Heidelberg 2009.},
   author = {Alfredo Monroy and Hiramand Calvo and Alexander Gelbukh},
   doi = {10.1007/978-3-642-00382-0_40/COVER},
   isbn = {3642003818},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {498-508},
   publisher = {Springer, Berlin, Heidelberg},
   title = {NLP for shallow question answering of legal documents using graphs},
   volume = {5449 LNCS},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-642-00382-0_40},
   year = {2009},
}
@article{Ganguly2023,
   abstract = {Artificial Intelligence (AI), Machine Learning (ML), Information Retrieval (IR) and Natural Language Processing (NLP) are transforming the way legal professionals and law firms approach their work. The significant potential for the application of AI to Law, for instance, by creating computational solutions for legal tasks, has intrigued researchers for decades. This appeal has only been amplified with the advent of Deep Learning (DL). It is worth noting that working with legal text is far more challenging as compared to the other subdomains of IR/NLP, mainly due to the typical characteristics of legal text, such as considerably longer documents, complex language and lack of large-scale annotated datasets. In this tutorial, we introduce the audience to these characteristics of legal text, and with it, the challenges associated with processing the legal documents. We touch upon the history of AI and Law research, and how it has evolved over the years from relatively simpler approaches to more complex ones, such as those involving DL. We organize the tutorial as follows. First, we provide a brief introduction to state-of-the-art research in the general domain of IR and NLP. We then discuss in more detail IR/NLP tasks specific to the legal domain. We outline the methodologies (both from an academic and industry perspective), and the available tools and datasets to evaluate the methodologies. This is then followed by a hands-on coding/demo session.},
   author = {Debasis Ganguly and Jack G. Conrad and Kripabandhu Ghosh and Saptarshi Ghosh and Pawan Goyal and Paheli Bhattacharya and Shubham Kumar Nigam and Shounak Paul},
   doi = {10.1007/978-3-031-28241-6_34/COVER},
   isbn = {9783031282409},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {AI & Law,Legal data analytics,Legal information retrieval,Natural language processing},
   pages = {331-340},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Legal IR and NLP: The History, Challenges, and State-of-the-Art},
   volume = {13982 LNCS},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-031-28241-6_34},
   year = {2023},
}
@article{Kaltenboeck2022,
   abstract = {This chapter provides insights about the work done and the results achieved by the Horizon 2020-funded Innovation Action “Lynx—Building the Legal Knowledge Graph for Smart Compliance Services in Multilingual Europe.” The main objective of Lynx is to...},
   author = {Martin Kaltenboeck and Pascual Boil and Pieter Verhoeven and Christian Sageder and Elena Montiel-Ponsoda and Pablo Calleja-Ibáñez},
   doi = {10.1007/978-3-030-78307-5_12},
   journal = {Technologies and Applications for Big Data Value},
   pages = {253-271},
   publisher = {Springer International Publishing},
   title = {Using a Legal Knowledge Graph for Multilingual Compliance Services in Labor Law, Contract Management, and Geothermal Energy},
   year = {2022},
}
@article{Constantin2016,
   abstract = {The availability in machine-readable form of descriptions of the structure of documents, as well as of the document discourse (e.g. the scientific discourse within scholarly articles), is crucial for facilitating semantic publishing and the overall comprehension of documents by both users and machines. In this paper we introduce DoCO, the Document Components Ontology, an OWL 2 DL ontology that provides a general-purpose structured vocabulary of document elements to describe both structural and rhetorical document components in RDF. In addition to giving a formal description of the ontology, this paper showcases its utility in practice in a variety of our own applications and other activities of the Semantic Publishing community that rely on DoCO to annotate and retrieve document components of scholarly articles.},
   author = {Alexandru Constantin and Silvio Peroni and Steve Pettifer and David Shotton and Fabio Vitali},
   doi = {10.3233/SW-150177},
   issn = {22104968},
   issue = {2},
   journal = {Semantic Web},
   keywords = {DEO,DoCO,PDFX,SPAR ontologies,Utopia Documents,document components,rhetoric,structural patterns},
   month = {2},
   pages = {167-181},
   publisher = {IOS Press},
   title = {The Document Components Ontology (DoCO)},
   volume = {7},
   year = {2016},
}
@article{Ngo2023,
   abstract = {Today, the legal document system is increasingly strict with different levels of influence and affects activities in many different fields. The increasing number of legal documents interwoven with each other also leads to difficulties in searching and applying in practice. The construction of knowledge maps that involve one or a group of legal documents is an effective approach to represent actual knowledge domains. A legal knowledge graph constructed from laws and legal documents can enable a number of applications, such as question answering, document similarity, and search. In this paper, we describe the process of building a system of knowledge maps for the Vietnamese legal system from the source of about 325,000 legal documents that span all fields of social life. This study also proposes an integrated ontology to represent the legal knowledge from legal documents. This model integrates the ontology of relational knowledge and the graph of key phrases and entities in the form of a concept graph. It can express the semantics of the content of a given legal document. In addition, this study also describes the process of building and exploiting natural language processing tools to build a VLegalKMaps system, which is a repository of Vietnamese legal knowledge maps. We also highlight open challenges in the realization of knowledge graphs in a technical legal system that enables this approach at scale.},
   author = {Hung Q. Ngo and Hien D. Nguyen and Nhien An Le-Khac},
   doi = {10.1007/978-3-031-36886-8_3/COVER},
   isbn = {9783031368851},
   issn = {23673389},
   journal = {Lecture Notes in Networks and Systems},
   keywords = {Knowledge Engineering,Knowledge Map,Legal AI,Legal Documents,Legal Linked Data},
   pages = {25-36},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Building Legal Knowledge Map Repository with NLP Toolkits},
   volume = {734 LNNS},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-031-36886-8_3},
   year = {2023},
}
@article{Lame2004,
   abstract = {A method to identify ontology components is presented in this article. The method relies on Natural Language Processing (NLP) techniques to extract concepts and relations among these concepts. This method is applied in the legal field to build an ontology dedicated to information retrieval. Legal texts on which the method is performed are carefully chosen as describing and conceptualizing the legal domain. We suggest that this method can help legal ontology designers and may be used while building ontologies dedicated to other tasks than information retrieval. © Springer 2006.},
   author = {Guiraude Lame},
   doi = {10.1007/S10506-005-4160-3/METRICS},
   issn = {09248463},
   issue = {4},
   journal = {Artificial Intelligence and Law},
   keywords = {Natural language processing techniques,Ontology},
   month = {12},
   pages = {379-396},
   publisher = {Springer},
   title = {Using NLP techniques to identify legal ontology components: Concepts and relations},
   volume = {12},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/article/10.1007/s10506-005-4160-3},
   year = {2004},
}
@article{Kapitsaki2017,
   abstract = {Open source software is nowadays widely used and any open source software must carry a prominent license. However, the legal, natural language text of open source licenses is not always easy to interpret and an extensive manual analysis of the text may be required, in order to fully understand its content. Existing approaches present license content based on such manual interpretation. In this paper, we propose an automated license term extraction system (FOSS-LTE) for the identification of the license terms from a specific license text and the creation of a representation of these terms divided into rights, obligations and additional conditions. We present the process employed for the creation of the license term extraction system using NLP techniques and we evaluate its accuracy on a set of sentences from available licenses collected for this purpose.},
   author = {Georgia M. Kapitsaki and Demetris Paschalides},
   doi = {10.1109/APSEC.2017.62},
   isbn = {9781538636817},
   issn = {15301362},
   journal = {Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
   keywords = {license terms,licensing,open source software,topic modeling},
   month = {7},
   pages = {540-545},
   publisher = {IEEE Computer Society},
   title = {Identifying Terms in Open Source Software License Texts},
   volume = {2017-December},
   year = {2017},
}
@article{Saha2017,
   abstract = {With increasing regulation of Big Data, it is becoming essential for organizations to ensure compliance with various data protection standards. The Federal Acquisition Regulations System (FARS) within the Code of Federal Regulations (CFR) includes facts and rules for individuals and organizations seeking to do business with the US Federal government. Parsing and gathering knowledge from such lengthy regulation documents is currently done manually and is time and human intensive. Hence, developing a cognitive assistant for automated analysis of such legal documents has become a necessity. We have developed semantically rich approach to automate the analysis of legal documents and have implemented a system to capture various facts and rules contributing towards building an efficient legal knowledge base that contains details of the relationships between various legal elements, semantically similar terminologies, deontic expressions and cross-referenced legal facts and rules. In this paper, we describe our framework along with the results of automating knowledge extraction from the FARS document (Title 48, CFR). Our approach can be used by Big Data Users to automate knowledge extraction from Large Legal documents.},
   author = {Srishty Saha and Karuna P. Joshi and Renee Frank and Michael Aebig and Jiayong Lin},
   doi = {10.1109/BIGDATA.2017.8258353},
   isbn = {9781538627143},
   journal = {Proceedings - 2017 IEEE International Conference on Big Data, Big Data 2017},
   keywords = {Code of Federal Regulations,Deep Learning,Information Retrieval,NLP},
   month = {7},
   pages = {3596-3603},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Automated knowledge extraction from the federal acquisition regulations system (FARS)},
   volume = {2018-January},
   year = {2017},
}
@article{Zhong2020,
   abstract = {Legal Artificial Intelligence (LegalAI) focuses on applying the technology of artificial intelligence, especially natural language processing, to benefit tasks in the legal domain. In recent years, LegalAI has drawn increasing attention rapidly from both AI researchers and legal professionals, as LegalAI is beneficial to the legal system for liberating legal professionals from a maze of paperwork. Legal professionals often think about how to solve tasks from rule-based and symbol-based methods, while NLP researchers concentrate more on data-driven and embedding methods. In this paper, we describe the history, the current state, and the future directions of research in LegalAI. We illustrate the tasks from the perspectives of legal professionals and NLP researchers and show several representative applications in LegalAI. We conduct experiments and provide an in-depth analysis of the advantages and disadvantages of existing works to explore possible future directions. You can find the implementation of our work from https://github.com/thunlp/CLAIM.},
   author = {Haoxi Zhong and Chaojun Xiao and Cunchao Tu and Tianyang Zhang and Zhiyuan Liu and Maosong Sun},
   doi = {10.18653/V1/2020.ACL-MAIN.466},
   isbn = {9781952148255},
   issn = {0736587X},
   journal = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
   pages = {5218-5230},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {How does NLP benefit legal system: A summary of legal artificial intelligence},
   year = {2020},
}
@article{Dan2023,
   abstract = {Legal question answering is a critical task in artificial intelligence. Since most legal data are presented in text, using natural language processing (NLP) to solve legal question answering is a current research direction. Compared with traditional question answering tasks, legal question answering often contains some potential information, such as legal events, crime process, litigants, and victims. This potential information suggests the legal question answering model reasoning's theme and can help the model improve its reasoning ability. In addition, the legal question answering task must answer based on relevant legal clauses, and the number of relevant legal clauses is usually a lot. Hence, the model needs to eliminate the influence of redundant and noisy clauses. Therefore, we propose a double-granularity-based graph neural network that can reason through potential legal events. Based on this research, we design an attention mechanism based on text interaction and calculate the attention by different window sizes score to decrease the influence of noise graph nodes. Finally, we evaluate the proposed model on the JEC-QA benchmark dataset to demonstrate our method's effectiveness. Experimental results show that the model performs well on the Chinese legal examination data and outperforms classical baselines. Out-performs the best baseline model by 7.08 in overall performance and outperforms the best baseline model in single-choice and multiple-choice questions; they are 7.52 and 6.41, respectively.},
   author = {Jingpei Dan and Tianyuan Zhang and Yuming Wang},
   doi = {10.1109/IJCNN54540.2023.10192015},
   isbn = {9781665488679},
   journal = {Proceedings of the International Joint Conference on Neural Networks},
   keywords = {Double-granularity,Graph neural network,Legal Question Answering},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Double Granularity Graph Network for Chinese Legal Question Answering},
   volume = {2023-June},
   year = {2023},
}
@article{Abualhaija2022,
   abstract = {Software systems are increasingly subject to regulatory compliance. Extracting compliance requirements from regulations is challenging. Ideally, locating compliance-related information in a regulation requires a joint effort from requirements engineers and legal experts, whose availability is limited. However, regulations are typically long documents spanning hundreds of pages, containing legal jargon, applying complicated natural language structures, and including cross-references, thus making their analysis effort-intensive. In this paper, we propose an automated question-answering (QA) approach that assists requirements engineers in finding the legal text passages relevant to compliance requirements. Our approach utilizes large-scale language models fine-tuned for QA, including BERT and three variants. We evaluate our approach on 107 question-answer pairs, manually curated by subject-matter experts, for four different European regulatory documents. Among these documents is the general data protection regulation (GDPR) - a major source for privacy-related requirements. Our empirical results show that, in $\approx 94$% of the cases, our approach finds the text passage containing the answer to a given question among the top five passages that our approach marks as most relevant. Further, our approach successfully demarcates, in the selected passage, the right answer with an average accuracy of $\approx$91%.},
   author = {Sallam Abualhaija and Chetan Arora and Amin Sleimi and Lionel C. Briand},
   doi = {10.1109/RE54965.2022.00011},
   isbn = {9781665470001},
   issn = {23326441},
   journal = {Proceedings of the IEEE International Conference on Requirements Engineering},
   keywords = {BERT,Language Models (LMs),Natural Language Processing (NLP),Question Answering,Regulatory Compliance,Requirements Engineering},
   pages = {39-50},
   publisher = {IEEE Computer Society},
   title = {Automated Question Answering for Improved Understanding of Compliance Requirements: A Multi-Document Study},
   volume = {2022-August},
   year = {2022},
}
@article{Tieu2021,
   abstract = {With robust development in NLP (Natural Language Processing) methods and Deep Learning, there are a variety of solutions to the problems in question answering systems that achieve extraordinary results. In this paper, we describe our approach using at the Automated Legal Question Answering Competition (ALQAC) 2021. In this competition, we achieved the first prize of all tasks with the scores of 88.07%, 71.02%, 69.89% in Task 1, Task 2 and Task 3 respectively.},
   author = {Truong Thinh Tieu and Chieu Nguyen Chau and Nguyen Minh Hoang Bui and Truong Son Nguyen and Le Minh Nguyen},
   doi = {10.1109/KSE53942.2021.9648727},
   isbn = {9781665499750},
   issn = {26944804},
   journal = {Proceedings - International Conference on Knowledge and Systems Engineering, KSE},
   keywords = {alqac 2021,answer selection,deep learning,language model,legal domain,natural language processing,question answering},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Apply Bert-based models and Domain knowledge for Automated Legal Question Answering tasks at ALQAC 2021},
   volume = {2021-November},
   year = {2021},
}
@article{Montelongo2020,
   abstract = {Deep Learning (DL) has become the state-of-the-art method for Natural Language Processing (NLP). During the last 5 years DL became the primary Artificial Intelligence (AI) method in the legal domain. In this work we provide a systematic bibliometric review of the publications that have utilized DL as the primary methodology. In particular we analyzed the performed objectives (performed tasks), the corpus utilized to train the models and promising areas of research. The sample includes a total of 137 works published between 1987 and 2020. This analysis starts with the first DL models (formerly Neural Networks) in the legal domain until the latest articles in the ongoing year. Our results show an increment of 300% on the total number of publications during the last 5 years, mainly on information extraction and classification tasks. Moreover, classification is the category with most publications with 39% of the total sample. Finally, we have identified that summarization and text generation as promising areas of research. These findings show that DL in the legal domain is currently in a growing stage, and hence it will be a promising topic of research in the coming years.},
   author = {Alfredo Montelongo and Joao Luiz Becker},
   doi = {10.1109/ICDMW51313.2020.00113},
   isbn = {9781728190129},
   issn = {23759259},
   journal = {IEEE International Conference on Data Mining Workshops, ICDMW},
   keywords = {Deep Learning,Legal Corpus,Neural Networks},
   month = {11},
   pages = {775-781},
   publisher = {IEEE Computer Society},
   title = {Tasks performed in the legal domain through Deep Learning: A bibliometric review (1987-2020)},
   volume = {2020-November},
   year = {2020},
}
@article{Vogel2018,
   abstract = {Law exists solely in and through language. Nonetheless, systematical empirical analysis of legal language has been rare. Yet, the tides are turning: After judges at various courts (including the US Supreme Court) have championed a method of analysis called corpus linguistics, the Michigan Supreme Court held in June 2016 that this method “is consistent with how courts have understood statutory interpretation.” The court illustrated how corpus analysis can benefit legal casework, thus sanctifying twenty years of previous research into the matter. The present article synthesizes this research and introduces computer-assisted legal linguistics (CAL2) as a novel approach to legal studies. Computer-supported analysis of carefully preprocessed collections of legal texts lets lawyers analyze legal semantics, language, and sociosemiotics in different working contexts (judiciary, legislature, legal academia). The article introduces the interdisciplinary CAL2 research group (www.cal2.eu), its Corpus of German Law, and other related projects that make law more transparent.},
   author = {Friedemann Vogel and Hanjo Hamann and Isabelle Gauer},
   doi = {10.1111/LSI.12305},
   issn = {17474469},
   issue = {4},
   journal = {Law and Social Inquiry},
   month = {9},
   pages = {1340-1363},
   publisher = {Blackwell Publishing Inc.},
   title = {Computer-Assisted Legal Linguistics: Corpus Analysis as a New Tool for Legal Studies},
   volume = {43},
   year = {2018},
}
@article{Veena2019,
   abstract = {This paper presents an ontology-driven question- answering system for Motor Vehicle Department(MVD). The MVD uses the motor vehicle act that regulates almost all the aspects of road transport vehicles. It defines a set of offenses and related penalties. At present all this information are unstructured, so in this work, we propose a Natural Language Processing(NLP) based structured representation for these offenses and its related penalties. We have used Semantic Role Labeling(SRL) to identify the roles of each word in the offenses. We use the result of SRL to create the ontology.},
   author = {G. Veena and Deepa Gupta and Akshay Anil and S. Akhil},
   doi = {10.1109/ICICICT46008.2019.8993168},
   isbn = {9781728102832},
   journal = {2019 2nd International Conference on Intelligent Computing, Instrumentation and Control Technologies, ICICICT 2019},
   keywords = {MVD,OWL,Offenses,Ontology,Penalties,SRL},
   month = {7},
   pages = {947-951},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {An Ontology Driven Question Answering System for Legal Documents},
   year = {2019},
}
@article{Amato2008,
   abstract = {In this paper we describe the use of NLP techniques and ontologies as the core for building novel e-gov based information systems, and in particular we define the main characteristics of a document management system in the legal domain, that manages a variety of paper documents, automatically transforming them into RDF statements, for suitable indexing, retrieval and long term preservation. Although we describe a general architecture that can be used for several application domains, our system is particularly suitable for the Italian notary realm. © 2008 IEEE.},
   author = {Flora Amato and Antonino Mazzeo and Antonio Penta and Antonio Picariello},
   doi = {10.1109/DEXA.2008.86},
   isbn = {9780769532998},
   issn = {15294188},
   journal = {Proceedings - International Workshop on Database and Expert Systems Applications, DEXA},
   pages = {67-71},
   title = {Using NLP and ontologies for notary document management systems},
   year = {2008},
}
@article{Cejas2023,
   abstract = {When the entity processing personal data (the processor) differs from the one collecting personal data (the controller), processing personal data is regulated in Europe by the General Data Protection Regulation (GDPR) through data processing agreements (DPAs). Checking the compliance of DPAs contributes to the compliance verification of software systems as DPAs are an important source of requirements for software development involving the processing of personal data. However, manually checking whether a given DPA complies with GDPR is challenging as it requires significant time and effort for understanding and identifying DPA-relevant compliance requirements in GDPR and then verifying these requirements in the DPA. Legal texts introduce additional complexity due to convoluted language and inherent ambiguity leading to potential misunderstandings. In this paper, we propose an automated solution to check the compliance of a given DPA against GDPR. In close interaction with legal experts, we first built two artifacts: (i) the 'shall' requirements extracted from the GDPR provisions relevant to DPA compliance and (ii) a glossary table defining the legal concepts in the requirements. Then, we developed an automated solution that leverages natural language processing (NLP) technologies to check the compliance of a given DPA against these 'shall' requirements. Specifically, our approach automatically generates phrasal-level representations for the textual content of the DPA and compares them against predefined representations of the 'shall' requirements. By comparing these two representations, the approach not only assesses whether the DPA is GDPR compliant but it further provides recommendations about missing information in the DPA. Over a dataset of 30 actual DPAs, the approach correctly finds 618 out of 750 genuine violations while raising 76 false violations, and further correctly identifies 524 satisfied requirements. The approach has thus an average precision of 89.1%, a recall of 82.4%, and an accuracy of 84.6%. Compared to a baseline that relies on off-the-shelf NLP tools, our approach provides an average accuracy gain of ≈20 percentage points. The accuracy of our approach can be improved to ≈94% with limited manual verification effort.},
   author = {Orlando Amaral Cejas and Muhammad Ilyas Azeem and Sallam Abualhaija and Lionel C. Briand},
   doi = {10.1109/TSE.2023.3288901},
   issn = {19393520},
   issue = {9},
   journal = {IEEE Transactions on Software Engineering},
   keywords = {Requirements engineering (RE),data processing agreement (DPA),natural language processing (NLP),privacy,regulatory compliance,the general data protection regulation (GDPR)},
   month = {9},
   pages = {4282-4303},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {NLP-Based Automated Compliance Checking of Data Processing Agreements Against GDPR},
   volume = {49},
   year = {2023},
}
@article{Kanhaiya2023,
   abstract = {The relationships between the law and fact, and the use of the facts to support a legal conclusion requires practice. This study introduces a technique called the "legal analysis of data extraction"which helps students/law practitioners to quickly assess a law article on the basis of the extracted attributes, and then apply their understanding in order to support a legal conclusion. To ease the processing of the law text data and retrieval of important information (entities) like names of Judges, appellants, respondents, judgment dates, etc. by utilizing NLP processes and ML techniques. This study demonstrates a novel approach of advanced information retrieval in the law articles leveraging natural language processing and machine learning techniques which are productized and used on live data as well. In this custom NER, aggregate accuracy of 95% is achieved using text classification followed by a regex fallback and flagging mechanism. This study consists of the following sections- a) Introduction b) Related Work c) About Dataset d) Methodology and approach e) Fallback mechanisms f) Results and Evaluations g) Conclusions & Future Scope with h) References.},
   author = {Kishan Kanhaiya and Institute Naveen and Arpit Kumar Sharma and Kamlesh Gautam and Pramod Singh Rathore},
   doi = {10.1109/ICAISS58487.2023.10250733},
   isbn = {9798350325799},
   journal = {Proceedings of the 2023 2nd International Conference on Augmented Intelligence and Sustainable Systems, ICAISS 2023},
   keywords = {AI-enabled-Information Retrival Engine (AI-IRE),Name Entity Recognition (NER),Natural Language Processing (NLP),Word Embeddings},
   pages = {206-210},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {AI Enabled-Information Retrival Engine (AI-IRE) in Legal Services: An Expert-Annotated NLP for Legal Judgements},
   year = {2023},
}
@article{Nayak2019,
   abstract = {Due to introduction of General Data Protection Regulation (GDPR) in EU; all the cloud hosted applications (span across multi geo location) wherein captures the personal data need to first identify data privacy protection (DPP) entities and handle it as per EU norms and regulations. The company's legal contracts or transactions in tie up with other parties (customers, partners, suppliers, etc.) are usually stored in to repository; on termination of the contracts either by agreement or mutual consent then the other parties' information are usually archived for historical reasons. The other parties are usually interested in knowing what all documents or transactions they were participated earlier and expects the data to be pruned on need basis. As these documents are unstructured in nature, this paper proposes a solution in identifying all DPP entities with in legal contract documents, index the corpus level accumulated knowledgebase, apply customized ranking algorithm for the retrieved legal contract documents based on DPP search query, derive DPP entities specific legal contract document dependency relation graph for which the parties are participating by using techniques from Information Retrieval, Information Extraction, Natural Language Processing (NLP) and Ontology.},
   author = {Shiva Prasad Nayak and Suresh Pasumarthi},
   doi = {10.1109/DDP.2019.00023},
   isbn = {9781728153636},
   journal = {Proceedings - 2019 1st International Conference on Digital Data Processing, DDP 2019},
   keywords = {DPP,GDPR,Information Extraction,Information Retrieval,Legal Contract Documents,Natural Language Processing,Ontology},
   month = {11},
   pages = {70-75},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Automatic Detection and Analysis of DPP Entities in Legal Contract Documents},
   year = {2019},
}
@article{Eidelman2019,
   abstract = {Automatic summarization methods have been studied on a variety of domains, including news and scientific articles. Yet, legislation has not previously been considered for this task, despite US Congress and state governments releasing tens of thousands of bills every year. In this paper, we introduce BillSum, the first dataset for summarization of US Congressional and California state bills (https://github.com/FiscalNote/BillSum). We explain the properties of the dataset that make it more challenging to process than other domains. Then, we benchmark extractive methods that consider neural sentence representations and traditional contextual features. Finally, we demonstrate that models built on Congressional bills can be used to summarize California bills, thus, showing that methods developed on this dataset can transfer to states without human-written summaries.},
   author = {Vladimir Eidelman},
   doi = {10.18653/v1/d19-5406},
   month = {11},
   pages = {48-56},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {BillSum: A Corpus for Automatic Summarization of US Legislation},
   year = {2019},
}
@article{Song2022,
   abstract = {We present the first comprehensive empirical evaluation of pre-trained language models (PLMs) for legal natural language processing (NLP) in order to examine their effectiveness in this domain. Our study covers eight representative and challenging legal datasets, ranging from 900 to 57K samples, across five NLP tasks: binary classification, multi-label classification, multiple choice question answering, summarization and information retrieval. We first run unsupervised, classical machine learning and/or non-PLM based deep learning methods on these datasets, and show that baseline systems' performance can be 4%35% lower than that of PLM-based methods. Next, we compare general-domain PLMs and those specifically pre-trained for the legal domain, and find that domain-specific PLMs demonstrate 1%5% higher performance than general-domain models, but only when the datasets are extremely close to the pre-training corpora. Finally, we evaluate six general-domain state-of-the-art systems, and show that they have limited generalizability to legal data, with performance gains from 0.1% to 1.2% over other PLM-based methods. Our experiments suggest that both general-domain and domain-specific PLM-based methods generally achieve better results than simpler methods on most tasks, with the exception of the retrieval task, where the best-performing baseline outperformed all PLM-based methods by at least 5%. Our findings can help legal NLP practitioners choose the appropriate methods for different tasks, and also shed light on potential future directions for legal NLP research.},
   author = {Dezhao Song and Sally Gao and Baosheng He and Frank Schilder},
   doi = {10.1109/ACCESS.2022.3190408},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Legal natural language processing,deep learning,machine learning,pre-trained language model},
   pages = {75835-75858},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {On the Effectiveness of Pre-Trained Language Models for Legal Natural Language Processing: An Empirical Study},
   volume = {10},
   year = {2022},
}
@article{Raj2022,
   abstract = {Open Information Extraction system (OIE) extracts textual tuples consisting of arguments and its relation within the input sentence. OIE can also be considered as an unrestricted variant of information extraction. Throughout the years these types of systems have become more and more popular. The main factor for considering such a system is that they are made for speed, then any other factor. Also, one of the major advantages of using such a system is that the relations that are extracted are human-readable. Relation extraction in the legal domain has always remained a challenge because there is no structured data available. hence making this process more challenging. Since we are working in the legal domain, extracting the relations that are present in the document is very important. This leads us to a system that is capable of assisting people in the domain. The introduction of such a system would be really important for professionals like judges, advocates and also for the common people if they want to have a quick glance at certain documents. The outcome of these systems can be really helpful for future downstream applications like question answering, sentence similarity etc.},
   author = {Nikhil Raj and Shiyon Thomas and G. Veena},
   doi = {10.1109/GCAT55367.2022.9971995},
   isbn = {9781665468534},
   journal = {2022 IEEE 3rd Global Conference for Advancement in Technology, GCAT 2022},
   keywords = {MinIE,NLP,Named Entity Recognition,Open information extraction system},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Open Information Extraction System For Extracting Relations in Legal Documents},
   year = {2022},
}
@article{Siena2009,
   abstract = {New laws, such as HIPAA and SOX, are increasingly impacting the design of software systems, as business organisations strive to comply. This paper studies the problem of generating a set of requirements for a new system which comply with a given law. Specifically, the paper proposes a systematic process for generating law-compliant requirements by using a taxonomy of legal concepts and a set of primitives to describe stakeholders and their strategic goals. Given a model of law and a model of stakeholders goals, legal alternatives are identified and explored. Strategic goals that can realise legal prescriptions are systematically analysed, and alternative ways of fulfilling a law are evaluated. The approach is demonstrated by means of a case study. This work is part of the Nomos framework, intended to support the design of law-compliant requirements models. © Springer-Verlag 2009.},
   author = {Alberto Siena and John Mylopoulos and Anna Perini and Angelo Susi},
   doi = {10.1007/978-3-642-04840-1_35},
   isbn = {3642048390},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {472-486},
   title = {Designing law-compliant software requirements},
   volume = {5829 LNCS},
   year = {2009},
}
@article{Sannier2017,
   abstract = {When identifying and elaborating compliance requirements, analysts need to follow the cross references in legal texts and consider the additional information in the cited provisions. Enabling easier navigation and handling of cross references requires automated support for the detection of the natural language expressions used in cross references, the interpretation of cross references in their context, and the linkage of cross references to the targeted provisions. In this article, we propose an approach and tool support for automated detection and resolution of cross references. The approach leverages the structure of legal texts, formalized into a schema, and a set of natural language patterns for legal cross reference expressions. These patterns were developed based on an investigation of Luxembourg’s legislation, written in French. To build confidence about their applicability beyond the context where they were observed, these patterns were validated against the Personal Health Information Protection Act (PHIPA) by the Government of Ontario, Canada, written in both French and English. We report on an empirical evaluation where we assess the accuracy and scalability of our framework over several Luxembourgish legislative texts as well as PHIPA.},
   author = {Nicolas Sannier and Morayo Adedjouma and Mehrdad Sabetzadeh and Lionel Briand},
   doi = {10.1007/S00766-015-0241-3},
   issn = {1432010X},
   issue = {2},
   journal = {Requirements Engineering},
   keywords = {Conceptual modeling,Cross references,Legal compliance,Natural language processing (NLP)},
   month = {6},
   pages = {215-237},
   publisher = {Springer London},
   title = {An automated framework for detection and resolution of cross references in legal texts},
   volume = {22},
   year = {2017},
}
@article{Zeni2015,
   abstract = {Ensuring compliance of software systems with government regulations, policies, and laws is a complex problem. Generally speaking, solutions to the problem first identify rights and obligations defined in the law and then treat these as requirements for the system under design. This work examines the challenge of developing tool support for extracting such requirements from legal documents. To address this challenge, we have developed a tool called GaiusT. The tool is founded on a framework for textual semantic annotation. It semiautomatically generates elements of requirements models, including actors, rights, and obligations. We present the complexities of annotating prescriptive text, the architecture of GaiusT, and the process by which annotation is accomplished. We also present experimental results from two case studies to illustrate the application of the tool and its effectiveness relative to manual efforts. The first case study is based on the US Health Insurance Portability and Accountability Act, while the second analyzes the Italian accessibility law for information technology instruments.},
   author = {Nicola Zeni and Nadzeya Kiyavitskaya and Luisa Mich and James R. Cordy and John Mylopoulos},
   doi = {10.1007/S00766-013-0181-8},
   issn = {1432010X},
   issue = {1},
   journal = {Requirements Engineering},
   keywords = {Legal documents,Legal requirements,Multilingual annotation,Regulation compliance problem,Requirements engineering,Semantic annotation},
   month = {3},
   pages = {1-22},
   publisher = {Springer London},
   title = {GaiusT: supporting the extraction of rights and obligations for regulatory compliance},
   volume = {20},
   year = {2015},
}
@article{Sleimi2018,
   abstract = {[Context] Semantic legal metadata provides information that helps with understanding and interpreting the meaning of legal provisions. Such metadata is important for the systematic analysis of legal requirements. [Objectives] Our work is motivated by two observations: (1) The existing requirements engineering (RE) literature does not provide a harmonized view on the semantic metadata types that are useful for legal requirements analysis. (2) Automated support for the extraction of semantic legal metadata is scarce, and further does not exploit the full potential of natural language processing (NLP). Our objective is to take steps toward addressing these limitations. [Methods] We review and reconcile the semantic legal metadata types proposed in RE. Subsequently, we conduct a qualitative study aimed at investigating how the identified metadata types can be extracted automatically. [Results and Conclusions] We propose (1) a harmonized conceptual model for the semantic metadata types pertinent to legal requirements analysis, and (2) automated extraction rules for these metadata types based on NLP. We evaluate the extraction rules through a case study. Our results indicate that the rules generate metadata annotations with high accuracy.},
   author = {Amin Sleimi and Nicolas Sannier and Mehrdad Sabetzadeh and Lionel Briand and John Dann},
   doi = {10.1109/RE.2018.00022},
   isbn = {9781538674185},
   journal = {Proceedings - 2018 IEEE 26th International Requirements Engineering Conference, RE 2018},
   keywords = {Legal requirements,Natural language processing,Semantic legal metadata},
   month = {10},
   pages = {124-135},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Automated extraction of semantic legal metadata using natural language processing},
   year = {2018},
}
@article{Mahabal2019,
   abstract = {Training data for text classification is often limited in practice, especially for applications with many output classes or involving many related classification problems. This means classifiers must generalize from limited evidence, but the manner and extent of generalization is task dependent. Current practice primarily relies on pre-trained word embeddings to map words unseen in training to similar seen ones. Unfortunately, this squishes many components of meaning into highly restricted capacity. Our alternative begins with sparse pre-trained representations derived from unlabeled parsed corpora; based on the available training data, we select features that offers the relevant generalizations. This produces task-specific semantic vectors; here, we show that a feed-forward network over these vectors is especially effective in low-data scenarios, compared to existing state-of-the-art methods. By further pairing this network with a convolutional neural network, we keep this edge in low data scenarios and remain competitive when using full training sets.},
   author = {Abhijit Mahabal and Jason Baldridge and Burcu Karagol Ayan and Vincent Perot and Dan Roth},
   doi = {10.18653/V1/N19-1319},
   isbn = {9781950737130},
   journal = {NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference},
   pages = {3158-3167},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {Text classification with few examples using controlled generalization},
   volume = {1},
   year = {2019},
}
@article{Bambroo2021,
   abstract = {Transformers have caused a paradigm shift in tasks related to natural language. From text summarization to classification, these models have established new state-of-the-art results on various general and closed domain tasks. Having said that, most of the popular transformer based models (BERT - Bidirectional Encoder Representations from Transformers, DistilBERT, and RoBERTa) face a limitation in terms of the content length they can accept. This is because the self-attention used by these models scales quadratically with the context sequence length. While this works well for short passages and questions, for longer documents this method fails to effectively capture both the local and global contexts. This shortcoming is underscored further for closed domain tasks like Legal Document classification, where the length of the documents can extend up to a couple of pages and which differ in their vocabulary from general English substantially. In this paper, a new architecture is proposed, where the concept of “long” attention is applied to a distilled BERT and then the model is pre-trained on legal-domain specific corpora. This helps combine a local windowed attention with task-motivated global attention, making the model contextually-aware for longer sequences. The proposed model is also able to outperform fine-tuned BERT and other transformer-based models at the task of legal document classification while also being faster.},
   author = {Purbid Bambroo and Aditi Awasthi},
   doi = {10.1109/ICAECT49130.2021.9392558},
   isbn = {9781728157900},
   journal = {Proceedings of the 2021 1st International Conference on Advances in Electrical, Computing, Communications and Sustainable Technologies, ICAECT 2021},
   keywords = {Contextual awareness,DistilBERT,Legal document classification,Longformer,Self attention},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {LegalDB: Long distilbert for legal document classification},
   year = {2021},
}
@article{Fujita2023,
   abstract = {We report our system architecture of COLIEE 2022 Task 4, which challenges to solve the textual entailment part of the Japanese legal bar examination problems. We successfully improved the correct answer ratio by an ensemble of a rule-based method and BERT-based method. Our proposed methods mainly consist of two parts: data augmentations of training dataset and an ensemble of the methods. Regarding training data augmentation, the civil law articles are segmented once and reconstructed again with all the combinations. Data expansion is then performed by replacing the data with negative forms and alphabetical symbols. Focusing on the characteristics that the rule-based method is high in its precision but low in its coverage, we employed a modular way in our ensemble. We integrated other proposed methods such as Sentence-BERT to select necessary data, person name inference to replace alphabetical anonymized symbols with the actual role name of the person. We confirmed that our suggested methods are effective by comparing with our baseline models, achieved 0.6789 correct answer ratio in accuracy on the formal run test dataset, which was the best score among the COLIEE 2022 Task 4 submissions.},
   author = {Masaki Fujita and Takaaki Onaga and Ayaka Ueyama and Yoshinobu Kano},
   doi = {10.1007/978-3-031-29168-5_10},
   isbn = {9783031291678},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {BERT,COLIEE,Legal Bar Exam,Legal information Extraction,Predicate Argument Structure Analysis,Question Answering},
   pages = {138-153},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Legal Textual Entailment Using Ensemble of Rule-Based and BERT-Based Method with Data Augmentation by Related Article Generation},
   volume = {13859 LNAI},
   year = {2023},
}
@article{Katz2023,
   abstract = {In this paper, we summarize the current state of the field of NLP & Law with a specific focus on recent technical and substantive developments. To support our analysis, we construct and analyze a nearly complete corpus of more than six hundred NLP & Law related papers published over the past decade. Our analysis highlights several major trends. Namely, we document an increasing number of papers written, tasks undertaken, and languages covered over the course of the past decade. We observe an increase in the sophistication of the methods which researchers deployed in this applied context. Slowly but surely, Legal NLP is beginning to match not only the methodological sophistication of general NLP but also the professional standards of data availability and code reproducibility observed within the broader scientific community. We believe all of these trends bode well for the future of the field, but many questions in both the academic and commercial sphere still remain open.},
   author = {Daniel Martin Katz and Dirk Hartung and Lauritz Gerlach and Abhik Jana and Michael James Bommarito},
   doi = {10.2139/SSRN.4336224},
   journal = {SSRN Electronic Journal},
   publisher = {Elsevier BV},
   title = {Natural Language Processing in the Legal Domain},
   year = {2023},
}
@article{Chalkidis2019,
   abstract = {Deep Learning has been widely used for tackling challenging natural language processing tasks over the recent years. Similarly, the application of Deep Neural Networks in legal analytics has increased significantly. In this survey, we study the early adaptation of Deep Learning in legal analytics focusing on three main fields; text classification, information extraction, and information retrieval. We focus on the semantic feature representations, a key instrument for the successful application of deep learning in natural language processing. Additionally, we share pre-trained legal word embeddings using the word2vec model over large corpora, comprised legislations from UK, EU, Canada, Australia, USA, and Japan among others.},
   author = {Ilias Chalkidis and Dimitrios Kampas},
   doi = {10.1007/S10506-018-9238-9},
   issn = {15728382},
   issue = {2},
   journal = {Artificial Intelligence and Law},
   keywords = {Deep learning,Legal word vectors,Natural language processing},
   month = {6},
   pages = {171-198},
   publisher = {Springer Netherlands},
   title = {Deep learning in law: early adaptation and legal word embeddings trained on large corpora},
   volume = {27},
   year = {2019},
}
@article{Nasar2021,
   abstract = {With the advent of Web 2.0, there exist many online platforms that result in massive textual-data production. With ever-increasing textual data at hand, it is of immense importance to extract infor...},
   author = {Zara Nasar and Syed Waqar Jaffry and Muhammad Kamran Malik},
   doi = {10.1145/3445965},
   isbn = {10.1145/3445965},
   issn = {15577341},
   issue = {1},
   journal = {ACM Computing Surveys (CSUR)},
   keywords = {Information extraction,deep learning,joint modeling,named entity recognition,relation extraction},
   month = {2},
   pages = {39},
   publisher = {
		ACM
		PUB27
		New York, NY, USA
	},
   title = {Named Entity Recognition and Relation Extraction},
   volume = {54},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3445965},
   year = {2021},
}
@article{Mumford2023,
   abstract = {Recent years have witnessed significant progress in the deployment of advanced Natural Language Processing (NLP) techniques based on transformer technology, across many domains and applications. However, in legal domains, due to the complexity, length, and sparsity of legal case documents, the use of these advanced NLP techniques has offered comparatively slight returns. Perhaps even more importantly, such methods are critically lacking in explain-ability and justification of outputs, which are essential for many legal applications. We propose that the direction of these NLP techniques should be aimed at ascription to a legal knowledge model, which can then provide the necessary and auditable justifications for the rationale of any case outcome. In this paper we investigate the effectiveness of using Hierarchical Bidirectional Encoder Representations from Transformers (H-BERT) models to ascribe to an Angelic Domain Model (ADM) that is able to represent the legal knowledge of a domain in a structured way, enabling justifications and improving performance. Our study involved an annotation task on a popular domain, cases from the European Court of Human Rights, to gain an understanding of the balance of complaints in the domain. The data set produced from this study enabled training of models for factor ascription using the classification targets derived from the annotations. We present results of experiments conducted to evaluate the performance of the ascription task at three different levels of abstraction within the structured model. CCS CONCEPTS • Applied computing → Law.},
   author = {Jack Mumford and Katie Atkinson and Trevor Bench-Capon},
   doi = {10.1145/3594536.3595158},
   isbn = {9798400701979},
   issue = {2023},
   keywords = {Transformers,case annotation,domain model,factor ascription},
   month = {6},
   pages = {167-176},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Combining a Legal Knowledge Model with Machine Learning for Reasoning with Legal Cases},
   volume = {10},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3594536.3595158},
   year = {2023},
}
@article{Behnke2023,
   abstract = {Legal language is considered to be a key obstacle to the comprehen-sibility of court decisions for laypeople. While differences between written 'standard' and legal language have already been analysed with regard to syntactic peculiarities, there is still a lack of findings on the influence of divergent word meanings on comprehensibility. We present the course and the preliminary results of a study elaborating such ambiguities on the basis of over half a million German court decisions. As these differences are highly language-dependent, our study consequentially relates (only) to German. CCS CONCEPTS • Applied Computing → Law.},
   author = {Gregor Behnke and Niklas Wais},
   doi = {10.1145/3594536.3595123},
   isbn = {9798400701979},
   keywords = {NLP,semantics of legal texts},
   month = {6},
   pages = {382-386},
   publisher = {Association for Computing Machinery (ACM)},
   title = {On the Semantic Difference of Judicial and Standard Language},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3594536.3595123},
   year = {2023},
}
@article{Chalkidis2020,
   abstract = {BERT has achieved impressive performance in several NLP tasks. However, there has been limited investigation on its adaptation guidelines in specialised domains. Here we focus on the legal domain, where we explore several approaches for applying BERT models to downstream legal tasks, evaluating on multiple datasets. Our findings indicate that the previous guidelines for pre-training and fine-tuning, often blindly followed, do not always generalize well in the legal domain. Thus we propose a systematic investigation of the available strategies when applying BERT in specialised domains. These are: (a) use the original BERT out of the box, (b) adapt BERT by additional pre-training on domain-specific corpora, and (c) pre-train BERT from scratch on domain-specific corpora. We also propose a broader hyper-parameter search space when fine-tuning for downstream tasks and we release LEGAL-BERT, a family of BERT models intended to assist legal NLP research, computational law, and legal technology applications.},
   author = {Ilias Chalkidis and Manos Fergadiotis and Prodromos Malakasiotis and Nikolaos Aletras and Ion Androutsopoulos},
   doi = {10.18653/v1/2020.findings-emnlp.261},
   isbn = {9781952148903},
   journal = {Findings of the Association for Computational Linguistics Findings of ACL: EMNLP 2020},
   pages = {2898-2904},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {LEGAL-BERT: The muppets straight out of law school},
   year = {2020},
}
@article{Jiang2023,
   abstract = {Legal syllogism is a form of deductive reasoning commonly used by legal professionals to analyze cases. In this paper, we propose legal syllogism prompting (LoT), a simple prompting method to teach large language models (LLMs) for legal judgment prediction. LoT teaches only that in the legal syllogism the major premise is law, the minor premise is the fact, and the conclusion is judgment. Then the models can produce a syllogism reasoning of the case and give the judgment without any learning, fine-tuning, or examples. On CAIL2018, a Chinese criminal case dataset, we performed zero-shot judgment prediction experiments with GPT-3 models. Our results show that LLMs with LoT achieve better performance than the baseline and chain of thought prompting, the state-of-art prompting method on diverse reasoning tasks. LoT enables the model to concentrate on the key information relevant to the judgment and to correctly understand the legal meaning of acts, as compared to other methods. Our method enables LLMs to predict judgment along with law articles and justification, which significantly enhances the explainability of models.},
   author = {Cong Jiang and Xiaolei Yang},
   doi = {10.1145/3594536.3595170},
   isbn = {9798400701979},
   keywords = {Artificial intelligence KEYWORDS large language models, legal syllogism, legal judgment prediction, chain of thought,CCS CONCEPTS • Applied computing → Law,• Computing methodologies → Natural language generation},
   month = {6},
   pages = {417-421},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Legal Syllogism Prompting},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3594536.3595170},
   year = {2023},
}
@article{Giaoui2023,
   abstract = {Motivated by the subjective decision making and lack of strict protocols in damages as a remedy for contract breach, this project uses natural legal language processing (NLLP) and artificial intelligence (AI) techniques to analyze patterns in contract law cases and reduce uncertainty in their outcome. A 'hybrid' model combining heuristics, NLLP & the results of an LSTM based model into an XGBoost regressor along with contextual information had the best performance for the classification of entity types from unstructured proceedings text. Linear regressors were developed to approximate the Recovery Rate and the Win Rate using a set of 6 engineered features likely to affect the outcome.},
   author = {Frank Giaoui and Luv Aggarwal and Diego Lobo and Joan Gondolo and Philippe Lachkeur and Satvik Jain},
   doi = {10.1145/3594536.3595119},
   isbn = {9798400701979},
   month = {6},
   pages = {468-469},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Applying NLLP and ML to Predict Damages as a Remedy for Contract Breach},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3594536.3595119},
   year = {2023},
}
@article{Goebel2023,
   abstract = {We summarize the 10th Competition on Legal Information Extraction and Entailment. In this edition, the competition included four tasks on case law and statute law. The case law …},
   author = {Randy Goebel and Yoshinobu Kano and Mi-Young Kim and Juliano Rabelo and Ken Satoh and Masaharu Yoshioka},
   doi = {10.1145/3594536.3595176},
   isbn = {9798400701979},
   keywords = {CCS CONCEPTS • Information systems → Content analysis and feature se-lection,Clustering and classification,Document topic models,Information extraction,Similarity measures,Specialized informa-tion retrieval KEYWORDS legal textual entailment, legal information retrieval, text classifica-tion, imbalanced datasets},
   month = {6},
   pages = {472-480},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Summary of the Competition on Legal Information, Extraction/Entailment (COLIEE) 2023},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3594536.3595176},
   year = {2023},
}
@article{Savelka2023,
   abstract = {We evaluated the capability of a state-of-the-art generative pre-trained transformer (GPT) model to perform semantic annotation of short text snippets (one to few sentences) coming from legal documents of various types. Discussions of potential uses (e.g., document drafting, summarization) of this emerging technology in legal domain have intensified, but to date there has not been a rigorous analysis of these large language models' (LLM) capacity in sentence-level semantic annotation of legal texts in zero-shot learning settings. Yet, this particular type of use could unlock many practical applications (e.g., in contract review) and research opportunities (e.g., in empirical legal studies). We fill the gap with this study. We examined if and how successfully the model can semantically annotate small batches of short text snippets (10-50) based exclusively on concise definitions of the semantic types. We found that the GPT model performs surprisingly well in zero-shot settings on diverse types of documents (F 1 = .73 on a task involving court opinions, .86 for contracts, and .54 for statutes and regulations). These findings can be leveraged by legal scholars and practicing lawyers alike to guide their decisions in integrating LLMs in wide range of workflows involving semantic annotation of legal texts.},
   author = {Jaromir Savelka},
   doi = {10.1145/3594536.3595161},
   isbn = {9798400701979},
   month = {6},
   pages = {447-451},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Unlocking Practical Applications in Legal Domain},
   year = {2023},
}
@article{Servantez2023,
   abstract = {The emergence of contract specific programming languages has struggled to translate into widespread adoption of computable contracts due largely to high conversion costs. In this work, we present the first system for converting natural language contracts into code through the extraction of key entities, relationships, and formulas into a graph representation called the Obligation Logic Graph (OLG). This approach allows the semantic meaning of contract obligations, including dependencies between obligations, to be captured through the OLG and mapped to code downstream. We also introduce OLG extraction as a new joint entity and relation prediction task for legal contracts, and present the Contract-OLG dataset, consisting of 1,876 contract provisions, 18,597 entities and 18,170 relationships. We perform detailed experiments to understand the capabilities of state-of-the-art Transformer and graph-based models at completing these tasks, and identify where there is currently a significant gap between human expert and machine performance, particularly for relation extraction.},
   author = {Sergio Servantez and Nedim Lipka and Alexa Siu and Milan Aggarwal and Balaji Krishnamurthy and Aparna Garimella and Kristian Hammond KristianHammond and Rajiv Jain and Balaji Krish-namurthy and Kristian Hammond},
   doi = {10.1145/3594536.3595152},
   isbn = {9798400701979},
   keywords = {CCS CONCEPTS • Computing methodologies → Natural language processing,Information extraction,Machine learning,• Applied com-puting → Law KEYWORDS natural language processing, information extraction, computable contracts, obligation logic graph},
   month = {6},
   pages = {371-380},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Argument Mining with Graph Representation Learning},
   volume = {10},
   url = {https://doi.org/10.1145/3594536.3595162},
   year = {2023},
}
@article{Xu2023,
   abstract = {Duties and powers are two fundamental notions in private law. In this work, we provide our conception of duties and powers and present a logic for reasoning about them. We treat duties as agents' obligations towards others to perform actions. We think that powers are agents' legal abilities, conferred by law, to change legal positions between agents. How to exercise powers is also specified by law. Many factors, including the exercise of powers, fulfillment of duties, violation of duties, and factual changes in the world, can change duties. The ontic level of the logic is a multi-agent dynamic logic, where agents have abilities to change atomic facts. At its normative level, agents have duties towards others to change atomic facts, and have powers to change duties by changing atomic facts. When agents behave, the ontic and normative aspects of the world change accordingly. The implications of the formalization are studied extensively. CCS CONCEPTS • Theory of computation → Modal and temporal logics.},
   author = {Tianwen Xu and Fengkui Ju},
   doi = {10.1145/3594536.3595133},
   isbn = {9798400701979},
   keywords = {abilities,actions,duties,exercise of powers,powers},
   month = {6},
   pages = {361-370},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Multi-agent logic for reasoning about duties and powers in private law},
   url = {https://doi.org/10.1145/3594536.3595133},
   year = {2023},
}
@article{Woerkom2023,
   abstract = {Duties and powers are two fundamental notions in private law. In this work, we provide our conception of duties and powers and present a logic for reasoning about them. We treat duties as agents' obligations towards others to perform actions. We think that powers are agents' legal abilities, conferred by law, to change legal positions between agents. How to exercise powers is also specified by law. Many factors, including the exercise of powers, fulfillment of duties, violation of duties, and factual changes in the world, can change duties. The ontic level of the logic is a multi-agent dynamic logic, where agents have abilities to change atomic facts. At its normative level, agents have duties towards others to change atomic facts, and have powers to change duties by changing atomic facts. When agents behave, the ontic and normative aspects of the world change accordingly. The implications of the formalization are studied extensively. CCS CONCEPTS • Theory of computation → Modal and temporal logics.},
   author = {Wijnand van Woerkom and Davide Grossi and Henry Prakken and Bart Verheij},
   doi = {10.1145/3594536.3595154},
   isbn = {9798400701979},
   keywords = {abilities,actions,duties,exercise of powers,powers},
   month = {6},
   pages = {333-342},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Hierarchical Precedential Constraint},
   url = {https://doi.org/10.1145/3594536.3595133},
   year = {2023},
}
@article{Servantez2023b,
   abstract = {The emergence of contract specific programming languages has struggled to translate into widespread adoption of computable contracts due largely to high conversion costs. In this work, we present the first system for converting natural language contracts into code through the extraction of key entities, relationships, and formulas into a graph representation called the Obligation Logic Graph (OLG). This approach allows the semantic meaning of contract obligations, including dependencies between obligations, to be captured through the OLG and mapped to code downstream. We also introduce OLG extraction as a new joint entity and relation prediction task for legal contracts, and present the Contract-OLG dataset, consisting of 1,876 contract provisions, 18,597 entities and 18,170 relationships. We perform detailed experiments to understand the capabilities of state-of-the-art Transformer and graph-based models at completing these tasks, and identify where there is currently a significant gap between human expert and machine performance, particularly for relation extraction.},
   author = {Sergio Servantez and Nedim Lipka and Alexa Siu and Milan Aggarwal and Balaji Krishnamurthy and Aparna Garimella and Kristian Hammond KristianHammond and Rajiv Jain and Balaji Krish-namurthy and Kristian Hammond},
   doi = {10.1145/3594536.3595162},
   isbn = {9798400701979},
   keywords = {CCS CONCEPTS • Computing methodologies → Natural language processing,Information extraction,Machine learning,• Applied com-puting → Law KEYWORDS natural language processing, information extraction, computable contracts, obligation logic graph},
   month = {6},
   pages = {267-276},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Computable Contracts by Extracting Obligation Logic Graphs},
   volume = {10},
   url = {https://doi.org/10.1145/3594536.3595162},
   year = {2023},
}
@article{Lewis2023,
   abstract = {What happens if the way in which we handle a genuine deontic conflict -i.e., a deontic ambiguity-matters regarding the application of other norms that are not directly affected by that conflict? We argue that the law requires sometimes propagating the ambiguity to other norms and sometimes confining it to some norms only. We explore this issue and model different reasoning patterns. The problem is addressed in a new variant of Defeasible Deontic Logic. The contribution of this paper is threefold: (a) we extend the treatment of ambiguity blocking and propagation to Defeasible Deontic Logic; (b) we discuss reasoning patterns in the law, especially in criminal law, where we need to deal with both ambiguity blocking and ambiguity propagation in the same legal system and logic; (c) we devise an annotated variant of Defeasible Deontic Logic where we distinguish literals that must be obtained through an ambiguity-blocking mechanism from those that are derived using an ambiguity-propagating mechanism.},
   author = {David D Lewis and Lenora Gray and Mark Noel},
   doi = {10.1145/3594536.3595175},
   isbn = {9798400701979},
   keywords = {SAVI,conformal prediction,e-disclosure,e-discovery,eDisclosure,effectiveness metrics,martingales,safe anytime-valid inference},
   month = {6},
   pages = {91-100},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Deontic Ambiguities in Legal Reasoning},
   url = {https://doi.org/10.1145/3594536.3595167},
   year = {2023},
}
@article{Lewis2023b,
   abstract = {Technology-assisted review (TAR) workflows are central to electronic discovery (eDiscovery). Researchers have proposed many methods for evaluating TAR workflows, but this research has had little impact on eDiscovery practice. We examine the operational constraints faced by eDiscovery reviewers and managers, and show how past evaluation proposals are inconsistent with their needs. We then present a new evaluation approach for one-phase TAR workflows based on confidence sequences. Our approach provides a review manager with complete control over the design and duration of the TAR workflow, as well as the amount and timing of review of evaluation documents. Evaluation documents can be reused for supervised learning while preserving valid frequentist confidence intervals on recall at all points during review. The method is expensive in terms of sample size but plausible for large scale reviews, and has many opportunities for improvement. CCS CONCEPTS • Information systems → Retrieval effectiveness; • Applied computing → Enterprise applications; • Human-centered computing → Interaction design process and methods; • Mathematics of computing → Stochastic processes.},
   author = {David D Lewis and Lenora Gray and Mark Noel},
   doi = {10.1145/3594536.3595148},
   isbn = {9798400701979},
   keywords = {SAVI,conformal prediction,e-disclosure,e-discovery,eDisclosure,effectiveness metrics,martingales,safe anytime-valid inference},
   month = {6},
   pages = {52-61},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Reasoning with hierarchies of open-textured predicates},
   url = {https://doi.org/10.1145/3594536.3595167},
   year = {2023},
}
@article{Hulstijn2023,
   abstract = {What happens if the way in which we handle a genuine deontic conflict -i.e., a deontic ambiguity-matters regarding the application of other norms that are not directly affected by that conflict? We argue that the law requires sometimes propagating the ambiguity to other norms and sometimes confining it to some norms only. We explore this issue and model different reasoning patterns. The problem is addressed in a new variant of Defeasible Deontic Logic. The contribution of this paper is threefold: (a) we extend the treatment of ambiguity blocking and propagation to Defeasible Deontic Logic; (b) we discuss reasoning patterns in the law, especially in criminal law, where we need to deal with both ambiguity blocking and ambiguity propagation in the same legal system and logic; (c) we devise an annotated variant of Defeasible Deontic Logic where we distinguish literals that must be obtained through an ambiguity-blocking mechanism from those that are derived using an ambiguity-propagating mechanism.},
   author = {Joris Hulstijn and Joris Hulstijn@uni Lu},
   doi = {10.1145/3594536.3595175},
   isbn = {9798400701979},
   keywords = {AI,computational accountability,ethics,internal controls},
   month = {6},
   pages = {91-100},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Deontic Ambiguities in Legal Reasoning},
   url = {https://doi.org/10.1145/3594536.3595122},
   year = {2023},
}
@article{Chen2023,
   abstract = {Legal Question Answering (LQA) is a promising artificial intelligence application with high practical value. A professional and effective legal question answering (QA) agent can assist in reducing the workload of lawyers and judges, and help to achieve judicial accessibility. However, the NLP community lacks a large-scale LQA dataset with high quality, making it difficult to develop practical data-driven LQA agents. To tackle this bottleneck, this work presents EQUALS, a well-annotated real-world dataset for lEgal QUestion Answering via reading Chinese LawS. EQUALS contains 6,914 \{question, article , answer\} triplets as well as a pool of articles of laws that covers 10 different collections of Chinese Laws. Questions and the corresponding answers in EQUALS are collected from a professional law consultation forum. More importantly, the exact spans of law articles are annotated by senior law students as the answers. In this way, we could assure the quality and professionalism of EQUALS. Furthermore, this work proposes a QA framework that encompasses a law article retrieval module and a machine reading comprehension module for extracting accurate answers from the law article. We conduct thorough experiments with representative baselines on EQUALS, and the results indicate that EQUALS is a challenging question answering task. To the best of our knowledge, EQUALS is the largest real-world LQA dataset which shall significantly promote the research of LQA tasks. The work has been open-sourced and is available at: https://github.com/andongBlue/EQUALS. * Corresponding Authors.},
   author = {Andong Chen and Feng Yao and Xinyan Zhao and Yating Zhang and Changlong Sun and Yun Liu and Weixing Shen},
   doi = {10.1145/3594536.3595121},
   isbn = {9798400701979},
   keywords = {CCS CONCEPTS • Information systems → Question answering,Recommender sys-tems,• Computing methodologies → Language resources KEYWORDS Legal Dataset, Legal Question Answering, Question Answering Framework},
   month = {6},
   pages = {32-41},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Analogical Reasoning, Generalization, and Rule Learning for Common Law Reasoning},
   url = {https://doi.org/10.1145/3594536.3595159},
   year = {2023},
}
@article{Blair-Stanek2023,
   abstract = {Statutory reasoning is the task of reasoning with facts and statutes, which are rules written in natural language by a legislature. It is a basic legal skill. In this paper we explore the capabilities of the most capable GPT-3 model, text-davinci-003, on an established statutory-reasoning dataset called SARA. We consider a variety of approaches, including dynamic few-shot prompting, chain-of-thought prompting, and zero-shot prompting. While we achieve results with GPT-3 that are better than the previous best published results, we also identify several types of clear errors it makes. In investigating why these happen, we discover that GPT-3 has imperfect prior knowledge of the actual U.S. statutes on which SARA is based. More importantly, GPT-3 performs poorly at answering straightforward questions about simple synthetic statutes. By also posing the same questions when the synthetic statutes are written in sentence form, we find that some of GPT-3's poor performance results from difficulty in parsing the typical structure of statutes, containing subsections and paragraphs.},
   author = {Andrew Blair-Stanek and Nils Holzenberger and Benjamin Van Durme},
   doi = {10.1145/3594536.3595163},
   isbn = {9798400701979},
   keywords = {AI,computational accountability,ethics,internal controls},
   month = {6},
   pages = {22-31},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Can GPT-3 Perform Statutory Reasoning?},
   url = {https://doi.org/10.1145/3594536.3595122},
   year = {2023},
}
@article{Kaur2023,
   abstract = {A number of datasets for Relation Extraction (RE) have been created to aide downstream tasks such as information retrieval, semantic search, question answering and textual entailment. However, these datasets fail to capture financial-domain specific challenges since most of these datasets are compiled using general knowledge sources, hindering real-life progress and adoption within the financial world. To address this limitation, we propose REFinD, the first large-scale annotated dataset of relations, with ∼29K instances and 22 relations amongst 8 types of entity pairs, generated entirely over financial documents. We also provide an empirical evaluation with various state-of-the-art models as benchmarks for the RE task and highlight the challenges posed by our dataset. We observed that various state-of-the-art deep learning models struggle with numeric inference, relational and directional ambiguity.},
   author = {Simerjot Kaur and Joy Sain and Charese Smiley and Dongsheng Wang and Akshat Gupta and Suchetha Siddagangappa and Toyin Aguda and Sameena Shah},
   doi = {10.1145/3539618.3591911},
   isbn = {9781450394086},
   journal = {SIGIR 2023 - Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
   keywords = {annotation datasets,benchmarking,finance,information retrieval,natural language processing,relation extraction},
   month = {7},
   pages = {3054-3063},
   publisher = {Association for Computing Machinery, Inc},
   title = {REFinD: Relation Extraction Financial Dataset},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3539618.3591911},
   year = {2023},
}
@article{Koshiyama2020,
   abstract = {This paper reviews Artificial Intelligence (AI), Machine Learning (ML) and associated algorithms in future Capital Markets. New AI algorithms are constantly emerging, with each 'strain' mimicking a new form of human learning, reasoning, knowledge, and decisionmaking. The current main disrupting forms of learning include Deep Learning, Adversarial Learning, Transfer and Meta Learning. Albeit these modes of learning have been in the AI/ML field more than a decade, they now are more applicable due to the availability of data, computing power and infrastructure. These forms of learning have produced new models (e.g., Long Short-Term Memory, Generative Adversarial Networks) and leverage important applications (e.g., Natural Language Processing, Adversarial Examples, Deep Fakes, etc.). These new models and applications will drive changes in future Capital Markets, so it is important to understand their computational strengths and weaknesses. Since ML algorithms effectively self-program and evolve dynamically, financial institutions and regulators are becoming increasingly concerned with ensuring there remains a modicum of human control, focusing on Algorithmic Interpretability/Explainability, Robustness and Legality. For example, the concern is that, in the future, an ecology of trading algorithms across different institutions may 'conspire' and become unintentionally fraudulent (cf. LIBOR) or subject to subversion through compromised datasets (e.g. Microsoft Tay). New and unique forms of systemic risks can emerge, potentially coming from excessive algorithmic complexity. The contribution of this paper is to review AI, ML and associated algorithms, their computational strengths and weaknesses, and discuss their future impact on the Capital Markets.},
   author = {Adriano Koshiyama and Nick Firoozye and Philip Treleaven},
   doi = {10.1145/3383455.3422539},
   isbn = {9781450375849},
   journal = {ICAIF 2020 - 1st ACM International Conference on AI in Finance},
   keywords = {Artificial intelligence,Deep learning,Finance,Generative adversarial networks,Machine learning,Transfer learning},
   month = {10},
   publisher = {Association for Computing Machinery, Inc},
   title = {Algorithms in future capital markets: A survey on AI, ML and associated algorithms in capital markets},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3383455.3422539},
   year = {2020},
}
@article{Takayanagi2023,
   abstract = {With the development of online platforms, people can share and obtain opinions quickly. It also makes individuals' preferences change dynamically and rapidly because they may change their minds when getting convincing opinions from other users. Unlike representative areas of recommendation research such as e-commerce platforms where items' features are fixed, in investment scenarios financial instruments' features such as stock price, also change dynamically over time. To capture these dynamic features and provide a better-personalized recommendation for amateur investors, this study proposes a Personalized Dynamic Recommender System for Investors, PDRSI. The proposed PDRSI considers two investor's personal features: dynamic preferences and historical interests, and two temporal environmental properties: recent discussions on the social media platform and the latest market information. The experimental results support the usefulness of the proposed PDRSI, and the ablation studies show the effect of each module. For reproduction, we follow Twitter's developer policy to share our dataset for future work.},
   author = {Takehiro Takayanagi and Chung Chi Chen and Kiyoshi Izumi},
   doi = {10.1145/3539618.3592035},
   isbn = {9781450394086},
   journal = {SIGIR 2023 - Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
   keywords = {financial data mining,investor modeling,stock recommendation},
   month = {7},
   pages = {2246-2250},
   publisher = {Association for Computing Machinery, Inc},
   title = {Personalized Dynamic Recommender System for Investors},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3539618.3592035},
   year = {2023},
}
@article{Tian2021,
   abstract = {Both authors contributed equally to this research. This paper presents the method that we tackled the FinSBD-3 shared task (structure boundary detection) to extract the boundaries of sentences, lists, and items, including structure elements like footer, header, tables from noisy unstructured English and French financial texts. The deep attention model based on word embedding using data augmentation and BERT model named as hybrid deep learning model to detect the sentence, list-item, footer, header, tables boundaries in noisy English and French texts and classify the list-item sentences into list & different item types using deep attention model. The experiment is shown that the proposed method could be an effective solution to deal with the FinSBD-3 shared task. The submitted result ranks first based on the task metrics in the final leader board.},
   author = {Ke Tian and Hua Chen},
   doi = {10.1145/3442442.3451380},
   isbn = {9781450383134},
   journal = {The Web Conference 2021 - Companion of the World Wide Web Conference, WWW 2021},
   keywords = {Attention,BERT,Data Augmentation,FinSBD-3 shared task,Financial Texts,LSTM,Structure Boundary Detec-tion},
   month = {4},
   pages = {283-287},
   publisher = {Association for Computing Machinery, Inc},
   title = {Aiai at the FinSBD-3 task: Structure Boundary Detection of Noisy Financial Texts in English and French Using Data Augmentation and Hybrid Deep Learning Model},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3442442.3451380},
   year = {2021},
}
@article{Zhang2023b,
   abstract = {Financial sentiment analysis is critical for valuation and investment decision-making. Traditional NLP models, however, are limited by their parameter size and the scope of their training datasets, which hampers their generalization capabilities and effectiveness in this field. Recently, Large Language Models (LLMs) pre-trained on extensive corpora have demonstrated superior performance across various NLP tasks due to their commendable zero-shot abilities. Yet, directly applying LLMs to financial sentiment analysis presents challenges: The discrepancy between the pre-training objective of LLMs and predicting the sentiment label can compromise their predictive performance. Furthermore, the succinct nature of financial news, often devoid of sufficient context, can significantly diminish the reliability of LLMs' sentiment analysis. To address these challenges, we introduce a retrieval-augmented LLMs framework for financial sentiment analysis. This framework includes an instruction-tuned LLMs module, which ensures LLMs behave as predictors of sentiment labels, and a retrieval-augmentation module which retrieves additional context from reliable external sources. Benchmarked against traditional models and LLMs like ChatGPT and LLaMA, our approach achieves 15% to 48% performance gain in accuracy and F1 score. CCS CONCEPTS • Computing methodologies → Natural language processing.},
   author = {Boyu Zhang and Australia Hongyang Yang and Tianyu Zhou and Ali Babar and Australia Xiao-Yang Liu and Hongyang Yang and Xiao-Yang Liu},
   city = {New York, NY, USA},
   doi = {10.1145/3604237.3626866},
   isbn = {9798400702402},
   journal = {4th ACM International Conference on AI in Finance},
   keywords = {Instruction Tuning,Large Language Models,Retrieval Augmented Generation * Authors contributed equally to this research † Corresponding author,Sentiment Analysis},
   month = {11},
   pages = {349-356},
   publisher = {ACM},
   title = {Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3604237.3626866},
   year = {2023},
}
@article{Ghosh2023,
   abstract = {Dealing with money has always been one of the basic skills one needs to live a comfortable life. However, financial literacy rates across the nations are extremely low. Furthermore, over the years the returns from traditional investment avenues like bank fixed deposits (FD), real estate, etc. have been diminishing. This entices new-age investors to trade and reap profits from the ever-growing stock markets. Nevertheless, in reality, only a handful of active traders are able to earn more than the FD rates. This is due to the lack of financial knowledge. The presence of complex concepts and jargons further reduces comprehensibility. In this paper, we present how financial texts can be demystified using Natural Language Processing (NLP). It consists of neural-based readability assessment and hypernym extraction tools to improve the readability of financial texts. Other modules include financial domain specific systems for automated claim detection, sustainability assessment, etc.},
   author = {Sohom Ghosh and Sudip Kumar Naskar},
   doi = {10.1145/3570991.3571051},
   isbn = {9781450397988},
   journal = {ACM International Conference Proceeding Series},
   keywords = {claim detection,financial text processing,hypernym detection,natural language processing,readability,• Computing methodologies → Language resources,• Information systems → Clustering and classification},
   month = {1},
   pages = {301-302},
   publisher = {Association for Computing Machinery},
   title = {Using Natural Language Processing to Enhance Understandability of Financial Texts},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3570991.3571051},
   year = {2023},
}
@article{Nath2023,
   abstract = {Dynamic Bipartite graph is naturally suited for modeling temporally evolving interaction in several domains, including digital payment and social media. Though dynamic graphs are widely studied, their focus remains on homogeneous graphs. This paper proposes a novel framework for representation learning in temporally evolving bi-partite graphs. It introduces a bipartite graph transformer layer, a temporal bipartite graph encoder based on an attention mechanism for learning node representations. It further extends the information maximization objective based on noise contrastive learning to temporal bipartite graphs. This combination of bipartite encoder layer and noise contrastive loss ensures each node-set in the temporal bipartite graph is represented uniquely and disentangled from other node-set. We use four public datasets with temporal bipar-tite characteristics in experimentation. The proposed model shows promising results on the transductive and inductive dynamic link prediction task and on the temporal recommendation task.},
   author = {Pritam Kumar Nath and Mastercard AI Garage India Govind Waghmare and Nikhil Tumbde and Mastercard AI Garage India Nitish Srivasatava and Mastercard AI Garage India Siddhartha Asthana},
   city = {New York, NY, USA},
   doi = {10.1145/3604237.3626908},
   isbn = {9798400702402},
   journal = {4th ACM International Conference on AI in Finance},
   keywords = {bipartite graphs,dynamic graph representation learning,graph neural network},
   month = {11},
   pages = {73-81},
   publisher = {ACM},
   title = {FlowMind: Automatic Workflow Generation with LLMs},
   url = {https://dl.acm.org/doi/10.1145/3604237.3626908},
   year = {2023},
}
@article{Stillman2023,
   abstract = {The ability to construct a realistic simulator of financial exchanges, including reproducing the dynamics of the limit order book, can give insight into many counterfactual scenarios, such as a flash crash, a margin call, or changes in macroeconomic outlook. In recent years, agent-based models have been developed that reproduce many features of an exchange, as summarised by a set of stylised facts and statistics. However, the ability to calibrate simulators to a specific period of trading remains an open challenge. In this work, we develop a novel approach to the calibration of market simulators by leveraging recent advances in deep learning, specifically using neural density estimators and embedding networks. We demonstrate that our approach is able to correctly identify high probability parameter sets, both when applied to synthetic and historical data, and without reliance on manually selected or weighted ensembles of stylised facts.},
   author = {Namid R Stillman and Rory Baggott and Justin Lyon and Jianfei Zhang and Hong Kong and Clearing Limited and Dingqiu Zhu and Tao Chen and Perukrishnen Vytelingum},
   city = {New York, NY, USA},
   doi = {10.1145/3604237.3626867},
   isbn = {9798400702402},
   journal = {4th ACM International Conference on AI in Finance},
   keywords = {Agent-based Models,Embedding networks,Market simulator,Neural density estimators,Simulation-based inference},
   month = {11},
   pages = {100-107},
   publisher = {ACM},
   title = {LLMs for Financial Advisement: A Fairness and Efficacy Study in Personal Decision Making},
   url = {https://dl.acm.org/doi/10.1145/3604237.3626867},
   year = {2023},
}
@article{Lee2021,
   abstract = {The Federal Reserve System (the Fed) plays a significant role in affecting monetary policy and financial conditions worldwide. Although it is important to analyse the Fed's communications to extract useful information, it is generally long-form and complex due to the ambiguous and esoteric nature of content. In this paper, we present FedNLP, an interpretable multi-component Natural Language Processing (NLP) system to decode Federal Reserve communications. This system is designed for end-users to explore how NLP techniques can assist their holistic understanding of the Fed's communications with NO coding. Behind the scenes, FedNLP uses multiple NLP models from traditional machine learning algorithms to deep neural network architectures in each downstream task. The demonstration shows multiple results at once including sentiment analysis, summary of the document, prediction of the Federal Funds Rate movement and visualization for interpreting the prediction model's result. Our application system and demonstration are available at https://fednlp.net.},
   author = {Jean Lee and Hoyoul Luis Youn and Nicholas Stevens and Josiah Poon and Soyeon Caren Han},
   doi = {10.1145/3404835.3462785},
   isbn = {9781450380379},
   journal = {SIGIR 2021 - Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
   keywords = {AI application,federal funds rate forecasting,federal reserve,interpretable machine learning,text analysis},
   month = {7},
   pages = {2560-2564},
   publisher = {Association for Computing Machinery, Inc},
   title = {FedNLP: An Interpretable NLP System to Decode Federal Reserve Communications},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3404835.3462785},
   year = {2021},
}
@article{Ghosh2023b,
   abstract = {Over the years, promising returns have enticed the masses to invest in the stock markets. However, most people do not have the financial knowledge needed for making investment decisions. Even seasoned investors find it difficult to grasp all the available information. This is primarily due to the ever-changing market dynamics and information overload. Natural Language Processing based automated systems are the rescue to such problems. In this paper, we present the Financial Language Understandability Enhancement Toolkit (FLUEnT) for processing financial text. It consists of eight different tools for tasks like hypernym detection, numeral claim analysis, readability assessment, sustainability assessment, etc. The objective of the toolkit is to empower the masses and enable investors in making data-driven decisions. It is open-source under MIT license and is openly accessible from Colab and HuggingFace.1,},
   author = {Sohom Ghosh and Sudip Kumar Naskar},
   doi = {10.1145/3570991.3571067},
   isbn = {9781450397988},
   journal = {ACM International Conference Proceeding Series},
   keywords = {financial text processing,natural language processing,toolkit},
   month = {1},
   pages = {258-262},
   publisher = {Association for Computing Machinery},
   title = {FLUEnT: Financial Language Understandability Enhancement Toolkit},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3570991.3571067},
   year = {2023},
}
@article{Arslan2021,
   abstract = {Neural networks for language modeling have been proven effective on several sub-Tasks of natural language processing. Training deep language models, however, is time-consuming and computationally intensive. Pre-Trained language models such as BERT are thus appealing since (1) they yielded state-of-The-Art performance, and (2) they offload practitioners from the burden of preparing the adequate resources (time, hardware, and data) to train models. Nevertheless, because pre-Trained models are generic, they may underperform on specific domains. In this study, we investigate the case of multi-class text classification, a task that is relatively less studied in the literature evaluating pre-Trained language models. Our work is further placed under the industrial settings of the financial domain. We thus leverage generic benchmark datasets from the literature and two proprietary datasets from our partners in the financial technological industry. After highlighting a challenge for generic pre-Trained models (BERT, DistilBERT, RoBERTa, XLNet, XLM) to classify a portion of the financial document dataset, we investigate the intuition that a specialized pre-Trained model for financial documents, such as FinBERT, should be leveraged. Nevertheless, our experiments show that the FinBERT model, even with an adapted vocabulary, does not lead to improvements compared to the generic BERT models.},
   author = {Yusuf Arslan and Kevin Allix and Lisa Veiber and Cedric Lothritz and Tegawendé F. Bissyandé and Jacques Klein and Anne Goujon},
   doi = {10.1145/3442442.3451375},
   isbn = {9781450383134},
   journal = {The Web Conference 2021 - Companion of the World Wide Web Conference, WWW 2021},
   keywords = {BERT,FinBERT,financial text classification},
   month = {4},
   pages = {260-268},
   publisher = {Association for Computing Machinery, Inc},
   title = {A Comparison of Pre-Trained Language Models for Multi-Class Text Classification in the Financial Domain},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3442442.3451375},
   year = {2021},
}
@article{Chersoni2021,
   abstract = {In this contribution, we describe the systems presented by the PolyU CBS Team at the second Shared Task on Learning Semantic Similarities for the Financial Domain (FinSim-2), where participating teams had to identify the right hypernyms for a list of target terms from the financial domain. For this task, we ran our classification experiments with several distributional, string-based, and Transformer features. Our results show that a simple logistic regression classifier, when trained on a combination of word embeddings, semantic and string similarity metrics and BERT-derived probabilities, achieves a strong performance (above 90%) in financial hypernymy detection.},
   author = {Emmanuele Chersoni and Chu Ren Huang},
   doi = {10.1145/3442442.3451387},
   isbn = {9781450383134},
   journal = {The Web Conference 2021 - Companion of the World Wide Web Conference, WWW 2021},
   keywords = {Distributional Models,Financial NLP,Hypernymy Detection,Nat-ural language processing},
   month = {4},
   pages = {316-319},
   publisher = {Association for Computing Machinery, Inc},
   title = {PolyU-CBS at the FinSim-2 Task: Combining Distributional, String-Based and Transformers-Based Features for Hypernymy Detection in the Financial Domain},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3442442.3451387},
   year = {2021},
}
@article{Goel2021,
   abstract = {Recent advancement in neural network architectures has provided several opportunities to develop systems to automatically extract and represent information from domain specific unstructured text sources. The Finsim-2021 shared task, collocated with the FinNLP workshop, offered the challenge to automatically learn effective and precise semantic models of financial domain concepts. Building such semantic representations of domain concepts requires knowledge about the specific domain. Such a thorough knowledge can be obtained through the contextual information available in raw text documents on those domains. In this paper, we proposed a transformer-based BERT architecture that captures such contextual information from a set of domain specific raw documents and then perform a classification task to segregate domain terms into fixed number of class labels. The proposed model not only considers the contextual BERT embeddings but also incorporates a TF-IDF vectorizer that gives a word level importance to the model. The performance of the model has been evaluated against several baseline architectures.},
   author = {Tushar Goel and Vipul Chauhan and Ishan Verma and Tirthankar Dasgupta and Lipika Dey},
   doi = {10.1145/3442442.3451386},
   isbn = {9781450383134},
   journal = {The Web Conference 2021 - Companion of the World Wide Web Conference, WWW 2021},
   keywords = {Automatic Classification of Financial Term,Ontology,TFIDF Vectors,Text Classification,Transformers},
   month = {4},
   pages = {311-315},
   publisher = {Association for Computing Machinery, Inc},
   title = {TCS-WITM-2021 @FinSim-2: Transformer based Models for Automatic Classification of Financial Terms},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3442442.3451386},
   year = {2021},
}
@article{Devlin2019,
   abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
   author = {Jacob Devlin and Ming Wei Chang and Kenton Lee and Kristina Toutanova},
   isbn = {9781950737130},
   journal = {NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference},
   keywords = {Attention,BERT,Catboost,FinSim-2 task,LSTM,Ontology,RoBERTa,Word2vec},
   month = {4},
   pages = {4171-4186},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {BERT: Pre-training of deep bidirectional transformers for language understanding},
   volume = {1},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3442442.3451388},
   year = {2019},
}
@article{Nguyen2021,
   abstract = {In this paper, we present the different methods proposed for the FinSIM-2 Shared Task 2021 on Learning Semantic Similarities for the Financial domain. The main focus of this task is to evaluate the classification of financial terms into corresponding top-level concepts (also known as hypernyms) that were extracted from an external ontology. We approached the task as a semantic textual similarity problem. By relying on a siamese network with pre-Trained language model encoders, we derived semantically meaningful term embeddings and computed similarity scores between them in a ranked manner. Additionally, we exhibit the results of different baselines in which the task is tackled as a multi-class classification problem. The proposed methods outperformed our baselines and proved the robustness of the models based on textual similarity siamese network.},
   author = {Nhu Khoa Nguyen and Emanuela Boros and Gaël Lejeune and Antoine Doucet and Thierry Delahaut},
   doi = {10.1145/3442442.3451384},
   isbn = {9781450383134},
   journal = {The Web Conference 2021 - Companion of the World Wide Web Conference, WWW 2021},
   keywords = {Hypernym detection,semantic similarities,siamese networks},
   month = {4},
   pages = {302-306},
   publisher = {Association for Computing Machinery, Inc},
   title = {L3i_LBPAM at the FinSim-2 task: Learning Financial Semantic Similarities with Siamese Transformers},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3442442.3451384},
   year = {2021},
}
@article{Perdih2021,
   abstract = {Ontologies are increasingly used for machine reasoning over the last few years. They can provide explanations of concepts or be used for concept classification if there exists a mapping from the desired labels to the relevant ontology. Another advantage of using ontologies is that they do not need a learning process, meaning that we do not need the train data or time before using them. This paper presents a practical use of an ontology for a classification problem from the financial domain. It first transforms a given ontology to a graph and proceeds with generalization with the aim to find common semantic descriptions of the input sets of financial concepts. We present a solution to the shared task on Learning Semantic Similarities for the Financial Domain (FinSim-2 task). The task is to design a system that can automatically classify concepts from the Financial domain into the most relevant hypernym concept in an external ontology-the Financial Industry Business Ontology. We propose a method that maps given concepts to the mentioned ontology and performs a graph search for the most relevant hypernyms. We also employ a word vectorization method and a machine learning classifier to supplement the method with a ranked list of labels for each concept.},
   author = {Timen Stepišnik Perdih and Senja Pollak and Blaå3/4 Škrlj},
   doi = {10.1145/3442442.3451383},
   isbn = {9781450383134},
   journal = {The Web Conference 2021 - Companion of the World Wide Web Conference, WWW 2021},
   keywords = {FIBO,concept classification,financial vocabulary,generalization,hypernym discovery,ontology},
   month = {4},
   pages = {298-301},
   publisher = {Association for Computing Machinery, Inc},
   title = {JSI at the FinSim-2 task: Ontology-Augmented Financial Concept Classification},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3442442.3451383},
   year = {2021},
}
@article{Mansar2021,
   abstract = {The FinSim-2 is a second edition of FinSim Shared Task on Learning Semantic Similarities for the Financial Domain, colocated with the FinWeb workshop. FinSim-2 proposed the challenge to automatically learn effective and precise semantic models for the financial domain. The second edition of the FinSim offered an enriched dataset in terms of volume and quality, and interested in systems which make creative use of relevant resources such as ontologies and lexica, as well as systems which make use of contextual word embeddings such as BERT[4]. Going beyond the mere representation of words is a key step to industrial applications that make use of Natural Language Processing (NLP). This is typically addressed using either unsupervised corpus-derived representations like word embeddings, which are typically opaque to human understanding but very useful in NLP applications or manually created resources such as taxonomies and ontologies, which typically have low coverage and contain inconsistencies, but provide a deeper understanding of the target domain. Finsim is inspired from previous endeavours in the Semeval community, which organized several competitions on semantic/lexical relation extraction between concepts/words. This year, 18 system runs were submitted by 7 teams and systems were ranked according to 2 metrics, Accuracy and Mean rank. All the systems beat our baseline 1 model by over 15 points and the best systems beat the baseline 2 by over 1 ~3 points in accuracy.},
   author = {Youness Mansar and Juyeon Kang and Ismail El Maarouf},
   doi = {10.1145/3442442.3451381},
   isbn = {9781450383134},
   journal = {The Web Conference 2021 - Companion of the World Wide Web Conference, WWW 2021},
   keywords = {Domain specific ontology,Financial documents processing,Hypernym-hyponym relation extraction,Natural Language Processing,Word embeddings},
   month = {4},
   pages = {288-292},
   publisher = {Association for Computing Machinery, Inc},
   title = {The FinSim-2 2021 Shared Task: Learning Semantic Similarities for the Financial Domain},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3442442.3451381},
   year = {2021},
}
@article{Pei2021,
   abstract = {In this paper, we present our approaches for the FinSim 2021 Shared Task on Learning Semantic Similarities for the Financial Domain. The aim of the FinSim shared task is to automatically classify a given list of terms from the financial domain into the most relevant hypernym (or top-level) concept in an external ontology. Two different word representations have been compared in our study, i.e., customized word2vec provided by the shared task and FinBERT. We first create a customized corpus from the given prospectuses and relevant articles from Investopedia. Then we train the domain-specific word2vec embeddings using the customized data with customized word2vec and FinBERT as the initialized embeddings respectively. Our experimental results demonstrate that these customized word embeddings can effectively improve the classification performance and achieve better results than the direct utilization of the provided word embeddings. The class imbalance issue of the given data is also explored. We empirically study the classification performance by employing several different strategies for imbalanced classification problems. Our system ranks 2nd on both Average Accuracy and Mean Rank metrics.},
   author = {Yulong Pei and Qian Zhang},
   doi = {10.1145/3442442.3451385},
   isbn = {9781450383134},
   journal = {The Web Conference 2021 - Companion of the World Wide Web Conference, WWW 2021},
   keywords = {BERT,Word representations,imbalance classification,word2vec},
   month = {4},
   pages = {307-310},
   publisher = {Association for Computing Machinery, Inc},
   title = {GOAT at the FinSim-2 task: Learning Word Representations of Financial Data with Customized Corpus},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3442442.3451385},
   year = {2021},
}
@article{Portisch2021,
   abstract = {This paper presents the FinMatcher system and its results for the FinSim 2021 shared task which is co-located with the Workshop on Financial Technology on the Web (FinWeb) in conjunction with The Web Conference. The FinSim-2 shared task consists of a set of concept labels from the financial services domain. The goal is to find the most relevant top-level concept from a given set of concepts. The FinMatcher system exploits three publicly available knowledge graphs, namely WordNet, Wikidata, and WebIsALOD. The graphs are used to generate explicit features as well as latent features which are fed into a neural classifier to predict the closest hypernym.},
   author = {Jan Portisch and Michael Hladik and Heiko Paulheim},
   doi = {10.1145/3442442.3451382},
   isbn = {9781450383134},
   journal = {The Web Conference 2021 - Companion of the World Wide Web Conference, WWW 2021},
   keywords = {RDF2vec,financial services,hypernymy detection,knowledge graph embeddings,knowledge graphs,wikidata},
   month = {4},
   pages = {293-297},
   publisher = {Association for Computing Machinery, Inc},
   title = {FinMatcher at FinSim-2: Hypernym Detection in the Financial Services Domain using Knowledge Graphs},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3442442.3451382},
   year = {2021},
}
@article{Zhang2023,
   abstract = {The use of Large Language Models (LLMs) is revolutionizing the legal industry. In this technical talk, we would like to explore the various use cases of LLMs in legal tasks, discuss the best practices, investigate the available resources, examine the ethical concerns, and suggest promising research directions. CCS CONCEPTS • Information systems → Information retrieval; • Computing methodologies → Natural language processing; Neural networks; • Applied computing → Law. KEYWORDS legal data mining, legal information retrieval, legal natural language processing, legal knowledge management, large language models ACM Reference Format:},
   author = {Dell Zhang and Alina Petrova and Dietrich Trautmann and Frank Schilder},
   doi = {10.1145/3583780.3615993},
   isbn = {9798400701245},
   month = {10},
   pages = {5257-5258},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Unleashing the Power of Large Language Models for Legal Applications},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3583780.3615993},
   year = {2023},
}
@article{Huang2023b,
   abstract = {In-context learning is the ability of a pretrained model to adapt to novel and diverse downstream tasks by conditioning on prompt examples, without optimizing any parameters. While large language models have demonstrated this ability, how in-context learning could be performed over graphs is unexplored. In this paper, we develop \textbf\{Pr\}etraining \textbf\{O\}ver \textbf\{D\}iverse \textbf\{I\}n-Context \textbf\{G\}raph S\textbf\{y\}stems (PRODIGY), the first pretraining framework that enables in-context learning over graphs. The key idea of our framework is to formulate in-context learning over graphs with a novel \emph\{prompt graph\} representation, which connects prompt examples and queries. We then propose a graph neural network architecture over the prompt graph and a corresponding family of in-context pretraining objectives. With PRODIGY, the pretrained model can directly perform novel downstream classification tasks on unseen graphs via in-context learning. We provide empirical evidence of the effectiveness of our framework by showcasing its strong in-context learning performance on tasks involving citation networks and knowledge graphs. Our approach outperforms the in-context learning accuracy of contrastive pretraining baselines with hard-coded adaptation by 18\% on average across all setups. Moreover, it also outperforms standard finetuning with limited data by 33\% on average with in-context learning.},
   author = {Qian Huang and Hongyu Ren and Peng Chen and Gregor Kržmanc and Daniel Zeng and Percy Liang and Jure Leskovec},
   month = {5},
   title = {PRODIGY: Enabling In-context Learning Over Graphs},
   url = {https://arxiv.org/abs/2305.12600v1},
   year = {2023},
}
@article{Kang2021,
   abstract = {With the recent rise in popularity of Transformer models in Natural Language Processing , research efforts have been dedicated to the development of domain-adapted versions of BERT-like architectures. In this study, we focus on FinBERT, a Transformer model trained on text from the financial domain. By comparing its performances with the original BERT on a wide variety of financial text processing tasks, we found continual pretraining from the original model to be the more beneficial option. Domain-specific pre-training from scratch, conversely, seems to be less effective.},
   author = {Juyeon Kang and Sandra Bellato and Mei Gan and IsmailEl Maarouf},
   journal = {aclanthology.org},
   pages = {37-44},
   title = {FinSim-3: The 3rd shared task on learning semantic similarities for the financial domain},
   url = {https://aclanthology.org/2021.finnlp-1.5.pdf},
   year = {2021}
}
@article{Boella2013,
   abstract = {Maintaining regulatory compliance is an increasing area of concern for business. Legal Knowledge Management systems that combine repositories of legislation with legal ontologies can support the work of in-house compliance managers. But there are challenges to overcome, of interpreting legal knowledge and mapping that knowledge onto business processes, and developing systems that can adequately handle the complexity with clarity and ease. In this paper we extend the Legal Knowledge Management system Eunomos to deal with alternative interpretations of norms connecting it with Business Process Management systems. Moreover, we propose a workflow involving the different roles in a company, which takes legal interpretation into account in mapping norms and processes, using Eunomos as a support. Copyright 2013 ACM.},
   author = {Guido Boella and Marijn Janssen and Joris Hulstijn and Llio Humphreys and Leendert Van Der Torre},
   doi = {10.1145/2514601.2514605},
   isbn = {9781450320801},
   journal = {Proceedings of the International Conference on Artificial Intelligence and Law},
   pages = {23-32},
   title = {Managing legal interpretation in regulatory compliance},
   year = {2013},
}
@article{ONeill2017,
   abstract = {Texts expressed in legal language are often dicult and time consuming for lawyers to read through, particularly for the purpose of identifying relevant deontic modalities (obligations, prohibitions and permissions). By nature, the language of law is strict, hence the predominant use of modal logic as a substitute for the syntactical ambiguity in natural language, specically, deontic and alethic logic for the respective modalities. However, deontic modalities which express obligations,prohibitions and permissions, can have varying degree and preciseness to which they correspond to a matter, strict deontic logic does not allow for such quantitative measures. Therefore, this paper outlines a data-driven approach by classifying deontic modalities using ensembled Articial Neural Networks (ANN) that incorporate domain specic legal distributional semantic model (DSM) representations, in combination with, a general DSM representation. We propose to use well calibrated probability estimates from these classiers as an approximation to the degree which an obligation/prohibition or permission belongs to a given class based on SME annotated sentences. Best results show 82.33 % accuracy on a held-out test set.},
   author = {James O’ Neill and Cecile Robin and Paul Buitelaar and Leona O’ Brien},
   doi = {10.1145/3086512.3086528},
   isbn = {9781450348911},
   journal = {Proceedings of the International Conference on Artificial Intelligence and Law},
   keywords = {Deontic modality,Financial law,Sentence classication},
   month = {6},
   pages = {159-168},
   publisher = {Association for Computing Machinery},
   title = {Classifying sentential modality in legal language: A use case in financial regulations, acts and directives},
   url = {https://dl.acm.org/doi/10.1145/3086512.3086528},
   year = {2017},
}
@article{Al-Shabandar2019,
   abstract = {The global financial crisis of 2008 has led to the increased scrutiny of governance and conduct of financial services firms. A key component of monitoring conduct within this area is Financial Services Compliance Management. Financial institutions need to adhere to legislation such as the European MiFID II and GDPR through to anti-money laundering compliance. A recent report by Thomson Routers in 2018 has found through a survey with 800 financial firms that 66% of firms expect the cost of senior compliance staff to increase, up from 60% of firms in 2017, indicating a continuing growth in spending on compliance. Effective solutions need to be in place to mitigate these increasing costs while enhancing the compliance workflow. Doing this would provide a market edge. Artificial intelligence (AI) and machine learning (ML) have been gaining traction within the compliance management domain from both regulators and financial institutions in areas such as trade and market surveillance to regulatory compliance assurance. These areas share a commonality in terms of the volume of data to monitor often in real-time and from disparate sources both structured and unstructured with an emphasis on ensuring data quality and handling underlying bias in data. In this paper, an overview of the key use case areas of AI and ML in the compliance management domain will be provided. Detailed analysis on the application of specific AI solutions such as natural language processing, data discovery and generative modelling is introduced.},
   author = {Raghad Al-Shabandar and Gaye Lightbody and Fiona Browne and Jun Liu and Haiying Wang and Huiru Zheng},
   doi = {10.1145/3358331.3358339},
   isbn = {9781450372022},
   journal = {ACM International Conference Proceeding Series},
   keywords = {Anti-Money Laundering (AML),Artificial General Intelligence (AGI),Artificial Narrow Intelligence (ANI),Natural Language Processing (NLP),You’re your Customer (KYC)},
   month = {10},
   publisher = {Association for Computing Machinery},
   title = {The application of artificial intelligence in financial compliance management},
   url = {https://dl.acm.org/doi/10.1145/3358331.3358339},
   year = {2019},
}
@article{Rahat2022,
   abstract = {Since the General Data Protection Regulation (GDPR) came into force in May 2018, companies have worked on their data practices to comply with the requirements of GDPR. In particular, since the privacy policy is the essential communication channel for users to understand and control their privacy when using companies' services, many companies updated their privacy policies after GDPR was enforced. However, most privacy policies are verbose, full of jargon, and vaguely describe companies' data practices and users' rights. In addition, our study shows that more than 32% of end users find it difficult to understand the privacy policies explaining GDPR requirements. Therefore, it is challenging for the end users and law enforcement authorities to manually check if companies' privacy policies comply with the requirements enforced by GDPR. In this paper, we create a privacy policy dataset of 1,080 websites annotated by experts with 18 GDPR requirements and develop a Convolutional Neural Network (CNN) based model that can classify the privacy policies into GDPR requirements with an accuracy of 89.2%. We apply our model to automatically measure GDPR compliance in the privacy policies of 9,761 most visited websites. Our results show that, even after four years since GDPR went into effect, 68% of websites still fail to comply with at least one requirement of GDPR.},
   author = {Tamjid Al Rahat and Minjun Long and Yuan Tian},
   doi = {10.1145/3559613.3563195},
   isbn = {9781450398732},
   journal = {WPES 2022 - Proceedings of the 21st Workshop on Privacy in the Electronic Society, co-located with CCS 2022},
   keywords = {active learning,cnn,compliance check,deep learning,gdpr,privacy policy},
   month = {11},
   pages = {89-102},
   publisher = {Association for Computing Machinery, Inc},
   title = {Is Your Policy Compliant? A Deep Learning-based Empirical Study of Privacy Policies Compliance with GDPR},
   url = {https://dl.acm.org/doi/10.1145/3559613.3563195},
   year = {2022},
}
@article{Sun2019,
   abstract = {SYNOPSIS: This paper aims to promote the application of deep learning to audit procedures by illustrating how the capabilities of deep learning for text understanding, speech recognition, visual recognition, and structured data analysis fit into the audit environment. Based on these four capabilities, deep learning serves two major functions in supporting audit decision making: information identification and judgment support. The paper proposes a framework for applying these two deep learning functions to a variety of audit procedures in different audit phases. An audit data warehouse of historical data can be used to construct prediction models, providing suggested actions for various audit procedures. The data warehouse will be updated and enriched with new data instances through the application of deep learning and a human auditor’s corrections. Finally, the paper discusses the challenges faced by the accounting profession, regulators, and educators when it comes to applying deep learning.},
   author = {Ting Sophia Sun},
   doi = {10.2308/ACCH-52455},
   issn = {15587975},
   issue = {3},
   journal = {Accounting Horizons},
   keywords = {Artificial intelligence,Audit procedure,Data analytics,Deep learning,Machine learning,Neural networks},
   pages = {89-109},
   publisher = {American Accounting Association},
   title = {Applying deep learning to audit procedures: An illustrative framework},
   volume = {33},
   year = {2019},
}
@article{Dinesh2008,
   abstract = {This paper considers the problem of checking whether an organization conforms to a body of regulation. Conformance is cast as a trace checking question - the regulation is represented in a logic that is evaluated against an abstract trace or run representing the operations of an organization. We focus on a problem in designing a logic to represent regulation. A common phenomenon in regulatory texts is for sentences to refer to others for conditions or exceptions. We motivate the need for a formal representation of regulation to accomodate such references between statements. We then extend linear temporal logic to allow statements to refer to others. The semantics of the resulting logic is defined via a combination of techniques from Reiter's default logic and Kripke's theory of truth. © 2008 Springer-Verlag.},
   author = {Nikhil Dinesh and Aravind Joshi and Insup Lee and Oleg Sokolsky},
   doi = {10.1007/978-3-540-70525-3_10/COVER},
   isbn = {3540705244},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {110-124},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Reasoning about conditions and exceptions to laws in regulatory conformance checking},
   volume = {5076 LNAI},
   url = {https://link.springer.com/chapter/10.1007/978-3-540-70525-3_10},
   year = {2008},
}
@article{Castiglione2023,
   abstract = {Legal language can be understood as the language typically used by those engaged in the legal profession and, as such, it may come both in spoken or written form. Recent legislation on cybersecurity obviously uses legal language in writing, thus inheriting all its interpretative complications due to the typical abundance of cases and sub-cases as well as to the general richness in detail. This paper faces the challenge of the essential interpretation of the legal language of cybersecurity, namely of the extraction of the essential Parts of Speech (POS) from the legal documents concerning cybersecurity. The challenge is overcome by our methodology for POS tagging of legal language. It leverages state-of-the-art open-source tools for Natural Language Processing (NLP) as well as manual analysis to validate the outcomes of the tools. As a result, the methodology is automated and, arguably, general for any legal language following minor tailoring of the preprocessing step. It is demonstrated over the most relevant EU legislation on cybersecurity, namely on the NIS 2 directive, producing the first, albeit essential, structured interpretation of such a relevant document. Moreover, our findings indicate that tools such as SpaCy and ClausIE reach their limits over the legal language of the NIS 2.},
   author = {Gianpietro Castiglione and Giampaolo Bella and Daniele Francesco Santamaria},
   doi = {10.1145/3600160.3605069},
   isbn = {9798400707728},
   journal = {ACM International Conference Proceeding Series},
   keywords = {Act,Data Protection,NLP,POS tagging,Privacy,Pronouncement},
   month = {8},
   publisher = {Association for Computing Machinery},
   title = {Towards Grammatical Tagging for the Legal Language of Cybersecurity},
   url = {https://dl.acm.org/doi/10.1145/3600160.3605069},
   year = {2023},
}
@article{Aires2017b,
   abstract = {The exchange of goods and services between individuals is often formalised by a contract in which the parties establish norms to define what is expected of each one. Norms use deontic statements of obligation, prohibition, and permission, which may be in conflict. The task of manually detecting norm conflicts can be time–consuming and error-prone since contracts can be vast and complex. To automate such tasks, we develop an approach to identify potential conflicts between norms. We show the effectiveness of our approach and its individual components empirically using two publicly available corpora, and contribute with a new annotated test corpus for norm conflict identification.},
   author = {João Paulo Aires and Daniele Pinheiro and Vera Strubede Lima and Felipe Meneguzzi},
   doi = {10.1007/S10506-017-9205-X},
   issn = {15728382},
   issue = {4},
   journal = {Artificial Intelligence and Law},
   keywords = {Deontic logic,Natural language processing,Normative conflicts,Norms},
   month = {12},
   pages = {397-428},
   publisher = {Springer Netherlands},
   title = {Norm conflict identification in contracts},
   volume = {25},
   year = {2017},
}
@article{Pires2022,
   abstract = {Artificial Intelligence has proven to be effective in streamlining processes in several domains. The Brazilian judiciary, specifically, has a very large number of cases, above the work capacity of the courts, generating urgency in the creation of methods that mainly support the access and manipulation of unstructured data. This paper presents the construction of a Knowledge Graph of the Brazilian Legislation using Semantic Web standards that allows an understanding of how Brazilian laws interact with each other. The Knowledge Graph was quantitatively evaluated using complex network analysis and it was found to be useful to support experts in understanding the Brazilian legislation by detecting special nodes, namely the "bridge-like nodes", that play an important role in the structure of the graph.},
   author = {Rilder S. Pires and Henrique Santos and Ricardo Guedes and João A. Monteiro Neto and Carlos Caminha and Vasco Furtado},
   issn = {1613-0073},
   keywords = {Article},
   month = {11},
   publisher = {Joint Proceedings of the 3th International Workshop on Artificial Intelligence Technologies for Legal Documents (AI4LEGAL 2022) and the 1st International Workshop on Knowledge Graph Summarization (KGSum 2022)},
   title = {Building and Analyzing the Brazilian Legal Knowledge Graph},
   url = {https://dspace.rpi.edu/handle/20.500.13015/6362},
   year = {2022},
}
@article{Tan2023,
   abstract = {We present a reality check on large language models and inspect the promise of retrieval augmented language models in comparison. Such language models are semi-parametric, where models integrate model parameters and knowledge from external data sources to make their predictions, as opposed to the parametric nature of vanilla large language models. We give initial experimental findings that semi-parametric architectures can be enhanced with views, a query analyzer/planner, and provenance to make a significantly more powerful system for question answering in terms of accuracy and efficiency, and potentially for other NLP tasks},
   author = {Wang-Chiew Tan and Yuliang Li and Pedro Rodriguez and Richard James and Victoria Xi and Alon Lin and Scott Halevy and Meta Yih},
   month = {6},
   title = {Reimagining Retrieval Augmented Language Models for Answering Queries},
   url = {https://arxiv.org/abs/2306.01061v1},
   year = {2023},
}
@article{Junior2019,
   abstract = {This paper presents our experience on building RDF knowledge graphs for an industrial use case in the legal domain. The information contained in legal information systems are often accessed through simple keyword interfaces and presented as a simple list of hits. In order to improve search accuracy one may avail of knowledge graphs, where the semantics of the data can be made explicit. Significant research effort has been invested in the area of building knowledge graphs from semi-structured text documents, such as XML, with the prevailing approach being the use of mapping languages. In this paper, we present a semantic model for representing legal documents together with an industrial use case. We also present a set of use case requirements based on the proposed semantic model, which are used to compare and discuss the use of state-of-the-art mapping languages for building knowledge graphs for legal data.},
   author = {Ademar Crotti Junior and Fabrizio Orlandi and Declan O'Sullivan and Christian Dirschl and Quentin Reul},
   issn = {16130073},
   journal = {CEUR Workshop Proceedings},
   keywords = {Legal Knowledge Graphs,Legal se-mantic model,Legal semantic model,Mapping languages,Mapping languages·},
   month = {11},
   publisher = {CEUR-WS},
   title = {Using Mapping Languages for Building Legal Knowledge Graphs from XML Files},
   volume = {2599},
   url = {https://arxiv.org/abs/1911.07673v1},
   year = {2019},
}
@article{Alchourron1981,
   abstract = {We study some of the ways in which the imposition of a partial ordering on a code of laws or regulations can serve to overcome logical imperfections in the code itself. In particular, we first show how partial orderings of a code, and derivative orderings of its...},
   author = {Carlos E. Alchourrón and David Makinson},
   doi = {10.1007/978-94-009-8484-4_5},
   isbn = {978-94-009-8484-4},
   journal = {New Studies in Deontic Logic},
   pages = {125-148},
   publisher = {Springer, Dordrecht},
   title = {Hierarchies of Regulations and their Logic},
   url = {https://link.springer.com/chapter/10.1007/978-94-009-8484-4_5},
   year = {1981},
}
@article{Shelton2006,
   abstract = {Systems of law usually establish a hierarchy of norms based on the particular source from which the norms derive. In national legal systems, it is commonplace for the fundamental values of society to be given constitutional status and afforded precedence in the event of a conflict with norms enacted by legislation or adopted by administrative regulation; administrative rules themselves must conform to legislative mandates, while written law usually takes precedence over unwritten law and legal norms prevail over nonlegal (political or moral) rules. Norms of equal status must be balanced and reconciled to the extent possible. The mode of legal reasoning applied in practice is thus naturally hierarchical, establishing relationships and order between normative statements and levels of authority.},
   author = {Dinah Shelton},
   doi = {10.1017/S0002930000016675},
   issn = {0002-9300},
   issue = {2},
   journal = {American Journal of International Law},
   pages = {291-323},
   publisher = {Cambridge University Press},
   title = {Normative Hierarchy in International Law},
   volume = {100},
   url = {https://www.cambridge.org/core/journals/american-journal-of-international-law/article/abs/normative-hierarchy-in-international-law/B17B59F4D46511BE55786728214856BB},
   year = {2006},
}
@article{Li2023,
   abstract = {Graph plays a significant role in representing and analyzing complex relationships in real-world applications such as citation networks, social networks, and biological data. Recently, Large Language Models (LLMs), which have achieved tremendous success in various domains, have also been leveraged in graph-related tasks to surpass traditional Graph Neural Networks (GNNs) based methods and yield state-of-the-art performance. In this survey, we first present a comprehensive review and analysis of existing methods that integrate LLMs with graphs. First of all, we propose a new taxonomy, which organizes existing methods into three categories based on the role (i.e., enhancer, predictor, and alignment component) played by LLMs in graph-related tasks. Then we systematically survey the representative methods along the three categories of the taxonomy. Finally, we discuss the remaining limitations of existing studies and highlight promising avenues for future research. The relevant papers are summarized and will be consistently updated at: https://github.com/yhLeeee/Awesome-LLMs-in-Graph-tasks.},
   author = {Yuhan Li and Zhixun Li and Peisong Wang and Jia Li and Xiangguo Sun and Hong Cheng and Jeffrey Xu Yu},
   month = {11},
   title = {A Survey of Graph Meets Large Language Model: Progress and Future Directions},
   url = {https://arxiv.org/abs/2311.12399v2},
   year = {2023},
}
@article{ShaoYunqiu2023,
   abstract = {Legal case retrieval is a special Information Retrieval (IR) task focusing on legal case documents. Depending on the downstream tasks of the retrieved case documents, users’ information needs in le...},
   author = {Yunqiu Shao and Haitao Li and Yueyue Wu and Yiqun Liu and Qingyao Ai and Jiaxin Mao and Yixiao Ma and Shaoping Ma},
   doi = {10.1145/3626093},
   issn = {1046-8188},
   journal = {ACM Transactions on Information Systems},
   keywords = {legal case retrieval,search intent,taxonomy,user behavior,user satisfaction},
   month = {1},
   publisher = {
ACM
PUB27
New York, NY
},
   title = {An Intent Taxonomy of Legal Case Retrieval},
   url = {https://dl.acm.org/doi/10.1145/3626093},
   year = {2023},
}
@article{Pandya2019,
   abstract = {Manual Summarization of large bodies of text involves a lot of human effort and time, especially in the legal domain. Lawyers spend a lot of time preparing legal briefs of their clients' case files. Automatic Text summarization is a constantly evolving field of Natural Language Processing(NLP), which is a subdiscipline of the Artificial Intelligence Field. In this paper a hybrid method for automatic text summarization of legal cases using k-means clustering technique and tf-idf(term frequency-inverse document frequency) word vectorizer is proposed. The summary generated by the proposed method is compared using ROGUE evaluation parameters with the case summary as prepared by the lawyer for appeal in court. Further, suggestions for improving the proposed method are also presented.},
   author = {Varun Pandya},
   doi = {10.5121/csit.2019.91004},
   keywords = {Automatic Text Summarization,Legal domain,k-means clustering,tf-idf word vectors},
   month = {8},
   pages = {37-43},
   publisher = {Academy and Industry Research Collaboration Center (AIRCC)},
   title = {Automatic Text Summarization of Legal Cases: A Hybrid Approach},
   url = {http://arxiv.org/abs/1908.09119 http://dx.doi.org/10.5121/csit.2019.91004},
   year = {2019},
}
@article{Song2020,
   abstract = {In contract analysis and contract automation, a Knowledge Base (KB) of legal entities is fundamental for performing tasks such as contract verification, contract generation and contract analytic. However, such a knowledge base does not always exist nor can be produced in a short time. In this paper, we propose a clustering-based approach to automatically generate a reliable knowledge base of legal entities from given contracts without any supplemental references. The proposed method is robust to different types of errors produced by preprocessing such as Optical Character Recognition (OCR) and Named Entity Recognition (NER), as well as editing errors such as typos. We evaluate our method on a dataset that consists of 800 real contracts with various qualities from 15 clients. Compared to the collected ground-truth data, our method is able to recall 84% of the knowledge.},
   author = {Fuqi Song and Eric De La Clergerie},
   doi = {10.1109/BIGDATA50022.2020.9378166},
   isbn = {9781728162515},
   journal = {Proceedings - 2020 IEEE International Conference on Big Data, Big Data 2020},
   keywords = {clustering,contract analysis,contract automation,legal entity extraction,ontology population},
   month = {12},
   pages = {2149-2152},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Clustering-based Automatic Construction of Legal Entity Knowledge Base from Contracts},
   year = {2020},
}
@article{Ostling2023,
   abstract = {We introduce the Cambridge Law Corpus (CLC), a corpus for legal AI research. It consists of over 250 000 court cases from the UK. Most cases are from the 21st century, but the corpus includes cases as old as the 16th century. This paper presents the first release of the corpus, containing the raw text and meta-data. Together with the corpus, we provide annotations on case outcomes for 638 cases, done by legal experts. Using our annotated data, we have trained and evaluated case outcome extraction with GPT-3, GPT-4 and RoBERTa models to provide benchmarks. We include an extensive legal and ethical discussion to address the potentially sensitive nature of this material. As a consequence, the corpus will only be released for research purposes under certain restrictions.},
   author = {Andreas Östling and Holli Sargeant and Huiyuan Xie and Ludwig Bull and Alexander Terenin and Leif Jonsson and Måns Magnusson and Felix Steffek},
   doi = {10.17863/CAM.100221},
   month = {9},
   title = {The Cambridge Law Corpus: A Corpus for Legal AI Research},
   url = {https://arxiv.org/abs/2309.12269v3},
   year = {2023},
}
@article{Iyer2022,
   abstract = {The increasing focus on Web 3.0 is leading to automated creation and enrichment of ontologies and other linked datasets. Alongside automation, quality evaluation of enriched ontologies can impact software reliability and reuse. Current quality evaluation approaches oftentimes seek to evaluate ontologies in either syntactic (degree of following ontology development guidelines) or semantic (degree of semantic validity of enriched concepts/relations) aspects. This paper proposes an ontology quality evaluation framework consisting of: (a) SynEvaluator and (b) SemValidator for evaluating syntactic and semantic aspects of ontologies respectively. SynEvaluator allows dynamic task-specific creation and updation of syntactic rules at run-time without any need for programming. SemValidator uses Twitter-based expertise of validators for semantic evaluation. The efficacy and validity of the framework is shown empirically on multiple ontologies.},
   author = {Vivek Iyer and Lalit Mohan Sanagavarapu and Y. Raghu Reddy},
   doi = {10.1007/978-3-030-97532-6_5/COVER},
   isbn = {9783030975319},
   issn = {18650937},
   journal = {Communications in Computer and Information Science},
   keywords = {Crowdsourcing,Ontology quality evaluation,Semantic validation,Syntactic evaluation,Twitter-based expertise},
   pages = {73-93},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {A Framework for Syntactic and Semantic Quality Evaluation of Ontologies},
   volume = {1549 CCIS},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-97532-6_5},
   year = {2022},
}
@article{Ramesh2023,
   abstract = {Indian court legal texts and processes are essential towards the integrity of the judicial system and towards maintaining the social and political order of the nation. Due to the increase in number of pending court cases, there is an urgent need to develop tools to automate many of the legal processes with the knowledge of artificial intelligence. In this paper, we employ knowledge extraction techniques, specially the named entity extraction of legal entities within court case judgements. We evaluate several state of the art architectures in the realm of sequence labeling using models trained on a curated dataset of legal texts. We observe that a Bi-LSTM model trained on Flair Embeddings achieves the best results, and we also publish the BIO formatted dataset as part of this paper.},
   author = {Vinay N Ramesh and Rohan Eswara},
   isbn = {2306.02182v1},
   month = {6},
   title = {FlairNLP at SemEval-2023 Task 6b: Extraction of Legal Named Entities from Legal Texts using Contextual String Embeddings},
   url = {https://arxiv.org/abs/2306.02182v1},
   year = {2023},
}
@article{Carvalho2017,
   abstract = {In the context of the Competition on Legal Information Extraction/Entailment (COLIEE), we propose a method comprising the necessary steps for finding relevant documents to a legal question and deciding on textual entailment evidence to provide a correct answer. The proposed method is based on the combination of several lexical and morphological characteristics, to build a language model and a set of features for Machine Learning algorithms. We provide a detailed study on the proposed method performance and failure cases, indicating that it is competitive with state-of-the-art approaches on Legal Information Retrieval and Question Answering, while not needing extensive training data nor depending on expert produced knowledge. The proposed method achieved significant results in the competition, indicating a substantial level of adequacy for the tasks addressed.},
   author = {Danilo S. Carvalho and Minh Tien Nguyen and Chien Xuan Tran and Minh Le Nguyen},
   doi = {10.1007/978-3-319-50953-2_21/COVER},
   isbn = {9783319509525},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {295-311},
   publisher = {Springer Verlag},
   title = {Lexical-morphological modeling for legal text analysis},
   volume = {10091 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-319-50953-2_21},
   year = {2017},
}
@article{Cutting-Decelle2018,
   abstract = {Problems faced by international standardization bodies become more and more crucial as the number and the size of the standards they produce increase. Sometimes, also, the lack of coordination among the committees in charge of the development of standards may lead to overlaps, mistakes or incompatibilities in the documents. The aim of this study is to present a methodology enabling an automatic extraction of the technical concepts (terms) found in normative documents, through the use of semantic tools coming from the field of language processing. The first part of the paper provides a description of the standardization world, its structure, its way of working and the problems faced; we then introduce the concepts of semantic annotation, information extraction and the software tools available in this domain. The next section explains the concept of ontology and its potential use in the field of standardization. We propose here a methodology enabling the extraction of technical information from a given normative corpus, based on a semantic annotation process done according to reference ontologies. The application to the ISO 15531 MANDATE corpus provides a first use case of the methodology described in this paper. The paper ends with the description of the first experimental results produced by this approach, and with some issues and perspectives, notably its application to other standards and, or Technical Committees and the possibility offered to create pre-defined technical dictionaries of terms.},
   author = {A. F. Cutting-Decelle and A. Digeon and R. I. Young and J. L. Barraud and P. Lamboley},
   keywords = {ISO 15531 MANDATE,Key-words : international standardization,SDO,industrial systems,information extraction,ontologies,semantic annotation},
   month = {6},
   title = {Extraction Of Technical Information From Normative Documents Using Automated Methods Based On Ontologies : Application To The Iso 15531 Mandate Standard - Methodology And First Results},
   url = {https://arxiv.org/abs/1806.02242v2},
   year = {2018},
}
@article{Hassanzadeh2011,
   abstract = {The Semantic Web is an extension of the current web in which information is given well-defined meaning. The perspective of Semantic Web is to promote the quality and intelligence of the current web by changing its contents into machine understandable form. Therefore, semantic level information is one of the cornerstones of the Semantic Web. The process of adding semantic metadata to web resources is called Semantic Annotation. There are many obstacles against the Semantic Annotation, such as multilinguality, scalability, and issues which are related to diversity and inconsistency in content of different web pages. Due to the wide range of domains and the dynamic environments that the Semantic Annotation systems must be performed on, the problem of automating annotation process is one of the significant challenges in this domain. To overcome this problem, different machine learning approaches such as supervised learning, unsupervised learning and more recent ones like, semi-supervised learning and active learning have been utilized. In this paper we present an inclusive layered classification of Semantic Annotation challenges and discuss the most important issues in this field. Also, we review and analyze machine learning applications for solving semantic annotation problems. For this goal, the article tries to closely study and categorize related researches for better understanding and to reach a framework that can map machine learning techniques into the Semantic Annotation challenges and requirements.},
   author = {Hamed Hassanzadeh and MohammadReza Keyvanpour},
   doi = {10.5121/ijwest.2011.2203},
   issue = {2},
   journal = {International journal of Web & Semantic Technology},
   keywords = {Machine Learning,Semantic Annotation,Semantic Web},
   month = {4},
   pages = {27-38},
   publisher = {Academy and Industry Research Collaboration Center (AIRCC)},
   title = {A Machine Learning Based Analytical Framework for Semantic Annotation Requirements},
   volume = {2},
   url = {http://arxiv.org/abs/1104.4950 http://dx.doi.org/10.5121/ijwest.2011.2203},
   year = {2011},
}
@article{Eliot2020,
   abstract = {A framework is proposed that seeks to identify and establish a set of robust autonomous levels articulating the realm of Artificial Intelligence and Legal Reasoning (AILR). Doing so provides a sound and parsimonious basis for being able to assess progress in the application of AI to the law, and can be utilized by scholars in academic pursuits of AI legal reasoning, along with being used by law practitioners and legal professionals in gauging how advances in AI are aiding the practice of law and the realization of aspirational versus achieved results. A set of seven levels of autonomy for AI and Legal Reasoning are meticulously proffered and mindfully discussed.},
   author = {Lance Eliot},
   keywords = {AI,artificial intelligence,autonomous levels,autonomy,framework,legal reasoning,ontology,taxonomy,the law},
   month = {8},
   title = {An Ontological AI-and-Law Framework for the Autonomous Levels of AI Legal Reasoning},
   url = {https://arxiv.org/abs/2008.07328v1},
   year = {2020},
}
@article{Prasad2023,
   abstract = {Automatic legal judgment prediction and its explanation suffer from the problem of long case documents exceeding tens of thousands of words, in general, and having a non-uniform structure. Predicting judgments from such documents and extracting their explanation becomes a challenging task, more so on documents with no structural annotation. We define this problem as "scarce annotated legal documents" and explore their lack of structural information and their long lengths with a deep-learning-based classification framework which we call MESc; "Multi-stage Encoder-based Supervised with-clustering"; for judgment prediction. We explore the adaptability of LLMs with multi-billion parameters (GPT-Neo, and GPT-J) to legal texts and their intra-domain(legal) transfer learning capacity. Alongside this, we compare their performance and adaptability with MESc and the impact of combining embeddings from their last layers. For such hierarchical models, we also propose an explanation extraction algorithm named ORSE; Occlusion sensitivity-based Relevant Sentence Extractor; based on the input-occlusion sensitivity of the model, to explain the predictions with the most relevant sentences from the document. We explore these methods and test their effectiveness with extensive experiments and ablation studies on legal documents from India, the European Union, and the United States with the ILDC dataset and a subset of the LexGLUE dataset. MESc achieves a minimum total performance gain of approximately 2 points over previous state-of-the-art proposed methods, while ORSE applied on MESc achieves a total average gain of 50% over the baseline explainability scores.},
   author = {Nishchal Prasad and Mohand Boughanem and Taoufik Dkaki},
   journal = {Proceedings of ACM Conference (Conference'17)},
   keywords = {Extractive explanation,Legal judgment prediction,Long document classification,Multi-stage classification framework,Scarce annotated documents},
   month = {9},
   title = {A Hierarchical Neural Framework for Classification and its Explanation in Large Unstructured Legal Documents},
   volume = {1},
   url = {https://arxiv.org/abs/2309.10563v2},
   year = {2023},
}
@article{Kalamkar2022,
   abstract = {In populous countries, pending legal cases have been growing exponentially. There is a need for developing techniques for processing and organizing legal documents. In this paper, we introduce a new corpus for structuring legal documents. In particular, we introduce a corpus of legal judgment documents in English that are segmented into topical and coherent parts. Each of these parts is annotated with a label coming from a list of pre-defined Rhetorical Roles. We develop baseline models for automatically predicting rhetorical roles in a legal document based on the annotated corpus. Further, we show the application of rhetorical roles to improve performance on the tasks of summarization and legal judgment prediction. We release the corpus and baseline model code along with the paper.},
   author = {Prathamesh Kalamkar and Aman Tiwari and Astha Agarwal and Saurabh Karn and Smita Gupta and Vivek Raghavan and Ashutosh Modi},
   isbn = {9791095546726},
   journal = {2022 Language Resources and Evaluation Conference, LREC 2022},
   keywords = {Legal Document Segmentation,Legal NLP,Rhetorical Roles},
   month = {1},
   pages = {4420-4429},
   publisher = {European Language Resources Association (ELRA)},
   title = {Corpus for Automatic Structuring of Legal Documents},
   url = {https://arxiv.org/abs/2201.13125v2},
   year = {2022},
}
@article{Mahoney2019,
   abstract = {Companies regularly spend millions of dollars producing electronically-stored documents in legal matters. Over the past two decades, attorneys have been using a variety of technologies to conduct this exercise, and most recently, parties on both sides of the 'legal aisle' are accepting the use of machine learning techniques like text classification to cull massive volumes of data and to identify responsive documents for use in these matters. While text classification is regularly used to reduce the discovery costs in legal matters, text classification also faces a peculiar perception challenge: amongst lawyers, this technology is sometimes looked upon as a black box Put simply, very little information is provided for attorneys to understand why documents are classified as responsive. In recent years, a group of AI and Machine Learning researchers have been actively researching Explainable AI. In an explainable AI system, actions or decisions are human understandable. In legal 'document review' scenarios, a document can be identified as responsive, as long as one or more of the text snippets (small passages of text) in a document are deemed responsive. In these scenarios, if text classification can be used to locate these responsive snippets, then attorneys could easily evaluate the model's document classification decision. When deployed with defined and explainable results, text classification can drastically enhance the overall quality and speed of the document review process by reducing the time it takes to review documents. Moreover, explainable predictive coding provides lawyers with greater confidence in the results of that supervised learning task. This paper describes a framework for explainable text classification as a valuable tool in legal services: for enhancing the quality and efficiency of legal document review and for assisting in locating responsive snippets within responsive documents. This framework has been implemented in our legal analytics product, which has been used in hundreds of legal matters. We also report our experimental results using the data from an actual legal matter that used this type of document review.},
   author = {Christian J. Mahoney and Jianping Zhang and Nathaniel Huber-Fliflet and Peter Gronvall and Haozhen Zhao},
   doi = {10.1109/BIGDATA47090.2019.9005659},
   isbn = {9781728108582},
   journal = {Proceedings - 2019 IEEE International Conference on Big Data, Big Data 2019},
   keywords = {XAI,explainable AI,false negatives,legal document review,machine learning,predictive coding,text categorization,text classification},
   month = {12},
   pages = {1858-1867},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A Framework for Explainable Text Classification in Legal Document Review},
   year = {2019},
}
@article{Soman2023,
   abstract = {Large Language Models (LLMs) have been driving progress in AI at an unprecedented rate, yet still face challenges in knowledge-intensive domains like biomedicine. Solutions such as pre-training and domain-specific fine-tuning add substantial computational overhead, and the latter require domain-expertise. External knowledge infusion is task-specific and requires model training. Here, we introduce a task-agnostic Knowledge Graph-based Retrieval Augmented Generation (KG-RAG) framework by leveraging the massive biomedical KG SPOKE with LLMs such as Llama-2-13b, GPT-3.5-Turbo and GPT-4, to generate meaningful biomedical text rooted in established knowledge. KG-RAG consistently enhanced the performance of LLMs across various prompt types, including one-hop and two-hop prompts, drug repurposing queries, biomedical true/false questions, and multiple-choice questions (MCQ). Notably, KG-RAG provides a remarkable 71% boost in the performance of the Llama-2 model on the challenging MCQ dataset, demonstrating the framework's capacity to empower open-source models with fewer parameters for domain-specific questions. Furthermore, KG-RAG enhanced the performance of proprietary GPT models, such as GPT-3.5 which exhibited improvement over GPT-4 in context utilization on MCQ data. Our approach was also able to address drug repurposing questions, returning meaningful repurposing suggestions. In summary, the proposed framework combines explicit and implicit knowledge of KG and LLM, respectively, in an optimized fashion, thus enhancing the adaptability of general-purpose LLMs to tackle domain-specific questions in a unified framework.},
   author = {Karthik Soman and Peter W Rose and John H Morris and Rabia E Akbas and Brett Smith and Braian Peetoom and Catalina Villouta-Reyes and Gabriel Cerono and Yongmei Shi and Angela Rizk-Jackson and Sharat Israni and Charlotte A Nelson and Sui Huang and Sergio E Baranzini and Francisco San Francisco},
   month = {11},
   title = {Biomedical knowledge graph-enhanced prompt generation for large language models},
   url = {https://arxiv.org/abs/2311.17330v1},
   year = {2023},
}
@article{Sarica2021,
   abstract = {Engineers often need to discover and learn designs from unfamiliar domains for inspiration or other particular uses. However, the complexity of the technical design descriptions and the unfamiliarity to the domain make it hard for engineers to comprehend the function, behavior, and structure of a design. To help engineers quickly understand a complex technical design description new to them, one approach is to represent it as a network graph of the design-related entities and their relations as an abstract summary of the design. While graph or network visualizations are widely adopted in the engineering design literature, the challenge remains in retrieving the design entities and deriving their relations. In this paper, we propose a network mapping method that is powered by Technology Semantic Network (TechNet). Through a case study, we showcase how TechNet’s unique characteristic of being trained on a large technology-related data source advantages itself over common-sense knowledge bases, such as WordNet and ConceptNet, for design knowledge representation.},
   author = {Serhad Sarica and Jianxi Luo},
   doi = {10.1017/PDS.2021.104},
   issn = {2732-527X},
   journal = {Proceedings of the Design Society},
   keywords = {Design informatics,Knowledge Representation,Semantic data processing,Technology Semantic Network,Visualisation},
   pages = {1043-1052},
   publisher = {Cambridge University Press},
   title = {DESIGN KNOWLEDGE REPRESENTATION WITH TECHNOLOGY SEMANTIC NETWORK},
   volume = {1},
   url = {https://www.cambridge.org/core/journals/proceedings-of-the-design-society/article/design-knowledge-representation-with-technology-semantic-network/6BB0FB6F96756646EAE3F6B95A096F60},
   year = {2021},
}
@article{Zimmermann2012,
   abstract = {We describe a generic framework for representing and reasoning with annotated Semantic Web data, a task becoming more important with the recent increased amount of inconsistent and non-reliable meta-data on the Web. We formalise the annotated language, the corresponding deductive system and address the query answering problem. Previous contributions on specific RDF annotation domains are encompassed by our unified reasoning formalism as we show by instantiating it on (i) temporal, (ii) fuzzy, and (iii) provenance annotations. Moreover, we provide a generic method for combining multiple annotation domains allowing to represent, e.g., temporally-annotated fuzzy RDF. Furthermore, we address the development of a query language - AnQL - that is inspired by SPARQL, including several features of SPARQL 1.1 (subqueries, aggregates, assignment, solution modifiers) along with the formal definitions of their semantics. © 2011 Elsevier B.V. All rights reserved.},
   author = {Antoine Zimmermann and Nuno Lopes and Axel Polleres and Umberto Straccia},
   doi = {10.1016/J.WEBSEM.2011.08.006},
   issn = {1570-8268},
   journal = {Journal of Web Semantics},
   keywords = {Annotations,Query,RDF,RDFS,SPARQL},
   month = {3},
   pages = {72-95},
   publisher = {Elsevier},
   title = {A general framework for representing, reasoning and querying with annotated Semantic Web data},
   volume = {11},
   year = {2012},
}
@article{Bus2019,
   abstract = {Manually checking models for compliance against building regulation is a time-consuming task for architects and construction engineers. There is thus a need for algorithms that process information from construction projects and report non-compliant elements. Still automated code-compliance checking raises several obstacles. Building regulations are usually published as human readable texts and their content is often ambiguous or incomplete. Also, the vocabulary used for expressing such regulations is very different from the vocabularies used to express Building Information Models (BIM). Furthermore, the high level of details associated to BIM-contained geometries induces complex calculations. Finally, the level of complexity of the IFC standard also hinders the automation of IFC processing tasks. Model chart, formal rules and pre-processors approach allows translating construction regulations into semantic queries. We further demonstrate the usefulness of this approach through several use cases. We argue our approach is a step forward in bridging the gap between regulation texts and automated checking algorithms. Finally with the recent building ontology BOT recommended by the W3C Linked Building Data Community Group, we identify perspectives for standardizing and extending our approach.},
   author = {Nicolas Bus and Ana Roxin and Guillaume Picinbono and Muhammad Fahad},
   issn = {16130073},
   journal = {CEUR Workshop Proceedings},
   keywords = {Automated compliance checking,BIM,Construction regulations,IFC,IfcOWL,OWL,SWRL,Semantic rules},
   month = {10},
   pages = {6-15},
   publisher = {CEUR-WS},
   title = {Towards French Smart Building Code: Compliance Checking Based on Semantic Rules},
   volume = {2159},
   url = {https://arxiv.org/abs/1910.00334v1},
   year = {2019},
}
@article{Sleimi2021,
   abstract = {Semantic legal metadata provides information that helps with understanding and interpreting legal provisions. Such metadata is therefore important for the systematic analysis of legal requirements. However, manually enhancing a large legal corpus with semantic metadata is prohibitively expensive. Our work is motivated by two observations: (1) the existing requirements engineering (RE) literature does not provide a harmonized view on the semantic metadata types that are useful for legal requirements analysis; (2) automated support for the extraction of semantic legal metadata is scarce, and it does not exploit the full potential of artificial intelligence technologies, notably natural language processing (NLP) and machine learning (ML). Our objective is to take steps toward overcoming these limitations. To do so, we review and reconcile the semantic legal metadata types proposed in the RE literature. Subsequently, we devise an automated extraction approach for the identified metadata types using NLP and ML. We evaluate our approach through two case studies over the Luxembourgish legislation. Our results indicate a high accuracy in the generation of metadata annotations. In particular, in the two case studies, we were able to obtain precision scores of 97,2% and 82,4%, and recall scores of 94,9% and 92,4%.},
   author = {Amin Sleimi and Nicolas Sannier and Mehrdad Sabetzadeh and Lionel Briand and Marcello Ceci and John Dann},
   doi = {10.1007/S10664-020-09933-5/TABLES/11},
   issn = {15737616},
   issue = {3},
   journal = {Empirical Software Engineering},
   keywords = {Legal requirements,Natural language processing (NLP),Semantic legal metadata},
   month = {5},
   pages = {1-50},
   publisher = {Springer},
   title = {An automated framework for the extraction of semantic legal metadata from legal texts},
   volume = {26},
   url = {https://link.springer.com/article/10.1007/s10664-020-09933-5},
   year = {2021},
}
@article{Adhikary2023,
   abstract = {The escalating number of pending cases is a growing concern world-wide. Recent advancements in digitization have opened up possibilities for leveraging artificial intelligence (AI) tools in the processing of legal documents. Adopting a structured representation for legal documents, as opposed to a mere bag-of-words flat text representation, can significantly enhance processing capabilities. With the aim of achieving this objective, we put forward a set of diverse attributes for criminal case proceedings. We use a state-of-the-art sequence labeling framework to automatically extract attributes from the legal documents. Moreover, we demonstrate the efficacy of the extracted attributes in a downstream task, namely legal judgment prediction.},
   author = {Subinay Adhikary and Sagnik Das and Sagnik Saha and Procheta Sen and Dwaipayan Roy and Kripabandhu Ghosh},
   doi = {10.1016/j.jksuci},
   journal = {Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (Conference acronym 'XX)},
   keywords = {Attribute Extraction,Legal AI},
   month = {10},
   title = {Automated Attribute Extraction from Legal Proceedings},
   volume = {1},
   url = {https://arxiv.org/abs/2310.12131v1},
   year = {2023},
}
@article{Keshavarz2022,
   abstract = {Named entity recognition (NER) is a fundamental task for several important applications such as knowledge base construction and semantic search. So far, the focus has been on building machine learning models, which identify generic named entities (e.g., person, date). Such models can be used off-the-shelf without requiring ground truth labels for training. However, such models cannot generalize to specialized domains that have domain-specific named entities (e.g., the legal domain). In these cases, it is inevitable to generate ground truth data and experiment with a variety of models in order to achieve good performance. Motivated by a real use case from the financial sector, we discuss the approach and lessons learned when solving the NER problem in the legal domain. This task is particularly challenging because it requires extensive human expertise to produce high quality ground-truth labels. For solving the legal domain NER problem, we first crawl a large dataset of legal documents and then introduce a semi-automated process to generate high-quality labels for a set of eleven predefined named entities. We validate that the proposed approach achieves high quality labels that outperform popular out-of-the-box NER methods. On top of that, our method once followed, can generate ground truth labels for the pre-defined named entities for an unbounded number of documents. Next, we experiment with a set of models and training procedures and report their performance on the NER task. Our experimental evaluation confirms that most of the models can generalize very well, achieving F1-score between 86% and 98.9%. The dataset, the labels produced by human annotators and our semi-supervised approach, as well as our code are made available to the research community.},
   author = {Hossein Keshavarz and Zografoula Vagena and Pigi Kouki and Ilias Fountalis and Mehdi Mabrouki and Aziz Belaweid and Nikolaos Vasiloglou},
   doi = {10.1109/BIGDATA55660.2022.10020873},
   isbn = {9781665480451},
   journal = {Proceedings - 2022 IEEE International Conference on Big Data, Big Data 2022},
   pages = {2024-2033},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Named Entity Recognition in Long Documents: An End-to-end Case Study in the Legal Domain},
   year = {2022},
}
@article{Alshugran2016,
   abstract = {Some organizations use software applications to manage their customers' personal, medical, or financial information. In the United States, those software applications are obligated to preserve users' privacy and to comply with the United States federal privacy laws and regulations. To formally guarantee compliance with those regulations, it is essential to extract and model the privacy rules from the text of the law using a formal framework. In this work we propose a goal-oriented framework for modeling and extracting the privacy requirements from regulatory text using natural language processing techniques.},
   author = {Tariq Alshugran and Julius Dichter},
   doi = {10.5121/hiij.2016.5101},
   issue = {1},
   journal = {Health Informatics - An International Journal},
   keywords = {Data Modelling,Data privacy,Law Formalization,Privacy Policies,Role engineering},
   month = {3},
   pages = {1-10},
   publisher = {Academy and Industry Research Collaboration Center (AIRCC)},
   title = {A Framework for Extracting and Modeling HIPAA Privacy Rules for Healthcare Applications},
   volume = {5},
   url = {http://arxiv.org/abs/1603.02964 http://dx.doi.org/10.5121/hiij.2016.5101},
   year = {2016},
}
@article{Danenas2019,
   abstract = {Being among the best-selling and most advanced features of model-driven development, model-to-model transformation could help improving one of the most time- and resource-consuming efforts in the process of model-driven information systems engineering, namely, discovery and specification of business vocabularies and business rules within the problem domain. Nonetheless, despite the relatively high levels of automation throughout the whole systems’ model-driven development process, business modeling stage remains among the most under re-searched areas throughout the whole process. In this paper, we introduce a novel natural language processing (NLP) technique to one of our latest developments for the automatic extraction of SBVR business vocabularies and business rules from UML use case diagrams. This development remains arguably the most comprehensive development of this kind currently available in public. The experiment provided proof that the developed NLP enhancement delivered even better extraction results compared to the already satisfactory performance of the previous development. This work contributes to the research in the areas of model transformations and NLP within the model-driven development of information systems, and beyond.},
   author = {Paulius Danenas and Tomas Skersys and Rimantas Butleris},
   doi = {10.1145/3368640.3368641},
   isbn = {9781450372923},
   journal = {ACM International Conference Proceeding Series},
   keywords = {Business rules,Business vocabulary,Model transformation,Natural language processing,SBVR,UML,Use case diagram},
   month = {11},
   publisher = {Association for Computing Machinery},
   title = {Enhancing the extraction of SBVR business vocabularies and business rules from UML use case diagrams with natural language processing},
   url = {https://dl.acm.org/doi/10.1145/3368640.3368641},
   year = {2019},
}
@article{Wang2023,
   abstract = {Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model. In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text "Columbus is a city" is transformed to generate the text sequence "@@Columbus## is a city", where special tokens @@## marks the entity to extract. To efficiently address the "hallucination" issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a labeled entity tag. We conduct experiments on five widely adopted NER datasets, and GPT-NER achieves comparable performances to fully supervised baselines, which is the first time as far as we are concerned. More importantly, we find that GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, GPT-NER performs significantly better than supervised models. This demonstrates the capabilities of GPT-NER in real-world NER applications where the number of labeled examples is limited.},
   author = {Shuhe Wang and Xiaofei Sun and Xiaoya Li and Rongbin Ouyang and Fei Wu and Tianwei Zhang and Jiwei Li and Guoyin Wang},
   month = {4},
   title = {GPT-NER: Named Entity Recognition via Large Language Models},
   url = {https://arxiv.org/abs/2304.10428v4},
   year = {2023},
}
@article{Rodrigues2019,
   abstract = {Over the last 30 years, AI & Law has provided breakthroughs in studies involving case-based reasoning, rule-based reasoning, information retrieval and, most recently, conceptual models for knowledge representation and reasoning, known as Legal Ontologies. Ontologies have been widely used by legal practitioners, scholars, and lay people in a variety of situations, such as simulating legal actions, semantic search and indexing, and to keep up-to-date with the continual change of laws and regulations. Given the high number of legal ontologies produced, the need to summarize this research realm through a well-defined methodological procedure is urgent need. This study presents the results of a systematic mapping of the literature, aiming at categorizing legal ontologies along certain dimensions, such as purpose, level of generality, underlying legal theories, among other aspects. The reasons to carry out a systematic mapping are twofold: in addition to explaining the maturation of the area over recent decades, it helps to avoid the old problem of reinventing the wheel. Through organizing and classifying what has already been produced, it is possible to realize that the development of legal ontologies can rise to the level of reusability where prefabricated models might be coupled with new and more complex ontologies for practical law.},
   author = {Cleyton Mário de Oliveira Rodrigues and Frederico Luiz Gonçalves de Freitas and Emanoel Francisco Spósito Barreiros and Ryan Ribeiro de Azevedo and Adauto Trigueiro de Almeida Filho},
   doi = {10.1016/J.ESWA.2019.04.009},
   issn = {0957-4174},
   journal = {Expert Systems with Applications},
   keywords = {Legal expert system,Legal ontology,Legal theory,Semantic web,Systematic mapping study},
   month = {9},
   pages = {12-30},
   publisher = {Pergamon},
   title = {Legal ontologies over time: A systematic mapping study},
   volume = {130},
   year = {2019},
}
@article{Casellas2012,
   abstract = {The application of Linked Open Data (LOD) principles to legal information (URI naming of resources, assertions about named relationships between resources or between resources and data values, and the possibility to easily extend, update and modify these relationships and resources) could offer better access and understanding of regulatory information to individual citizens, businesses and government agencies and administrations, and allow its sharing and reuse across applications, organizations and jurisdictions. © 2012 Authors.},
   author = {Nuria Casellas and Thomas R. Bruce and Sara S. Frug and Sarah Bouwman and Dallas Dias and Jie Lin and Sharvari Marathe and Krithi Rai and Ankit Singh and Debraj Sinha and Sanjna Venkataraman},
   doi = {10.1145/2307729.2307785},
   isbn = {9781450314039},
   journal = {ACM International Conference Proceeding Series},
   keywords = {RDF,SKOS,information extraction,linked data},
   pages = {280-281},
   title = {Linked legal data: Improving access to regulations},
   url = {https://dl.acm.org/doi/10.1145/2307729.2307785},
   year = {2012},
}
@article{Gordon2010,
   abstract = {The Legal Knowledge Interchange Format (LKIF) is an XML Schema for representing theories and arguments (proofs) constructed from theories ESTRELLA Project (2008). A theory in LKIF consists of a set of axioms and defeasible inference rules. The language of individuals, predicate and function symbols used by the theory can be imported from an ontology represented in the Web Ontology Language (OWL). Importing an ontology also imports the axioms of the ontology. All symbols are represented using Universal Resource Identifiers (URIs). Other LKIF files may also be imported, enabling complex theories to modularized. © 2010 Springer-Verlag Berlin Heidelberg.},
   author = {Thomas F. Gordon},
   doi = {10.1007/978-3-642-15402-7_30/COVER},
   isbn = {3642154018},
   issn = {18651348},
   journal = {Lecture Notes in Business Information Processing},
   pages = {240-242},
   publisher = {Springer Verlag},
   title = {An overview of the legal knowledge interchange format},
   volume = {57 LNBIP},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-15402-7_30},
   year = {2010},
}
@article{Sharifi2020,
   abstract = {Legal contracts specify the terms and conditions (in essence, requirements) that apply to business transactions. Smart contracts are software systems that monitor and control the execution of contracts to ensure compliance. This paper proposes a formal specification language for contracts, called Symboleo, where contracts consist of collections of obligations and powers that define the legal contract's compliant executions. The formal semantics of Symboleo is based on an extension of an ontology for Law and is described in terms of logical axioms on statecharts that describe the lifetimes of contracts, obligations and powers. Our proposal includes a preliminary evaluation through the specification of a real life-inspired Sale-of-Goods contract, with a prototype execution engine. We envision this language to enable formally verifying contracts to detect requirements-level issues and to generate executable smart contracts (e.g., on blockchain technology).},
   author = {Sepehr Sharifi and Alireza Parvizimosaed and Daniel Amyot and Luigi Logrippo and John Mylopoulos},
   doi = {10.1109/RE48521.2020.00049},
   isbn = {9781728174389},
   issn = {23326441},
   journal = {Proceedings of the IEEE International Conference on Requirements Engineering},
   keywords = {Legal contracts,formal specification languages,smart contracts,software requirements specifications},
   month = {8},
   pages = {364-369},
   publisher = {IEEE Computer Society},
   title = {Symboleo: Towards a Specification Language for Legal Contracts},
   volume = {2020-August},
   year = {2020},
}
@article{Hohfeld1913,
   abstract = {PRINTED},
   author = {Wesley Newcomb Hohfeld},
   doi = {10.2307/1275798},
   issn = {00262234},
   issue = {8},
   journal = {Michigan Law Review},
   month = {6},
   pages = {537},
   publisher = {JSTOR},
   title = {The Relations between Equity and Law},
   volume = {11},
   year = {1913},
}
@article{Hohfeld1917,
   abstract = {The present discussion, while intended to be intrinsically complete so far as intelligent and convenient perusal is concerned, represents, as originally planned, a continuation of an article which appeared under the same title more than three years ago. It therefore seems desirable to indicate, in very general form, the scope and purpose of the latter. The main divisions were entitled: Legal Conceptions Contrasted with Non-legal Conceptions; Operative Facts Contrasted with Evidential Facts; and Fundamental Jural Relations Contrasted with One Another. The jural relations analyzed and discussed under the last subtitle were, at the outset, grouped in a convenient "scheme of opposites and correlatives"; and it will greatly facilitate the presentation of the matters to be hereafter considered if that scheme be reproduced at the present point:},
   author = {Wesley Newcomb Hohfeld},
   doi = {10.2307/786270},
   issn = {00440094},
   issue = {8},
   journal = {The Yale Law Journal},
   month = {6},
   pages = {710},
   publisher = {JSTOR},
   title = {Fundamental Legal Conceptions as Applied in Judicial Reasoning},
   volume = {26},
   year = {1917},
}
@article{Hogan2021,
   abstract = {In this article, we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting...},
   author = {Aidan Hogan and Eva Blomqvist and Michael Cochez and Claudia D'Amato and Gerard De Melo and Claudio Gutierrez and Sabrina Kirrane and José Emilio Labra Gayo and Roberto Navigli and Sebastian Neumaier and Axel Cyrille Ngonga Ngomo and Axel Polleres and Sabbir M. Rashid and Anisa Rula and Lukas Schmelzeisen and Juan Sequeda and Steffen Staab and Antoine Zimmermann},
   doi = {10.1145/3447772},
   issn = {15577341},
   issue = {4},
   journal = {ACM Computing Surveys (CSUR)},
   keywords = {Knowledge graphs,embeddings,graph algorithms,graph databases,graph neural networks,graph query languages,ontologies,rule mining,shapes},
   month = {7},
   publisher = {
		ACM
		PUB27
		New York, NY, USA
	},
   title = {Knowledge Graphs},
   volume = {54},
   url = {https://dl.acm.org/doi/10.1145/3447772},
   year = {2021},
}
@article{Yao2023,
   abstract = {With the widespread use of large language models (LLMs) in NLP tasks, researchers have discovered the potential of Chain-of-thought (CoT) to assist LLMs in accomplishing complex reasoning tasks by generating intermediate steps. However, human thought processes are often non-linear, rather than simply sequential chains of thoughts. Therefore, we propose Graph-of-Thought (GoT) reasoning, which models human thought processes not only as a chain but also as a graph. By representing thought units as nodes and connections between them as edges, our approach captures the non-sequential nature of human thinking and allows for a more realistic modeling of thought processes. Similar to Multimodal-CoT, we modeled GoT reasoning as a two-stage framework, generating rationales first and then producing the final answer. Specifically, we employ an additional graph-of-thoughts encoder for GoT representation learning and fuse the GoT representation with the original input representation through a gated fusion mechanism. We implement a GoT reasoning model on the T5 pre-trained model and evaluate its performance on a text-only reasoning task (GSM8K) and a multimodal reasoning task (ScienceQA). Our model achieves significant improvement over the strong CoT baseline with 3.41% and 5.08% on the GSM8K test set with T5-base and T5-large architectures, respectively. Additionally, our model boosts accuracy from 84.91% to 91.54% using the T5-base model and from 91.68% to 92.77% using the T5-large model over the state-of-the-art Multimodal-CoT on the ScienceQA test set. Experiments have shown that GoT achieves comparable results to Multimodal-CoT(large) with over 700M parameters, despite having fewer than 250M backbone model parameters, demonstrating the effectiveness of GoT.},
   author = {Yao Yao and Zuchao Li and Hai Zhao},
   month = {5},
   title = {Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models},
   url = {https://arxiv.org/abs/2305.16582v1},
   year = {2023},
}
@article{Brachman1977,
   abstract = {Semantic networks constitute one of the many attempts to capture human knowledge in an abstraction suitable for processing by computer program. While semantic nets enjoy widespread popularity, they seem never to live up to their authors' expectations of expressive power and ease of construction. This paper examines the fundamentals of network notation, in order to understand why the “formalism” has not been the panacea it was once hoped to be. We focus here on “concepts”—what net-authors think they are, and how network nodes might represent them. The simplistic view of concept nodes as representing extensional sets is examined, and found wanting in several respects. In an effort to solve the foundational problems exposed, we emphasize the importance of considering an “epistemological foundation” on which to consistently build representations for complex concepts. A level of representation above that of completely uniform nodes and links, but below the level of conceptual knowledge itself, seems to be the key to using previously learned concepts to interpret and structure new ones. A particular foundation is proposed here, based on the notion of a set of functional roles bound together by a structuring interrelationship. Procedures for using this foundation to automatically build instances and conceptual modifications are presented. In addition, the intensional nature of such a representation and its implications are discussed. © 1977, All rights reserved.},
   author = {Ronald J. Brachman},
   doi = {10.1016/S0020-7373(77)80017-5},
   issn = {0020-7373},
   issue = {2},
   journal = {International Journal of Man-Machine Studies},
   month = {3},
   pages = {127-152},
   publisher = {Academic Press},
   title = {What's in a concept: structural foundations for semantic networks},
   volume = {9},
   year = {1977},
}
@article{Agrawal2023,
   abstract = {The contemporary LLMs are prone to producing hallucinations, stemming mainly from the knowledge gaps within the models. To address this critical limitation, researchers employ diverse strategies to augment the LLMs by incorporating external knowledge, aiming to reduce hallucinations and enhance reasoning accuracy. Among these strategies, leveraging knowledge graphs as a source of external information has demonstrated promising results. In this survey, we conduct a comprehensive review of these knowledge-graph-based knowledge augmentation techniques in LLMs, focusing on their efficacy in mitigating hallucinations. We systematically categorize these methods into three overarching groups, offering both methodological comparisons and empirical evaluations of their performance. Lastly, the paper explores the challenges associated with these techniques and outlines potential avenues for future research in this emerging field.},
   author = {Garima Agrawal and Tharindu Kumarage and Zeyad Alghami and Huan Liu},
   month = {11},
   title = {Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey},
   url = {https://arxiv.org/abs/2311.07914v1},
   year = {2023},
}
@article{Abu-Salih2021,
   abstract = {Knowledge Graphs (KGs) have made a qualitative leap and effected a real revolution in knowledge representation. This is leveraged by the underlying structure of the KG which underpins a better comprehension, reasoning and interpretation of knowledge for both human and machine. Therefore, KGs continue to be used as the main means of tackling a plethora of real-life problems in various domains. However, there is no consensus in regard to a plausible and inclusive definition of a domain-specific KG. Further, in conjunction with several limitations and deficiencies, various domain-specific KG construction approaches are far from perfect. This survey is the first to offer a comprehensive definition of a domain-specific KG. Also, the paper presents a thorough review of the state-of-the-art approaches drawn from academic works relevant to seven domains of knowledge. An examination of current approaches reveals a range of limitations and deficiencies. At the same time, uncharted territories on the research map are highlighted to tackle extant issues in the literature and point to directions for future research.},
   author = {Bilal Abu-Salih},
   doi = {10.1016/J.JNCA.2021.103076},
   issn = {1084-8045},
   journal = {Journal of Network and Computer Applications},
   keywords = {Domain ontology,Domain-specific knowledge graph,Knowledge graph,Knowledge graph construction,Knowledge graph embeddings,Knowledge graph evaluation,Survey},
   month = {7},
   pages = {103076},
   publisher = {Academic Press},
   title = {Domain-specific knowledge graphs: A survey},
   volume = {185},
   year = {2021},
}
@article{Sequeda2023,
   abstract = {Enterprise applications of Large Language Models (LLMs) hold promise for question answering on enterprise SQL databases. However, the extent to which LLMs can accurately respond to enterprise questions in such databases remains unclear, given the absence of suitable Text-to-SQL benchmarks tailored to enterprise settings. Additionally, the potential of Knowledge Graphs (KGs) to enhance LLM-based question answering by providing business context is not well understood. This study aims to evaluate the accuracy of LLM-powered question answering systems in the context of enterprise questions and SQL databases, while also exploring the role of knowledge graphs in improving accuracy. To achieve this, we introduce a benchmark comprising an enterprise SQL schema in the insurance domain, a range of enterprise queries encompassing reporting to metrics, and a contextual layer incorporating an ontology and mappings that define a knowledge graph. Our primary finding reveals that question answering using GPT-4, with zero-shot prompts directly on SQL databases, achieves an accuracy of 16%. Notably, this accuracy increases to 54% when questions are posed over a Knowledge Graph representation of the enterprise SQL database. Therefore, investing in Knowledge Graph provides higher accuracy for LLM powered question answering systems.},
   author = {Juan Sequeda and Dean Allemang and Bryon Jacob},
   keywords = {Answering · SQL,Augmented,Benchmark ·,Databases ·,Generation (RAG),Graphs ·,Knowledge,Language,Large,Models ·,Question,Retrieval},
   month = {11},
   title = {A Benchmark to Understand the Role of Knowledge Graphs on Large Language Model's Accuracy for Question Answering on Enterprise SQL Databases},
   url = {https://arxiv.org/abs/2311.07509v1},
   year = {2023},
}
@article{Joshi2020,
   abstract = {Federal government agencies and organizations doing business with them have to adhere to the Code of Federal Regulations (CFR). The CFRs are currently available as large text documents that are not...},
   author = {Karuna Pande Joshi and Srishty Saha},
   doi = {10.1145/3425192},
   issn = {26390175},
   issue = {3},
   journal = {Digital Government: Research and Practice},
   keywords = {Deep learning,Semantic Web,compliance,legal text analytics},
   month = {11},
   publisher = {
		ACM
		PUB27
		New York, NY, USA
	},
   title = {A Semantically Rich Framework for Knowledge Representation of Code of Federal Regulations},
   volume = {1},
   url = {https://dl.acm.org/doi/10.1145/3425192},
   year = {2020},
}
@article{Casellas2011,
   abstract = {The online publication of legal materials by governments and administrations is not recent, nevertheless, nowadays some of these materials are being made available in machine readable formats, and follow the Linked Open Data principles: URI naming of resources, assertions about named relationships between resources or between resources and data values, and the possibility to easily extend, update and modify these relationships and resources.The combination of the publication of legislation, regulations, judicial cases, or administrative decisions as Linked Open Legal Data together with the availability of legal OWL/RDF ontologies and SKOS thesauri, could offer new possibilities to enhance not only access but also understanding of legal information.  This paper outlines the possibilities offered by the use Open Legal Data, and gives a preliminary description of the ongoing work on the reuse of the Code of Federal Regulations and one of its finding aids, the Thesaurus of Indexing Terms.  An improved version of this paper has been accepted for presentation at the AAAI Fall Symposium on Open Government Knowledge: AI Opportunities and Challenges, November 5-6, Arlington, VA. It will be published as part of the Technical Reports.},
   author = {Núria Casellas and Joan-Josep Vallbé and Thomas Robert Bruce},
   doi = {10.2139/SSRN.1959931},
   journal = {SSRN Electronic Journal},
   keywords = {CFR,From Legal Information to Open Legal Data: A Case Study in U.S. Federal Legal Information,Joan-Josep Vallbé,Núria Casellas,RDF,SKOS,SSRN,Thomas Robert Bruce,legal information,legal ontologies,open legal data,thesaurus},
   month = {9},
   publisher = {Elsevier BV},
   title = {From Legal Information to Open Legal Data: A Case Study in U.S. Federal Legal Information},
   url = {https://papers.ssrn.com/abstract=1959931},
   year = {2011},
}
@article{Wang2023b,
   abstract = {Graph embedding methods such as Graph Neural Networks (GNNs) and Graph Transformers have contributed to the development of graph reasoning algorithms for various tasks on knowledge graphs. However, the lack of interpretability and explainability of graph embedding methods has limited their applicability in scenarios requiring explicit reasoning. In this paper, we introduce the Graph Agent (GA), an intelligent agent methodology of leveraging large language models (LLMs), inductive-deductive reasoning modules, and long-term memory for knowledge graph reasoning tasks. GA integrates aspects of symbolic reasoning and existing graph embedding methods to provide an innovative approach for complex graph reasoning tasks. By converting graph structures into textual data, GA enables LLMs to process, reason, and provide predictions alongside human-interpretable explanations. The effectiveness of the GA was evaluated on node classification and link prediction tasks. Results showed that GA reached state-of-the-art performance, demonstrating accuracy of 90.65%, 95.48%, and 89.32% on Cora, PubMed, and PrimeKG datasets, respectively. Compared to existing GNN and transformer models, GA offered advantages of explicit reasoning ability, free-of-training, easy adaption to various graph reasoning tasks},
   author = {Qinyong Wang and Zhenxiang Gao and Rong Xu},
   month = {10},
   title = {Graph Agent: Explicit Reasoning Agent for Graphs},
   url = {https://arxiv.org/abs/2310.16421v1},
   year = {2023},
}
@article{Ding2018,
   abstract = {Embedding knowledge graphs (KGs) into continuous vector spaces is a focus of current research. Early works performed this task via simple models developed over KG triples. Recent attempts focused on either designing more complicated triple scoring models, or incorporating extra information beyond triples. This paper, by contrast, investigates the potential of using very simple constraints to improve KG embedding. We examine non-negativity constraints on entity representations and approximate entailment constraints on relation representations. The former help to learn compact and interpretable representations for entities. The latter further encode regularities of logical entailment between relations into their distributed representations. These constraints impose prior beliefs upon the structure of the embedding space, without negative impacts on efficiency or scalability. Evaluation on WordNet, Freebase, and DBpedia shows that our approach is simple yet surprisingly effective, significantly and consistently outperforming competitive baselines. The constraints imposed indeed improve model interpretability, leading to a substantially increased structuring of the embedding space. Code and data are available at https://github.com/iieir-km/ComplEx-NNE_AER.},
   author = {Boyang Ding and Quan Wang and Bin Wang and Li Guo},
   doi = {10.18653/v1/p18-1011},
   isbn = {9781948087322},
   journal = {ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
   month = {5},
   pages = {110-121},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {Improving Knowledge Graph Embedding Using Simple Constraints},
   volume = {1},
   url = {https://arxiv.org/abs/1805.02408v2},
   year = {2018},
}
@article{Tunstall2023,
   abstract = {We aim to produce a smaller language model that is aligned to user intent. Previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, we experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access RLHF-based model. Code, models, data, and tutorials for the system are available at https://github.com/huggingface/alignment-handbook.},
   author = {Lewis Tunstall and Edward Beeching and Nathan Lambert and Nazneen Rajani and Kashif Rasul and Younes Belkada and Shengyi Huang and Leandro von Werra and Clémentine Fourrier and Nathan Habib and Nathan Sarrazin and Omar Sanseviero and Alexander M. Rush and Thomas Wolf},
   month = {10},
   title = {Zephyr: Direct Distillation of LM Alignment},
   url = {https://arxiv.org/abs/2310.16944v1},
   year = {2023},
}
@article{Galkin2023,
   abstract = {Foundation models in language and vision have the ability to run inference on any textual and visual inputs thanks to the transferable representations such as a vocabulary of tokens in language. Knowledge graphs (KGs) have different entity and relation vocabularies that generally do not overlap. The key challenge of designing foundation models on KGs is to learn such transferable representations that enable inference on any graph with arbitrary entity and relation vocabularies. In this work, we make a step towards such foundation models and present ULTRA, an approach for learning universal and transferable graph representations. ULTRA builds relational representations as a function conditioned on their interactions. Such a conditioning strategy allows a pre-trained ULTRA model to inductively generalize to any unseen KG with any relation vocabulary and to be fine-tuned on any graph. Conducting link prediction experiments on 57 different KGs, we find that the zero-shot inductive inference performance of a single pre-trained ULTRA model on unseen graphs of various sizes is often on par or better than strong baselines trained on specific graphs. Fine-tuning further boosts the performance.},
   author = {Mikhail Galkin and Xinyu Yuan and Hesham Mostafa and Jian Tang and Zhaocheng Zhu},
   month = {10},
   title = {Towards Foundation Models for Knowledge Graph Reasoning},
   url = {https://arxiv.org/abs/2310.04562v1},
   year = {2023},
}
@article{Akehurst2006,
   abstract = {A number of different Model Transformation Frameworks (MTF) are being developed, each of them requiring a user to learn a different language and each possessing its own specific language peculiarities, even if they are based on the QVT standard. To write even a simple transformation, these MTFs require a large amount of learning time. We describe in this paper a minimal, Java based, library that can be used to support the implementation of many practical transformations. Use of this library enables simple transformations to be implemented simply, whilst still providing some support for more complex transformations. © Springer-Verlag Berlin Heidelberg 2006.},
   author = {D. H. Akehurst and B. Bordbar and M. J. Evans and W. G.J. Howells and K. D. McDonald-Maier},
   doi = {10.1007/11880240_25/COVER},
   isbn = {3540457720},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {351-364},
   publisher = {Springer Verlag},
   title = {SiTra: Simple transformations in Java},
   volume = {4199 LNCS},
   url = {https://link.springer.com/chapter/10.1007/11880240_25},
   year = {2006},
}
@article{Su2022,
   abstract = {We introduce INSTRUCTOR, a new method for computing text embeddings given task instructions: every text input is embedded together with instructions explaining the use case (e.g., task and domain descriptions). Unlike encoders from prior work that are more specialized, INSTRUCTOR is a single embedder that can generate text embeddings tailored to different downstream tasks and domains, without any further training. We first annotate instructions for 330 diverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive loss. We evaluate INSTRUCTOR on 70 embedding evaluation tasks (66 of which are unseen during training), ranging from classification and information retrieval to semantic textual similarity and text generation evaluation. INSTRUCTOR, while having an order of magnitude fewer parameters than the previous best model, achieves state-of-the-art performance, with an average improvement of 3.4% compared to the previous best results on the 70 diverse datasets. Our analysis suggests that INSTRUCTOR is robust to changes in instructions, and that instruction finetuning mitigates the challenge of training a single model on diverse datasets. Our model, code, and data are available at https://instructor-embedding.github.io.},
   author = {Hongjin Su and Weijia Shi and Jungo Kasai and Yizhong Wang and Yushi Hu and Mari Ostendorf and Wen-tau Yih and Noah A. Smith and Luke Zettlemoyer and Tao Yu},
   doi = {10.18653/v1/2023.findings-acl.71},
   month = {12},
   pages = {1102-1121},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {One Embedder, Any Task: Instruction-Finetuned Text Embeddings},
   url = {https://arxiv.org/abs/2212.09741v3},
   year = {2022},
}
@article{Reimers2019,
   abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
   author = {Nils Reimers and Iryna Gurevych},
   doi = {10.18653/v1/d19-1410},
   isbn = {9781950737901},
   journal = {EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference},
   month = {8},
   pages = {3982-3992},
   publisher = {Association for Computational Linguistics},
   title = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
   url = {https://arxiv.org/abs/1908.10084v1},
   year = {2019},
}
@article{Akl2021,
   abstract = {In this paper, we present our approaches for the FinSim-3 Shared Task 2021: Learning Semantic Similarities for the Financial Domain. The aim of this shared task is to correctly classify a list of given terms from the financial domain into the most relevant hypernym (or top-level) concept in an external ontology. For our system submission, we evaluate two methods: a Sentence-RoBERTa (SRoBERTa) embeddings model pre-trained on a custom corpus, and a dual word-sentence embeddings model that builds on the first method by improving the proposed baseline word embeddings construction using the FastText model to boost the classification performance. Our system ranks 2nd overall on both metrics, scoring 0.917 on Average Accuracy and 1.141 on Mean Rank.},
   author = {Hanna Abi Akl and Dominique Mariko and Hugues De and Mazancourt Yseop},
   month = {8},
   title = {Yseop at FinSim-3 Shared Task 2021: Specializing Financial Domain Learning with Phrase Representations},
   url = {https://arxiv.org/abs/2108.09485v1},
   year = {2021},
}
@article{Corro2013,
   abstract = {We propose ClausIE, a novel, clause-based approach to open information extraction, which extracts relations and their arguments from natural language text. ClausIE fundamentally differs from previous approaches in that it separates the detection of "useful" pieces of information expressed in a sentence from their representation in terms of extractions. In more detail, ClausIE exploits linguistic knowledge about the grammar of the English language to first detect clauses in an input sentence and to subsequently identify the type of each clause according to the grammatical function of its constituents. Based on this information, ClausIE is able to generate high-precision extractions; the representation of these extractions can be flexibly customized to the underlying application. ClausIE is based on dependency parsing and a small set of domain-independent lexica, operates sentence by sentence without any post-processing, and requires no training data (whether labeled or unlabeled). Our experimental study on various real-world datasets suggests that ClausIE obtains higher recall and higher precision than existing approaches, both on high-quality text as well as on noisy text as found in the web.},
   author = {Luciano Del Corro and Rainer Gemulla},
   doi = {10.1145/2488388.2488420},
   title = {Clausie: clause-based open information extraction},
   month = {5},
   pages = {355-366},
   publisher = {Association for Computing Machinery (ACM)},
   url = {https://dl.acm.org/doi/10.1145/2488388.2488420},
   year = {2013},
}
@article{Fatemi2023,
   abstract = {Graphs are a powerful tool for representing and analyzing complex relationships in real-world applications such as social networks, recommender systems, and computational finance. Reasoning on graphs is essential for drawing inferences about the relationships between entities in a complex system, and to identify hidden patterns and trends. Despite the remarkable progress in automated reasoning with natural text, reasoning on graphs with large language models (LLMs) remains an understudied problem. In this work, we perform the first comprehensive study of encoding graph-structured data as text for consumption by LLMs. We show that LLM performance on graph reasoning tasks varies on three fundamental levels: (1) the graph encoding method, (2) the nature of the graph task itself, and (3) interestingly, the very structure of the graph considered. These novel results provide valuable insight on strategies for encoding graphs as text. Using these insights we illustrate how the correct choice of encoders can boost performance on graph reasoning tasks inside LLMs by 4.8% to 61.8%, depending on the task.},
   author = {Bahare Fatemi and Jonathan Halcrow and Bryan Perozzi and Google Research},
   month = {10},
   title = {Talk like a Graph: Encoding Graphs for Large Language Models},
   url = {https://arxiv.org/abs/2310.04560v1},
   year = {2023},
}
@article{Bouzidi2013,
   abstract = {In this paper, we present a semantic web approach for modelling the process of creating new technical and regulatory documents related to the Building sector. This industry, among other industries, is currently experiencing a phenomenal growth in its technical and regulatory texts. Therefore, it is urgent and crucial to improve the process of creating regulations by automating it as much as possible. We focus on the creation of particular technical documents issued by the French Scientific and Technical Centre for Building (CSTB), called Technical Assessments, and we propose services based on Semantic Web models and techniques for modelling the process of their creation.},
   author = {Khalil Riad Bouzidi and Bruno Fies and Marc Bourdeau and Catherine Faron-Zucker and Nhan Le-Thanh},
   keywords = {Building Industry,Knowledge Management,Ontology,Semantic Web,e-regulations},
   month = {2},
   pages = {26-28},
   title = {An Ontology for Modelling and Supporting the Process of Authoring Technical Assessments},
   url = {https://arxiv.org/abs/1302.4726v1},
   year = {2013},
}
@article{Gao2012,
   abstract = {A contract is a legally binding agreement between real-world business entities whom we treat as providing services to one another. We focus on business rather than technical services. We think of a business contract as specifying the functional and nonfunctional behaviors of and interactions among the services. In current practice, contracts are produced as text documents. Thus the relevant service capabilities, requirements, qualities, and risks are hidden and difficult to access and reason about. We describe a simple but effective unsupervised information extraction approach and tool, Contract Miner, for discovering service exceptions at the phrase level from a large contract repository. Our approach involves preprocessing followed by an application of linguistic patterns and parsing to extract the service exception phrases. Identifying such (noun) phrases can help build service exception vocabularies that support the development of a taxonomy of business terms, and also facilitate modeling and analyzing service engagements. A lightweight online tool that comes with Contract Miner highlights the relevant text in service contracts and thereby assists users in reviewing contracts. Contract Miner produces promising results in terms of precision and recall when evaluated over a corpus of manually annotated contracts. © 2012 IEEE.},
   author = {Xibin Gao and Munindar P. Singh and Pankaj Mehra},
   doi = {10.1109/TSC.2011.1},
   issn = {19391374},
   issue = {3},
   journal = {IEEE Transactions on Services Computing},
   keywords = {Contract analysis,service exceptions,text mining},
   pages = {333-344},
   title = {Mining business contracts for service exceptions},
   volume = {5},
   year = {2012},
}
@article{Asai2023,
   abstract = {Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.},
   author = {Akari Asai and Zeqiu Wu and Yizhong Wang and Avirup Sil and Hannaneh Hajishirzi},
   month = {10},
   title = {Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection},
   url = {https://arxiv.org/abs/2310.11511v1},
   year = {2023},
}
@article{Saad-Falcon2023,
   abstract = {Large Language Models (LLMs) have issues with document question answering (QA) in situations where the document is unable to fit in the small context length of an LLM. To overcome this issue, most existing works focus on retrieving the relevant context from the document, representing them as plain text. However, documents such as PDFs, web pages, and presentations are naturally structured with different pages, tables, sections, and so on. Representing such structured documents as plain text is incongruous with the user's mental model of these documents with rich structure. When a system has to query the document for context, this incongruity is brought to the fore, and seemingly trivial questions can trip up the QA system. To bridge this fundamental gap in handling structured documents, we propose an approach called PDFTriage that enables models to retrieve the context based on either structure or content. Our experiments demonstrate the effectiveness of the proposed PDFTriage-augmented models across several classes of questions where existing retrieval-augmented LLMs fail. To facilitate further research on this fundamental problem, we release our benchmark dataset consisting of 900+ human-generated questions over 80 structured documents from 10 different categories of question types for document QA.},
   author = {Jon Saad-Falcon and Joe Barrow and Alexa Siu and Ani Nenkova and Ryan A Rossi and Adobe Research and Franck Dernoncourt},
   month = {9},
   title = {PDFTriage: Question Answering over Long, Structured Documents},
   url = {https://arxiv.org/abs/2309.08872v1},
   year = {2023},
}
@article{Koreeda2021,
   abstract = {While many NLP pipelines assume raw, clean texts, many texts we encounter in the wild, including a vast majority of legal documents, are not so clean, with many of them being visually structured documents (VSDs) such as PDFs. Conventional preprocessing tools for VSDs mainly focused on word segmentation and coarse layout analysis, whereas fine-grained logical structure analysis (such as identifying paragraph boundaries and their hierarchies) of VSDs is underexplored. To that end, we proposed to formulate the task as prediction of "transition labels" between text fragments that maps the fragments to a tree, and developed a feature-based machine learning system that fuses visual, textual and semantic cues.Our system is easily customizable to different types of VSDs and it significantly outperformed baselines in identifying different structures in VSDs. For example, our system obtained a paragraph boundary detection F1 score of 0.953 which is significantly better than a popular PDF-to-text tool with an F1 score of 0.739.},
   author = {Yuta Koreeda and Christopher D. Manning},
   doi = {10.18653/v1/2021.nllp-1.15},
   isbn = {9781954085985},
   journal = {Natural Legal Language Processing, NLLP 2021 - Proceedings of the 2021 Workshop},
   month = {5},
   pages = {144-154},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {Capturing Logical Structure of Visually Structured Documents with Multimodal Transition Parser},
   url = {https://arxiv.org/abs/2105.00150v2},
   year = {2021},
}
@article{Liu2023b,
   abstract = {While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze language model performance on two tasks that require identifying relevant information within their input contexts: multi-document question answering and key-value retrieval. We find that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts. Furthermore, performance substantially decreases as the input context grows longer, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context models.},
   author = {Nelson F. Liu and Kevin Lin and John Hewitt and Ashwin Paranjape and Michele Bevilacqua and Fabio Petroni and Percy Liang},
   month = {7},
   title = {Lost in the Middle: How Language Models Use Long Contexts},
   url = {https://arxiv.org/abs/2307.03172v2},
   year = {2023},
}
@article{Demuth2007,
   abstract = {Business rules should improve the human communication inside of an enterprise or between business partners and must be therefore independent of implementations in IT systems. As a long-term goal, business rules should be guaranteed by all IT applications of an enterprise. A first step to define and to standardize what business rules are is an OMG initiative to specify a metamodel for business rules and the vocabulary on which business rules are defined. The result of OMG's effort is the SBVR (Semantics of Business Vocabulary and Business Rules) specification that we took as starting point of our investigations to automate business rules. There are multiple ways for transforming business rules. In this paper we show how SBVR based vocabulary and rules can be translated by model transformation chains into Semantic Web Languages. In our approach we use OWL and R2ML (REWERSE Rule Markup Language). Both are languages with a high potential for a broad usage in future rule-based applications. © Springer-Verlag Berlin Heidelberg 2007.},
   author = {Birgit Demuth and Hans Bernhard Liebau},
   doi = {10.1007/978-3-540-75975-1_10/COVER},
   isbn = {9783540759744},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {119-133},
   publisher = {Springer Verlag},
   title = {An approach for bridging the gap between business rules and the semantic Web},
   volume = {4824 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-540-75975-1_10},
   year = {2007},
}
@article{Linehan2008,
   abstract = {Semantics of Business Vocabulary and Rules (SBVR) is a new standard from the OMG that combines aspects of ontologies and of rule systems. This paper summarizes SBVR, reviews some possible use cases for SBVR, and discusses ways that vocabularies and rules given in SBVR could relate to established ontology standards, to rules technologies, and to other IT implementation technologies. It also describes experience with an SBVR prototype that transforms a subset of SBVR rules types to several types of runtime implementations. © 2008 Springer Berlin Heidelberg.},
   author = {Mark H. Linehan},
   doi = {10.1007/978-3-540-88808-6_20/COVER},
   isbn = {3540888071},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Business rules,Ontology,Rules,SBVR,Semantics,Vocabulary},
   pages = {182-196},
   publisher = {Springer Verlag},
   title = {SBVR use cases},
   volume = {5321 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-540-88808-6_20},
   year = {2008},
}
@article{Boley2007,
   abstract = {Four principal Web rule issues constitute our starting points: I1) Formal knowledge representation can act as content in its own right and/or as metadata for content. 12) Knowledge on the open Web is typically inconsistent but closed 'intranet' reasoning can exploit local consistency. 13) Scalability of reasoning calls for representation layering on top of quite inexpressive languages. 14) Rule representation should stay compatible with relevant Web standards. To address these, four corresponding essentials of Web rules have emerged: El) Combine formal and informal knowledge in a Rule Wiki, where the formal parts can be taken as code (or as metadata) for the informal parts, and the informal parts as documentation (or as content) for the formal parts. This can be supported by tools for Controlled Natural Language: mapping a subset of, e.g., English into rules and back. E2) Represent distributed knowledge via a module construct, supporting local consistency, permitting scoped negation as failure, and reducing the search space of scoped queries. Modules are embedded into an 'Entails' element: prove whether a query is entailed by a module. E3) Develop a dual layering of assertional and terminological knowledge as well as their blends. To permit the specification of terminologies independent of assertions, the CARIN principle is adopted: a terminological predicate is not permitted in the head of a rule. E4) Differentiate the Web notion of URIs as in URLs, for access, vs. URNs, for naming. A URI can then be used: URL-like, for module import, where it is an error if dereferencing does not yield a valid knowledge document; URN-like, as an identifier, where dereferencing is not intended; or, as a name whose dereferencing can access its (partial) definition. © Springer-Verlag Berlin Heidelberg 2007.},
   author = {Harold Boley},
   doi = {10.1007/978-3-540-75975-1_2/COVER},
   isbn = {9783540759744},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {7-24},
   publisher = {Springer Verlag},
   title = {Are your rules online? Four Web rule essentials},
   volume = {4824 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-540-75975-1_2},
   year = {2007},
}
@article{Aiello2014,
   abstract = {This paper proposes a mapping technique for automatically translating rules expressed in a format based on natural language, i.e. Semantics of Business Vocabulary and Business Rules (SBVR) standard, into production rules that can be executed by a computer (i.e. Rule engine). The proposed approach achieves a twofold purpose: on the one hand non IT skilled people (i.e. Domain expert) can effectively focus on business rules definition by using statements in natural language, and on the other hand the IT staff will have to manage business rules in a format ready to be executed by a rule engine. The main goal is to overcome some weaknesses in the software development process that could produce inconsistencies between the domain requirements identification and the implemented software functionalities. An exhaustive analysis of the mapping technique is provided and a real case study is presented in order to prove the validity of our work.},
   author = {Giovanni Aiello and Roberto Di Bernardo and Martino Maggio and Daniele Di Bona and Giuseppe Lo Re},
   doi = {10.1109/SOCA.2014.39},
   isbn = {9781479968336},
   journal = {Proceedings - IEEE 7th International Conference on Service-Oriented Computing and Applications, SOCA 2014},
   keywords = {Business Rules,Drools,Natural Language,SBVR},
   month = {12},
   pages = {131-136},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Inferring business rules from natural language expressions},
   year = {2014},
}
@article{Bollen2008,
   abstract = {In this paper we will give an introduction to the recently established OMG SBVR standard on business rules. This standard is a major step forward in improving the productivity of business rule- modelers and analysts. The paper furthermore, illustrates how the mature fact-oriented approaches, e.g. ORM and CogNiam, are related to this new standard and how they can contribute to deliver high quality SBVR models.},
   author = {Peter Bollen},
   doi = {10.1007/978-3-540-88875-8_96/COVER},
   isbn = {9783540888741},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Business rules,CogNIAM,Fact-Orientation,NIAM,OMG,ORM,SBVR,Semantics of business vocabulary and business rules},
   pages = {718-727},
   publisher = {Springer Verlag},
   title = {SBVR: A fact-oriented OMG standard},
   volume = {5333},
   url = {https://link.springer.com/chapter/10.1007/978-3-540-88875-8_96},
   year = {2008},
}
@article{Gruzitis2010,
   abstract = {Computational semantics and logic-based controlled natural languages (CNL) do not address systematically the word sense disambiguation problem of content words, i.e., they tend to interpret only some functional words that are crucial for construction of discourse representation structures. We show that micro-ontologies and multi-word units allow integration of the rich and polysemous multi-domain background knowledge into CNL thus providing interpretation for the content words. The proposed approach is demonstrated by extending the Attempto Controlled English (ACE) with polysemous and procedural constructs resulting in a more natural CNL named PAO covering narrative multi-domain texts. © 2010 Springer-Verlag Berlin Heidelberg.},
   author = {Normunds Gruzitis and Guntis Barzdins},
   doi = {10.1007/978-3-642-14418-9_7/COVER},
   isbn = {3642144179},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {102-120},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Polysemy in controlled natural language texts},
   volume = {5972 LNAI},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-14418-9_7},
   year = {2010},
}
@article{Hough2019,
   abstract = {Understanding is at the core of higher-level information processing and has a long history in the cognitive sciences. It is often described as a complex phenomenon with many dimensions, which makes it difficult to define with precision. Many researchers have noted that understanding is often ill defined, indirectly addressed, or avoided altogether. This is particularly disappointing considering that understanding has been a topic of interest since the ancient Greeks. In order to address this problem with our understanding of understanding, we reviewed literature from philosophy, psychology , education, neuroscience, and computer science. Here we summarize insights from that review, focusing on similarities and differences across those domains, as well as implications for the nature, measurement, and modeling of understanding.},
   author = {Alexander R. Hough and Kevin Gluck},
   journal = {cogsys.org, Advances in Cognitive Systems, 2019 - cogsys.org},
   pages = {13-32},
   title = {The understanding problem in cognitive science},
   volume = {8},
   url = {http://www.cogsys.org/journal/volume8/article-8-3.pdf},
   year = {2019},
}
@article{Jackson2020,
   abstract = {How can we define and understand the nature of understanding itself? This paper discusses cognitive processes for understanding the world in general and for understanding natural language. The discussion considers whether and how an artificial cognitive system could use a 'natural language of thought', and whether the ambiguities of natural language would be a theoretical barrier or could be a theoretical advantage for such a system, in a research approach toward human-level artificial intelligence.},
   author = {Philip Jackson},
   doi = {10.1016/J.PROCS.2020.02.138},
   issn = {1877-0509},
   journal = {Procedia Computer Science},
   keywords = {TalaMind,ambiguity,language of thought,mentalese,natural language,understanding},
   month = {1},
   pages = {209-225},
   publisher = {Elsevier},
   title = {Understanding understanding and ambiguity in natural language},
   volume = {169},
   year = {2020},
}
@article{Csaki2022,
   abstract = {Over the last three decades legal service providers as well as legal departments of various firms have embraced the opportunity to apply the latest digital technology to improve the efficiency and effectiveness of their work. Since language is central to both law-making and during the application of the law, Natural Language Processing solutions have found their way to this profession. One particular research area relates to the issue of small languages. The problem is rooted in the size of the population speaking a given language: in a small market, it is not economically feasible to develop NLP technologies as they require considerable time and effort to develop a sufficient language corpus. This paper reviews the challenges countries and jurisdictions of small languages face in light of increasing NLP applications in legal contexts, while also examining the role of the public sector in relation to addressing such issues.},
   author = {Csaba Csáki and Péter Homoki and György Görög and Pál Vadász},
   keywords = {AI regulations,Legal Tech,Natural Language Processing,small languages},
   title = {NLP in the Legal Profession: How about Small Languages?},
   url = {https://ceur-ws.org/Vol-3399/paper19.pdf},
   year = {2022},
}
@article{Allahyari2017,
   abstract = {In recent years, there has been a explosion in the amount of text data from a variety of sources. This volume of text is an invaluable source of information and knowledge which needs to be effectively summarized to be useful. In this review, the main approaches to automatic text summarization are described. We review the different processes for summarization and describe the effectiveness and shortcomings of the different methods.},
   author = {Mehdi Allahyari and Seyedamin Pouriyeh and Mehdi Assefi and Saeid Safaei and Elizabeth D. and Juan B. and Krys Kochut},
   doi = {10.14569/ijacsa.2017.081052},
   issn = {2158107X},
   issue = {10},
   journal = {International Journal of Advanced Computer Science and Applications},
   publisher = {The Science and Information Organization},
   title = {Text Summarization Techniques: A Brief Survey},
   volume = {8},
   year = {2017},
}
@article{Eidelman2019b,
   abstract = {Automatic summarization methods have been studied on a variety of domains, including news and scientific articles. Yet, legislation has not previously been considered for this task, despite US Congress and state governments releasing tens of thousands of bills every year. In this paper, we introduce BillSum, the first dataset for summarization of US Congressional and California state bills (https://github.com/FiscalNote/BillSum). We explain the properties of the dataset that make it more challenging to process than other domains. Then, we benchmark extractive methods that consider neural sentence representations and traditional contextual features. Finally, we demonstrate that models built on Congressional bills can be used to summarize California bills, thus, showing that methods developed on this dataset can transfer to states without human-written summaries.},
   author = {Vladimir Eidelman},
   doi = {10.18653/v1/d19-5406},
   month = {11},
   pages = {48-56},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {BillSum: A Corpus for Automatic Summarization of US Legislation},
   year = {2019},
}
@article{See2017,
   abstract = {Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.},
   author = {Abigail See and Peter J. Liu and Christopher D. Manning},
   doi = {10.18653/v1/P17-1099},
   isbn = {9781945626753},
   journal = {ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)},
   pages = {1073-1083},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {Get to the point: Summarization with pointer-generator networks},
   volume = {1},
   year = {2017},
}
@article{Merchant2018,
   abstract = {It is very essential for lawyers and ordinary citizens to do an exhaustive research related to their case before they answer questions in court. For quite some time they have had to read extremely long judgements and try to pick out the useful information from them or hire legal editors to create summaries. We propose an automated text summarization system that generates short and useful summaries from lengthy judgements. We make use of a natural language processing technique called latent semantic analysis (LSA) to capture concepts within a single document. We use two approaches-a single document untrained approach and a multi-document trained approach depending on the type of input case (criminal or civil). Our data was collected from official government sites that included Supreme Court, high court and district court cases and our model achieved an average ROGUE-1 score of 0.58. Finally, our system was approved by professional lawyers. In the future we aim to provide better continuity within our generated summaries and evaluate our system more accurately.},
   author = {Kaiz Merchant and Yash Pande},
   doi = {10.1109/ICACCI.2018.8554831},
   isbn = {9781538653142},
   journal = {2018 International Conference on Advances in Computing, Communications and Informatics, ICACCI 2018},
   keywords = {Latent semantic analysis,Legal,Natural language processing,Text summarization},
   month = {11},
   pages = {1803-1807},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {NLP Based Latent Semantic Analysis for Legal Text Summarization},
   year = {2018},
}
@article{Kanapala2019,
   abstract = {Enormous amount of online information, available in legal domain, has made legal text processing an important area of research. In this paper, we attempt to survey different text summarization techniques that have taken place in the recent past. We put special emphasis on the issue of legal text summarization, as it is one of the most important areas in legal domain. We start with general introduction to text summarization, briefly touch the recent advances in single and multi-document summarization, and then delve into extraction based legal text summarization. We discuss different datasets and metrics used in summarization and compare performances of different approaches, first in general and then focused to legal text. we also mention highlights of different summarization techniques. We briefly cover a few software tools used in legal text summarization. We finally conclude with some future research directions.},
   author = {Ambedkar Kanapala and Sukomal Pal and Rajendra Pamula},
   doi = {10.1007/S10462-017-9566-2/METRICS},
   issn = {15737462},
   issue = {3},
   journal = {Artificial Intelligence Review},
   keywords = {Legal domain,Multi-document summarization,Single-document summarization,Summarization from legal documents},
   month = {3},
   pages = {371-402},
   publisher = {Springer Netherlands},
   title = {Text summarization from legal documents: a survey},
   volume = {51},
   url = {https://link.springer.com/article/10.1007/s10462-017-9566-2},
   year = {2019},
}
@article{Katz2023b,
   abstract = {In this paper, we summarize the current state of the field of NLP & Law with a specific focus on recent technical and substantive developments. To support our analysis, we construct and analyze a nearly complete corpus of more than six hundred NLP & Law related papers published over the past decade. Our analysis highlights several major trends. Namely, we document an increasing number of papers written, tasks undertaken, and languages covered over the course of the past decade. We observe an increase in the sophistication of the methods which researchers deployed in this applied context. Slowly but surely, Legal NLP is beginning to match not only the methodological sophistication of general NLP but also the professional standards of data availability and code reproducibility observed within the broader scientific community. We believe all of these trends bode well for the future of the field, but many questions in both the academic and commercial sphere still remain open.},
   author = {Daniel Martin Katz and Dirk Hartung and Lauritz Gerlach and Abhik Jana and Michael J. Bommarito II},
   doi = {10.2139/ssrn.4336224},
   journal = {SSRN Electronic Journal},
   month = {2},
   publisher = {Elsevier BV},
   title = {Natural Language Processing in the Legal Domain},
   url = {https://arxiv.org/abs/2302.12039v1},
   year = {2023},
}
@article{Hendrycks2021,
   abstract = {Many specialized domains remain untouched by deep learning, as large labeled datasets require expensive expert annotators. We address this bottleneck within the legal domain by introducing the Contract Understanding Atticus Dataset (CUAD), a new dataset for legal contract review. CUAD was created with dozens of legal experts from The Atticus Project and consists of over 13,000 annotations. The task is to highlight salient portions of a contract that are important for a human to review. We find that Transformer models have nascent performance, but that this performance is strongly influenced by model design and training dataset size. Despite these promising results, there is still substantial room for improvement. As one of the only large, specialized NLP benchmarks annotated by experts, CUAD can serve as a challenging research benchmark for the broader NLP community.},
   author = {Dan Hendrycks and Collin Burns and Anya Chen and Spencer Ball},
   month = {3},
   title = {CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review},
   url = {https://arxiv.org/abs/2103.06268v2},
   year = {2021},
}
@article{Hupkes2022,
   abstract = {The ability to generalise well is one of the primary desiderata of natural language processing (NLP). Yet, what 'good generalisation' entails and how it should be evaluated is not well understood, nor are there any evaluation standards for generalisation. In this paper, we lay the groundwork to address both of these issues. We present a taxonomy for characterising and understanding generalisation research in NLP. Our taxonomy is based on an extensive literature review of generalisation research, and contains five axes along which studies can differ: their main motivation, the type of generalisation they investigate, the type of data shift they consider, the source of this data shift, and the locus of the shift within the modelling pipeline. We use our taxonomy to classify over 400 papers that test generalisation, for a total of more than 600 individual experiments. Considering the results of this review, we present an in-depth analysis that maps out the current state of generalisation research in NLP, and we make recommendations for which areas might deserve attention in the future. Along with this paper, we release a webpage where the results of our review can be dynamically explored, and which we intend to update as new NLP generalisation studies are published. With this work, we aim to take steps towards making state-of-the-art generalisation testing the new status quo in NLP.},
   author = {Dieuwke Hupkes and Mario Giulianelli and Verna Dankers and Mikel Artetxe and Yanai Elazar and Tiago Pimentel and Christos Christodoulopoulos and Karim Lasri and Naomi Saphra and Arabella Sinclair and Dennis Ulmer and Florian Schottmann and Khuyagbaatar Batsuren and Kaiser Sun and Koustuv Sinha and Leila Khalatbari and Maria Ryskina and Rita Frieske and Ryan Cotterell and Zhijing Jin},
   month = {10},
   title = {State-of-the-art generalisation research in NLP: A taxonomy and review},
   url = {https://arxiv.org/abs/2210.03050v3},
   year = {2022},
}
@article{Hupkes2023,
   abstract = {The ability to generalize well is one of the primary desiderata for models of natural language processing (NLP), but what ‘good generalization’ entails and how it should be evaluated is not well understood. In this Analysis we present a taxonomy for characterizing and understanding generalization research in NLP. The proposed taxonomy is based on an extensive literature review and contains five axes along which generalization studies can differ: their main motivation, the type of generalization they aim to solve, the type of data shift they consider, the source by which this data shift originated, and the locus of the shift within the NLP modelling pipeline. We use our taxonomy to classify over 700 experiments, and we use the results to present an in-depth analysis that maps out the current state of generalization research in NLP and make recommendations for which areas deserve attention in the future. With the rapid development of natural language processing (NLP) models in the last decade came the realization that high performance levels on test sets do not imply that a model robustly generalizes to a wide range of scenarios. Hupkes et al. review generalization approaches in the NLP literature and propose a taxonomy based on five axes to analyse such studies: motivation, type of generalization, type of data shift, the source of this data shift, and the locus of the shift within the modelling pipeline.},
   author = {Dieuwke Hupkes and Mario Giulianelli and Verna Dankers and Mikel Artetxe and Yanai Elazar and Tiago Pimentel and Christos Christodoulopoulos and Karim Lasri and Naomi Saphra and Arabella Sinclair and Dennis Ulmer and Florian Schottmann and Khuyagbaatar Batsuren and Kaiser Sun and Koustuv Sinha and Leila Khalatbari and Maria Ryskina and Rita Frieske and Ryan Cotterell and Zhijing Jin and Hong Kong},
   doi = {10.1038/s42256-023-00729-y},
   issn = {2522-5839},
   issue = {10},
   journal = {Nature Machine Intelligence 2023 5:10},
   keywords = {Computer science,Language and linguistics},
   month = {10},
   pages = {1161-1174},
   publisher = {Nature Publishing Group},
   title = {A taxonomy and review of generalization research in NLP},
   volume = {5},
   url = {https://www.nature.com/articles/s42256-023-00729-y},
   year = {2023},
}
@article{Zhu2023,
   abstract = {When prompted with a few examples and intermediate steps, large language models (LLMs) have demonstrated impressive performance in various reasoning tasks. However, prompting methods that rely on implicit knowledge in an LLM often hallucinate incorrect answers when the implicit knowledge is wrong or inconsistent with the task. To tackle this problem, we present Hypotheses-to-Theories (HtT), a framework that learns a rule library for reasoning with LLMs. HtT contains two stages, an induction stage and a deduction stage. In the induction stage, an LLM is first asked to generate and verify rules over a set of training examples. Rules that appear and lead to correct answers sufficiently often are collected to form a rule library. In the deduction stage, the LLM is then prompted to employ the learned rule library to perform reasoning to answer test questions. Experiments on both numerical reasoning and relational reasoning problems show that HtT improves existing prompting methods, with an absolute gain of 11-27% in accuracy. The learned rules are also transferable to different models and to different forms of the same problem.},
   author = {Zhaocheng Zhu and Yuan Xue and Xinyun Chen and Denny Zhou and Jian Tang and Dale Schuurmans and Hanjun Dai},
   month = {10},
   title = {Large Language Models can Learn Rules},
   url = {https://arxiv.org/abs/2310.07064v1},
   year = {2023},
}
@article{Zheng2023,
   abstract = {We present Step-Back Prompting, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide the reasoning steps, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of Step-Back Prompting with PaLM-2L models and observe substantial performance gains on a wide range of challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting improves PaLM-2L performance on MMLU Physics and Chemistry by 7% and 11%, TimeQA by 27%, and MuSiQue by 7%.},
   author = {Huaixiu Steven Zheng and Swaroop Mishra and Xinyun Chen and Heng-Tze Cheng and Ed H Chi Quoc and V Le and Denny Zhou and Google Deepmind},
   month = {10},
   title = {Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models},
   url = {https://arxiv.org/abs/2310.06117v1},
   year = {2023},
}
@article{Showkat2022,
   abstract = {The rise of automated text processing systems has led to the development of tools designed for a wide variety of application domains. These technologies are often developed to support non-technical users such as domain experts and are often developed in isolation of the tools primary user. While such developments are exciting, less attention has been paid to domain experts' expectations about the values embedded in these automated systems. As a step toward addressing that gap, we examined values expectations of journalists and legal experts. Both these domains involve extensive text processing and place high importance on values in professional practice. We engaged participants from two non-profit organizations in two separate co-speculation design workshops centered around several speculative automated text processing systems. This study makes three interrelated contributions. First, we provide a detailed investigation of domain experts' values expectations around future NLP systems. Second, the speculative design fiction concepts, which we specifically crafted for these investigative journalists and legal experts, illuminated a series of tensions around the technical implementation details of automation. Third, our findings highlight the utility of design fiction in eliciting not-to-design implications, not only about automated NLP but also about technology more broadly. Overall, our study findings provide groundwork for the inclusion of domain experts values whose expertise lies outside of the field of computing into the design of automated NLP systems.},
   author = {Dilruba Showkat and Eric P.S. Baumer},
   doi = {10.1145/3532106.3533483},
   isbn = {9781450393584},
   journal = {DIS 2022 - Proceedings of the 2022 ACM Designing Interactive Systems Conference: Digital Wellbeing},
   keywords = {Design Fiction,Natural Language Processing (NLP) Automation,Participatory Design Fiction Workshop,Values in Automated NLP},
   month = {6},
   pages = {100-122},
   publisher = {Association for Computing Machinery, Inc},
   title = {"It's Like the Value System in the Loop": Domain Experts' Values Expectations for NLP Automation},
   year = {2022},
}
@article{Shea-Blymyer2022,
   abstract = {This work gives a logical characterization of the (ethical and social) obligations of an agent trained with Reinforcement Learning (RL). An RL agent takes actions by following a utility-maximizing policy. We maintain that the choice of utility function embeds ethical and social values implicitly, and that it is necessary to make these values explicit. This work provides a basis for doing so. First, we propose a probabilistic deontic logic that is suited for formally specifying the obligations of a stochastic system, including its ethical obligations. We prove some useful validities about this logic, and how its semantics are compatible with those of Markov Decision Processes (MDPs). Second, we show that model checking allows us to prove that an agent has a given obligation to bring about some state of affairs-meaning that by acting optimally, it is seeking to reach that state of affairs. We develop a model checker for our logic against MDPs. Third, we observe that it is useful for a system designer to obtain a logical characterization of her system's obligations, which is potentially more interpretable and helpful in debugging than the expression of a utility function. Enumerating all the obligations of an agent is impractical, so we propose a Bayesian optimization routine that learns to generate a system's obligations that the system designer deems interesting. We implement the model checking and Bayesian optimization routines, and demonstrate their effectiveness with an initial pilot study. This work provides a rigorous method to characterize utility-maximizing agents in terms of the (ethical and social) obligations that they implicitly seek to satisfy.},
   author = {Colin Shea-Blymyer and Houssam Abbas},
   doi = {10.1145/3514094.3534163},
   isbn = {9781450392471},
   journal = {AIES 2022 - Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
   keywords = {deontic logic,explainability,machine ethics,model checking,normative systems},
   month = {7},
   pages = {653-663},
   publisher = {Association for Computing Machinery, Inc},
   title = {Generating Deontic Obligations from Utility-Maximizing Systems},
   url = {https://doi.org/10.1145/3514094.3534163},
   year = {2022},
}
@article{Jiang2023b,
   abstract = {Legal syllogism is a form of deductive reasoning commonly used by legal professionals to analyze cases. In this paper, we propose legal syllogism prompting (LoT), a simple prompting method to teach large language models (LLMs) for legal judgment prediction. LoT teaches only that in the legal syllogism the major premise is law, the minor premise is the fact, and the conclusion is judgment. Then the models can produce a syllogism reasoning of the case and give the judgment without any learning, fine-tuning, or examples. On CAIL2018, a Chinese criminal case dataset, we performed zero-shot judgment prediction experiments with GPT-3 models. Our results show that LLMs with LoT achieve better performance than the baseline and chain of thought prompting, the state-of-art prompting method on diverse reasoning tasks. LoT enables the model to concentrate on the key information relevant to the judgment and to correctly understand the legal meaning of acts, as compared to other methods. Our method enables LLMs to predict judgment along with law articles and justification, which significantly enhances the explainability of models.},
   author = {Cong Jiang and Xiaolei Yang},
   doi = {10.1145/3594536.3595170},
   isbn = {9798400701979},
   keywords = {Artificial intelligence KEYWORDS large language models, legal syllogism, legal judgment prediction, chain of thought,CCS CONCEPTS • Applied computing → Law,• Computing methodologies → Natural language generation},
   month = {6},
   pages = {417-421},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Legal Syllogism Prompting},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3594536.3595170},
   year = {2023},
}
@article{Wright2020,
   abstract = {A "rightful machine" is an explicitly moral, autonomous machine agent whose behavior conforms to principles of justice and the positive public law of a legitimate state. In this paper, I set out some basic elements of a deontic logic appropriate for capturing conflicting legal obligations for purposes of programming rightful machines. Justice demands that the prescriptive system of enforceable public laws be consistent, yet statutes or case holdings may often describe legal obligations that contradict; moreover, even fundamental constitutional rights may come into conflict. I argue that a deontic logic of the law should not try to work around such conflicts but, instead, identify and expose them so that the rights and duties that generate inconsistencies in public law can be explicitly qualified and the conflicts resolved. I then argue that a credulous, non-monotonic deontic logic can describe inconsistent legal obligations while meeting the normative demand for consistency in the prescriptive system of public law. I propose an implementation of this logic via a modified form of "answer set programming," which I demonstrate with some simple examples.},
   author = {Ava Thomas Wright},
   doi = {10.1145/3375627.3375867},
   isbn = {9781450371100},
   keywords = {Answer Set Programming,Conflicts,Deontic Logic,Justice,Law,Logic Programming,Machine Ethics,Rightful Machines},
   month = {2},
   pages = {392-392},
   publisher = {Association for Computing Machinery (ACM)},
   title = {A Deontic Logic for Programming Rightful Machines},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3375627.3375867},
   year = {2020},
}
@article{Rotolo2015,
   abstract = {This paper offers a new logical machinery for reasoning about interpretive canons. We identify some options for modeling reasoning about interpretations and show that interpretative argumentation has a distinctive structure where the claim that a legal text ought or may be interpreted in a certain way can be supported or attacked by arguments, whose conicts may have to be assessed according to further arguments.},
   author = {Antonino Rotolo and Guido Governatori and Giovanni Sartor},
   doi = {10.1145/2746090.2746100},
   isbn = {9781450335225},
   journal = {Proceedings of the International Conference on Artificial Intelligence and Law},
   keywords = {Argumentation,Defeasible Logic,Legal Interpretation},
   month = {6},
   pages = {99-108},
   publisher = {Association for Computing Machinery},
   title = {Deontic defeasible reasoning in legal interpretation: Two options for modelling interpretive arguments},
   volume = {08-12-June-2015},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/2746090.2746100},
   year = {2015},
}
@article{Joshi2020b,
   abstract = {Federal government agencies and organizations doing business with them have to adhere to the Code of Federal Regulations (CFR). The CFRs are currently available as large text documents that are not...},
   author = {Karuna Pande Joshi and Srishty Saha},
   doi = {10.1145/3425192},
   issn = {26390175},
   issue = {3},
   journal = {Digital Government: Research and Practice},
   keywords = {Deep learning,Semantic Web,compliance,legal text analytics},
   month = {11},
   publisher = {
		ACM
		PUB27
		New York, NY, USA
	},
   title = {A Semantically Rich Framework for Knowledge Representation of Code of Federal Regulations},
   volume = {1},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3425192},
   year = {2020},
}
@article{Libal2019,
   abstract = {In this article we introduce a logical structure for normative reasoning, called Normative Detachment Structure with Ideal Conditions, that can be used to represent the content of certain legal texts in a normalized way. The structure exploits the deductive properties of a system of bimodal logic able to distinguish between ideal and actual normative statements, as well as a novel formalization of conditional normative statements able to capture interesting cases of contrary-to-duty reasoning and to avoid deontic paradoxes. Furthermore, we illustrate how the theoretical framework proposed can be mechanized to get an automated procedure of query-answering on an example of legal text.},
   author = {Tomer Libal and Matteo Pascucci},
   city = {New York, NY, USA},
   doi = {10.1145/3322640.3326707},
   isbn = {9781450367547},
   journal = {Proceedings of the 17th International Conference on Artificial Intelligence and Law, ICAIL 2019},
   keywords = {Deontic logic,Legal reasoning,Normative ideality},
   month = {6},
   pages = {63-72},
   publisher = {Association for Computing Machinery, Inc},
   title = {Automated reasoning in normative detachment structures with ideal conditions},
   url = {https://doi.org/10.1145/3322640.3326721},
   year = {2019},
}
@article{Araszkiewicz2023,
   abstract = {We present an approach designed to support the process of legislative drafting by helping to detect errors in a normative text. It is based on a framework allowing for representation and comparison of structure and semantic content of legal provisions. Such comparison serves as a starting point for detection of (potential) legislative errors. The approach provides in particular criteria to select provisions to be compared, related to the phenomenon of provisions overlapping. We show that specific cases of such an overlap may amount to legislative errors. The presented framework enables a precise and transparent account of these errors. We also acknowledge that textual provisions enable various interpretations, while the error methodology detection assumes that the semantic representation of provisions is a result of a specific interpretation. We introduce the notion of Constraining Interpretive Rules which are used to evaluate the acceptability of specific interpretations of legal provisions. We discuss the features of the model on a real example and we present an implementation of the approach by using semantic technologies. CCS CONCEPTS • Computing methodologies → Reasoning about belief and knowledge ; • Applied computing → Law.},
   author = {Michał Araszkiewicz and Enrico Francesconi and European Parliament and Tomasz Zurek},
   doi = {10.1145/3594536.3595172},
   isbn = {9798400701979},
   keywords = {Enrico Francesconi,Interpretation of statutory law ACM Reference Format: Michał Araszkiewicz,Legal knowledge representation,Legislative drafting support,Legislative errors,Semantic tech-nologies,and Tomasz Zurek 2023 Iden-tification of Legislative},
   month = {6},
   pages = {2-11},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Identification of Legislative Errors},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3594536.3595172},
   year = {2023},
}
@article{Witt2021,
   abstract = {A critical challenge in "Rules as Code"("RaC") initiatives is enhancing legal accuracy. In this paper, we present the preliminary results of a two-week, first of its kind experiment that aims to shed light on how different legally trained people interpret and convert Australian Commonwealth legislation into machine-executable code. We find that coders collaboratively agreeing on key legal terms, or atoms, before commencing independent coding work can significantly increase the similarity of their encoded rules. Participants nonetheless made a range of divergent interpretive choices, which we argue are most likely due to: (1) the complexity of statutory interpretation, (2) encoded provisions having varying levels of granularity, and (3) the functionality of our coding language. Based on these findings, we draw an important distinction between processes for technical validation of encoded rules, which focus on ensuring rules adhere to select coding languages and conventions, and processes of legal alignment, which we conceptualise as enhancing congruence between the encoded provisions and the true meaning of the statutory text in line with the modern approach to statutory interpretation. We argue that these processes are distinct but both critically important in enhancing the accuracy of encoded rules. We conclude by emphasising the need in RaC initiatives for multi-disciplinary expertise across specific legal subject matters, statutory interpretation and technical programming.},
   author = {Alice Witt and Anna Huggins and Guido Governatori and Joshua Buckley},
   doi = {10.1145/3462757.3466083},
   isbn = {9781450385268},
   journal = {Proceedings of the 18th International Conference on Artificial Intelligence and Law, ICAIL 2021},
   keywords = {digitising legislation,legal alignment,machine-executable code,rules as code,statutory interpretation,technical processes},
   month = {6},
   pages = {139-148},
   publisher = {Association for Computing Machinery, Inc},
   title = {Converting copyright legislation into machine-executable code: Interpretation, coding validation and legal alignment},
   volume = {21},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3462757.3466083},
   year = {2021},
}
@article{Rotolo2021,
   abstract = {A legal procedure in court proceedings is a sequence of actions in which the last action is (the creation of) a(n individual) norm, where the court settles that it is obligatory in the interest of some agents that other agents bring about a certain state of affairs. This paper models legal procedures by using a variant of Propositional Dynamic Logic (PDL) enriched with a preference operator for prioritising procedural actions. The key reason towards the usage of PDL is that, in procedural law, claims and resolutions resemble programs to be executed. Requests are organised in a preference order and resolutions have their own dynamics of execution (either spontaneously by the one obliged and/or by force of law).},
   author = {Antonino Rotolo and Clara Smith},
   doi = {10.1145/3462757.3466089},
   isbn = {9781450385268},
   journal = {Proceedings of the 18th International Conference on Artificial Intelligence and Law, ICAIL 2021},
   keywords = {PDL,legal procedures,preferences},
   month = {6},
   pages = {220-224},
   publisher = {Association for Computing Machinery, Inc},
   title = {Modelling legal procedures},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3462757.3466089},
   year = {2021},
}
@article{Maranhao2019,
   abstract = {The research aims at a formal definition of constructive interpretation in law as the dynamic of revision of theories about the normative system, embedding a model of balancing values [13] into an architecture of i/o logics representing conceptual, deontological and axiological rules [11]. We also introduce new revision operators which are relevant in the context of value assessments.},
   author = {Juliano Maranhão and Giovanni Sartor},
   doi = {10.1145/3322640.3326709},
   isbn = {9781450367547},
   journal = {Proceedings of the 17th International Conference on Artificial Intelligence and Law, ICAIL 2019},
   keywords = {Balancing values,Input/output logics,Normative systems},
   month = {6},
   pages = {219-223},
   publisher = {Association for Computing Machinery, Inc},
   title = {Value assessment and revision in legal interpretation},
   volume = {5},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3322640.3326709},
   year = {2019},
}
@article{Parvizimosaed2022,
   abstract = {Legal contracts specify requirements for business transactions. As any other requirements specification, contracts may contain errors and violate properties expected by contracting parties. Symboleo was recently proposed as a formal specification language for legal contracts. This paper presents SymboleoPC, a tool for analyzing Symboleo contracts using model checking. It highlights the architecture, implementation and testing of the tool, as well as a scalability evaluation with respect to the size of contracts and properties to be checked through a series of experiments. The results suggest that SymboleoPC can be usefully applied to the analysis of formal specifications of contracts with real-life sizes and structures.},
   author = {Alireza Parvizimosaed and Marco Roveri and Aidin Rasti and Daniel Amyot and Luigi Logrippo and John Mylopoulos},
   doi = {10.1145/3550355.3552449},
   isbn = {9781450394666},
   journal = {Proceedings - 25th ACM/IEEE International Conference on Model Driven Engineering Languages and Systems, MODELS 2022},
   keywords = {formal specification languages,legal contracts,model checking,nuXmv,performance analysis,smart contracts,software requirements specifications},
   month = {10},
   pages = {278-288},
   publisher = {Association for Computing Machinery, Inc},
   title = {Model-checking legal contracts with SymboleoPC},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3550355.3552449},
   year = {2022},
}
@article{Shea-Blymyer2020,
   abstract = {We consider the pressing question of how to model, verify, and ensure that autonomous systems meet certain obligations (like the obligation to respect traffic laws), and refrain from impermissible behavior (like recklessly changing lanes). Temporal logics are heavily used in autonomous system design; however, as we illustrate here, temporal (alethic) logics alone are inappropriate for reasoning about obligations of autonomous systems. This paper proposes the use of Dominance Act Utilitarianism (DAU), a deontic logic of agency, to encode and reason about obligations of autonomous systems. We use DAU to analyze Intel's Responsibility-Sensitive Safety (RSS) proposal as a real-world case study. We demonstrate that DAU can express well-posed RSS rules, formally derive undesirable consequences of these rules, illustrate how DAU could help design systems that have specific obligations, and how to model-check DAU obligations.},
   author = {Colin Shea-Blymyer and Houssam Abbas},
   doi = {10.1145/3365365.3382203},
   isbn = {9781450370189},
   journal = {HSCC 2020 - Proceedings of the 23rd International Conference on Hybrid Systems: Computation and Control ,part of CPS-IoT Week},
   month = {4},
   publisher = {Association for Computing Machinery, Inc},
   title = {A deontic logic analysis of autonomous systems' safety},
   volume = {11},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3365365.3382203},
   year = {2020},
}
@article{Joshi2021,
   abstract = {Contracts are agreements between parties engaging in economic transactions. They specify deontic modalities that the signatories should be held responsible for and state the penalties or actions to be taken if the stated agreements are not met. Additionally, contracts have also been known to be source of Software Engineering (SE) requirements. Identifying the deontic modalities in contracts can therefore add value to the Requirements Engineering (RE) phase of SE. The complex and ambiguous language of contracts make it difficult and time-consuming to identify the deontic modalities (obligations, permissions, prohibitions), embedded in the text. State-of-art neural network models are effective for text classification; however, they require substantial amounts of training data. The availability of contracts data is sparse owing to the confidentiality concerns of customers. In this paper, we leverage the linguistic and taxonomical similarities between regulations (available abundantly in the public domain) and contracts to demonstrate that it is possible to use regulations as training data for classifying deontic modalities in real-life contracts. We discuss the results of a range of experiments from the use of rule-based approach to Bidirectional Encoder Representations from Transformers (BERT) for automating the classification of deontic modalities. With BERT, we obtained an average precision and recall of 90% and 89.66% respectively.},
   author = {Vivek Joshi and Preethu Rose Anish and Smita Ghaisas},
   doi = {10.1145/3468264.3473921},
   isbn = {9781450385626},
   journal = {ESEC/FSE 2021 - Proceedings of the 29th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
   keywords = {BERT,BiLSTM,Business Contract,Deep Learning Models,Deontic Modality,Domain Adaptation,Regulation},
   month = {8},
   pages = {1275-1280},
   publisher = {Association for Computing Machinery, Inc},
   title = {Domain adaptation for an automated classification of deontic modalities in software engineering contracts},
   volume = {21},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3468264.3473921},
   year = {2021},
}
@article{Areces2023,
   abstract = {We introduce a logic for representing the deontic notion of knowingly complying-associated to an agent's conciousness of taking a normative course of action for achieving a certain goal. Our logic features an operator for describing normative courses of actions, and another operator for describing what each agent knowingly complies with. We provide a sound and complete axiom system for our logic, and study the computational complexity of its satis-fiability problem. Finally, we extend our logic with an additional operator for capturing the general abilities of the agents. This operator enables us to distinguish 'what agents can do' and 'what agents do according to norms'. For this extension, we also provide a sound and complete axiom system.},
   author = {Carlos Areces and Valentin Cassano and Pablo F Castro and Raul Fervari and Andrés R Saravia},
   doi = {10.5555/3545946.3598659},
   journal = {IFAAMAS},
   keywords = {Deontic Logic,Knowingly Complying,Multiagent},
   title = {A Deontic Logic of Knowingly Complying},
   volume = {9},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.5555/3545946.3598659},
   year = {2023},
}
@article{Palmirani2011,
   abstract = {Legal reasoning involves multiple temporal dimensions but the existing state of the art of legal representation languages does not allow us to easily combine expressiveness, performance and legal reasoning requirements. Moreover we also aim at the combination of legal temporal reasoning with the defeasible logic approach, maintaining a computable complexity. The contribution of this work is to extend LKIF-rules with temporal dimensions and defeasible tools, extending our previous work [17]. © 2011 Authors.},
   author = {Monica Palmirani and Guido Governatori and Giuseppe Contissa},
   doi = {10.1145/2018358.2018378},
   isbn = {9781450307550},
   journal = {Proceedings of the International Conference on Artificial Intelligence and Law},
   keywords = {LKIF-rule,rule modelling,temporal dimension},
   pages = {131-135},
   title = {Modelling temporal legal rules},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/2018358.2018378},
   year = {2011},
}
@article{Cheng2006,
   abstract = {This paper discusses why classical mathematical logic, its various classical conservatives extensions, or its non-classical alternatives are not suitable candidates for the right fundamental logic to underlie legal information systems, and show that deontic relevant logic is a more hopeful candidate for the right fundamental logic. Copyright 2006 ACM.},
   author = {Jingde Cheng},
   doi = {10.1145/1141277.1141353},
   isbn = {1595931082},
   journal = {Proceedings of the ACM Symposium on Applied Computing},
   keywords = {Legal knowledge representation and reasoning,Relevant logic},
   pages = {319-320},
   publisher = {Association for Computing Machinery},
   title = {Deontic relevant logic as the logical basis for legal information systems},
   volume = {1},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/1141277.1141353},
   year = {2006},
}
@article{Governatori2021,
   abstract = {Legal documents often contain references to either other documents, or other parts (of the same document). The use of references is meant to reduce the complexity of the documents; however, they pose serious concerns for the formal (logical) representation of the norms stipulated in the document itself. We propose an approach to directly model the references in a logic language and to resolve them during the computation of the legal effects in force in a case. The approach is proved to be computationally feasible and to have an efficient algorithmic implementation.},
   author = {Guido Governatori and Francesco Olivieri},
   city = {New York, NY, USA},
   doi = {10.1145/3462757},
   isbn = {9781450385268},
   journal = {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Law},
   keywords = {Automated reasoning KEYWORDS Defeasible Deontic Logic, legal references,CCS CONCEPTS • Computing methodologies → Artificial intelligence,• Ap-plied computing → Law,• Theory of computation → Proof theory},
   pages = {10},
   publisher = {ACM},
   title = {Unravel Legal References in Defeasible Deontic Logic},
   url = {https://doi.org/10.1145/3462757.3466080},
   year = {2021},
}
@article{Governatori2023,
   abstract = {What happens if the way in which we handle a genuine deontic conflict -i.e., a deontic ambiguity-matters regarding the application of other norms that are not directly affected by that conflict? We argue that the law requires sometimes propagating the ambiguity to other norms and sometimes confining it to some norms only. We explore this issue and model different reasoning patterns. The problem is addressed in a new variant of Defeasible Deontic Logic. The contribution of this paper is threefold: (a) we extend the treatment of ambiguity blocking and propagation to Defeasible Deontic Logic; (b) we discuss reasoning patterns in the law, especially in criminal law, where we need to deal with both ambiguity blocking and ambiguity propagation in the same legal system and logic; (c) we devise an annotated variant of Defeasible Deontic Logic where we distinguish literals that must be obtained through an ambiguity-blocking mechanism from those that are derived using an ambiguity-propagating mechanism.},
   author = {Guido Governatori and Antonino Rotolo},
   doi = {10.1145/3594536.3595175},
   isbn = {9798400701979},
   keywords = {Automated reason-ing,CCS CONCEPTS • Theory of computation → Proof theory,• Applied computing → Law KEYWORDS Defeasible Deontic Logic, Deontic Ambiguities, Ambiguity Block-ing, Ambiguity Propagation},
   month = {6},
   pages = {91-100},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Deontic Ambiguities in Legal Reasoning},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3594536.3595175},
   year = {2023},
}
@article{Savelka2023b,
   abstract = {We evaluated the capability of a state-of-the-art generative pre-trained transformer (GPT) model to perform semantic annotation of short text snippets (one to few sentences) coming from legal documents of various types. Discussions of potential uses (e.g., document drafting, summarization) of this emerging technology in legal domain have intensified, but to date there has not been a rigorous analysis of these large language models' (LLM) capacity in sentence-level semantic annotation of legal texts in zero-shot learning settings. Yet, this particular type of use could unlock many practical applications (e.g., in contract review) and research opportunities (e.g., in empirical legal studies). We fill the gap with this study. We examined if and how successfully the model can semantically annotate small batches of short text snippets (10-50) based exclusively on concise definitions of the semantic types. We found that the GPT model performs surprisingly well in zero-shot settings on diverse types of documents (F 1 = .73 on a task involving court opinions, .86 for contracts, and .54 for statutes and regulations). These findings can be leveraged by legal scholars and practicing lawyers alike to guide their decisions in integrating LLMs in wide range of workflows involving semantic annotation of legal texts.},
   author = {Jaromir Savelka},
   doi = {10.1145/3594536.3595161},
   isbn = {9798400701979},
   keywords = {CCS CONCEPTS • Applied computing → Law; Annotation KEYWORDS Semantic legal annotation,GPT,adjudicatory decisions,contracts,generative pre-trained transformers,statutory and regulatory provisions,transfer learning,zero-shot},
   month = {6},
   pages = {447-451},
   publisher = {Association for Computing Machinery (ACM)},
   title = {Unlocking Practical Applications in Legal Domain},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3594536.3595161},
   year = {2023},
}
@article{Stavropoulou2020,
   abstract = {One of the main objectives of the paper is to present a big open legal data (BOLD) platform that will enable easy and advanced access to open legal information across the European Union. We envision facilitating public access for all types of users to open legal data accessed and served via a Legal Web platform in a customizable, structured, intuitive and easy-To-handle way. The user-centric suite of services to be offered include: i) research through legal corpora, analyzing the alignment of national legislation with EU legislation, ii) comparing national laws which target the same life events, iii) analyzing the references to European legislation by national laws, iv) analyzing related laws within the same Member State, v) timeline analysis for all legal acts, vi) visualization of the progress and current status of a specific national or European piece of legislation and vii) sentiment analysis towards new legislation. To implement these, the Manylaws platform builds upon data analytics, text mining, semantic analysis and interactive visualization approaches, for multilingual resources, that will allow users to pose advanced queries, upon which the system will semantically correlate them with annotated resources, to allow the user to retrieve relevant and detailed results, visualized in intuitive ways, and thus allowing better understanding of legal information or building innovative data-centric services. The paper thus describes the novel ManyLaws platform architecture and design specification that will drive the implementation and operation of the ManyLaws portal service infrastructure. The ManyLaws architecture and design specification have been devised following co-design paradigms with legal and parliamentary stakeholders, and software engineering practices for elaboration of its components and their connectors.},
   author = {Stefania Stavropoulou and Ilias Romas and Sofia Tsekeridou and Michalis Avgerinos Loutsaris and Thomas Lampoltshammer and LÅ'rinc Thurnay and Shefali Virkar and Guenther Schefbeck and Nektarios Kyriakou and Zoi Lachana and Charalampos Alexopoulos and Yannis Charalambidis},
   doi = {10.1145/3428502.3428610},
   isbn = {9781450376747},
   journal = {ACM International Conference Proceeding Series},
   keywords = {European legislation,legal informatics,legal ontologies,national legal framework,open legal data,semantic legal annotation,semantics,text mining,visualization},
   month = {9},
   pages = {723-730},
   publisher = {Association for Computing Machinery},
   title = {Architecting an innovative big open legal data analytics, search and retrieval platform},
   volume = {8},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3428502.3428610},
   year = {2020},
}
@article{Lachana2020,
   abstract = {One of the most promising developments comes with the use of innovative technologies and thus with the availability of novel services. The combination of text mining with legal elements may contrib...},
   author = {Zoi Lachana and Michalis Avgerinos Loutsaris and Charalampos Alexopoulos and Yannis Charalabidis},
   doi = {10.4018/IJESMA.2020040105},
   issn = {19416288},
   issue = {2},
   journal = {International Journal of E-Services and Mobile Applications (IJESMA)},
   keywords = {Automated Codification,Automated Interrelation,Laws/Connecting Graphs,Legal Elements Intercon-Nections,Legal Text Mining,Parsing Legal Texts},
   month = {4},
   pages = {79-96},
   publisher = {IGI Global},
   title = {Automated Analysis and Interrelation of Legal Elements Based on Text Mining},
   volume = {12},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.4018/IJESMA.2020040105},
   year = {2020},
}
@article{Casanovas2016,
   abstract = {Ontology-driven systems with reasoning capabilities in the legal field are now better understood. Legal concepts are not discrete, but make up a dynamic continuum between common sense terms, specific technical use, and professional knowledge, in an evolving institutional reality. Thus, the tension between a plural understanding of regulations and a more general understanding of law is bringing into view a new landscape in which general legal frameworks -grounded in well-known legal theories stemming from 20th-century c. legal positivism or sociological jurisprudence -are made compatible with specific forms of rights management on the Web. In this sense, Semantic Web tools are not only being designed for information retrieval, classification, clustering, and knowledge management. They can also be understood as regulatory tools, i.e. as components of the contemporary legal architecture, to be used by multiple stakeholders -front-line practitioners, policymakers, legal drafters, companies, market agents, and citizens. That is the issue broadly addressed in this Special Issue on the Semantic Web for the Legal Domain, overviewing the work carried out over the last fifteen years, and seeking to foster new research in this field, beyond the state of the art.},
   author = {Pompeu Casanovas and Monica Palmirani and Silvio Peroni and Tom Van Engers and Fabio Vitali},
   doi = {10.3233/SW-160224},
   issn = {22104968},
   issue = {3},
   journal = {Semantic Web},
   keywords = {Linked Data,Semantic Web and its applications,Semantic Web for the Legal Domain,ontologies,regulations,rights},
   month = {3},
   pages = {213-227},
   publisher = {IOS Press},
   title = {Semantic Web for the Legal Domain: The'next step},
   volume = {7},
   year = {2016},
}
@article{Loutsaris2021,
   abstract = {The globalization of communication networks and the possibilities offered by the information and communication technologies (ICTs) significantly change the public sector's operation and services. Digital Governance is now integrated into administrations' policies and programs at all levels: local, regional, national, European. At the national level, there is a requirement to provide electronic public services according to citizens' needs while, in the sense of globalization, at the European level, there are many programs (e.g., the Europe 2005 and i2010 program) emphasizing the Digital Governance world (or better Digital Governance community) that indicates rapid changes not only in the sense of the change in the public sector's systems but also in the mentality that the public sector operates. On the other hand, Digital Governance's evolution affects societies intensively, emphasizing the importance of cross-border interaction and information sharing between them. [6]. Concerning the legal informatics domain, this can result in changing governments' operations in many ways [2]. By now, the massive amount of each country's legal information currently remains fragmented across multiple national databases and systems or even better legal databases. Most of these legal databases result from the significant advancements in the "legal informatics"research field that observed since governments have started to promote the development of legal information systems [9]. This research contributes to this purpose by developing an open and automated legal system capable of providing any EU country's legal information based on the existing ontologies.},
   author = {Michalis Avgerinos Loutsaris and Zoi Lachana and Charalampos Alexopoulos and Yannis Charalabidis},
   doi = {10.1145/3463677.3463730},
   isbn = {9781450384926},
   journal = {ACM International Conference Proceeding Series},
   keywords = {big open legal data,legal information systems,legal ontologies},
   month = {6},
   pages = {522-532},
   publisher = {Association for Computing Machinery},
   title = {Legal Text Processing: Combing two legal ontological approaches through text mining},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/3463677.3463730},
   year = {2021},
}
@article{Gopalan2012,
   abstract = {Developing an access control system that satisfies the requirements expressed in regulations, such as the Health Insurance Portability and Accountability Act (HIPAA), can help ensure regulatory compliance in software systems. A usage control model that specifies the rules governing information access and usage, as expressed in law, is an important step towards achieving such compliance. Software systems that handle health records must comply with regulations in the HIPAA Privacy and Security Rules. Herein, we analyze the HIPAA Privacy Rule using a grounded theory methodology coupled with an inquiry driven approach to determine the components that must be supported by a usage control model to achieve regulatorycompliant health records usage. In this paper, we propose a usage control model, UCON LEGAL, which extends UCON ABC with components to model purposes, cross-references, exceptions, conditions, and logs. We also employ UCON LEGAL to show how to express the access and usage rules we identified in the HIPAA Privacy Rule. Our analysis yielded seven types of conditions specific to HIPAA that we include in UCON LEGAL; these conditions were previously unsupported by existing usage control models. Copyright © 2012 ACM.},
   author = {Ramya Gopalan and Annie Antön and Jon Doyle},
   doi = {10.1145/2110363.2110391},
   isbn = {9781450307819},
   journal = {IHI'12 - Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium},
   keywords = {Access control,Electronic health records,HIPAA-compliance,Legal compliance,Personal health information,Regulatory compliance,Software requirements,Usage control},
   pages = {227-236},
   title = {UCON LEGAL: A usage control model for HIPAA},
   url = {https://dl-acm-org.ez67.periodicos.capes.gov.br/doi/10.1145/2110363.2110391},
   year = {2012},
}
@article{Akaichi2023b,
   abstract = {Robust Usage Control (UC) mechanisms are necessary to protect sensitive data and resources, especially when these are distributed across multiple nodes or users. Existing solutions have limitations in expressing and enforcing usage control policies due to difficulties in capturing complex requirements and the lack of formal semantics necessary for automated compliance checking. To address these challenges, we propose GUCON, a generic policy framework that allows for the expression of and reasoning over granular UC policies. This is achieved by leveraging the expressiveness and semantics of graph pattern expressions, as well as the flexibility of deontic concepts. Additionally , GUCON incorporates algorithms for conflict detection, resolution, compliance and requirements checking, ensuring active policy enforcement. We demonstrate the effectiveness of our framework by proposing instantiations using SHACL, OWL and ODRL. We show how instantiations provide a bridge between abstract formalism and concrete implementations, thus allowing existing reasoners and implementations to be leveraged.},
   author = {I Akaichi and G Flouris and I Fundulaki and S Kirrane},
   journal = {penni.wu.ac.atI Akaichi, G Flouris, I Fundulaki, S Kirranepenni.wu.ac.at},
   keywords = {Control ·,Deontic,Enforcement,Policy ·,Reasoning ·,Rules ·,Usage},
   title = {GUCON: A Generic Graph Pattern based Policy Framework for Usage Control Enforcement},
   url = {https://penni.wu.ac.at/papers/RuleML%20RR%202023%20GUCON-A%20Generic%20Graph%20Pattern%20based%20Policy%20Framework%20for%20Usage%20Control%20Enforcement.pdf},
   year = {2023},
}
@article{Bench-Capon2022,
   abstract = {We present argumentation schemes to model reasoning with legal cases. We provide schemes for each of the three stages that take place after the facts are established: factor ascription, issue resolution and outcome determination. The schemes are illustrated with examples from a specific legal domain, US Trade Secrets law, and the wider applicability of these schemes is discussed.},
   author = {Trevor Bench-Capon and Katie Atkinson},
   month = {10},
   title = {Using Argumentation Schemes to Model Legal Reasoning},
   url = {https://arxiv.org/abs/2210.00315v1},
   year = {2022},
}
@article{Wyner2011,
   abstract = {Rules in regulations such as found in the US Federal Code of Regulations can be expressed using conditional and deontic rules. Identifying and extracting such rules from the language of the source material would be useful for automating rulebook management and translating into an executable logic. The paper presents a linguistically-oriented, rule-based approach, which is in contrast to a machine learning approach. It outlines use cases, discusses the source materials, reviews the methodology, then provides initial results and future steps. © 2011 The authors and IOS Press. All rights reserved.},
   author = {Adam Wyner and Wim Peters},
   doi = {10.3233/978-1-60750-981-3-113},
   isbn = {9781607509806},
   issn = {09226389},
   journal = {Frontiers in Artificial Intelligence and Applications},
   keywords = {Conditionals,Deontic Logic,Regulation,Text analysis},
   pages = {113-122},
   publisher = {IOS Press},
   title = {On Rule Extraction from Regulations},
   volume = {235},
   url = {https://ebooks.iospress.nl/doi/10.3233/978-1-60750-981-3-113},
   year = {2011},
}
@article{Zhong2020b,
   abstract = {Legal Artificial Intelligence (LegalAI) focuses on applying the technology of artificial intelligence, especially natural language processing, to benefit tasks in the legal domain. In recent years, LegalAI has drawn increasing attention rapidly from both AI researchers and legal professionals, as LegalAI is beneficial to the legal system for liberating legal professionals from a maze of paperwork. Legal professionals often think about how to solve tasks from rule-based and symbol-based methods, while NLP researchers concentrate more on data-driven and embedding methods. In this paper, we introduce the history, the current state, and the future directions of research in LegalAI. We illustrate the tasks from the perspectives of legal professionals and NLP researchers and show several representative applications in LegalAI. We conduct experiments and provide an in-depth analysis of the advantages and disadvantages of existing works to explore possible future directions. You can find the implementation of our work from https://github.com/thunlp/CLAIM.},
   author = {Haoxi Zhong and Chaojun Xiao and Cunchao Tu and Tianyang Zhang and Zhiyuan Liu and Maosong Sun},
   doi = {10.18653/v1/2020.acl-main.466},
   isbn = {9781952148255},
   issn = {0736587X},
   journal = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
   month = {4},
   pages = {5218-5230},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {How Does NLP Benefit Legal System: A Summary of Legal Artificial Intelligence},
   url = {https://arxiv.org/abs/2004.12158v5},
   year = {2020},
}
@article{Boella2013b,
   abstract = {Legal ontology is one of the most researched areas of Artificial Intelligence & Law, but is less applied in the commercial world. This is mainly due to a historical focus on general purpose legal ontologies that do not capture the variety of definitions and interpretations that apply in different contexts, and a focus on automated extraction over manual verification in a domain where accuracy is of utmost importance. In this paper, we show how the use of a domain-specific ontology within a sophisticated legal monitoring software managed by legal experts can help compliance officers in banks and insurance companies comply with strict regulatory duties in a highly complex and constantly evolving area of law.},
   author = {Guido Boella and Llio Humphreys and Marco Martin and Piercarlo Rossi and Leendert Van Der Torre and Andrea Violato},
   doi = {10.1007/978-3-7908-2789-7_62/COVER},
   isbn = {9783790827897},
   journal = {Information Systems: Crossroads for Organization, Management, Accounting and Engineering: ItAIS: The Italian Association for Information Systems},
   month = {9},
   pages = {571-578},
   publisher = {Physica-Verlag},
   title = {Eunomos, A legal document and knowledge management system for regulatory compliance},
   volume = {9783790827897},
   url = {https://link.springer.com/chapter/10.1007/978-3-7908-2789-7_62},
   year = {2013},
}
@article{Winkels2005,
   abstract = {The Dutch Tax and Customs Administration (DTCA) is one of many organizations that deal with a multitude of electronic legal data, from various sources and in different formats. In this paper, we describe the results of a study aimed at better access to these sources by having a supplier and format independent knowledge store that describes the sources and their interrelations in a semantic network. Furthermore we developed parsers to automatically detect the identity of sources and typed references within the sources to other legal documents. These parsers can be used to fill and update the semantic network as new documents are added. Copyright 2005 ACM.},
   author = {Radboud Winkels and Alexander Boer and Emile De Maat and Tom Van Engers and Matthijs Breebaart and Henri Melger},
   doi = {10.1145/1165485.1165505},
   isbn = {1595930817},
   journal = {Proceedings of the International Conference on Artificial Intelligence and Law},
   keywords = {Legal CMS,Reference parsing,Semantic web,Standardisation},
   pages = {125-132},
   title = {Constructing a semantic network for legal content},
   url = {https://dl.acm.org/doi/10.1145/1165485.1165505},
   year = {2005},
}
@article{Junior2019b,
   abstract = {This paper presents our experience on building RDF knowledge graphs for an industrial use case in the legal domain. The information contained in legal information systems are often accessed through simple keyword interfaces and presented as a simple list of hits. In order to improve search accuracy one may avail of knowledge graphs, where the semantics of the data can be made explicit. Significant research effort has been invested in the area of building knowledge graphs from semi-structured text documents, such as XML, with the prevailing approach being the use of mapping languages. In this paper, we present a semantic model for representing legal documents together with an industrial use case. We also present a set of use case requirements based on the proposed semantic model, which are used to compare and discuss the use of state-of-the-art mapping languages for building knowledge graphs for legal data.},
   author = {Ademar Crotti Junior and Fabrizio Orlandi and Declan O'Sullivan and Christian Dirschl and Quentin Reul},
   issn = {16130073},
   journal = {CEUR Workshop Proceedings},
   keywords = {Legal Knowledge Graphs,Legal se-mantic model,Legal semantic model,Mapping languages,Mapping languages·},
   month = {11},
   publisher = {CEUR-WS},
   title = {Using Mapping Languages for Building Legal Knowledge Graphs from XML Files},
   volume = {2599},
   url = {https://arxiv.org/abs/1911.07673v1},
   year = {2019},
}
@article{Tang2020,
   abstract = {Knowledge graph has become an essential tool for semantic analysis with the development of natural language processing and deep learning. A high-quality knowledge graph is handy for building a high-performance knowledge-driven application. Despite recent advances in information extraction (IE) techniques, no suitable automated methods can be applied to constructing a domain-specific, comprehensive, and high-quality knowledge graph. However, a semi-automatic strategy, which can ensure the basic quality requirements of a knowledge graph, has been successfully implemented in the elementary science domain. This paper presents a semantic annotation system developed for building a high-quality legal knowledge graph (SALKG) using the semi-automatic strategy. We introduce its system design, architecture, algorithms, functions, and implementation. To investigate the effectiveness of SALKG, we conduct a preliminary annotation experiment with 280 legal texts which were collected from the Harvard Caselaw Access Project. The user evaluation from 32 graduate students demonstrates the high usability of SALKG in semantic annotation and the potential for building a high-quality legal knowledge graph. The system can also be adapted to other fields for constructing domain-specific knowledge graphs.},
   author = {Mingwei Tang and Cui Su and Haihua Chen and Jingye Qu and Junhua Ding},
   doi = {10.1109/BIGDATA50022.2020.9378107},
   isbn = {9781728162515},
   journal = {Proceedings - 2020 IEEE International Conference on Big Data, Big Data 2020},
   keywords = {Annotation System,Knowledge Graph,Legal Text,Semantic Annotation},
   month = {12},
   pages = {2153-2159},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {SALKG: A Semantic Annotation System for Building a High-quality Legal Knowledge Graph},
   year = {2020},
}
@article{Bos2008,
   abstract = {Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorial Grammar (CCG) and Discourse Representation Theory (DRT). Used together with the C&C tools, Boxer reaches more than 95% coverage on newswire texts. The semantic representations produced by Boxer, known as Discourse Representation Structures (DRSs), incorporate a neo-Davidsonian representations for events, using the VerbNet inventory of thematic roles. The resulting DRSs can be translated to ordinary first-order logic formulas and be processing by standard theorem provers for first-order logic. Boxer's performance on the shared task for comparing semantic represtations was promising. It was able to produce complete DRSs for all seven texts. Manually inspecting the output revealed that: (a) the computed predicate argument structure was generally of high quality, in particular dealing with hard constructions involving control or coordination; (b) discourse structure triggered by conditionals, negation or discourse adverbs was overall correctly computed; (c) some measure and time expressions are correctly analysed, others aren't; (d) several shallow analyses are given for lexical phrases that require deep analysis; (e) bridging references and pronouns are not resolved in most cases. Boxer is distributed with the C&C tools and freely available for research purposes.},
   author = {Johan Bos},
   doi = {10.3115/1626481.1626503},
   journal = {Semantics in Text Processing, STEP 2008 - Conference Proceedings},
   pages = {277-286},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {Wide-coverage semantic analysis with Boxer},
   url = {https://www.researchgate.net/publication/254502455_Wide-Coverage_Semantic_Analysis_with_Boxer},
   year = {2008},
}
@article{Ngo2010,
   abstract = {Traditional information retrieval systems represent documents and queries by keyword sets. However, the content of a document or a query is mainly defined by both keywords and named entities occurring in it. Named entities have ontological features, namely, their aliases, classes, and identifiers, which are hidden from their textual appearance. Besides, the meaning of a query may imply latent named entities that are related to the apparent ones in the query. We propose an ontology-based generalized vector space model to semantic text search. It exploits ontological features of named entities and their latently related ones to reveal the semantics of documents and queries. We also propose a framework to combine different ontologies to take their complementary advantages for semantic annotation and searching. Experiments on a benchmark dataset show better search quality of our model to other ones. © 2010 Springer-Verlag Berlin Heidelberg.},
   author = {Vuong M. Ngo and Tru H. Cao},
   doi = {10.1007/978-3-642-12090-9_4/COVER},
   isbn = {9783642120893},
   issn = {1860949X},
   journal = {Studies in Computational Intelligence},
   pages = {41-52},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Ontology-based query expansion with latently related named entities for semantic text searchfs},
   volume = {283},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-12090-9_4},
   year = {2010},
}
@article{Erekhinskaya2020,
   abstract = {In the last years, Artificial Intelligence and Deep Learning have matured from a facinating research area to real-word applications across multiple domains. Enterprises adopt data-driven approaches for various use cases. With the increased adoption, such issues as governance of the models, deployment, scalability, reusablity and maintenance are widely addressed on the engineering side, but not so much on the knowledge side. In this paper, we demonstrate 10 ways of leveraging ontology for Natural Language Processing. Specifically, we explore the usage of ontologies and related standards for labeling schema, configuration, providing lexical data, powering rule engine and automated generation of rules, as well as providing a standard output format. Additionally, we discuss three NLP-based applications: semantic search, question answering and natural language querying and show how they can benefit from ontology usage. The paper summarizes our experience of using ontology in a number of projects for medical, enterprise, financial, legal and security domains.},
   author = {Tatiana Erekhinskaya and Dmitriy Strebkov and Sujal Patel and Mithun Balakrishna and Marta Tatu and Dan Moldovan},
   doi = {10.1145/3391274.3393639},
   isbn = {9781450379748},
   journal = {Proceedings of the International Workshop on on Semantic Big Data, SBD 2020 - In conjunction with the 2020 ACM SIGMOD/PODS Conference},
   keywords = {domain-specific knowledge,labeling,natural language processing,natural language querying,ontologies,semantic graph},
   month = {6},
   publisher = {Association for Computing Machinery, Inc},
   title = {Ten ways of leveraging ontologies for natural language processing and its enterprise applications},
   url = {https://dl.acm.org/doi/10.1145/3391274.3393639},
   year = {2020},
}
@article{Vallet2007,
   abstract = {Personalized content retrieval aims at improving the retrieval process by taking into account the particular interests of individual users. However, not all user preferences are relevant in all situations. It is well known that human preferences are complex, multiple, heterogeneous, changing, even contradictory, and should be understood in context with the user goals and tasks at hand. In this paper, we propose a method to build a dynamic representation of the semantic context of ongoing retrieval tasks, which is used to activate different subsets of user interests at runtime, in a way that out-of-context preferences are discarded. Our approach is based on an ontology-driven representation of the domain of discourse, providing enriched descriptions of the semantics involved in retrieval actions and preferences, and enabling the definition of effective means to relate preferences and context. © 2007 IEEE.},
   author = {David Vallet and Pablo Castells and Miriam Fernández and Phivos Mylonas and Yannis Avrithis},
   doi = {10.1109/TCSVT.2007.890633},
   issn = {10518215},
   issue = {3},
   journal = {IEEE Transactions on Circuits and Systems for Video Technology},
   keywords = {Content search and retrieval,Context modeling,Ontology,Personalization},
   month = {3},
   pages = {336-345},
   title = {Personalized content retrieval in context using ontological knowledge},
   volume = {17},
   year = {2007},
}
@article{Fernandez2011,
   abstract = {Currently, techniques for content description and query processing in Information Retrieval (IR) are based on keywords, and therefore provide limited capabilities to capture the conceptualizations associated with user needs and contents. Aiming to solve the limitations of keyword-based models, the idea of conceptual search, understood as searching by meanings rather than literal strings, has been the focus of a wide body of research in the IR field. More recently, it has been used as a prototypical scenario (or even envisioned as a potential "killer app") in the Semantic Web (SW) vision, since its emergence in the late nineties. However, current approaches to semantic search developed in the SW area have not yet taken full advantage of the acquired knowledge, accumulated experience, and technological sophistication achieved through several decades of work in the IR field. Starting from this position, this work investigates the definition of an ontology-based IR model, oriented to the exploitation of domain Knowledge Bases to support semantic search capabilities in large document repositories, stressing on the one hand the use of fully fledged ontologies in the semantic-based perspective, and on the other hand the consideration of unstructured content as the target search space. The major contribution of this work is an innovative, comprehensive semantic search model, which extends the classic IR model, addresses the challenges of the massive and heterogeneous Web environment, and integrates the benefits of both keyword and semantic-based search. Additional contributions include: an innovative rank fusion technique that minimizes the undesired effects of knowledge sparseness on the yet juvenile SW, and the creation of a large-scale evaluation benchmark, based on TREC IR evaluation standards, which allows a rigorous comparison between IR and SW approaches. Conducted experiments show that our semantic search model obtained comparable and better performance results (in terms of MAP and P@10 values) than the best TREC automatic system. © 2010 Elsevier B.V. All rights reserved.},
   author = {Miriam Fernández and Iván Cantador and Vanesa López and David Vallet and Pablo Castells and Enrico Motta},
   doi = {10.1016/J.WEBSEM.2010.11.003},
   issn = {1570-8268},
   issue = {4},
   journal = {Journal of Web Semantics},
   keywords = {Information Retrieval,Semantic Web,Semantic search},
   month = {12},
   pages = {434-452},
   publisher = {Elsevier},
   title = {Semantically enhanced Information Retrieval: An ontology-based approach},
   volume = {9},
   year = {2011},
}
@article{Marie2011,
   abstract = {The objective of ONTORULE is to enable users, from business executives over business analysts to IT developers, to interact in their own way with the part of a business application that is relevant to them. This extended abstract describes the approach the ONTORULE project proposes to business rule application development, and it introduces the architecture and the semantic technologies that we develop for that purpose and that are validated and demonstrated in two pilot applications. © 2011 Springer-Verlag.},
   author = {Christian De Sainte Marie and Miguel Iglesias Escudero and Peter Rosina},
   doi = {10.1007/978-3-642-23580-1_3/COVER},
   isbn = {9783642235795},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Business rule management systems,ONTORULE,knowledge management,ontology,rules},
   pages = {24-29},
   publisher = {Springer, Berlin, Heidelberg},
   title = {The ONTORULE project: Where ontology meets business rules},
   volume = {6902 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-23580-1_3},
   year = {2011},
}
@article{Kholkar2017,
   abstract = {Enterprises today face the problem of complying with ever-increasing regulation. Use of rule engines for implementing compliance is widespread, however, the rule base needs to be encoded manually. We present a method using model-driven architecture (MDA) to automate generation of rules in a rule language, from a platform-independent model derived from a specification given by domain experts. We demonstrate how a Semantics of Business Vocabulary and Rules (SBVR) model of regulation rules can serve as the common source model for generating rules on various categories of rule engine platforms. The approach is illustrated using a real-life case study from the MiFID-2 financial regulation.},
   author = {Deepali Kholkar and Sagar Sunkle and Vinay Kulkarni},
   doi = {10.5220/0006216406170628},
   isbn = {9789897582103},
   journal = {MODELSWARD 2017 - Proceedings of the 5th International Conference on Model-Driven Engineering and Software Development},
   keywords = {Business Rule Management Systems,CIM,Defeasible Logic,Fact-oriented Model,Formal Compliance Checking,Model Driven Architecture,Model Transformation,PIM,PSM,Production Rule Systems,Rule Engines,SBVR},
   pages = {617-628},
   publisher = {SciTePress},
   title = {Towards automated generation of regulation rule bases using MDA},
   volume = {2017-January},
   url = {https://dl.acm.org/doi/10.5220/0006216406170628},
   year = {2017},
}
@article{Njonko2012,
   abstract = {This paper presents a methodology for transforming business rules (BR) written in natural language (NL) such as English into a set of executable models as Unified Modeling Language (UML), Structured Query language (SQL), etc. As the direct automatic transformation of NL specifications to executable models is very difficult due to the inherent ambiguities of NL, this methodology aims at using the Semantics of Business Vocabulary and Business Rules (SBVR) as an intermediate model front-ended by Micro-Systemic Linguistic Analysis (MSLA) because of their mathematical underpinnings. SBVR is a Semantic Metamodel (SMM) introduced by the Object Management Group (OMG) for specifying semantic models of business using NL. SBVR is not only easy to process by machine since it is grounded in formal logic, but it is also easy to understand both by software developers and other stakeholders. Given that SBVR is fully integrated in OMG's Model Driven Architecture (MDA) and behaves as a Computational Independent Model (CIM), our approach advocates model transformation which is the key constituent of the MDA standard. © 2012 IEEE.},
   author = {Paul Brillant Feuto Njonko and Walid El Abed},
   doi = {10.1109/ICSAI.2012.6223550},
   isbn = {9781467301992},
   journal = {2012 International Conference on Systems and Informatics, ICSAI 2012},
   keywords = {Business Rules,Executable Models,MDA,MSLA,Natural Language,SBVR,Semantic Metamodel},
   pages = {2453-2457},
   title = {From natural language business requirements to executable models via SBVR},
   year = {2012},
}
@article{Ghali2012,
   abstract = {Ontologies are known to be suitable to represent business knowledge. However, in the Business Rules community the business models are usually represented using object models (OM). Many of the existing Business Rules Management Systems (BRMS) allow the Business Users to represent Business Object Models in their own proprietary languages. Some work has been done in the last years to bridge the gap between the ontologies and the Business Rules. A pragmatic approach consist in projecting ontologies into the Object Models used by the BRMS, to ease the use of ontologies by the Business Users. The main issue with this approach is that the expressive power of the targeted Object Model is not enough to cope with the content of the ontology. Hence, the translation looses some of the information contained in the ontology, such as axioms. The aim of this paper, is to go a step further using this approach by translating some of the axioms defined in an OWL ontology into Business Rules. This translation brings at least two benefits: (i) it allow the Business Users to understand better the content of the Ontology by having some of its axioms in the rule language they are used to. (ii) at the run-time level, the translated axioms will be handled by the rule engine. We explain the basic mechanism of this translation and detail its implementation in the JRules BRMS system. © 2012 Springer-Verlag.},
   author = {Adil El Ghali and Amina Chniti and Hugues Citeau},
   doi = {10.1007/978-3-642-32689-9_6/COVER},
   isbn = {9783642326882},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Business Rules,OWL,Ontologies,RIF-PRD},
   pages = {62-76},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Bringing OWL ontologies to the business rules users},
   volume = {7438 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-32689-9_6},
   year = {2012},
}
@article{Skersys2018,
   abstract = {In model-driven information systems engineering, model transformations reside at the very core of this paradigm. Indeed, model transformations (in particular, model-to-model, or M2M) are a must-have feature of any modern model-driven approach supported by CASE technology. Model transformations are intended to raise quality of the models under development, and also speed-up the modeling itself by bringing in certain level of automation into the development process. Nevertheless, due to certain objective reasons, the level of such automation is spread unevenly throughout the development process – in this respect, Business Modeling and System Analysis are, arguably, the most underdeveloped phases of the model-driven information systems development life cycle. In this paper, we show how M2M transformation technology was used to extract well-structured business vocabularies and business rules from formal use case models represented through a set of use case diagrams; Object Management Group's (OMG) standards Semantics for Business Vocabulary and Rules (SBVR) and Unified Modeling Language (UML) were used for this purpose. The proposed solution consists of two concurrent approaches, namely, automatic and semi-automatic, which may be used selectively to achieve the best expected result. Basic implementation aspects of the solution integrating both approaches are also briefly presented in the paper. While UML use case models is the main subject in this research, the proposed solution may be adopted for other UML and MOF-based models as well.},
   author = {Tomas Skersys and Paulius Danenas and Rimantas Butleris},
   doi = {10.1016/J.JSS.2018.03.061},
   issn = {0164-1212},
   journal = {Journal of Systems and Software},
   keywords = {Model-to-model transformation,SBVR business rules,SBVR business vocabulary,UML use case diagram},
   month = {7},
   pages = {111-130},
   publisher = {Elsevier},
   title = {Extracting SBVR business vocabularies and business rules from UML use case diagrams},
   volume = {141},
   year = {2018},
}
@article{Omrane2011,
   abstract = {This paper describes a platform that helps industrial domain experts to preserve the connection between textual sources and formalized business rules by using lexicalized ontologies both for links and for storage of the conceptual knowledge. Business Rules Management Systems (BRMSs) are used to update and query business rules of an automotive use case. They rely strongly on domain ontologies, which model the business knowledge and provide a conceptual vocabulary for the formalization of the rules that are expressed in written policies. We show that lexicalized ontologies are a key component of such BRMSs and how such knowledge can be encoded. Our proposed solution supports domain experts in the automotive industry in understanding and maintaining their business rules by presenting the relevant source documents that were used to create the ontological concepts. The use case is based on a car development scenario that models the connection between car testing scenarios, e.g., safety tests, and the methods and tools used to analyze and prepare these tests. The intended solution has been developed in the ONTORULE project and is still work in progress. © 2011 Springer-Verlag.},
   author = {Nouha Omrane and Adeline Nazarenko and Peter Rosina and Sylvie Szulman and Christoph Westphal},
   doi = {10.1007/978-3-642-24908-2_21/COVER},
   isbn = {9783642249075},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Business rules,domain ontology,semantic annotation},
   pages = {179-192},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Lexicalized ontology for a business rules management platform: An automotive use case},
   volume = {7018 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-24908-2_21},
   year = {2011},
}
@article{Sullivan2023,
   abstract = {Decision trees remain one of the most popular machine learning models today, largely due to their out-of-the-box performance and interpretability. In this work, we present a Bayesian approach to decision tree induction via maximum a posteriori inference of a posterior distribution over trees. We first demonstrate a connection between maximum a posteriori inference of decision trees and AND/OR search. Using this connection, we propose an AND/OR search algorithm, dubbed MAPTree, which is able to recover the maximum a posteriori tree. Lastly, we demonstrate the empirical performance of the maximum a posteriori tree both on synthetic data and in real world settings. On 16 real world datasets, MAPTree either outperforms baselines or demonstrates comparable performance but with much smaller trees. On a synthetic dataset, MAPTree also demonstrates greater robustness to noise and better generalization than existing approaches. Finally, MAPTree recovers the maxiumum a posteriori tree faster than existing sampling approaches and, in contrast with those algorithms, is able to provide a certificate of optimality. The code for our experiments is available at https://github.com/ThrunGroup/maptree.},
   author = {Colin Sullivan and Mo Tiwari and Sebastian Thrun},
   month = {9},
   title = {MAPTree: Beating "Optimal" Decision Trees with Bayesian Decision Trees},
   url = {https://arxiv.org/abs/2309.15312v1},
   year = {2023},
}
@article{Ye2023,
   abstract = {The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scalable prompts based on natural language instructions, and use natural language to describe the geometric structure and node features of the graph for instruction tuning an LLM to perform learning and inference on graphs in a generative manner. Our method exceeds all competitive GNN baselines on ogbn-arxiv, Cora and PubMed datasets, which demonstrates the effectiveness of our method and sheds light on generative large language models as the foundation model for graph machine learning.},
   author = {Ruosong Ye and Caiqi Zhang and Runhui Wang and Shuyuan Xu and Yongfeng Zhang},
   month = {8},
   title = {Natural Language is All a Graph Needs},
   url = {https://arxiv.org/abs/2308.07134v3},
   year = {2023},
}
@article{Guo2023,
   abstract = {Large language models~(LLM) like ChatGPT have become indispensable to artificial general intelligence~(AGI), demonstrating excellent performance in various natural language processing tasks. In the real world, graph data is ubiquitous and an essential part of AGI and prevails in domains like social network analysis, bioinformatics and recommender systems. The training corpus of large language models often includes some algorithmic components, which allows them to achieve certain effects on some graph data-related problems. However, there is still little research on their performance on a broader range of graph-structured data. In this study, we conduct an extensive investigation to assess the proficiency of LLMs in comprehending graph data, employing a diverse range of structural and semantic-related tasks. Our analysis encompasses 10 distinct tasks that evaluate the LLMs' capabilities in graph understanding. Through our study, we not only uncover the current limitations of language models in comprehending graph structures and performing associated reasoning tasks but also emphasize the necessity for further advancements and novel approaches to enhance their graph processing capabilities. Our findings contribute valuable insights towards bridging the gap between language models and graph understanding, paving the way for more effective graph mining and knowledge extraction.},
   author = {Jiayan Guo and Lun Du and Hengyu Liu and Mengyu Zhou and Xinyi He and Shi Han},
   month = {5},
   title = {GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking},
   url = {https://arxiv.org/abs/2305.15066v2},
   year = {2023},
}
@article{Nazarenko2011,
   abstract = {Knowledge acquisition is a key issue in the business rule methodology. As Natural Language (NL) policies and regulations are often important or even contractual sources of knowledge, we propose a framework for acquisition and maintenance of business rules based on NL texts. It enables business experts to author the specification of rule applications without the help of knowledge engineers. This framework has been created as part of the ONTORULE project, which is defining an integrated platform for acquisition, maintenance and execution of business-oriented knowledge bases combining ontologies and rules. Our framework relies on a data structure, called "index", encompassing and connecting the source text, the ontology and a textual representation of rules. Textual rules are as close to the Structured English representation of SBVR as possible for business users in charge of rule elicitation. The index relies on W3C technologies, which makes the tools interoperable and enable semantic search. We show that such an index structure supports the parallel maintenance of policy documents and knowledge bases (acquisition, consistency check and update). Two detailed examples with preliminary results are provided, one from air travel and the other from the automotive industry. © Springer-Verlag Berlin Heidelberg 2011.},
   author = {Adeline Nazarenko and Abdoulaye Guissé and François Lévy and Nouha Omrane and Sylvie Szulman},
   doi = {10.1007/978-3-642-22546-8_9/COVER},
   isbn = {9783642225451},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {99-113},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Integrating written policies in business rule management systems},
   volume = {6826 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-22546-8_9},
   year = {2011},
}
@article{Anand2018,
   abstract = {Business rules control and constrain the behavior and structure of the business system in terms of its policies and principles. Business rules are restructured frequently as per the internal or external circumstances based on market opportunities, statutory regulations, and business focus. The current practice in industry, of detecting inconsistencies manually, is error prone, due to the size, complexity and ambiguity in representation using natural language. Our work detects inconsistencies in business rules based on model checking that exploits the FOL basis of SBVR specification. We aim to reduce the burden on solvers and obtain effective system level test data, leading to the development of a novel inconsistency rule checker based on extracting the unsatisfiable cores using solvers like Z3, CVC4, etc. We introduce the concept of graphical clusters, to partition SBVR vocabularies and represent the former exploiting the many-sorted logic and graph reachability algorithm, thus reducing the domain of quantification and the number of uninterpreted functions. The translation of SBVR to SMT-LIBv2 is implemented as part of our tool BuRRiTo. Experimental results are shown on industrial level rule sets.},
   author = {Kritika Anand and Pavan Kumar Chittimalli and Ravindra Naik},
   doi = {10.1007/978-3-319-73305-0_6/COVER},
   isbn = {9783319733043},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Business rules,First order logic,SBVR,SMT solvers},
   pages = {80-96},
   publisher = {Springer Verlag},
   title = {An automated detection of inconsistencies in SBVR-based business rules using many-sorted logic},
   volume = {10702 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-319-73305-0_6},
   year = {2018},
}
@article{Ceci2016,
   abstract = {The user has requested enhancement of the downloaded file.},
   author = {Marcello Ceci and Firas Al Khalil and Leona O'brien},
   journal = {researchgate.net},
   title = {Making Sense of Regulations with SBVR.},
   url = {https://www.researchgate.net/profile/Firas-Al-Khalil-2/publication/305333061_Making_Sense_of_Regulations_with_SBVR/links/5788f3b708ae7a588ee853ec/Making-Sense-of-Regulations-with-SBVR.pdf},
   year = {2016},
}
@article{Emani2019,
   abstract = {Many standards exist to formalize legal texts and rules. The same is true for legal ontologies. However, there is no proof theory to draw conclusions for these ontologically modeled rules. We address this gap by the proposal of a new modeling of deontic statements, and then we use this modeling to propose reasoning mechanisms to answer deontic questions i.e., questions like “Is it mandatory/permitted/prohibited to..”. We also show that using this modeling, it is possible to check the consistency of a deontic rule base. This work stands as a first important step towards a proof theory over a deontic rule base.},
   author = {Cheikh Kacfah Emani and Yannis Haralambous},
   doi = {10.1007/978-3-030-21348-0_14/COVER},
   isbn = {9783030213473},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {209-224},
   publisher = {Springer Verlag},
   title = {Deontic reasoning for legal ontologies},
   volume = {11503 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-21348-0_14},
   year = {2019},
}
@article{Sukys2012,
   abstract = {The goal of the paper is to present question patterns in structured natural language and their transformations into ontology query language SPARQL for allowing business participants to communicate with business software services and data in more flexible and friendly way. The structured language is based on Semantics of Business Vocabulary and Business Rules (SBVR) metamodel, which allows creating and managing business vocabularies and business rules in specific domains. The current paper is focused on transforming question patterns, including usage of synonyms and synonymous forms; projecting formulations constrained by atomic formulations based on facts and fact types; projections on several variables; restricting query results by auxiliary variables constrained by various logical formulations; supplementing questions with derivation rules from SBVR vocabulary of business rules, etc. Patterns are followed by examples, proved by implemented SBVR query editor and SBVR to SPARQL transformations. © 2012 Springer-Verlag.},
   author = {Algirdas Šukys and Lina Nemuraite and Bronius Paradauskas},
   doi = {10.1007/978-3-642-33308-8_36},
   journal = {Communications in Computer and Information Science},
   keywords = {OWL2,SBVR,SBVR question,SPARQL,SWRL,business rule,business vocabulary,ontology},
   pages = {436-451},
   title = {Representing and Transforming SBVR Question Patterns into SPARQL},
   volume = {319 CCIS},
   url = {https://www.researchgate.net/profile/Algirdas-Sukys/publication/283555035_Representing_and_transforming_SBVR_question_patterns_into_SPARQL/links/56a492ad08ae1b65113252df/Representing-and-transforming-SBVR-question-patterns-into-SPARQL.pdf},
   year = {2012},
}
@article{Bouzidi2011,
   abstract = {This paper gives an overview of a formal semantic-based approach of modeling some regulations in the photovoltaic field to help the delivering of technical assessments at the French scientific center on Building Industry (CSTB). Starting from regulatory texts, we first explicit SBVR rules and then formalize them into ontology-based rules in the SPARQL language. These are exploited in the modeling of the compliance checking process required for the delivering of technical assessments.},
   author = {Khalil Bouzidi and Catherine Faron-Zucker and Bruno Fiés and Nhan Le Thanh and Khalil Riad Bouzidi and Bruno Fies and Nhan Le Thanh An and Nhan Le Than},
   doi = {10.1007/978-3-642-23580-1_19},
   journal = {researchgate.net},
   keywords = {Building Industry,E-Government,E-regulations,Knowledge Management,Ontology,Semantic Web},
   pages = {244-249},
   title = {An ontological approach for modeling technical standards for compliance checking},
   volume = {6902 LNCS},
   url = {https://www.researchgate.net/profile/Catherine-Faron-Zucker/publication/221211655_An_Ontological_Approach_for_Modeling_Technical_Standards_for_Compliance_Checking/links/5832f60208aef19cb81c885e/An-Ontological-Approach-for-Modeling-Technical-Standards-for-Compliance-Checking.pdf},
   year = {2011},
}
@article{Sukys2012b,
   abstract = {The paper presents transformation framework from questions in structured language based on Semantics of Business Vocabulary and Rules (SBVR) into SPARQL queries over ontologies defined in Web Ontology Language OWL 2 and, possibly, supplemented with Semantic Web rules SWRL. Such transformation depends on OWL 2 ontology related with corresponding SBVR vocabulary and rules. The current work considers a family of transformations and metamodels required for relating ontologies, rules, SPARQL queries and real business data supported by computerised information systems, as well as establishes requirements for harmonizing the coexistence and preserving semantics of these different representations.},
   author = {Algirdas Sukys and Lina Nemuraite and Bronius Paradauskas and Edvinas Sinkevicius},
   isbn = {9781612082233},
   journal = {Citeseer},
   keywords = {SBVR,SBVR question,SPARQL,business rule,business vocabulary,ontology},
   title = {Transformation Framework for SBVR based Semantic Queries in Business Information Systems},
   url = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=4dbd014ad97245712e8e64291034b9f1aef175c7},
   year = {2012},
}
@article{Leopold2012,
   abstract = {Process Modeling is a widely used concept for understanding, documenting and also redesigning the operations of organizations. The validation and usage of process models is however affected by the fact that only business analysts fully understand them in detail. This is in particular a problem because they are typically not domain experts. In this paper, we investigate in how far the concept of verbalization can be adapted from object-role modeling to process models. To this end, we define an approach which automatically transforms BPMN process models into natural language texts and combines different techniques from linguistics and graph decomposition in a flexible and accurate manner. The evaluation of the technique is based on a prototypical implementation and involves a test set of 53 BPMN process models showing that natural language texts can be generated in a reliable fashion. © 2012 Springer-Verlag Berlin Heidelberg.},
   author = {Henrik Leopold and Jan Mendling and Artem Polyvyanyy},
   doi = {10.1007/978-3-642-31095-9_5},
   isbn = {9783642310942},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Business Process Models,Natural Language Generation,Verbalization},
   pages = {64-79},
   title = {Generating natural language texts from business process models},
   volume = {7328 LNCS},
   year = {2012},
}
@article{Skersys2022,
   abstract = {The Object Management Group (OMG) has put considerable effort into the standardization of various business modeling aspects within the context of model-driven systems development. Indeed, the Business Process Model and Notation (BPMN) is now arguably the most popular process modeling language. At the same time, the Semantics of Business Vocabulary and Business Rules (SBVR), which is a novel and formally sound standard for the specification of virtually any kind of knowledge using controlled natural language, is also gaining its grounds. Nonetheless, the integration between these two very much related standards remains weak. In this paper, we present one such integration effort, namely an approach for the extraction of SBVR process rules from BPMN processes. To accomplish this, we utilized model-to-model transformation technology, which is one of the core features of Model-Driven Architecture. At the core of the presented solution stands a set of model transformation rules and two algorithms specifying the formation of formally defined process rules from process models. Basic implementation aspects, together with the source code of the solution, are also presented in the paper. The experimental results acquired from the automatic model transformation have shown full compliance with the benchmark results and cover the entirety of the specified flow of work defined in the experimental process models. Following this, it is safe to conclude that the set of specified transformation rules and algorithms was sufficient for the given scope of the experiment, providing a solid background for the practical application and future developments of the solution.},
   author = {Tomas Skersys and Paulius Danenas and Egle Mickeviciute and Rimantas Butleris},
   doi = {10.3390/APP12188976},
   issn = {2076-3417},
   issue = {18},
   journal = {Applied Sciences 2022, Vol. 12, Page 8976},
   keywords = {BPMN,SBVR,business process model,model transformation,process rules},
   month = {9},
   pages = {8976},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {Transforming BPMN Processes to SBVR Process Rules with Deontic Modalities},
   volume = {12},
   url = {https://www.mdpi.com/2076-3417/12/18/8976/htm https://www.mdpi.com/2076-3417/12/18/8976},
   year = {2022},
}
@article{Racz2010,
   abstract = {Governance, Risk and Compliance (GRC) is an emerging topic in the business and information technology world. However to this day the concept behind the acronym has neither been adequately researched, nor is there a common understanding among professionals. The research at hand provides a frame of reference for research of integrated GRC that was derived from the first scientifically grounded definition of the term. By means of a literature review the authors merge observations, an analysis of existing definitions and results from prior surveys in the derivation of a single-phrase definition. The definition is evaluated and improved through a survey among GRC professionals. Finally a frame of reference for GRC research is constructed. © 2010 Springer-Verlag.},
   author = {Nicolas Racz and Edgar Weippl and Andreas Seufert},
   doi = {10.1007/978-3-642-13241-4_11/COVER},
   isbn = {3642132405},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {GRC,compliance,definition,governance,integrated,risk},
   pages = {106-117},
   publisher = {Springer, Berlin, Heidelberg},
   title = {A frame of reference for research of integrated Governance, Risk and Compliance (GRC)},
   volume = {6109 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-13241-4_11},
   year = {2010},
}
@article{Douzon2023,
   abstract = {Transformer-based Language Models are widely used in Natural Language Processing related tasks. Thanks to their pre-training, they have been successfully adapted to Information Extraction in business documents. However, most pre-training tasks proposed in the literature for business documents are too generic and not sufficient to learn more complex structures. In this paper, we use LayoutLM, a language model pre-trained on a collection of business documents, and introduce two new pre-training tasks that further improve its capacity to extract relevant information. The first is aimed at better understanding the complex layout of documents, and the second focuses on numeric values and their order of magnitude. These tasks force the model to learn better-contextualized representations of the scanned documents. We further introduce a new post-processing algorithm to decode BIESO tags in Information Extraction that performs better with complex entities. Our method significantly improves extraction performance on both public (from 93.88 to 95.50 F1 score) and private (from 84.35 to 84.84 F1 score) datasets composed of expense receipts, invoices, and purchase orders.},
   author = {Thibault Douzon and Stefan Duffner and Christophe Garcia and Jérémy Espinas},
   doi = {10.1007/978-3-031-06555-2_8},
   isbn = {9783031065545},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {BIESO Decoding,Business documents,Document understanding,Information extraction,Pre-,Pre-training,Training · BIESO,Transformer,Understanding ·},
   month = {9},
   pages = {111-125},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Improving Information Extraction on Business Documents with Specific Pre-Training Tasks},
   volume = {13237 LNCS},
   url = {https://arxiv.org/abs/2309.05429v1},
   year = {2023},
}
@article{Dragoni2016,
   abstract = {Legal texts express conditions in natural language describing what is permitted, forbidden or mandatory in the context they regulate. Despite the numerous approaches tackling the problem of moving from a natural language legal text to the respective set of machine-readable conditions, results are still unsatisfiable and it remains a major open challenge. In this paper, we propose a preliminary approach which combines different Natural Language Processing techniques towards the extraction of rules from legal documents. More precisely, we combine the linguistic information provided by WordNet together with a syntax-based extraction of rules from legal texts, and a logic-based extraction of dependencies between chunks of such texts. Such a combined approach leads to a powerful solution towards the extraction of machine-readable rules from legal documents. We evaluate the proposed approach over the Australian " Telecommunications consumer protections code " .},
   author = {Mauro Dragoni and Serena Villata and Williams Rizzi and Guido Governatori},
   title = {Combining NLP Approaches for Rule Extraction from Legal Documents},
   url = {https://hal.science/hal-01572443 https://hal.science/hal-01572443/document},
   year = {2016},
}
@article{Kotis2022,
   abstract = {Semantics of Business Vocabulary and Rules (SBVR) is a standard that is applied in describing business knowledge in the form of controlled natural language. Business process designers develop SBVR from formal documents and later translate it into business process models. In many immature companies, these documents are often unavailable and could hinder resource efficiency efforts. This study introduced a novel approach called informal document to SBVR (ID2SBVR). This approach is used to extract operational rules of SBVR from informal documents. ID2SBVR mines fact type candidates using word patterns or extracting triplets (actor, action, and object) from sentences. A candidate fact type can be a complex, compound, or complex-compound sentence. ID2SBVR extracts fact types from candidate fact types and transforms them into a set of SBVR operational rules. The experimental results show that our approach can be used to generate the operational rules of SBVR from informal documents with an accuracy of 0.91. Moreover, ID2SBVR can also be used to extract fact types with an accuracy of 0.96. The unstructured data is successfully converted into semi-structured data for use in pre-processing. ID2SBVR allows the designer to automatically generate business process models from informal documents.},
   author = {Konstantinos Kotis and Dimitris Spiliotopoulos and Irene Tangkawarow and Riyanarto Sarno and Daniel Siahaan},
   doi = {10.3390/BDCC6040119},
   issn = {2504-2289},
   issue = {4},
   journal = {Big Data and Cognitive Computing 2022, Vol. 6, Page 119},
   keywords = {SBVR,fact type,informal document to SBVR,natural language,operational rules,resource efficiency},
   month = {10},
   pages = {119},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {ID2SBVR: A Method for Extracting Business Vocabulary and Rules from an Informal Document},
   volume = {6},
   url = {https://www.mdpi.com/2504-2289/6/4/119/htm https://www.mdpi.com/2504-2289/6/4/119},
   year = {2022},
}
@article{Amaral2022,
   abstract = {Finance and economics are wide domains, where ontologies are useful instruments for dealing with semantic interoperability and information integration problems, as well as improving communication and problem solving among people. In particular, reference ontologies have been widely recognized as powerful tools for representing a model of consensus within a community to support communication, meaning negotiation, consensus establishment, as well as semantic interoperability and information integration. In domains like economics and finance, which are too large and complex to be represented as a single, large and monolithic ontology, it is necessary to create an ontological framework, built incrementally and in an integrated way, as a network. Therefore, in this paper we introduce OntoFINE, an Ontology Network in Finance and Economics that organizes and integrates knowledge in the realm on finance and economics, serving as a basis to several applications. We discuss the development of OntoFINE and present some of its applications.},
   author = {Glenda Amaral and Tiago Prince Sales and Giancarlo Guizzardi},
   doi = {10.1007/978-3-031-11520-2_4/COVER},
   isbn = {9783031115196},
   issn = {18651356},
   journal = {Lecture Notes in Business Information Processing},
   keywords = {Economic Exchanges,Money,Ontology Network,Risk,Trust,Value},
   pages = {42-57},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {Towards an Ontology Network in Finance and Economics},
   volume = {441 LNBIP},
   url = {https://link.springer.com/chapter/10.1007/978-3-031-11520-2_4},
   year = {2022},
}
@article{Roychoudhury2018,
   abstract = {Modern enterprises operate in an unprecedented regulatory environment where increasing regulation and heavy penalties on non-compliance have placed regulatory compliance among the topmost concerns of enterprises worldwide. Previous research in the field of compliance has established that the manual specification/tagging of the regulations not only fails to ensure their proper coverage but also negatively affects the turnaround time both in proving and maintaining the compliance. Our contribution in this paper is a case study using a subset of European Union Regulation in the financial markets, namely, Money Market Statistical Reporting (MMSR) and that we validated it in the context of our model-driven semi-automated compliance framework. The novelty of the framework is the key participation of domain experts to author regulatory rules in a controlled natural language to enable compliance checking. We demonstrate transformation of regulations present in legal natural language text (English) to a model form via authoring of Structured English rules in the context of MMSR regulations for a large European bank. This generated regulatory model is eventually translated to formal logic that enables formal compliance checking contrary to current industry practice, that provides content management-based, document-driven and expert-dependent ways of managing regulatory compliance.},
   author = {Suman Roychoudhury and Sagar Sunkle and Namrata Choudhary and Deepali Kholkar and Vinay Kulkarni},
   doi = {10.1007/978-3-030-02302-7_18/FIGURES/8},
   isbn = {9783030023010},
   issn = {18651348},
   journal = {Lecture Notes in Business Information Processing},
   keywords = {Compliance checking,Money market statistical reporting,Regulatory compliance,Structured English},
   pages = {288-302},
   publisher = {Springer Verlag},
   title = {A case study on modeling and validating financial regulations using (semi-) automated compliance framework},
   volume = {335},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-02302-7_18},
   year = {2018},
}
@article{Sunkle2016,
   abstract = {Enterprises today face an unprecedented regulatory regime and are increasingly looking to technology to ease their regulatory compliance concerns. Formal approaches in research focus on checking compliance of business processes against rules, and assume usage of matching terminology on both sides. We focus on run-time compliance of enterprise data, and the specific problem of identifying enterprise data relevant to a regulation, in an automated manner. We present a knowledge representation approach and semi-automated solution using models and model transformations to extract the same from distributed enterprise databases. We use a Semantics of Business Vocabulary and Rules (SBVR) model of regulation rules as the basis to arrive at the necessary and sufficient model of enterprise data. The approach is illustrated using a real-life case study of the MiFID-II financial regulation.},
   author = {Sagar Sunkle and Vinay Kulkarni and Deepali Kholkar},
   doi = {10.5220/0006002600600071},
   journal = {researchgate.net},
   keywords = {Defeasible Logic,Enterprise Data Integration,Fact-oriented Model,Formal Compliance Checking,Knowledge Base,Knowledge Representation,Model Transformation,Reasoning,SBVR},
   title = {From Natural-language Regulations to Enterprise Data using Knowledge Representation and Model Transformations},
   url = {https://www.researchgate.net/profile/Sagar-Sunkle/publication/305720703_From_Natural-language_Regulations_to_Enterprise_Data_using_Knowledge_Representation_and_Model_Transformations/links/5b0783c5aca2725783e25575/From-Natural-language-Regulations-to-Enterprise-Data-using-Knowledge-Representation-and-Model-Transformations.pdf},
   year = {2016},
}
@article{Seppala2018,
   abstract = {This paper presents a visualization technique to assist legal experts in formalising their interpretation of legal texts in terms of regulatory requirements. (Semi-)automation of compliance processes requires a machine-readable version of legal requirements in a format that enables effective compliance assessment. The use of a semi-structured controlled natural language as an intermediate step of the translation from a human-readable text to a machine-readable and understandable format ensures that the process of interpretation of those requirements is as simple as possible. However, it does not ensure that the formal representation resulting from the interpretation faithfully represents the intended semantics provided by the legal expert. Visualization techniques such as property graphs in Neo4j could fill this gap, allowing legal experts to understand and control the formal representation of the result of their act of interpretation.},
   author = {Selja Seppälä and Marcello Ceci and Hai Huang and Leona O'Brien and Tom Butler},
   issn = {1613-0073},
   keywords = {Controlled natural languages,Neo4j,RegTech,SBVR},
   month = {1},
   pages = {73-85},
   publisher = {CEUR Workshop Proceedings},
   title = {SmaRT visualisation of legal rules for compliance},
   volume = {2049},
   url = {https://hdl.handle.net/10468/11829},
   year = {2018},
}
@article{Sunkle2015,
   abstract = {Modern enterprises increasingly face the challenge of keeping pace with regulatory compliances. Semantic disparity between regulation texts, their interpretations, and operational specifics of enterprise often leads enterprises to situations where it becomes difficult for them to establish what compliance means, how they are supposed to affect it in the operational practices, and how to prove that they comply when asked for explanations of (non-)compliance. We take a step toward reducing the semantic disparity by using semantic vocabularies to map regulations with available operational details of enterprise and utilize them in enacting compliance. We also propose to provide explanations of proofs of (non-)compliance.We report our ongoing work in this regard using the design science research (DSR) paradigm. Initial iterations of design cycle from DSR have been useful to us in identifying and matching stakeholderspecific goals in solving these problems.},
   author = {Sagar Sunkle and Deepali Kholkar and Vinay Kulkarni},
   doi = {10.1007/978-3-319-19237-6_21/COVER},
   isbn = {9783319192369},
   issn = {18651348},
   journal = {Lecture Notes in Business Information Processing},
   keywords = {Business process,Design cycle,Design science research,Enterprise data,GRC,Regulatory compliance,SBVR},
   pages = {326-341},
   publisher = {Springer Verlag},
   title = {Solving semantic disparity and explanation problems in regulatory compliance-a research-in-progress report with design science research perspective},
   volume = {214},
   url = {https://link.springer.com/chapter/10.1007/978-3-319-19237-6_21},
   year = {2015},
}
@article{Johnsen2010,
   abstract = {The paper reports on how the two separate worlds of legislator and IT-technologist can be bridged through the formalization of legal rules with SBVR. The legislator can use SBVR to transform legal rules expressed in natural language to legal rules expressed in controlled natural language. During the transformation, impreciseness and inconsistence in the law formulation may be revealed and entail improved quality of the law formulation. The institutions implementing the legal rules can use SBVR documents published by the legislator to save time in their analyzing phase and even to automate the transformation from vocabulary and rules in SBVR to vocabulary and rules in a business rules management system (BRMS). The more institutions that are affected by the same legislation the more time and effort will be saved.},
   author = {Åshild Johnsen and Arne-Jørgen Berre#},
   journal = {Citeseer},
   keywords = {SBVR,interpreting,legal rules,legislator,semantic ontology},
   title = {A bridge between legislator and technologist-Formalization in SBVR for improved quality and understanding of legal rules},
   url = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=71e5ceca50474b7945bc44ba0739a6aec8d206d8},
   year = {2010},
}
@article{Sunkle2015b,
   abstract = {With recent regulatory advances, modern enterprises have to not only comply with regulations but have to be prepared to provide explanation of proof of (non-)compliance. On top of compliance checking, this necessitates modeling concepts from regulations and enterprise operations so that stakeholder-specific and close to natural language explanations could be generated. We take a step in this direction by using Semantics of Business Vocabulary and Rules to model and map vocabularies of regulations and operations of enterprise. Using these vocabularies and leveraging proof generation abilities of an existing compliance engine, we show how such explanations can be created. Basic natural language explanations that we generate can be easily enriched by adding requisite domain knowledge to the vocabularies.},
   author = {Sagar Sunkle and Deepali Kholkar and Vinay Kulkarni},
   doi = {10.1007/978-3-319-21542-6_25/COVER},
   isbn = {9783319215419},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Explanation of proof, Proof of compliance, Regulatory compliance, SBVR},
   pages = {388-403},
   publisher = {Springer Verlag},
   title = {Explanation of proofs of regulatory (Non-)compliance using semantic vocabularies},
   volume = {9202},
   url = {https://link.springer.com/chapter/10.1007/978-3-319-21542-6_25},
   year = {2015},
}
@article{Elgammal2015,
   abstract = {Following the crisis in 2008, the financial industry has faced growing numbers of laws and regulations globally. The number and complexity of these regulations is creating significant issues for governance, risk and compliance management in almost all industrial sectors; however some of these sectors are characterized by being heavily-regulated including the financial industry. This paper proposes a semantically-enabled compliance management framework. In the heart of the framework is an integrated semantic repository incorporating regulatory, business and compliance knowledge; i.e., CMKB. The approach is underpinned by legal Subject Matter Experts (SMEs) interpreting financial regulations and encoding them in the Semantics of Business vocabulary and Business Rule (SBVR) standard. As a proof-of-concept, we have integrated the SBVR and CMKB repositories with a validated compliance solution for design-time compliance verification. However, the approach could be integrated with other compliance solutions at different phases of the business process lifecycle.},
   author = {Amal Elgammal and Tom Butler},
   doi = {10.1007/978-3-319-22885-3_15/COVER},
   isbn = {9783319228846},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Business process compliance management,Compliance patterns,Financial services,SBVR,Semantic compliance management},
   pages = {171-184},
   publisher = {Springer Verlag},
   title = {Towards a framework for semantically-enabled compliance management in financial services},
   volume = {8954},
   url = {https://link.springer.com/chapter/10.1007/978-3-319-22885-3_15},
   year = {2015},
}
@article{Kumar2020,
   abstract = {Enterprises model the behavior of their business to prepare a communication standard for business analysts and to specify requirements to Information Technology (IT) people. The communication gap between IT group and business analysts, who lie on the opposite end of the business spectrum exists due to the different terminologies used in their respective fields regarding the same context. This gap has led to major software failures which prompted the OMG group has come up with a new standard - Semantic of Business Vocabulary and Business Rules (SBVR). Declarative models are provided by SBVR to represent Business Vocabulary and Business Rules which can be understood by everyone working throughout the business spectrum. Each business is governed by business rules which are constrained by the regulation policy set up by the policy guidelines of the organization and government regulations set up on the organization. Business rules are specified in documents like user guides, requirement documents, terms and conditions, do's and don'ts. Typically a Business Analyst interprets the document and manually extracts rules based on his understanding which leads to potential discrepancies, ambiguities and quality issues in the software system. To minimize such errors, in this paper we present an unsupervised approach to automatically extract SBVR vocabularies and rules from domain-specific business documents. We also present our initial results and comparative study with our earlier approach.},
   author = {Pavan Kumar and Chandan Prakash and Ravindra Naik and Abhidip Bhattacharyya},
   doi = {10.1145/3385032.3385046},
   isbn = {9781450375948},
   journal = {ACM International Conference Proceeding Series},
   keywords = {Business rules extraction,Natural language processing,Rule components,Rule document,Sbvr,Text mining},
   month = {2},
   publisher = {Association for Computing Machinery},
   title = {An Approach to Mine SBVR Vocabularies and Rules from Business Documents},
   url = {https://dl.acm.org/doi/10.1145/3385032.3385046},
   year = {2020},
}
@article{Khalil2016,
   abstract = {The Semantics of Business Vocabulary and Business Rules (SBVR) is a specification created by the Object Management Group (OMG) to provide a way to semantically describe business concepts and specify business rules. However, reasoning with SBVR is still an open subject, and current efforts to provide reasoning are done through the Web Ontology Language (OWL), by providing a mapping between SBVR and OWL. In this paper we focus on the problem of mapping SBVR vocabulary and rulebook to OWL 2, but unlike previous mappings described in the literature, we provide a novel and unorthodox mapping that allows to describe legal rules which have their own intricate anatomy.},
   author = {Firas Al Khalil and Marcello Ceci and Kosala Yapa and Leona O’Brien},
   doi = {10.1007/978-3-319-42019-6_17/COVER},
   isbn = {9783319420189},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Legal,OWL,Rule,SBVR},
   pages = {258-266},
   publisher = {Springer Verlag},
   title = {SBVR to OWL 2 mapping in the domain of legal rules},
   volume = {9718},
   url = {https://link.springer.com/chapter/10.1007/978-3-319-42019-6_17},
   year = {2016},
}
@article{Abi-Lahoud2014,
   abstract = {Regulatory compliance has proved to be difficult and time consuming across business domains. In Financial Services, the wide and complex spectrum of regulations calls for machine assistance in making sense of, and in consuming, the regulatory text. Semantic...},
   author = {Elie Abi-Lahoud and Leona O’Brien and Tom Butler},
   doi = {10.1007/978-3-662-45960-7_14},
   pages = {188-201},
   publisher = {Springer, Berlin, Heidelberg},
   title = {On the Road to Regulatory Ontologies},
   url = {https://link.springer.com/chapter/10.1007/978-3-662-45960-7_14},
   year = {2014},
}
@article{Chuang2023c,
   abstract = {Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this Decoding by Contrasting Layers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA by 12-17% absolute points, demonstrating its potential in making LLMs reliably generate truthful facts.},
   author = {Yung-Sung Chuang and Yujia Xie and Hongyin Luo and Yoon Kim and James Glass and Pengcheng He},
   month = {9},
   title = {DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models},
   url = {https://arxiv.org/abs/2309.03883v1},
   year = {2023},
}
@article{Qian2023,
   abstract = {Large language models (LLMs) acquire extensive knowledge during pre-training, known as their parametric knowledge. However, in order to remain up-to-date and align with human instructions, LLMs inevitably require external knowledge during their interactions with users. This raises a crucial question: How will LLMs respond when external knowledge interferes with their parametric knowledge? To investigate this question, we propose a framework that systematically elicits LLM parametric knowledge and introduces external knowledge. Specifically, we uncover the impacts by constructing a parametric knowledge graph to reveal the different knowledge structures of LLMs, and introduce external knowledge through distractors of varying degrees, methods, positions, and formats. Our experiments on both black-box and open-source models demonstrate that LLMs tend to produce responses that deviate from their parametric knowledge, particularly when they encounter direct conflicts or confounding changes of information within detailed contexts. We also find that while LLMs are sensitive to the veracity of external knowledge, they can still be distracted by unrelated information. These findings highlight the risk of hallucination when integrating external knowledge, even indirectly, during interactions with current LLMs. All the data and results are publicly available.},
   author = {Cheng Qian and Xinran Zhao and Sherry Tongshuang Wu},
   month = {9},
   title = {"Merge Conflicts!" Exploring the Impacts of External Distractors to Parametric Knowledge Graphs},
   url = {https://arxiv.org/abs/2309.08594v1},
   year = {2023},
}
@article{Wang2023c,
   abstract = {The 'pre-train, prompt, predict' paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in the scenario of multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of different documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or intra-document structural relations. For graph traversal, we design an LM-guided graph traverser that navigates across nodes and gathers supporting passages assisting LLMs in MD-QA. The constructed graph serves as the global ruler that regulates the transitional space among passages and reduces retrieval latency. Concurrently, the LM-guided traverser acts as a local navigator that gathers pertinent context to progressively approach the question and guarantee retrieval quality. Extensive experiments underscore the efficacy of KGP for MD-QA, signifying the potential of leveraging graphs in enhancing the prompt design for LLMs. Our code is at https://github.com/YuWVandy/KG-LLM-MDQA.},
   author = {Yu Wang and Nedim Lipka and Ryan A. Rossi and Alexa Siu and Ruiyi Zhang and Tyler Derr},
   month = {8},
   title = {Knowledge Graph Prompting for Multi-Document Question Answering},
   url = {https://arxiv.org/abs/2308.11730v1},
   year = {2023},
}
@article{Sunkle2015c,
   abstract = {Industry governance, risk, and compliance (GRC) solutions stand to gain from various analyses offered by formal compliance checking approaches. Such adoption is made difficult by the fact that most formal approaches assume that a mapping between concepts of regulations and models of operational specifics exists. Industry solutions offer tagging mechanisms to map regulations to operational specifics; however, they are mostly semi-formal in nature and tend to rely extensively on experts. We propose to use Semantics of Business Vocabularies and Rules along with similarity measures to create an explicit mapping between concepts of regulations and models of operational specifics of the enterprise. We believe that our work-in-progress takes a step toward adapting and leveraging formal compliance checking approaches in industry GRC solutions.},
   author = {Sagar Sunkle and Deepali Kholkar and Vinay Kulkarni},
   doi = {10.7250/CSIMQ.2015-5.04},
   issn = {2255-9922},
   issue = {5},
   journal = {Complex Systems Informatics and Modeling Quarterly},
   keywords = {Regulatory compliance,SBVR,business process models,operations,semantic similarity},
   month = {12},
   pages = {39-60},
   publisher = {CEUR-WS},
   title = {Toward Better Mapping between Regulations and Operations of Enterprises Using Vocabularies and Semantic Similarity},
   volume = {0},
   url = {https://csimq-journals.rtu.lv/article/view/csimq.2015-5.04},
   year = {2015},
}
@article{Mitra2018,
   abstract = {In the modern age, the need for automation has led to Business Organizations representing their functionality as structured Business Rules. SBVR has come up as an universally popular format for representation of Business Rules. The presence of different Business Organizations working in a particular real life domain results in generation of different rules for each of the organization. Due to the varying business practices, like mergers & acquisitions, upgrades, incorporation of a new application, etc., it becomes necessary to compare a set of Business Rules of a particular organization with the rules of a reference model, to get a measure of similarity among the business functionality of the two. Presently, this comparison is carried out manually by business experts or by executing the rules of one organization with the data of another and checking if they are compliant. Both the approaches are extremely tedious and expensive as modern organizations have huge rule sets and data sets.We present MatGap, a tool which performs a systematic Match and Gap Analysis between two sets of SBVR-based Business Rules applicable to a specific domain, using Global Vectors(GloVe) model and SMT-LIBv2. The analysis report gives a measure of Match among the rules and entities, thus providing the best alignment and aids to identify the representational Gaps(if any) among the rules and entities. The tool also checks whether the embedded logic in the reference Business Rule set is covered by the other Rule set, thus highlighting the business functionality gap that is present in the latter.},
   author = {Sayandeep Mitra and Chandan Prakash and Shayak Chakraborty and Pavan Kumar Chittimalli},
   doi = {10.1109/APSEC.2018.00070},
   isbn = {9781728119700},
   issn = {15301362},
   journal = {Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
   keywords = {Business Rules,Clustering,Gap Analysis,Matching,Natural Language Processing,SBVR,SMT,Verification},
   month = {7},
   pages = {551-560},
   publisher = {IEEE Computer Society},
   title = {MatGap: A Systematic Approach to Perform Match and Gap Analysis among SBVR-Based Domain Specific Business Rules},
   volume = {2018-December},
   year = {2018},
}
@article{Haj2019,
   abstract = {Business rules are generally captured in a natural language. The inherit ambiguity of the latter is often seen as a cause for project failure, which makes it necessary to translate natural language business rules statements to another language sufficiently formal. However, business experts are generally not familiar with formal languages, which can complicate the communication between stakeholders. For this reason, Object Management Group (OMG) had proposed SBVR Standard (2008) for modeling complex organizations in a natural language but in a formal and detailed way. As a result, several studies have succeeded to increase the accuracy of their approaches by transforming their models from/to SBVR standard. Clearly then, the success of these approaches depends on the quality of the SBVR based statements used or generated. This paper presents an approach for checking conformance of both lexicon and syntax of Business Rules (BR) expressed with Semantic of Business Vocabulary and Rules (SBVR), to SBVR Structured English notation (SBVR-SE) using Natural Language Processing (NLP).},
   author = {Abdellatif Haj and Youssef Balouki and Taoufiq Gadi},
   doi = {10.1007/978-3-030-11928-7_63/COVER},
   isbn = {9783030119270},
   issn = {21945365},
   journal = {Advances in Intelligent Systems and Computing},
   keywords = {BR,Business rules,NLP,Natural language processing,SBVR,SBVR structured english,Semantic of business vocabulary and rules},
   pages = {697-706},
   publisher = {Springer Verlag},
   title = {Automated checking of conformance to SBVR structured english notation},
   volume = {915},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-11928-7_63},
   year = {2019},
}
@article{Chittimalli2019,
   abstract = {An enterprise system operates business by providing various services that are guided by set of certain business rules (BR) and constraints. These BR are usually written using plain Natural Language in operating procedures, terms and conditions, and other documents or in source code of legacy enterprise systems. For implementing the BR in a software system, expressing them as UML use-case specifications, or preparing for Merger & Acquisition (M&A) activity, analysts manually interpret the documents or try to identify constraints from the source code, leading to potential discrepancies and ambiguities. These issues in the software system can be resolved only after testing, which is a very tedious and expensive activity. To minimize such errors and efforts, we propose BuRRiTo framework consisting of automatic extraction of BR by mining documents and source code, ability to clean them of various anomalies like inconsistency, redundancies, conflicts, etc. and able to analyze the functional gaps present and performing semantic querying and searching.},
   author = {Pavan Kumar Chittimalli and Kritika Anand and Shrishti Pradhan and Sayandeep Mitra and Chandan Prakash and Rohit Shere and Ravindra Naik},
   doi = {10.1109/ASE.2019.00134},
   isbn = {9781728125084},
   journal = {Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019},
   keywords = {Business Rules Extraction,Graphs,Match and Gap,Natural Language Processing,Rule Components,Rule Document,SBVR,Search and Query,Source Code,Text Mining},
   month = {11},
   pages = {1190-1193},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {BuRRiTo: A framework to extract, specify, verify and analyze business rules},
   year = {2019},
}
@article{Meyer2023,
   abstract = {Knowledge Graphs (KG) provide us with a structured, flexible, transparent, cross-system, and collaborative way of organizing our knowledge and data across various domains in society and industrial as well as scientific disciplines. KGs surpass any other form of representation in terms of effectiveness. However, Knowledge Graph Engineering (KGE) requires in-depth experiences of graph structures, web technologies, existing models and vocabularies, rule sets, logic, as well as best practices. It also demands a significant amount of work. Considering the advancements in large language models (LLMs) and their interfaces and applications in recent years, we have conducted comprehensive experiments with ChatGPT to explore its potential in supporting KGE. In this paper, we present a selection of these experiments and their results to demonstrate how ChatGPT can assist us in the development and management of KGs.},
   author = {Lars-Peter Meyer and Claus Stadler and Johannes Frey and Norman Radtke and Kurt Junghanns and Roy Meissner and Gordian Dziwis and Kirill Bulert and Michael Martin},
   month = {7},
   title = {LLM-assisted Knowledge Graph Engineering: Experiments with ChatGPT},
   url = {https://arxiv.org/abs/2307.06917v1},
   year = {2023},
}
@article{Lewis2020b,
   abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG)-models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
   author = {Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-Tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
   doi = {10.5555/3495724.3496517},
   journal = {NIPS'20: Proceedings of the 34th International Conference on Neural Information Processing Systems},
   pages = {9459-9474},
   title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
   url = {https://dl.acm.org/doi/10.5555/3495724.3496517},
   year = {2020},
}
@article{Wolczyk2023,
   abstract = {Recently, foundation models have achieved remarkable results in fields such as computer vision and language processing. Although there has been a significant push to introduce similar approaches in reinforcement learning, these have not yet succeeded on a comparable scale. In this paper, we take a step towards understanding and closing this gap by highlighting one of the problems specific to foundation RL models, namely the data shift occurring during fine-tuning. We show that fine-tuning on compositional tasks, where parts of the environment might only be available after a long training period, is inherently prone to catastrophic forgetting. In such a scenario, a pre-trained model might forget useful knowledge before even seeing parts of the state space it can solve. We provide examples of both a grid world and realistic robotic scenarios where catastrophic forgetting occurs. Finally, we show how this problem can be mitigated by using tools from continual learning. We discuss the potential impact of this finding and propose further research directions.},
   author = {Maciej Wołczyk and Bartłomiej Cupiał and Jagiellonian Unviersity and Michał Zaj and Razvan Pascanu Deepmind and Łukasz Kucí and Piotr Miło´s Miło´s},
   doi = {10.18653/v1/w18-5413},
   isbn = {9781948087711},
   journal = {EMNLP 2018 - 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, Proceedings of the 1st Workshop},
   month = {3},
   pages = {108-114},
   publisher = {Association for Computational Linguistics (ACL)},
   title = {On The Role of Forgetting in Fine-Tuning Reinforcement Learning Models},
   year = {2023},
}
@article{Ahmad2022,
   abstract = {In the area of natural language processing, measuring sentence similarity is an essential problem. Searching for semantic meaning in natural language is a related issue. The task of measuring sentence similarity is to find semantic symmetry in two sentences, not matter how they are arranged. It is important to measure the similarity of sentences accurately. To compute the similarity between sentences, existing methods have been constructed from approaches for large texts. Since these methods work in very high-dimensional spaces, they are inefficient, require human input, and are not flexible enough for some applications. In this study, we propose a hybrid method (HydMethod) which considers not only semantic information including lexical databases, word embeddings, and corpus statistics, but also implied word order information. With lexical databases, our method models human common sense knowledge, and that knowledge can then be adapted to be used in different domains with the incorporation of corpus statistics. Therefore, the methodology is applicable across several domains. As part of our experiments, we used two standard datasets - Pilot Short Text Semantic Similarity Benchmark and MS paraphrase - in order to demonstrate the efficacy of our proposed method. As a result, the proposed method outperforms the existing approaches when tested on these two datasets, giving the highest correlation value for both word and sentence similarity. Moreover, it achieves a maximum of 32% higher increase than only using word vector or WorldNet based methodology. With Rubenstein and Goodenough word & sentence pairs, our algorithm's similarity measure shows a high Pearson correlation coefficient of 0.8953.},
   author = {Farooq Ahmad and Dr Mohammad Faisal},
   doi = {10.1016/J.IJCCE.2022.02.001},
   issn = {2666-3074},
   journal = {International Journal of Cognitive Computing in Engineering},
   keywords = {Corpus,Lexical database,Natural language processing,Semantic search,Semantic similarity,Word embedding,Word overlap,WordNet},
   month = {6},
   pages = {58-77},
   publisher = {Elsevier},
   title = {A novel hybrid methodology for computing semantic similarity between sentences through various word senses},
   volume = {3},
   year = {2022},
}
@article{Ahmad2023,
   abstract = {The majority of projects fail to achieve their intended objectives, according to research. This could arise for a number of reasons, such as ensuring requirements are managed, excessive documentation of the code, or the difficulty in delivering software that includes all the requested features on time. An effort could be made to overcome such failure rates by establishing a proper management of requirements and concept of reusability. The correct requirements can be identified by checking similarity between the requirements received from the various stakeholders. A reusable software component can result in substantial savings in both time and money. It can be challenging to make a choice regarding the reuse of certain software components. A comparison of the requirements of a new project with those of previous projects prior to starting a new project or even at a later stage during development is useful for identifying reusable components. This paper proposes a framework (ReSim) for identifying software requirements' similarities, in an attempt to improve reusability and identify the correct requirements. A crucial component of ReSim is to measure similarity between software requirements. Different well-known similarity measurement techniques used by the researchers to evaluate the similarity between the software requirements. Some of the methods used to measure this include dice, jaccard, and cosine coefficients, but in this paper, we have used recently developed hybrid method which considers not only semantic information including lexical databases, word embeddings, and corpus statistics, but also implied word order information and produced significant improvements in the results related to the measurement of semantic similarity between words and sentences. As part of the experiments, the study used PURE dataset-in order to demonstrate the efficacy of the proposed framework. As a result, recently developed hybrid method of measuring the requirements similarity is more accurate than Dice, Jaccard, and Cosine, while Cosine is a better choice than Dice, and Jaccard is more accurate than Dice. Thus, ReSim outperforms existing approaches when tested on the PURE dataset, providing the most accurate results for both functional and non-functional requirements.},
   author = {Farooq Ahmad and Mohammad Faisal},
   doi = {10.5815/ijieeb.2023.02.05},
   keywords = {Framework,Index Terms: Measurement,Requirements,Reusability,Semantic,Similarity},
   pages = {38-53},
   title = {Assessing Similarity between Software Requirements: A Semantic Approach},
   volume = {2},
   url = {https://orcid.org/0000-0002-6120-5259},
   year = {2023},
}
@article{Ferrari2017,
   abstract = {This paper presents PURE (PUblic REquirements dataset), a dataset of 79 publicly available natural language requirements documents collected from the Web. The dataset includes 34,268 sentences and can be used for natural language processing tasks that are typical in requirements engineering, such as model synthesis, abstraction identification and document structure assessment. It can be further annotated to work as a benchmark for other tasks, such as ambiguity detection, requirements categorisation and identification of equivalent re-quirements. In the paper, we present the dataset and we compare its language with generic English texts, showing the peculiarities of the requirements jargon, made of a restricted vocabulary of domain-specific acronyms and words, and long sentences. We also present the common XML format to which we have manually ported a subset of the documents, with the goal of facilitating replication of NLP experiments.},
   author = {Alessio Ferrari and Giorgio Oronzo Spagnolo and Stefania Gnesi},
   doi = {10.1109/RE.2017.29},
   isbn = {9781538631911},
   journal = {Proceedings - 2017 IEEE 25th International Requirements Engineering Conference, RE 2017},
   keywords = {Empirical Software Engineering,Empirical Studies,Model Synthesis,NLP,NLP Tasks,Natural Language Requirements,PURE,Public Requirements,Requirements Abstraction,Requirements Ambiguity Detection,Requirements Categorisation,Requirements Dataset,XML},
   month = {9},
   pages = {502-505},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {PURE: A Dataset of Public Requirements Documents},
   year = {2017},
}
@article{Yao2019,
abstract = {Knowledge graphs are important resources for many artificial intelligence tasks but often suffer from incompleteness. In this work, we propose to use pre-trained language models for knowledge graph completion. We treat triples in knowledge graphs as textual sequences and propose a novel framework named Knowledge Graph Bidirectional Encoder Representations from Transformer (KG-BERT) to model these triples. Our method takes entity and relation descriptions of a triple as input and computes scoring function of the triple with the KG-BERT language model. Experimental results on multiple benchmark knowledge graphs show that our method can achieve state-of-the-art performance in triple classification, link prediction and relation prediction tasks.},
author = {Liang Yao and Chengsheng Mao and Yuan Luo},
   month = {9},
   title = {KG-BERT: BERT for Knowledge Graph Completion},
   url = {https://arxiv.org/abs/1909.03193v2},
   year = {2019},
}
@article{Pan2023,
   abstract = {Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolving by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.},
   author = {Shirui Pan and Senior Member and Linhao Luo and Yufei Wang and Chen Chen and Jiapu Wang and Xindong Wu},
   isbn = {00000000/00$00.0},
   keywords = {Bidirectional Reasoning,Generative Pre-Training,Index Terms-Natural Language Processing,Knowledge Graphs,Large Language Models,Roadmap},
   month = {6},
   title = {Unifying Large Language Models and Knowledge Graphs: A Roadmap},
   url = {https://arxiv.org/abs/2306.08302v2},
   year = {2023},
}
@article{Trajanoska2023,
   abstract = {The growing trend of Large Language Models (LLM) development has attracted significant attention, with models for various applications emerging consistently. However, the combined application of Large Language Models with semantic technologies for reasoning and inference is still a challenging task. This paper analyzes how the current advances in foundational LLM, like ChatGPT, can be compared with the specialized pretrained models, like REBEL, for joint entity and relation extraction. To evaluate this approach, we conducted several experiments using sustainability-related text as our use case. We created pipelines for the automatic creation of Knowledge Graphs from raw texts, and our findings indicate that using advanced LLM models can improve the accuracy of the process of creating these graphs from unstructured text. Furthermore, we explored the potential of automatic ontology creation using foundation LLM models, which resulted in even more relevant and accurate knowledge graphs.},
   author = {Milena Trajanoska and Riste Stojanov and Dimitar Trajanov},
   keywords = {Index Terms-ChatGPT,LLMs,NLP,REBEL,Relation-extraction,Sustainability},
   month = {5},
   title = {Enhancing Knowledge Graph Construction Using Large Language Models},
   url = {https://arxiv.org/abs/2305.04676v1},
   year = {2023},
}
@article{Mihindukulasooriya2023,
   abstract = {The recent advances in large language models (LLM) and foundation models with emergent capabilities have been shown to improve the performance of many NLP tasks. LLMs and Knowledge Graphs (KG) can complement each other such that LLMs can be used for KG construction or completion while existing KGs can be used for different tasks such as making LLM outputs explainable or fact-checking in Neuro-Symbolic manner. In this paper, we present Text2KGBench, a benchmark to evaluate the capabilities of language models to generate KGs from natural language text guided by an ontology. Given an input ontology and a set of sentences, the task is to extract facts from the text while complying with the given ontology (concepts, relations, domain/range constraints) and being faithful to the input sentences. We provide two datasets (i) Wikidata-TekGen with 10 ontologies and 13,474 sentences and (ii) DBpedia-WebNLG with 19 ontologies and 4,860 sentences. We define seven evaluation metrics to measure fact extraction performance, ontology conformance, and hallucinations by LLMs. Furthermore, we provide results for two baseline models, Vicuna-13B and Alpaca-LoRA-13B using automatic prompt generation from test cases. The baseline results show that there is room for improvement using both Semantic Web and Natural Language Processing techniques.},
   author = {Nandana Mihindukulasooriya and Sanju Tiwari and Carlos F. Enguix and Kusum Lata},
   doi = {10.5281/zenodo.7916716},
   keywords = {Benchmark ·,Extraction ·,Generation ·,Graph,Graph ·,Knowledge,Language,Large,Models,Relation},
   month = {8},
   title = {Text2KGBench: A Benchmark for Ontology-Driven Knowledge Graph Generation from Text},
   url = {https://arxiv.org/abs/2308.02357v1},
   year = {2023},
}
@article{Butler2019,
   abstract = {The world's first digital computer, Electronic Numerical Integrator and Computer (ENIAC), would have turned 73 this year. Since ENIAC's birth in 1946, we have used computers to create a digital version of our analogue world. Human intelligence (and natural stupidity) evolved for the analogue world; however, human cognitive capabilities are limited when it comes to the complexity of understanding and decision-making in the digital world. This paper explores the capability of artificial intelligence (AI) to transform the financial industry. Banks and insurance companies have effectively digitised their businesses, with financial institutions reportedly spending more than any others on data; however, they find themselves caught between the Scylla of big regulation and the Charybdis of big data, particularly where financial compliance and risk management is concerned. Supervisory authorities are equally challenged. It is no surprise, then, to discover that AI is shaping the FinTech, RegTech and SupTech landscapes, in addition to related activities in the legal and professional services sectors. In the face of unbridled enthusiasm and unquestioning acceptance of many of the claims made for AI, this paper takes a balanced, critical stance in explaining the what, why and how of AI in the financial industry, with a particular focus on the art of the possible in regulatory compliance.},
   author = {Tom Butler and Leona O'Brien and Tom Butler and Leona O'Brien},
   issue = {1},
   journal = {Journal of Financial Compliance},
   keywords = {RegTech,SupTech,artificial intelligence (AI),digital technologies,finance,financial regulators,regulatory compliance,semantic technologies},
   pages = {44-59},
   publisher = {Henry Stewart Publications},
   title = {Artificial intelligence for regulatory compliance: Are we there yet?},
   volume = {3},
   url = {https://EconPapers.repec.org/RePEc:aza:jfc000:y:2019:v:3:i:1:p:44-59},
   year = {2019},
}
@article{Palagin2023,
   abstract = {This research presents a comprehensive methodology for utilizing an ontology-driven structured prompts system in interplay with ChatGPT, a widely used large language model (LLM). The study develops formal models, both information and functional, and establishes the methodological foundations for integrating ontology-driven prompts with ChatGPT's meta-learning capabilities. The resulting productive triad comprises the methodological foundations, advanced information technology, and the OntoChatGPT system, which collectively enhance the effectiveness and performance of chatbot systems. The implementation of this technology is demonstrated using the Ukrainian language within the domain of rehabilitation. By applying the proposed methodology, the OntoChatGPT system effectively extracts entities from contexts, classifies them, and generates relevant responses. The study highlights the versatility of the methodology, emphasizing its applicability not only to ChatGPT but also to other chatbot systems based on LLMs, such as Google's Bard utilizing the PaLM 2 LLM. The underlying principles of meta-learning, structured prompts, and ontology-driven information retrieval form the core of the proposed methodology, enabling their adaptation and utilization in various LLM-based systems. This versatile approach opens up new possibilities for NLP and dialogue systems, empowering developers to enhance the performance and functionality of chatbot systems across different domains and languages.},
   author = {Oleksandr Palagin and Vladislav Kaverinskiy and Anna Litvin and Kyrylo Malakhov},
   doi = {10.47839/ijc.22.2.3086},
   issue = {2},
   journal = {International Journal of Computing},
   keywords = {ChatGPT,OntoChatGPT,chatbot,composite service,meta-learning,ontology engineering,ontology-driven information system,prompt engineering,prompt-based learning,transdisciplinary research},
   month = {7},
   pages = {170-183},
   publisher = {Research Institute for Intelligent Computer Systems},
   title = {OntoChatGPT Information System: Ontology-Driven Structured Prompts for ChatGPT Meta-Learning},
   volume = {170},
   url = {http://arxiv.org/abs/2307.05082 http://dx.doi.org/10.47839/ijc.22.2.3086},
   year = {2023},
}
@article{Haj2020,
   abstract = {Abstract: Business Rules (BR) are usually written by different stakeholders, which makes them vulnerable to contain different designations for a same concept. Such problem can be the source of a not well orchestrated behaviors. Whereas identification of synonyms is manual or totally neglected in most approaches dealing with natural language Business Rules. In this paper, we present an automated approach to identify semantic similarity between terms in textual BR using Natural Language Processing and knowledge-based algorithm refined using heuristics. Our method is unique in that it also identifies abbreviations/expansions (as a special case of synonym) which is not possible using a dictionary. Then, results are saved in a standard format (SBVR) for reusability purposes. Our approach was applied on more than 160 BR statements divided on three cases with an accuracy between 69% and 87% which suggests it to be an indispensable enhancement for other methods dealing with textual BR.},
   author = {Abdellatif Haj and Youssef Balouki and Taoufiq Gadi},
   doi = {10.22266/IJIES2021.0228.15},
   issn = {21853118},
   issue = {1},
   journal = {International Journal of Intelligent Engineering and Systems},
   keywords = {Abbreviation identification,Business rules,NLP.,Semantic similarity,Synonym extraction},
   pages = {147-156},
   publisher = {Intelligent Network and Systems Society},
   title = {Automated Identification of Semantic Similarity between Concepts of Textual Business Rules},
   volume = {14},
   year = {2020},
}
@article{Karpovic2012,
   abstract = {Structured language, based on Semantics of Business Vocabulary and Business Rules (SBVR), can be seen as domain expert friendly means for developing OWL2 ontologies, which are becoming more and more important in Semantic Web and Enterprise applications. The goal of the paper is to present transformations from SBVR specifications to ontologies and to describe conditions for creating "right" vocabularies in order to obtain consistent ontologies without losing information. The need for such approach is caused by several reasons. Concept models rely on the closed world assumption, whereas ontologies rely on the open one where every constraint should be explicitly specified. Both SBVR and OWL2 have terminology related part, desirable being separated from the substantial ontology. We suggest rules that can help creating meaningful SBVR vocabularies regarding consequences of affecting the behavior of ontology reasoners, taking advantages of ontologies and retaining terminological information separately from the main ontology. © 2012 Springer-Verlag.},
   author = {Jaroslav Karpovic and Lina Nemuraite and Milda Stankeviciene},
   doi = {10.1007/978-3-642-33308-8_35/COVER},
   isbn = {9783642333071},
   issn = {18650929},
   journal = {Communications in Computer and Information Science},
   keywords = {ATL,OWL2,SBVR,Semantics,business rules,business vocabulary,consistency,ontology,transformations},
   pages = {420-435},
   publisher = {Springer, Berlin, Heidelberg},
   title = {Requirements for semantic business vocabularies and rules for transforming them into consistent OWL2 ontologies},
   volume = {319 CCIS},
   url = {https://link-springer-com.ez67.periodicos.capes.gov.br/chapter/10.1007/978-3-642-33308-8_35},
   year = {2012},
}
@article{Veizaga2021,
   abstract = {Natural language (NL) is pervasive in software requirements specifications (SRSs). However, despite its popularity and widespread use, NL is highly prone to quality issues such as vagueness, ambiguity, and incompleteness. Controlled natural languages (CNLs) have been proposed as a way to prevent quality problems in requirements documents, while maintaining the flexibility to write and communicate requirements in an intuitive and universally understood manner. In collaboration with an industrial partner from the financial domain, we systematically develop and evaluate a CNL, named Rimay, intended at helping analysts write functional requirements. We rely on Grounded Theory for building Rimay and follow well-known guidelines for conducting and reporting industrial case study research. Our main contributions are: (1) a qualitative methodology to systematically define a CNL for functional requirements; this methodology is intended to be general for use across information-system domains, (2) a CNL grammar to represent functional requirements; this grammar is derived from our experience in the financial domain, but should be applicable, possibly with adaptations, to other information-system domains, and (3) an empirical evaluation of our CNL (Rimay) through an industrial case study. Our contributions draw on 15 representative SRSs, collectively containing 3215 NL requirements statements from the financial domain. Our evaluation shows that Rimay is expressive enough to capture, on average, 88% (405 out of 460) of the NL requirements statements in four previously unseen SRSs from the financial domain.},
   author = {Alvaro Veizaga and Mauricio Alferez and Damiano Torre and Mehrdad Sabetzadeh and Lionel Briand},
   doi = {10.1007/S10664-021-09956-6/TABLES/13},
   issn = {15737616},
   issue = {4},
   journal = {Empirical Software Engineering},
   keywords = {Case study research,Controlled natural language,Functional requirements,Natural language requirements,Qualitative study},
   month = {7},
   pages = {1-53},
   publisher = {Springer},
   title = {On systematically building a controlled natural language for functional requirements},
   volume = {26},
   url = {https://link.springer.com/article/10.1007/s10664-021-09956-6},
   year = {2021},
}
@article{Kowalski2022,
   abstract = {In this paper, we present an informal introduction to Logical English (LE) and illustrate its use to standardise the legal wording of the Automatic Early Termination (AET) clauses of International Swaps and Derivatives Association (ISDA) Agreements. LE can be viewed both as an alternative to conventional legal English for expressing legal documents, and as an alternative to conventional computer languages for automating legal documents. LE is a controlled natural language (CNL), which is designed both to be computer-executable and to be readable by English speakers without special training. The basic form of LE is syntactic sugar for logic programs, in which all sentences have the same standard form, either as rules of the form conclusion if conditions or as unconditional sentences of the form conclusion. However, LE extends normal logic programming by introducing features that are present in other computer languages and other logics. These features include typed variables signalled by common nouns, and existentially quantified variables in the conclusions of sentences signalled by indefinite articles. Although LE translates naturally into a logic programming language such as Prolog or ASP, it can also serve as a neutral standard, which can be compiled into other lower-level computer languages.},
   author = {Robert Kowalski and Akber Datoo},
   doi = {10.1007/S10506-021-09295-3/METRICS},
   issn = {15728382},
   issue = {2},
   journal = {Artificial Intelligence and Law},
   keywords = {Clause taxonomy,Close-out netting,Controlled Natural Language,ISDA,Logic Programming,Logical English},
   month = {6},
   pages = {163-197},
   publisher = {Springer Science and Business Media B.V.},
   title = {Logical English meets legal English for swaps and derivatives},
   volume = {30},
   url = {https://link.springer.com/article/10.1007/s10506-021-09295-3},
   year = {2022},
}
@article{Draicchio2013,
   abstract = {FRED is an online tool for converting text into internally well-connected and quality linked-data-ready ontologies in web-service-acceptable time. It implements a novel approach for ontology design from natural language sentences. In this paper we present a demonstration of such tool combining Discourse Representation Theory (DRT), linguistic frame semantics, and Ontology Design Patterns (ODP). The tool is based on Boxer which implements a DRT-compliant deep parser. The logical output of Boxer enriched with semantic data from Verbnet or Framenet frames is transformed into RDF/OWL by means of a mapping model and a set of heuristics following ODP best-practice [5] of OWL ontologies and RDF data design. © Springer-Verlag 2013.},
   author = {Francesco Draicchio and Aldo Gangemi and Valentina Presutti and Andrea Giovanni Nuzzolese},
   doi = {10.1007/978-3-642-41242-4_36},
   isbn = {9783642412417},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {263-267},
   title = {FRED: From natural language text to RDF and OWL in one click},
   volume = {7955 LNCS},
   year = {2013},
}
@article{Ramakrishna2014,
   abstract = {Structured English has been applied as computational independent language for defining business vocabularies and business rules, e.g., in the context of OMG's Semantics and Business Vocabulary Representation (SBVR). It allows non-technical domain experts to engineer knowledge in natural language, but with an underlying semi-formal semantics which eases the automation of machine transformation into formal knowledge representations and logic-based machine interpretation. We adapt this approach to the legal domain in order to support legal domain experts in their task to build legal vocabularies and legal rules in Structured English from legal texts. In this paper we contribute with a semi-automated vocabulary and rule development process which is supported by automated suggestions of legal concepts computed by a semantic legal text analysis. We implement a proof-of-concept in the KR4IPLaw tool, which enables legal domain experts to represent their knowledge in Structured English. We evaluate the proposed approach on the basis of use cases in the domain of IP and patent law. © 2014 Springer International Publishing.},
   author = {Shashishekar Ramakrishna and Adrian Paschke},
   doi = {10.1007/978-3-319-09870-8_15/COVER},
   isbn = {9783319098692},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Controlled Natural Language,Legal Norms,LegalRuleML,SBVR,Structured English},
   pages = {201-215},
   publisher = {Springer Verlag},
   title = {Semi-automated vocabulary building for structured legal English},
   volume = {8620 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-319-09870-8_15},
   year = {2014},
}
@article{Njonko2014,
   abstract = {Business rules represent the primary means by which companies define their business, perform their actions in order to reach their objectives. Thus, they need to be expressed unambiguously to avoid inconsistencies between business stakeholders and formally in order to be machine-processed. A promising solution is the use of a controlled natural language (CNL) which is a good mediator between natural and formal languages. This paper presents RuleCNL, which is a CNL for defining business rules. Its core feature is the alignment of the business rule definition with the business vocabulary which ensures traceability and consistency with the business domain. The RuleCNL tool provides editors that assist end-users in the writing process and automatic mappings into the Semantics of Business Vocabulary and Business Rules (SBVR) standard. SBVR is grounded in first order logic and includes constructs called semantic formulations that structure the meaning of rules. © 2014 Springer International Publishing Switzerland.},
   author = {Paul Brillant Feuto Njonko and Sylviane Cardey and Peter Greenfield and Walid El Abed},
   doi = {10.1007/978-3-319-10223-8_7/COVER},
   isbn = {9783319102221},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Automatic Mapping,Business Rule,Controlled Natural Language,Semantics of Business Vocabulary and Business Rules},
   pages = {66-77},
   publisher = {Springer Verlag},
   title = {RuleCNL: A controlled natural language for business rule specifications},
   volume = {8625 LNAI},
   url = {https://link.springer.com/chapter/10.1007/978-3-319-10223-8_7},
   year = {2014},
}
@article{Fuchs2008,
   abstract = {Attempto Controlled English (ACE) is a controlled natural language, i.e. a precisely defined subset of English that can automatically and unambiguously be translated into first-order logic. ACE may seem to be completely natural, but is actually a formal language, concretely it is a first-order logic language with an English syntax. Thus ACE is human and machine understandable. ACE was originally intended to specify software, but has since been used as a general knowledge representation language in several application domains, most recently for the semantic web. ACE is supported by a number of tools, predominantly by the Attempto Parsing Engine (APE) that translates ACE texts into Discourse Representation Structures (DRS), a variant of first-order logic. Other tools include the Attempto Reasoner RACE, the AceRules system, the ACE View plug-in for the Protégé ontology editor, AceWiki, and the OWL verbaliser. © 2008 Springer-Verlag Berlin Heidelberg.},
   author = {Norbert E. Fuchs and Kaarel Kaljurand and Tobias Kuhn},
   doi = {10.1007/978-3-540-85658-0_3},
   isbn = {3540856560},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {104-124},
   title = {Attempto controlled english for knowledge representation},
   volume = {5224 LNCS},
   year = {2008},
}
@article{Levy2013,
   abstract = {This paper presents an original use of SBVR to help building a set of business rules out of regulatory documents. The formalization is analyzed as a three-step process, in which SBVR-SE stands in an intermediate position between the Natural Language on the one hand and the formal language on the other hand. The rules are extracted, clarified and simplified at the general regulatory level (expert task) before being refined according to the business application (engineer task). A methodology for these first two steps is described, with different operations composing each step. It is illustrated with examples from the literature and from the Ontorule use cases. © 2013 Springer-Verlag Berlin Heidelberg.},
   author = {François Lévy and Adeline Nazarenko},
   doi = {10.1007/978-3-642-39617-5_5},
   isbn = {9783642396168},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   month = {1},
   pages = {19-33},
   publisher = {Springer Verlag},
   title = {Formalization of natural language regulations through sbvr structured english (tutorial)},
   volume = {8035 LNCS},
   year = {2013},
}
@article{Lin2008,
   abstract = {Nowadays, many ontologies are used in industry, public adminstration and academia. Although these ontologies are developed for various purposes and domains, they often contain overlapping information. To build a collaborative semantic web, which allows data to be shared and reused across applications, enterprises, and community boundaries, it is necessary to find ways to compare, match and integrate various ontologies. Different strategies (e.g., string similarity, synonyms, structure similarity and based on instances) for determining similarity between entities are used in current ontology matching systems. Synonyms can help to solve the problem of using different terms in the ontologies for the same concept. The WordNet thesauri can support improving similarity measures. This paper provides an overview of how to apply WordNet in the ontology matching research area. © 2008 International Federation for Information Processing.},
   author = {Feiyu Lin and Kurt Sandkuhl},
   doi = {10.1007/978-0-387-09695-7_33},
   issn = {15715736},
   journal = {IFIP International Federation for Information Processing},
   pages = {341-350},
   title = {A survey of exploiting WordNet in ontology matching},
   volume = {276},
   year = {2008},
}
@article{Bernotaityte2013,
   abstract = {Semantics of Business Vocabulary and Business Rules (SBVR) is OMG adopted metamodel allowing defining noun concepts, verb concepts and business rules of a problem domain in structured natural language based on formal logics. SBVR business vocabulary and business rules are capable of representing ontologies. There are some research works devoted to transforming SBVR into Web Ontology Language OWL2. The reverse way of representing ontology concepts with SBVR structured language was not investigated though there are much more ontologies than SBVR vocabularies. Our research is concentrated on methodology for creating SBVR vocabularies and rules from OWL2 ontologies without a loss of the expressive power, characteristic for ontologies, as some ontology-specific concepts have no direct representation in SBVR. The particular attention is devoted to applying SBVR vocabulary in semantic search. © Springer-Verlag Berlin Heidelberg 2013.},
   author = {Gintare Bernotaityte and Lina Nemuraite and Rita Butkiene and Bronius Paradauskas},
   doi = {10.1007/978-3-642-41947-8_13/COVER},
   isbn = {9783642419461},
   issn = {18650929},
   journal = {Communications in Computer and Information Science},
   keywords = {OWL 2,SBVR,business rules,business vocabulary,domain ontology,lexical ontology},
   pages = {134-145},
   publisher = {Springer Verlag},
   title = {Developing SBVR vocabularies and business rules from OWL2 ontologies},
   volume = {403},
   url = {https://link.springer.com/chapter/10.1007/978-3-642-41947-8_13},
   year = {2013},
}
@article{Martinez-Fernandez2008,
   abstract = {This paper addresses the problem of extracting formal statements, in the form of business rules, from free text descriptions of financial products or services. This automatic process is integrated in the banking software factory, permitting business analysts the formal specification, direct implementation and fast deployment of new products. This system is fully integrated with the typical software methodologies and architectures used in the banking industry for conventional development of backoffice or online applications. © 2008 Springer-Verlag Berlin Heidelberg.},
   author = {José L. Martínez-Fernández and José C. González and Julio Villena and Paloma Martínez},
   doi = {10.1007/978-3-540-69858-6_29/COVER},
   isbn = {3540698574},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Banking industry,Business rules,Financial ontologies,Natural language processing},
   pages = {299-310},
   publisher = {Springer, Berlin, Heidelberg},
   title = {A preliminary approach to the automatic extraction of business rules from unrestricted text in the banking industry},
   volume = {5039 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-540-69858-6_29},
   year = {2008},
}
@article{Islam2023,
   abstract = {FinanceBench is a first-of-its-kind test suite for evaluating the performance of LLMs on open book financial question answering (QA). It comprises 10,231 questions about publicly traded companies, with corresponding answers and evidence strings. The questions in FinanceBench are ecologically valid and cover a diverse set of scenarios. They are intended to be clear-cut and straightforward to answer to serve as a minimum performance standard. We test 16 state of the art model configurations (including GPT-4-Turbo, Llama2 and Claude2, with vector stores and long context prompts) on a sample of 150 cases from FinanceBench, and manually review their answers (n=2,400). The cases are available open-source. We show that existing LLMs have clear limitations for financial QA. Notably, GPT-4-Turbo used with a retrieval system incorrectly answered or refused to answer 81% of questions. While augmentation techniques such as using longer context window to feed in relevant evidence improve performance, they are unrealistic for enterprise settings due to increased latency and cannot support larger financial documents. We find that all models examined exhibit weaknesses, such as hallucinations, that limit their suitability for use by enterprises.},
   author = {Pranab Islam and Anand Kannappan and Douwe Kiela and Rebecca Qian and Nino Scherrer and Bertie Vidgen},
   month = {11},
   title = {FinanceBench: A New Benchmark for Financial Question Answering},
   url = {https://arxiv.org/abs/2311.11944v1},
   year = {2023},
}
